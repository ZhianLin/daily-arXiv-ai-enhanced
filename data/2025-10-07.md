<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 169]
- [cs.CL](#cs.CL) [Total: 111]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LG](#cs.LG) [Total: 285]
- [cs.SE](#cs.SE) [Total: 46]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.NE](#cs.NE) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: 提出了AgentAug数据增强框架，通过模拟典型创作过程生成多样化的假新闻视频，结合基于不确定性采样的主动学习策略，显著提升了短视频假新闻检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 短视频平台假新闻泛滥成为重要社会问题，现有检测器因训练数据有限且多样性不足，导致模式偏见和性能受限。真实场景中视频素材与虚假新闻事件存在复杂的多对多关系，但现有数据集未能充分反映这种关系。

Method: 开发了基于LLM的多条流水线，模拟四种假新闻视频创作类别，结合基于不确定性采样的主动学习策略，在训练过程中选择潜在有用的增强样本。

Result: 在两个基准数据集上的实验结果表明，AgentAug能够持续提升短视频假新闻检测器的性能。

Conclusion: AgentAug通过模拟真实创作过程和主动学习策略，有效解决了假新闻视频检测中的数据不足和多样性缺乏问题，为自动视频新闻检测提供了有效的数据增强解决方案。

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [2] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: 提出SoC-DT框架，结合反应-扩散肿瘤生长模型和标准治疗干预，通过可微分求解器预测治疗后肿瘤结构，在合成和真实胶质瘤数据上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测标准治疗下的肿瘤轨迹是肿瘤学的重要需求，传统反应-扩散模型无法捕捉异质性治疗范式下的肿瘤动态，需要能模拟标准治疗干预并考虑患者间差异的计算框架。

Method: 提出SoC-DT可微分框架，统一反应-扩散肿瘤生长模型、离散标准治疗干预（手术、化疗、放疗）以及基因组和人口统计学个性化；开发IMEX-SoC隐式-显式指数时间差分求解器，确保稳定性、正性和可扩展性。

Result: 在合成数据和真实世界胶质瘤数据上评估，SoC-DT在预测肿瘤动态方面始终优于经典PDE基线和纯数据驱动的神经模型。

Conclusion: SoC-DT通过将机制可解释性与现代可微分求解器相结合，为肿瘤学中患者特异性数字孪生建立了原则性基础，实现了生物学一致的肿瘤动态估计。

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [3] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 提出了一种结合Transformer架构和纹理方法的集成框架，用于检测深度伪造媒体，在DFWild-Cup数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的兴起，检测伪造媒体变得日益重要。现有方法难以在不同数据集和生成技术间实现良好泛化。

Method: 使用集成框架结合Swin Transformers、ViTs和纹理方法，引入创新数据分割、顺序训练、频率分割、基于补丁的注意力和人脸分割技术。

Result: 在包含八个深度伪造数据集的DFWild-Cup数据集上实现了最先进的检测性能。

Conclusion: 混合模型能有效应对深度伪造检测的挑战，为实际应用提供鲁棒解决方案，Transformer擅长全局特征提取，纹理方法提供可解释性。

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [4] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: 提出一个结合分布式多GPU推理系统和交互式可视化平台的混合框架，用于分析视频剧集中的名人动态，通过高效处理大规模视频数据并提供多维可视化分析。


<details>
  <summary>Details</summary>
Motivation: 在视频内容主导的时代，理解视频结构和动态变得越来越重要，需要高效处理大规模视频数据并提供深入分析的工具。

Method: 采用分布式多GPU推理系统，使用优化的ONNX模型、异构批量推理和高吞吐量并行处理，生成带时间戳的出现记录，然后通过交互式可视化平台进行多维分析。

Result: 系统能够高效处理大规模视频数据，生成包括出现频率图、时长分析、饼图、共现矩阵、网络图、堆叠面积图、季间比较和热力图等多种可视化，提供对名人动态的深入洞察。

Conclusion: 通过将分布式识别与结构化、视觉驱动的分析相结合，这项工作为娱乐分析、内容创作策略和观众参与研究开辟了新的可能性。

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [5] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: ReactDiff是一个时序扩散框架，用于在对话中生成多样且逼真的人脸反应，通过融入时空面部运动学和动作单元依赖关系来提升反应质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法模拟真实人类反应的随机性和动态特性，导致生成的面部反应缺乏多样性和自然度。

Method: 提出ReactDiff框架，在扩散过程中融入两个关键先验：时序面部行为运动学和面部动作单元依赖关系，以指导模型生成符合人类面部解剖约束的逼真反应。

Result: 在REACT2024数据集上的实验表明，该方法在反应质量、多样性和反应适切性方面均达到最先进水平。

Conclusion: ReactDiff通过融入面部运动学和动作单元约束，成功生成了多样且逼真的面部反应，解决了现有方法在随机性和动态建模方面的不足。

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [6] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: 本研究评估了多种深度学习模型在水下塑料垃圾跨域检测中的性能，发现轻量级CNN模型MobileNetV2表现最佳，而零样本模型CLIP和Gemini在精度和召回率上呈现互补特征。


<details>
  <summary>Details</summary>
Motivation: 海洋塑料污染是紧迫的环境威胁，需要可靠的自动化检测系统。但由于域偏移问题，在一个数据集上训练的系统在新图像上性能会下降，因此需要评估模型在跨域场景下的鲁棒性。

Method: 在标记的水下数据集上训练CNN（MobileNetV2、ResNet-18、EfficientNet-B0）和视觉变换器（DeiT-Tiny、ViT-B16），然后在来自不同来源的塑料阳性图像和训练域阴性图像构建的平衡跨域测试集上评估。同时评估了两个零样本模型CLIP ViT-L14和Google Gemini 2.0 Flash。

Result: 轻量级MobileNetV2在跨域性能上表现最佳（F1 0.97），所有微调模型都实现了高精度（约99%），但在召回率上存在差异。零样本CLIP召回率约80%但精度较低（约56%），而Gemini则相反（精度约99%，召回率约81%）。

Conclusion: 经过监督训练的紧凑CNN能够有效泛化用于跨域水下检测，而大型预训练视觉语言模型提供了互补优势。错误分析显示主要混淆源包括珊瑚纹理、悬浮颗粒和镜面反射。

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [7] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: ExposureEngine是一个端到端系统，通过预测定向边界框(OBB)来精确量化体育转播中的赞助商可见度，解决了传统水平边界框(HBB)在旋转或倾斜标志检测中的不准确问题。


<details>
  <summary>Details</summary>
Motivation: 传统赞助商可见度分析方法依赖手动、主观且不可扩展的方法，而现有自动化系统使用水平边界框在标志旋转或倾斜时会产生不准确的曝光指标。

Method: 开发了包含1103帧瑞典精英足球比赛图像的新数据集，使用定向边界框(OBB)标注670个独特赞助商标志。构建了旋转感知的检测模型，并集成语言驱动代理层支持自然语言查询。

Result: 模型在mAP@0.5上达到0.859，精度0.96，召回率0.87，在多样化转播条件下稳健地定位标志。系统能够计算精确的可见度指标如曝光时长和屏幕覆盖率。

Conclusion: ExposureEngine提供了一个全面的解决方案，用于体育媒体中可审计和可解释的赞助商测量，包括数据集和分析仪表板。

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [8] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP是一个阿拉伯语图像描述框架，结合CLIP视觉标签检索和多模态文本生成，通过可解释的阿拉伯语视觉概念生成文化连贯的图像描述。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端图像描述方法缺乏可解释性，VLCAP旨在通过基于视觉概念检索的生成方法，为阿拉伯语图像描述提供更可解释和上下文准确的解决方案。

Method: 使用三种多语言编码器(mCLIP、AraCLIP、Jina V4)进行视觉标签检索，构建包含训练描述和21K通用领域标签的混合词汇表，然后将检索到的标签转换为阿拉伯语提示，与原始图像一起输入视觉语言模型(Qwen-VL和Gemini Pro Vision)生成描述。

Result: mCLIP + Gemini Pro Vision组合在BLEU-1(5.34%)和余弦相似度(60.01%)上表现最佳，AraCLIP + Qwen-VL在LLM-judge评分(36.33%)上最高。

Conclusion: VLCAP的可解释流水线能够生成文化连贯和上下文准确的阿拉伯语图像描述，为阿拉伯语多模态理解提供了有效解决方案。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [9] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker是第一个学术演示视频生成的多智能体框架，通过集成幻灯片生成、字幕、语音合成和说话人渲染等技术，自动将研究论文转化为演示视频。


<details>
  <summary>Details</summary>
Motivation: 学术演示视频制作过程高度劳动密集，需要大量时间进行幻灯片设计、录制和编辑。现有方法难以处理研究论文的密集多模态信息和协调多个对齐通道。

Method: 提出多智能体框架，集成幻灯片生成与布局优化、光标定位、字幕生成、语音合成和说话人渲染，并采用并行化幻灯片生成提高效率。

Result: 在Paper2Video数据集上的实验表明，该方法生成的演示视频比现有基线更忠实和富有信息量。

Conclusion: PaperTalker为自动化学术视频生成迈出了实用的一步，提供了可立即使用的解决方案。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


### [10] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: 对比EfficientNet-B0和ViT-Base在SpaceNet数据集上的性能，包括不平衡和平衡标签分布两种情况，评估准确率、F1分数、模型大小和延迟等指标。


<details>
  <summary>Details</summary>
Motivation: 比较卷积神经网络和视觉Transformer在遥感图像分类任务中的表现差异，特别是在不同标签分布情况下的性能对比。

Method: 在SpaceNet数据集上进行控制实验，使用不平衡的五类分割和平衡重采样分割（每类700张图像），采用相同的预处理、轻量级数据增强和40轮训练预算，在单个NVIDIA P100上运行。

Result: 在不平衡分割中，EfficientNet-B0达到93%测试准确率，具有更强的宏F1和更低延迟；ViT-Base也达到93%但参数更多、运行时间更长。在平衡分割中，两者表现都很强，EfficientNet-B0达到99%，ViT-Base保持竞争力。

Conclusion: 平衡数据分布缩小了架构差距，但CNN在效率方面仍保持优势。

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [11] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: 本文综述了基于摄像头的AI感知系统在弱势道路使用者安全保护方面的最新进展，系统分析了检测分类、跟踪重识别、轨迹预测和意图识别四个核心任务，并指出了数据、模型和部署方面的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基础设施措施在动态城市环境中保护弱势道路使用者效果有限，而现有AI应用调查主要关注检测任务，缺乏对全面VRU理解和保护所需的其他视觉任务的覆盖。

Method: 系统回顾了过去五年基于摄像头的AI感知系统在VRU安全方面的进展，重点分析了检测分类、跟踪重识别、轨迹预测和意图识别四个核心任务。

Result: 识别了AI赋能的主动VRU保护解决方案的关键技术组成，为智能交通系统中下一代感知系统的开发提供了基础参考。

Conclusion: 通过将视觉AI进展与现实世界实施的实际考虑联系起来，本综述旨在为开发增强VRU安全性的下一代感知系统提供基础参考，并指出了数据、模型和部署方面的四个主要开放挑战。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [12] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: 地球观测基础模型（EOFMs）的表示空间对传感器架构高度敏感，理解这种差异对于当前EOFMs设计的缺陷和未来发展具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有EOFMs大多在单一模态数据上训练，然后通过跨模态匹配波段来应用或评估，但不同传感器架构对EOFMs内部表示的影响尚不清楚。

Method: 分析EOFMs表示空间对传感器架构的敏感性，探讨不同传感器设计对模型内部表示的影响。

Result: 研究发现EOFMs的表示空间高度依赖于传感器架构，这种敏感性揭示了当前EOFMs设计的局限性。

Conclusion: 理解传感器架构对EOFMs表示空间的影响，为模型开发者、用户和遥感科学社区提供了重要指导，指明了未来发展的方向。

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [13] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 提出一种基于修复引导的扰动解释技术，通过生成逼真的掩码局部编辑来揭示视觉模型在生态监测中的决策依据，特别应用于海豹检测任务。


<details>
  <summary>Details</summary>
Motivation: 生态监测中自动化视觉模型的预测不透明限制了信任和实际应用，需要可解释的方法来支持AI在生态学中的可信部署。

Method: 使用修复引导的扰动解释技术，结合Segment-Anything-Model精炼的掩码，进行物体移除/替换和背景替换两种干预，生成保持场景上下文的逼真编辑。

Result: 该方法能够定位诊断性结构，避免传统扰动常见的删除伪影，产生领域相关的见解，支持专家验证和生态学AI的可靠部署。

Conclusion: 该解释技术为生态监测中的视觉模型提供了可信的决策依据，增强了AI在生态学应用中的透明度和可靠性。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [14] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: 这是一篇关于医学图像分割方法的系统性综述，涵盖了从传统图像处理技术到现代深度学习方法，特别关注了深度学习架构如CNN、FCN、U-Net及其变体，以及注意力机制、半监督学习、GAN和Transformer等新兴技术。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割是医学图像分析的核心，在精确诊断、治疗规划和疾病监测中起着关键作用。本文旨在弥合传统图像处理技术与现代深度学习方法之间的差距，为研究者提供全面的方法概览。

Method: 系统性地综述了阈值分割、边缘检测、区域分割、聚类算法和基于模型的技术等传统方法，以及CNN、FCN、U-Net等深度学习架构，并深入探讨了注意力机制、半监督学习、GAN和Transformer等新兴技术。

Result: 综述涵盖了医学图像分割领域的主要方法和发展趋势，包括混合架构、跨模态学习、联邦学习、分布式学习和主动学习等新兴方向，并提供了腰椎分割的专门案例研究。

Conclusion: 尽管医学图像分割领域取得了显著进展，但仍面临数据集偏差、领域适应、深度学习模型可解释性以及临床工作流集成等关键挑战。

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [15] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: DECOR是一个具有方向鲁棒性的深度聚类框架，用于对晶圆图中的复杂缺陷模式进行一致聚类，在MixedWM38数据集上表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中晶圆缺陷的早期检测对产品良率优化至关重要，但原始晶圆数据通常复杂、无标签、不平衡且单个晶圆可能包含多个缺陷，需要设计在非理想数据条件下仍可靠的聚类方法。

Method: DECOR框架通过显式考虑晶圆图中的方向变化，确保空间相似的缺陷无论其旋转或对齐方式如何都能被一致聚类，无需手动调参即可发现簇。

Result: 在MixedWM38数据集上的实验表明，该方法优于现有的聚类基线方法。

Conclusion: DECOR为自动化视觉检测系统提供了一个可靠且可扩展的解决方案。

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [16] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: 该论文提出了一种基于LSTM和注意力机制的面部表情分类方法，专门针对类别不平衡问题，通过错误校正技术提高对小类别的识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决面部表情分类中类别不平衡的问题，特别是某些情绪类别显著占主导地位的情况，这对于实际应用如反欺诈系统中寻找罕见事件具有重要意义。

Method: 使用基于LSTM的神经网络模型，结合注意力机制聚焦于面部关键区域，在六个类别的子集上训练模型，然后对第七个被排除的类别进行错误校正。

Result: 实验表明所有类别都可以进行校正，但成功率不同；在测试样本中，校正某些类别时小类别的关键质量指标有所提升。

Conclusion: 该方法可有效应用于面部表情分析系统，以及在类别分布偏斜情况下需要稳定分类的任务中。

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [17] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: 提出了DCG-Bench基准测试，评估多模态大语言模型在动态图表生成任务中的能力，包括三个维度：简单文本到图表、详细文本到图表和视频到图表任务。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在静态图表生成和理解方面取得了显著进展，但它们在动态图表生成和处理方面的潜力仍未得到充分探索。

Method: 构建了DCG-8K高质量数据集，包含指令-代码-视频三元组和QA对，采用两阶段训练方法，提出联合代码视觉奖励进行组相对策略优化。

Result: 基准测试结果显示现有MLLMs在视觉到图表任务中存在不足，提出的模型Qwen2.5-VL-DCG-3B在三个任务上平均性能提升8.31%，仅用30亿参数就达到了与专有模型相当的性能。

Conclusion: 提出的训练方法有效，证明了在动态图表生成任务中，通过适当的训练策略，小规模模型也能达到优秀性能。

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [18] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出VoT（Visual odometry Transformer）端到端单目视觉里程计方法，无需手工组件如束调整、特征匹配、相机标定或密集3D重建，直接预测相机运动。


<details>
  <summary>Details</summary>
Motivation: 传统单目视觉里程计方法依赖预训练深度学习组件和优化模块，形成复杂流水线，严重依赖相机标定和超参数调优，在未见过的真实场景中表现不佳。现有大规模3D模型虽能提供通用化密集重建和相机位姿估计，但处理长视频和提供准确逐帧估计方面仍有局限。

Method: VoT通过时空注意力处理单目帧序列，提取特征并建模全局关系。该方法直接预测相机运动而不估计密集几何，仅依赖相机位姿进行监督。框架模块化灵活，可无缝集成各种预训练编码器作为特征提取器。

Result: 实验表明VoT能有效扩展到更大数据集，从更强预训练骨干网络中显著受益，在不同相机运动和标定设置下具有良好泛化能力，性能优于传统方法且运行速度快3倍以上。

Conclusion: 单目视觉里程计可通过端到端方式有效解决，无需手工组件，VoT展示了在性能、速度和泛化能力方面的优势。

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [19] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: SiteShield是一个基于多模态大视觉语言模型和检索增强生成的框架，用于自动化建筑安全检查报告生成，整合视觉和音频输入，在真实数据上表现优于单模态LLM。


<details>
  <summary>Details</summary>
Motivation: 传统建筑安全检查方法效率低下，需要处理大量信息。现有大模型应用存在响应不相关、模态输入受限和幻觉问题，LLM应用受限于训练数据和实时适应性。

Method: 开发了SiteShield框架，结合多模态LVLM和检索增强生成技术，整合视觉和音频输入来自动化安全检查报告生成。

Result: 在真实数据上，SiteShield的F1得分为0.82，汉明损失为0.04，精确率为0.76，召回率为0.96，优于无RAG的单模态LLM。

Conclusion: SiteShield为增强安全检查报告的信息检索和生成效率提供了新途径。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [20] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 提出了一种新的推理时搜索算法，利用侧信息指导扩散模型的采样过程，在图像重建任务中实现更准确可靠的恢复效果


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型方法通常忽略侧信息，而这些信息在严重不适定问题中能显著提升重建质量，且基于梯度的引导方法容易产生奖励破解伪影

Method: 设计了一种平衡探索与利用的推理时搜索算法，将侧信息无缝集成到扩散采样过程中，可广泛应用于现有扩散重建流程

Result: 在多种逆问题（框内修复、超分辨率、运动/高斯/非线性/盲去模糊等）上实验表明，该方法能持续提升扩散重建算法的定性和定量性能

Conclusion: 该方法优于包括基于奖励梯度引导在内的其他基线方法，为扩散模型在逆问题求解中提供了更可靠的侧信息利用方案

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [21] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: 对声纳图像数据集现状的综述研究，系统梳理了各类声纳模态的公开数据集，分析了应用场景，并提供了数据集比较和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 声纳图像在水下探索、自主导航和生态系统监测中很重要，但公开可用的标注数据集稀缺，这限制了机器学习模型的发展。

Method: 系统性地映射了各种声纳模态的公开数据集，包括侧扫声纳、前视声纳、合成孔径声纳等，分析了分类、检测、分割和3D重建等应用，并创建了主表和时序线图进行比较。

Result: 提供了声纳图像数据集的全面概览，包括数据集特征、大小和标注细节的清晰比较，识别了现有资源中的空白领域。

Conclusion: 该研究为水下声学数据分析领域的研究人员提供了基础指南，有助于推动声纳图像处理技术的发展。

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [22] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了一种结合无透镜相机和隐式神经表示的方法，无需专业设备即可从多角度捕获显示器特性，实现显示器校准。


<details>
  <summary>Details</summary>
Motivation: 显示器校准是内容创作者必须定期执行的基本任务，但传统方法需要专业设备和暗室，对大多数用户来说难以实现。

Method: 联合设计无透镜相机和基于隐式神经表示的算法，从不同视角捕获显示器特性，能够重建46.6°×37.6°视角锥内的显示器光场。

Result: 开发的新流程能够高效重建显示器发出的光场，为显示器校准和特性分析提供了初步解决方案。

Conclusion: 该新兴流程为实现无需专业设备的显示器校准和特性分析迈出了重要一步。

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [23] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: 提出溯源网络，一种新型神经网络模型，通过将预测直接关联到支持性训练样本来实现端到端的可解释性，类似于学习的KNN方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习模型的不透明性、幻觉问题以及数据贡献者信用分配问题，提高模型的透明度、鲁棒性和可信度。

Method: 在模型架构中嵌入可解释性，联合优化主任务和可解释性目标，学习将每个预测与相关训练样本直接关联，基于特征空间中的相关性进行加权。

Result: 能够系统研究记忆与泛化的权衡，验证输入是否在训练集中，检测错误标签或异常数据点，增强对输入扰动的鲁棒性，识别相似输入对新数据点的贡献。

Conclusion: 溯源网络为现有可解释性技术提供了补充方法，虽然带来额外计算成本且目前适用于中等规模数据集，但显著提升了神经模型的透明度和可信度。

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [24] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: 提出了统一成本过滤（UCF）框架，用于改进无监督异常检测方法，通过匹配视角统一处理单模态和多模态场景，通过可学习的过滤模块减少匹配噪声并突出细微异常。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法存在匹配噪声问题，且单模态和多模态方法相互隔离，阻碍了知识迁移和全面理解。

Method: 构建测试样本与正常样本的匹配成本体积，使用多层注意力引导的可学习过滤模块来优化异常检测结果。

Result: 在22个不同基准测试中，UCF显著提升了多种无监督异常检测方法的性能，在单模态和多模态场景下均达到新的最先进水平。

Conclusion: UCF是一个通用的后处理框架，能有效减少匹配噪声，提高异常检测精度，适用于各种无监督异常检测方法。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [25] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: 提出了一种使用视觉语言模型(VLMs)自动评估工业图纸中物体检测结果质量的框架，能够识别缺失或不一致的检测，从而提高复杂工业图纸的整体检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业图纸数字化是构建数字孪生和实现智能工业自动化的重要步骤，但目前缺乏自动评估物体检测输出质量的方法。

Method: 利用视觉语言模型的多模态能力来识别缺失或不一致的检测结果，实现自动质量评估并指导检测结果的优化。

Result: 该框架能够自动评估物体检测质量，识别检测中的问题，并提高复杂工业图纸的整体检测性能。

Conclusion: 视觉语言模型为工业图纸物体检测的质量评估提供了有效的自动化解决方案，有助于推进工业图纸的数字化进程。

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [26] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: 提出了SpatialViLT，一种增强的视觉语言模型，通过整合深度图、3D坐标和边缘图等空间特征来改进3D场景和复杂物体配置的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在多模态推理方面取得进展，但在3D场景和复杂物体配置的空间推理方面仍面临挑战。

Method: 采用多任务学习框架，整合空间特征（深度图、3D坐标、边缘图）到多模态嵌入中。提出了SpatialViLT和MaskedSpatialViLT两个变体，以及结合两者的SpatialEnsemble方法。

Result: 在具有挑战性的视觉空间推理（VSR）数据集上，模型在方向性、拓扑和邻近关系等空间推理类别中表现出色，达到了最先进的准确率。

Conclusion: 这项工作在增强AI系统的空间智能方面迈出了重要一步，对于高级多模态理解和实际应用至关重要。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [27] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: 使用编码器-解码器网络减少两相光学切片结构照明显微镜中的伪影，通过合成数据训练实现监督去噪


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法难以抑制两相OS-SI中因采集时间减少而产生的残留伪影，且监督训练缺乏干净的ground-truth数据

Method: 使用合成训练对（将真实伪影场应用于合成图像），训练非对称去噪自编码器和U-Net网络

Result: 两种网络均提高了图像清晰度，在不同类型伪影上各有优势

Conclusion: 合成训练能够实现OS-SI图像的监督去噪，编码器-解码器网络有潜力简化重建工作流程

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [28] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: PEaRL是一个多模态框架，通过ssGSEA计算通路激活分数来表示转录组学，使用transformer编码生物学通路信号，并通过对比学习与组织学特征对齐，在三个癌症空间转录组数据集中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法主要依赖少量高变基因，限制了预测范围并忽略了协调的生物学程序，需要更全面地整合组织形态和分子功能。

Method: 使用ssGSEA计算通路激活分数，通过transformer编码通路信号，利用对比学习将通路特征与组织学特征对齐。

Result: 在三个癌症ST数据集中，PEaRL在基因和通路水平表达预测方面均优于现有方法，Pearson相关系数分别提高达58.9%和20.4%。

Conclusion: 基于通路的转录组表示能产生更生物学忠实和可解释的多模态模型，推动计算病理学超越基因水平嵌入。

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [29] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS是一个用于多模态医学图像分析的新型深度学习框架，通过层次化语义提示和双提示机制实现细粒度任务控制，在分割任务中超越现有方法，并能扩展到预后预测等任务。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像分析中任务特定模型缺乏通用性、现有通用方法条件控制简单且医学语义理解不足的问题。

Method: 提出基于视觉-语言的层次化语义提示框架，采用双提示机制实现文本控制架构，支持参数高效微调快速适应新任务。

Result: 在10个不同解剖结构的医学数据集上，在8个数据集上超越现有最优方法；在头颈癌数据集上预后预测的CI达到0.69。

Conclusion: DuPLUS是一个通用且临床相关的医学图像分析解决方案，具有出色的泛化能力和扩展性。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [30] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 提出了一个移动优化的两阶段深度学习框架，通过线程化检测模型并行化YOLOv10检测和MobileSAM分割，在资源受限环境下实现实时动物检测和分割。


<details>
  <summary>Details</summary>
Motivation: 自然环境中实时动物检测和分割对野生动物保护至关重要，但由于计算资源有限和许多物种的隐蔽外观，这些任务仍然具有挑战性。

Method: 集成线程化检测模型(TDM)，并行执行YOLOv10检测和MobileSAM轻量级分割，通过线程化减少延迟提高实时性能。

Result: 在Houbara Bustard物种上，模型达到mAP50为0.9627，mAP75为0.7731，mAP95为0.7178，MobileSAM mIoU为0.7421，YOLOv10每帧处理时间为43.7毫秒。

Conclusion: 该框架在资源受限环境下实现了实时动物检测和分割，并提供了包含40,000张标注图像的Houbara数据集支持模型训练和评估。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [31] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: Platonic Transformer通过引入柏拉图立体对称群的参考框架，在保持标准Transformer架构和计算成本的同时，实现了对连续平移和柏拉图对称性的等变性，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer缺乏对几何对称性的归纳偏置，而现有的等变方法往往牺牲了Transformer的效率和灵活性。

Method: 通过定义相对于柏拉图立体对称群参考框架的注意力机制，引入原则性的权重共享方案。

Result: 在计算机视觉、3D点云和分子属性预测等多个基准测试中，Platonic Transformer实现了竞争性性能，且无需额外计算成本。

Conclusion: Platonic Transformer成功解决了Transformer在几何对称性方面的局限性，同时保持了其效率和灵活性。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [32] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 这篇论文是关于领域泛化语义分割的综述，重点讨论了深度神经网络在未知领域泛化的挑战，以及基础模型对领域泛化的重大影响。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在未知领域的泛化能力是一个重大挑战，特别是在语义分割任务中。领域泛化方法旨在在多个未见过的目标领域上实现泛化，这对生物医学和自动驾驶等应用至关重要。

Method: 作者对现有方法进行了聚类和回顾，识别了向基于基础模型的领域泛化的范式转变，并提供了所有方法的广泛性能比较。

Result: 性能比较突显了基础模型对领域泛化的显著影响，表明基础模型在这一领域具有重要作用。

Conclusion: 本综述旨在推动领域泛化研究，并激励科学家探索新的研究方向，特别是在基础模型的应用方面。

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [33] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: 提出基于Transformer的自动化内窥镜报告生成模型，通过两阶段训练框架减轻胃肠科医生的文档负担


<details>
  <summary>Details</summary>
Motivation: 内窥镜检查的文档记录工作给胃肠科医生带来沉重负担，导致临床工作流程效率低下和医生职业倦怠

Method: 采用基于Transformer的视觉编码器和文本解码器，通过两阶段训练：先在图像/文本描述对上预训练，然后在图像/报告对上微调

Result: 模型能够生成具有临床意义的检查发现，简化文档记录流程

Conclusion: 该方法有望减轻医生工作负担并改善患者护理

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [34] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan是一个基于扩散模型的规划器，通过2D手绘草图在深度图像上生成无人机3D飞行路径，实现零样本从仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 解决如何让无人机通过人类直观的2D手绘草图来理解导航意图，并生成安全准确的3D飞行路径，特别是在未见过的真实环境中。

Method: 包含SketchAdapter（将手绘草图映射到2D投影路径）和DiffPath（从2D投影和第一人称深度图像推断3D轨迹）两个组件，使用合成数据集训练，结合人工标注和自动标注数据。

Result: 在真实世界无人机测试中，低/中等障碍物环境下达到100%成功率，高障碍物环境下达到40%成功率，比关键消融实验高出20-60%的任务完成率。

Conclusion: 混合人工标注和自动标注数据的训练方法以及模块化设计显著提升了模型理解人类意图和推断3D路径的能力，实现了有效的零样本仿真到现实迁移。

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [35] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: 提出了一种针对AI视频会议中身份劫持攻击的防御方法，通过分析姿态-表情潜在空间中的生物特征信息来检测非法身份交换，而无需查看重建的RGB视频。


<details>
  <summary>Details</summary>
Motivation: AI视频会议系统通过传输紧凑的姿态-表情潜在表示来减少带宽，但这些潜在表示可能被恶意操控，导致攻击者实时劫持受害者形象。由于每帧都是合成的，传统的深度伪造检测器完全失效。

Method: 使用姿态条件化的大间隔对比编码器，从传输的潜在表示中分离出持久的身份线索，同时消除瞬时的姿态和表情信息。通过简单的余弦测试来检测非法身份交换。

Result: 在多个说话头生成模型上的实验表明，该方法始终优于现有的操控防御方法，能够实时运行，并在分布外场景中表现出强大的泛化能力。

Conclusion: 该方法首次实现了无需查看重建RGB视频的生物特征泄露防御，有效解决了AI视频会议中的身份劫持安全问题。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [36] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: 提出了REVEL任务和DragStream方法，实现自回归视频扩散模型的流式、细粒度拖拽式交互视频编辑，支持平移、变形和旋转效果。


<details>
  <summary>Details</summary>
Motivation: 解决自回归视频扩散模型难以实现流式、细粒度控制的问题，确保输出结果与用户期望一致。

Method: 提出训练无关的DragStream方法，包含自适应分布自校正策略和空间频率选择性优化机制，利用相邻帧统计约束潜在嵌入漂移，选择性传播视觉线索。

Result: 方法可无缝集成到现有自回归视频扩散模型中，大量实验验证了DragStream的有效性。

Conclusion: REVEL任务和DragStream方法成功解决了流式拖拽视频编辑中的潜在分布漂移和上下文干扰问题，实现了高质量的交互式视频操控。

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [37] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: GAS-MIL是一种集成框架，可无缝整合多个基础模型的特征，无需手动特征选择或大量任务特定微调，在三种癌症数据集的分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 基础模型为计算病理学提供了强大的特征提取器，但针对特定诊断任务进行适配和基准测试通常耗时且资源密集，特别是考虑到其规模和多样性。

Method: 提出了Group-Aggregative Selection Multi-Instance Learning (GAS-MIL)，这是一个灵活的集成框架，能够整合多个基础模型的特征，保留它们的互补优势。

Result: 在前列腺癌(PANDA)、卵巢癌(UBC-OCEAN)和乳腺癌(TCGA-BrCa)三个数据集的分类任务中，GAS-MIL相对于单个基础模型和已建立的MIL方法，始终实现优越或相当的性能。

Conclusion: GAS-MIL通过实现异构基础模型的高效集成，简化了病理学模型部署，并为未来多模态和精准肿瘤学应用提供了可扩展的基础。

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [38] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: 提出无人机辅助纳洛酮递送模拟数据集和基于视频的实时情境感知评估框架，用于评估旁观者在阿片类药物过量紧急情况下的情境感知能力


<details>
  <summary>Details</summary>
Motivation: 通过无人机快速递送纳洛酮可以挽救阿片类药物过量患者的生命，但需要评估旁观者的情境感知能力以确保有效的人机协作

Method: 使用图嵌入和transformer模型，整合几何、运动学和交互图特征，构建实时情境感知评估框架

Result: 实现了高性能的情境感知预测，在时间分割准确率上比FINCH基线提高了9%的MoF和5%的IoU

Conclusion: 该研究支持开发能够有效指导旁观者的自适应无人机系统，改善紧急响应结果并拯救生命

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [39] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 评估四种开源OCR系统在食品包装图像上的性能，重点关注成分表和营养事实面板的文本提取能力。Tesseract在准确率上表现最佳，EasyOCR在准确率和多语言支持间取得平衡，PaddleOCR覆盖率高但速度慢，TrOCR表现最弱。


<details>
  <summary>Details</summary>
Motivation: 食品包装上的OCR对于合规性和营养监测很重要，但由于多语言文本、密集布局、字体变化、反光和曲面等挑战而困难。需要评估现有OCR系统在此特定场景下的表现。

Method: 使用231个产品（1,628张图像）的数据集，评估Tesseract、EasyOCR、PaddleOCR和TrOCR四种OCR系统的速度和覆盖率。创建113张图像（60个产品）的真实标注子集进行准确率评估，使用CER、WER、BLEU、ROUGE-L、F1、覆盖率和执行时间等指标。

Result: 在真实标注子集上，Tesseract获得最低的字符错误率（0.912）和最高的BLEU分数（0.245）。EasyOCR在准确率和多语言支持间取得良好平衡。PaddleOCR实现近乎完整的覆盖率但因GPU不兼容只能在CPU上运行而速度较慢。TrOCR尽管使用GPU加速但表现最差。

Conclusion: 研究为包装特定场景提供了基准，建立了基线，并指出了布局感知方法和文本定位的发展方向。Tesseract在准确率上表现最佳，而不同OCR系统在不同指标上各有优势。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [40] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: FrameOracle是一个轻量级即插即用模块，通过预测最相关帧和所需帧数来优化视频理解，在保持准确性的同时显著减少输入帧数。


<details>
  <summary>Details</summary>
Motivation: 现有帧采样策略无法适应信息密度和任务复杂度的变化，导致效率低下和信息丢失。

Method: 使用四阶段课程学习训练，前三阶段依赖跨模态相似度等弱代理信号，最后阶段利用新数据集FrameOracle-41K的关键帧标注进行强监督。

Result: 在5个VLMs和6个基准测试中，FrameOracle将16帧输入平均减少到10.4帧而不损失准确性；从64帧候选帧中平均减少到13.9帧，同时准确性提高1.4%。

Conclusion: FrameOracle实现了可扩展视频理解的最优效率-准确性权衡。

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [41] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 提出了一种混合协同微调(CFT)方法，通过整合标记和未标记数据来改进游戏视觉bug检测，减少对目标游戏标记数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 游戏视觉bug的人工识别成本高且需要专业知识，而监督学习模型依赖大量标记数据，但这类bug出现频率低，标记数据获取困难。

Method: 采用混合协同微调方法，利用目标游戏和同领域游戏的标记样本，同时结合未标记数据来增强特征表示学习。

Result: 实验结果显示该方法在多个游戏环境中优于传统基线方法，即使在仅使用50%目标游戏标记数据时仍保持竞争力。

Conclusion: CFT方法提高了游戏视觉bug检测的可扩展性和适应性，有效降低了特定目标游戏标记数据的依赖。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [42] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: HRM模型在MNIST上表现良好（≈98%准确率），但在CIFAR-10和CIFAR-100上过拟合严重，性能不如简单的CNN基线模型，表明在无数据增强的小分辨率图像分类任务中，HRM缺乏足够的图像特定归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 探究HRM模型（包含两个Transformer模块、DEQ风格单步训练、深度监督、旋转位置编码和RMSNorm）是否可以作为实用的图像分类器。

Method: 在MNIST、CIFAR-10和CIFAR-100数据集上评估HRM，采用原始训练策略：无数据增强、相同优化器家族、一轮预热后余弦衰减、标签平滑。

Result: HRM在MNIST上达到≈98%测试准确率，但在CIFAR-10上仅65.0%（CNN基线77.2%），CIFAR-100上仅29.7%（CNN基线45.3%），且训练速度慢30倍。

Conclusion: 对于无增强的小分辨率图像分类，现有HRM模型不如简单卷积架构有竞争力，但模型修改可能带来显著改进。

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [43] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: 本文调查了DINOv2自监督学习方法，分析了其核心技术（多裁剪视图增强和自蒸馏）的发展历程，并与SSL和WSL方法在多个下游任务上进行比较，展示了其学习特征的显著特性。


<details>
  <summary>Details</summary>
Motivation: 自监督学习（SSL）的最新进展使得能够学习捕捉图像高级语义和细粒度空间结构的通用视觉特征。DINOv2在大多数基准测试中超越了弱监督方法（WSL），建立了新的最先进水平。

Method: 分析DINOv2的核心方法：多裁剪视图增强和基于平均教师的自蒸馏，并追溯这些方法在先前工作中的发展历程。

Result: 比较DINO和DINOv2与其他SSL和WSL方法在各种下游任务上的性能，突出展示了基于transformer骨干网络学习特征的显著涌现特性。

Conclusion: 简要讨论了DINOv2的局限性、影响力以及未来的研究方向。

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [44] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: 提出DCS框架，通过扩散模型与FSCIL分类器的协同进化循环，解决小样本类增量学习中的数据稀缺和遗忘问题，在FSCIL基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前FSCIL方法因依赖有限数据集而泛化能力不足，直接应用扩散模型进行数据增强会导致语义错位或引导无效，需要建立更有效的协同机制。

Method: 采用奖励对齐学习策略，构建动态多维度奖励函数，在特征层面确保语义一致性和多样性，在logits层面促进探索性图像生成和类间区分性。

Result: 在FSCIL基准测试中实现了最先进的性能，显著提升了知识保留和新类学习能力。

Conclusion: DCS框架通过扩散模型与分类器的协同进化，有效解决了FSCIL中的数据稀缺和稳定性-可塑性困境，为小样本增量学习提供了新思路。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [45] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: MonitorVLM是一个基于视觉-语言模型的智能安全监控框架，专门用于从监控视频流中自动检测采矿行业的安全违规行为。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查在大型动态环境中劳动密集、易出错且不足，迫切需要智能自动化安全监控来解决高风险领域（如采矿）中不安全行为导致的事故问题。

Method: 提出三个关键创新：1) 包含9,000个VQA样本的领域特定违规数据集；2) 动态选择Top-K最相关条款的条款过滤模块，减少推理延迟13.56%；3) 增强工人区域的行为放大模块，提高细粒度动作识别。

Result: 显著优于基线视觉-语言模型，在精度上提升22.01%，召回率提升34.22%，F1分数提升28.37%。轻量级Web界面集成到实际工作流中，实现自动违规报告和视频时间戳记录。

Conclusion: 该研究展示了多模态大模型在增强采矿及其他领域职业安全监控方面的潜力。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [46] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: 提出了一种将扩散模型集成到智能交通系统中的混合模型，用于交通事故检测，通过结合异常检测网络和扩散技术，在公开数据集上达到了97.32%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在处理复杂数据分布方面存在局限性，扩散模型具有理解复杂数据分布的固有能力，可以克服传统方法的不足。

Method: 使用微调的ExceptionNet架构输出作为扩散模型的输入，图像张量作为条件输入，构建包含多个条件模块的混合模型，通过时间嵌入和图像协变量嵌入动态调整网络行为。

Result: 在公开数据集上的评估显示，所提出的扩散模型在基于图像的交通事故检测中表现最佳，准确率达到97.32%。

Conclusion: 该混合模型通过集成扩散技术和异常检测网络，为智能交通系统中的事故检测提供了更鲁棒的解决方案，并通过消融研究验证了关键扩散参数的重要性。

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [47] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: 提出SAMSOD模型，通过单模态监督增强非主导模态学习，使用梯度解冲突减少冲突梯度对收敛的影响，并利用解耦适配器分别处理高/低激活神经元以增强背景学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法在RGB-T显著性目标检测中忽视了两模态收敛不平衡以及高/低激活之间的显著梯度差异，限制了性能提升空间。

Method: 采用单模态监督增强非主导模态学习，使用梯度解冲突技术，并利用两个解耦适配器分别屏蔽高激活和低激活神经元来强调前景目标。

Result: 在RGB-T SOD基准数据集、涂鸦监督RGB-T SOD、全监督RGB-D SOD数据集以及全监督RGB-D轨道表面缺陷检测上的实验均证明了方法的有效性。

Conclusion: SAMSOD模型通过解决模态收敛不平衡和梯度差异问题，在多个RGB-T和RGB-D任务中表现出优越性能。

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [48] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 提出了针对小目标指代表达理解的新数据集SOREC和参数高效微调方法PIZA，显著提升了小目标定位性能


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言学习在指代表达理解方面取得了显著进展，但在自动驾驶等实际应用中，定位极小的目标仍然是一个重大挑战

Method: 提出了小目标指代表达理解数据集SOREC（包含10万个驾驶场景中的小目标样本），以及渐进迭代缩放适配器PIZA用于参数高效微调

Result: 在SOREC数据集上应用PIZA到GroundingDINO模型，准确率显著提升

Conclusion: 提出的SOREC数据集和PIZA方法有效解决了小目标定位的挑战，相关资源已公开

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [49] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: 提出了一种基于注意力机制的Attention-WNet深度学习模型，用于视网膜血管的动静脉分割，在HRF和DRIVE数据集上表现优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管的动静脉分割对于视网膜血管分析至关重要，能够为识别和诊断各种视网膜眼病提供潜在见解和生物标志物。血管规律性和宽度的改变可作为全身血管系统健康的指标，帮助识别中风和心肌梗死等血管疾病的高风险患者。

Method: 将注意力机制整合到WNet深度学习模型中，构建了Attention-WNet模型，用于视网膜血管的动静脉分割。

Result: 在公开可用的HRF和DRIVE数据集上测试，提出的方法在性能上超越了文献中其他最先进的模型。

Conclusion: 基于注意力机制的Attention-WNet模型在视网膜血管动静脉分割任务中表现出色，为血管疾病的风险评估提供了有效的工具。

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [50] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 本研究通过为LAION-400M数据集创建人物中心注释，揭示了训练数据中的人口统计不平衡与有害关联，并证明CLIP和Stable Diffusion中60-70%的性别偏见可由数据中的直接共现线性解释。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多模态数据集中缺乏人口统计注释的问题，以明确训练数据在产生模型偏见中的作用。

Method: 通过结合目标检测、多模态字幕生成和微调分类器的验证自动标注流程，为LAION-400M数据集创建人物中心注释，包括2.76亿个边界框、感知性别和种族/民族标签以及自动生成的字幕。

Result: 发现数据集中存在人口统计不平衡和有害关联，如男性和被感知为黑人或中东裔的个体与犯罪相关及负面内容的不成比例关联；CLIP和Stable Diffusion中60-70%的性别偏见可由数据中的直接共现线性解释。

Conclusion: 建立了首个大规模实证联系，证明数据集构成与下游模型偏见之间存在直接关联，为理解模型偏见来源提供了重要依据。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [51] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: 比较两种预训练神经网络在里约热内卢贫民窟检测中的表现：通用网络（大数据量）vs专业卫星图像网络（任务特异性），探究任务特异性与数据量哪个对性能影响更大


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法检测非正规住区尚未充分利用预训练神经网络的潜力，需要比较任务特异性与数据量对检测性能的影响

Method: 使用两种预训练神经网络：1) 在大型多样化数据集上预训练的通用网络；2) 在卫星图像上预训练的专业网络，对比它们在里约热内卢贫民窟检测任务中的表现

Result: 论文比较了两种预训练策略的性能，但具体结果未在摘要中提供

Conclusion: 研究旨在确定在非正规住区检测中，任务特异性与数据量哪个因素对模型性能贡献更大

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [52] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: 提出LoRA修补方法，通过在Deepfake生成器中注入LoRA补丁来绕过现有防御系统，并引入防御性LoRA修补作为补充解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有针对Deepfake的主动防御系统存在鲁棒性和可靠性不足的问题，需要开发能够绕过这些防御的新方法。

Method: 使用低秩适应(LoRA)补丁技术，结合可学习门控机制防止梯度爆炸，并引入多模态特征对齐损失函数实现语义级特征对齐。

Result: 仅需1000个面部样本和单轮微调，LoRA修补就能成功击败多种主动防御系统。

Conclusion: 揭示了当前Deepfake防御范式的关键弱点，强调需要更鲁棒的防御策略。

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [53] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 提出参考集微调(RSF)方法，通过在测试时参考集上进行简单微调，显著提升视觉位置识别在具有挑战性数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VPR方法在测试环境与训练数据差异较大时表现不佳，需要弥补训练-测试领域差距的新信息源。

Method: 利用测试时参考集（地图）中的图像和姿态信息，对VPR模型进行参考集微调(RSF)。

Result: 在具有挑战性的数据集上平均提升Recall@1约2.3%，微调后的模型保持泛化能力，且RSF在不同测试数据集上均有效。

Conclusion: 参考集微调是一种简单有效的方法，能够显著提升SOTA VPR方法在领域差距较大情况下的性能。

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [54] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: ARSAM通过自适应采样-重用-混合分解梯度的方法，显著加速了Sharpness-Aware Minimization (SAM)算法，在保持模型泛化能力的同时将计算速度提升约40%。


<details>
  <summary>Details</summary>
Motivation: SAM算法虽然能提高模型泛化能力，但其计算成本是SGD的两倍，需要每步计算两次梯度。为了降低这种计算开销，作者提出了ARSAM方法。

Method: 将SAM的梯度分解为SGD梯度和二阶梯度在一阶梯度上的投影(PSF)，然后自适应地重用PSF和及时更新PSF来维持模型泛化能力。

Result: 在CIFAR-10/100等数据集上，ARSAM达到了与SAM相当的最先进准确率，同时提供约40%的速度提升。在人体姿态估计、模型量化等挑战性任务中也能加速优化而不牺牲性能。

Conclusion: ARSAM是一种高效且实用的SAM加速方法，在多种网络架构和任务中都能保持性能的同时显著降低计算成本。

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [55] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: CoPA框架通过概念提示和聚合机制，从视觉编码器的多层提取概念表示，并使用这些表示作为提示来增强关键概念相关的视觉线索，从而提升概念和疾病预测性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在临床诊断中需要透明度，但现有概念瓶颈方法仅依赖最终层特征，忽略了浅层和多尺度特征，且缺乏有效的概念编码指导，限制了细粒度概念提取能力。

Method: 提出概念提示和聚合框架(CoPA)，包含概念感知嵌入生成器(CEG)从视觉编码器各层提取概念表示，以及概念提示调优(CPT)使用这些表示作为提示来增强关键概念相关的视觉线索，最后聚合各层视觉表示与文本概念表示对齐。

Result: 在三个公共数据集上的广泛实验结果表明，CoPA在概念和疾病预测方面优于现有最先进方法。

Conclusion: CoPA框架能有效捕获和利用图像中有价值的概念信息，显著提升了概念和疾病预测性能，为临床诊断提供了更透明的决策过程。

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [56] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: ZFP压缩技术可在保持脑血管分割质量的同时，实现高达22.89:1的数据压缩比，促进大规模医学影像研究的协作和可转移性。


<details>
  <summary>Details</summary>
Motivation: 解决3D医学影像数据集规模增大和复杂性提高带来的协作研究和可转移性障碍。

Method: 在包含真实血管分割标注的大规模3D医学数据集上，应用ZFP压缩技术的误差容忍和固定速率两种模式，并与未压缩基线进行分割质量对比。

Result: ZFP在误差容忍模式下实现22.89:1的数据压缩比，同时保持高保真度，平均Dice系数为0.87656（基线为0.8774）。

Conclusion: ZFP是促进大规模医学数据集高效和可访问研究的可行且强大工具，有助于推动社区内的广泛协作。

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [57] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: 提出了一种混合分割架构，集成CNN、Transformer和Mamba注意力融合机制，用于医学图像分割，在保持计算效率的同时提升准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型多为任务特定，性能在不同模态和解剖区域间差异大，难以平衡模型复杂度和性能，特别是在临床环境中需要兼顾准确性和效率。

Method: 采用三分支编码器整合CNN、Transformer和Mamba注意力融合机制，捕捉局部、全局和长程依赖；多尺度注意力CNN解码器重建细粒度分割图；协同注意力门增强特征选择和跨尺度通信。

Result: 在多个基准数据集上的实验表明，该方法在准确性和泛化能力上优于现有最优方法，同时保持相当的计算复杂度。

Conclusion: 该架构通过有效平衡效率和效果，为多样化医学成像任务提供了实用且可扩展的解决方案。

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [58] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 提出基于YOLOv9算法的道路损坏和井盖检测方法，使用多边形标注提高定位精度，在孟加拉国达卡收集的数据集上训练，整体准确率达78.1%。


<details>
  <summary>Details</summary>
Motivation: 城市安全和基础设施维护是智慧城市发展的关键，手动监测道路损坏耗时、成本高且容易出错，需要自动化解决方案。

Method: 使用YOLOv9算法结合多边形标注进行道路损坏和井盖检测，构建包含1000多张图像的数据集，分为Broken、Not Broken和Manhole三个类别。

Result: 整体图像级准确率78.1%，Broken类F1分数86.7%，Not Broken类F1分数89.2%，Manhole类F1分数18.2%（因类别不平衡问题）。

Conclusion: 该方法为发展中国家城市基础设施监控提供了高效且可扩展的解决方案。

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [59] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: 提出了一种结合对比学习和分数扩散模型的无配对图像翻译方法，通过时间相关的对比学习保留域不变特征，用对比模型指导预训练SDE的推理过程。


<details>
  <summary>Details</summary>
Motivation: 无配对图像翻译需要学习源域和目标域之间的映射，但缺乏对齐样本。扩散模型能生成高质量多样化输出，对比学习能学习语义相似性，两者都适合无配对场景。

Method: 使用时间相关的对比学习方法，将图像与其域不变特征作为正对进行SimCLR训练，然后用学到的对比模型指导预训练SDE进行图像翻译。

Result: 在三个常见无配对图像翻译任务上与多个基线方法比较，使用四个评估指标。Contrastive-SDE在多个指标上达到与最先进方法相当的结果，收敛速度显著更快，且无需标签监督或分类器训练。

Conclusion: 该方法是无配对图像翻译任务中更高效的替代方案，结合了对比学习和扩散模型的优势，实现了快速收敛和无监督学习。

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [60] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO是LIBERO基准的扩展版本，通过系统性地引入四个维度的扰动（操作对象、初始状态、任务指令和环境），揭示了现有VLA模型在标准评估中90%以上的准确率在广义设置下会崩溃至0.0%，暴露了模型依赖训练集记忆而非真正理解的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LIBERO基准的训练和评估设置存在问题，导致性能估计被夸大且无法公平比较模型。需要更严格的评估方法来测试模型的泛化能力和真实理解。

Method: 扩展LIBERO基准为LIBERO-PRO，在四个维度引入合理扰动：操作对象替换、初始状态变化、任务指令修改和环境变化，系统评估模型在扰动下的表现。

Result: 实验结果显示，现有模型在标准LIBERO评估中达到90%以上准确率，但在LIBERO-PRO的广义设置下性能崩溃至0.0%。模型表现出对训练集动作序列和环境布局的机械记忆，而非真正的任务理解。

Conclusion: 当前评估方法存在严重缺陷，呼吁社区放弃误导性的评估方法，采用能够真正测试模型泛化能力和理解的鲁棒评估标准。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [61] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: Mirage数据集包含具有可见伪影的AI生成图像，现有检测方法大多失败。研究发现大型视觉语言模型能有效检测有伪影的AI图像，但对无伪影图像检测性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成图像越来越难以被标准检测器识别，但人类仍能区分。研究旨在解决检测器与人类判断之间的差异，并探索大型视觉语言模型在可解释AI图像检测中的应用。

Method: 引入Mirage数据集，包含各种具有可见伪影的AI生成图像；研究大型视觉语言模型在AI图像检测任务中的表现，比较其在有伪影和无伪影图像上的检测能力。

Result: 大型视觉语言模型在检测具有可见伪影的AI生成图像方面非常有效，但在面对缺乏此类线索的图像时性能下降。

Conclusion: 大型视觉语言模型可作为AI图像检测的有力工具，特别是在图像包含可见伪影时，但对于更逼真的无伪影图像仍需改进检测方法。

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [62] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround提出了一种统一的视觉定位新范式，通过动态选择Transformer中间层作为"mask as prompt"，解决了现有方法依赖固定最后一层和"<SEG> as prompt"的两个主要问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法存在两个问题：(1)依赖固定的最后一隐藏层，导致层间传播误差累积；(2)使用<SEG>作为提示，将文本嵌入隐式投影到视觉空间而缺乏明确的空间线索。

Method: 提出Policy-Prompted Masking方法，包含两个核心组件：Stochastic Skip Connection（SSC）通过强化学习策略动态选择连接层，Mask as Prompt（MasP）使用相似度图作为软逻辑掩码来提示SAM生成掩码。

Result: UGround首次在单一框架内统一了从传统指代表达到推理分割、从单目标到多目标、从正查询到错误前提的视觉定位任务。

Conclusion: UGround通过动态层选择和mask作为提示的新范式，有效解决了现有视觉定位方法的局限性，提供了更灵活和准确的解决方案。

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [63] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: OMG4是一个优化的4D高斯泼溅框架，通过三阶段渐进式修剪（采样、剪枝、合并）和隐式外观压缩，显著减少模型存储开销60%以上，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 4D高斯泼溅虽然能实时渲染复杂动态场景，但需要数百万高斯原语导致存储开销巨大。现有方法在压缩比或视觉质量方面仍有局限。

Method: 三阶段渐进式修剪：高斯采样识别关键原语、高斯剪枝去除冗余、高斯合并融合相似特征；集成隐式外观压缩和广义子向量量化技术。

Result: 在标准基准数据集上的实验表明，OMG4显著优于现有方法，模型大小减少超过60%的同时保持重建质量。

Conclusion: OMG4在紧凑4D场景表示方面迈出重要一步，为广泛应用开辟新可能性。

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [64] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种新颖的开放词汇目标检测框架，通过结构化域对齐将地面视图的预训练模型知识迁移到航拍图像领域，解决了跨域知识转移的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测模型只能识别固定类别的对象，缺乏灵活性。开放词汇目标检测能够识别未见过的类别，但直接跨域知识转移在航拍图像中效果不佳，需要专门的适应策略。

Method: 采用对比性图像到图像对齐增强航拍和地面视图嵌入的相似性，并使用多实例词汇关联来对齐航拍图像与文本嵌入。

Result: 在多个数据集上的实验验证了方法的有效性，在零样本设置下相比微调的封闭词汇模型，在DOTAv2上提升+6.32 mAP，VisDrone上提升+4.16 mAP，HRRSD上提升+3.46 mAP。

Conclusion: 该方法为航拍应用中的目标检测系统提供了更灵活和可扩展的解决方案。

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [65] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 这篇综述论文讨论了深度学习在皮肤癌诊断中的应用，重点分析了当前面临的挑战（如复杂特征、图像噪声、类内变异等）以及应对这些挑战的创新方法（如数据增强、混合模型、特征融合等）。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是全球最常见和致命的癌症之一，早期检测对改善患者预后至关重要。深度学习在提高皮肤疾病自动诊断的准确性和效率方面显示出巨大潜力，但仍面临诸多挑战。

Method: 本文采用基于PRISMA框架的综合方法，综述了深度学习在皮肤癌诊断中的最新研究，重点关注数据增强、混合模型和特征融合等创新方法。

Result: 研究表明深度学习模型能够有效应对皮肤癌诊断中的复杂挑战，通过整合到临床工作流程中，有望彻底改变皮肤疾病诊断方式并改善临床决策。

Conclusion: 深度学习在皮肤癌诊断中具有变革性潜力，但需要持续的技术进步来充分释放其在皮肤病学护理中的价值。

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [66] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出SDAKD方法解决GAN知识蒸馏中的容量不匹配问题，通过引入学生判别器和三阶段训练策略，在图像超分辨率任务上取得优于现有方法的性能


<details>
  <summary>Details</summary>
Motivation: GANs在生成任务中表现优异但计算需求大，难以部署到资源受限设备。知识蒸馏是GAN压缩的有前景方向，但由于学生生成器与教师判别器之间的容量不匹配，有效训练小型学生生成器具有挑战性

Method: 提出学生判别器辅助知识蒸馏(SDAKD)，引入学生判别器缓解容量不匹配问题，采用三阶段训练策略，并在最后两个训练阶段集成适配的特征图蒸馏方法

Result: 在GCFSR和Real-ESRGAN两个高性能超分辨率GAN上评估，实验表明相比基线和最先进的GAN知识蒸馏方法有持续改进

Conclusion: SDAKD方法通过解决容量不匹配问题，有效提升了GAN知识蒸馏的性能，为资源受限设备部署GAN提供了可行方案

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [67] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 提出了PoseGaze-AHP数据集，这是首个专门用于AI驱动眼源性异常头位诊断的公开资源，同步捕捉头位和注视运动信息。


<details>
  <summary>Details</summary>
Motivation: 现有数据集分别关注头位和眼部运动，限制了眼源性异常头位综合诊断方法的发展，阻碍了AI在该领域的进步。

Method: 使用大型语言模型从医学文献中提取结构化临床数据，通过迭代过程结合分步、分层和复杂提示策略，使用神经头像框架将记录转换为3D表示。

Result: 数据集包含7,920张图像，覆盖广泛的眼部疾病谱，提取方法总体准确率达到91.92%。

Conclusion: PoseGaze-AHP是首个公开可用的眼源性异常头位AI诊断资源，支持开发准确且符合隐私要求的诊断工具。

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [68] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了DHQA-4D数据集和DynaMesh-Rater方法，用于动态4D数字人网格的质量评估。DHQA-4D包含32个高质量4D人体网格序列和1920个失真样本，DynaMesh-Rater通过提取视觉、运动和几何特征，利用大语言模型进行质量评分。


<details>
  <summary>Details</summary>
Motivation: 随着3D扫描和重建技术的发展，动态数字人化身在游戏、动画和远程通信中应用广泛。但4D人体网格在采集、压缩和传输过程中容易受到噪声影响，影响用户体验，因此需要有效的质量评估方法。

Method: 构建了DHQA-4D数据集，包含真实扫描的4D人体网格序列和失真样本。提出了DynaMesh-Rater方法，从投影的2D视频提取视觉特征，从裁剪视频提取运动特征，从4D网格提取几何特征，然后利用大语言模型整合这些多维度特征并进行LoRA指令微调来预测质量分数。

Result: 在DHQA-4D数据集上的大量实验结果表明，DynaMesh-Rater方法在质量评估方面优于先前的方法。

Conclusion: DHQA-4D数据集为动态4D数字人质量评估提供了重要基准，DynaMesh-Rater方法通过多维度特征融合和大语言模型的有效利用，在质量评估任务中表现出优越性能。

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [69] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于自适应空间特征融合(ASFF)的改进ResNet-50模型，用于皮肤癌分类，在ISIC 2020数据集上达到93.18%的准确率。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌分类面临类间相似度高、类内变异大和图像噪声等挑战，需要更有效的特征表示方法来减少过拟合。

Method: 在ResNet-50基础上引入自适应特征融合机制，通过双分支设计融合高层语义和中层细节特征，使用全局平均池化和全连接层生成自适应权重进行加权融合。

Result: 在ISIC 2020数据集(3297张图像)上，相比5种经典CNN模型取得最佳性能：准确率93.18%，AUC值0.9670(PR曲线)和0.9717(ROC曲线)。Grad-CAM验证模型能聚焦病变相关区域。

Conclusion: 该方法为计算机辅助皮肤癌诊断提供了更有效和高效的解决方案，通过自适应特征融合增强了特征学习能力。

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [70] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: 开发了一个多模态深度学习框架，通过集成临床、放射学和病理学图像来改善口腔鳞状细胞癌的早期检测。


<details>
  <summary>Details</summary>
Motivation: 口腔鳞状细胞癌的晚期诊断导致高死亡率，超过50%的病例在晚期被发现，5年生存率低于50%。需要改进早期检测方法。

Method: 使用公开数据集训练三个DenseNet-121 CNN模型，分别处理不同医学成像模态。通过数据增强和模态特定预处理提高鲁棒性，采用验证加权集成策略融合预测结果。

Result: 放射学模态验证准确率100%，病理学模态95.12%，临床图像较低为63.10%。集成模型在多模态验证集上达到84.58%的总体准确率。

Conclusion: 该多模态集成框架提供了一种非侵入性的AI辅助分诊工具，能够增强高风险病变的早期识别，支持临床决策，减少诊断延迟并改善患者预后。

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [71] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文挑战了扩展定律，提出IQA-Select方法，通过聚类选择高质量指令调优数据，仅用10%数据就能达到甚至超越全量数据训练的效果。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在可解释图像质量评估中指令调优数据冗余和计算成本高的问题，质疑扩展定律的普适性。

Method: 提出基于聚类的三阶段数据选择框架：聚类特征提取、集群配额分配、集群采样策略，开发了IQA-Select方法。

Result: 在Q-Bench和AesBench上，仅使用10%选择的数据就能分别达到全量数据训练102.1%和103.7%的性能。

Conclusion: 数据质量比数据量更重要，IQA-Select能显著降低计算成本同时获得更好性能，挑战了传统的扩展定律。

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [72] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种基于通用动作专家的框架，使用稀疏3D轨迹作为中间表示，将VLM的高级规划能力与低级物理动作模块连接起来，解决了传统VLA模型的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-语言-动作模型在将推理能力转化为物理世界时泛化能力差，而双系统方法又受到动作模块语义模糊性的限制，需要在新环境中重新收集数据进行微调。

Method: 采用稀疏3D轨迹作为中间表示，VLM仅需生成粗略3D路径点，然后由通用动作专家通过采样实时点云观测将其细化为密集可执行动作序列，并引入"动作预训练、点云微调"的训练范式。

Result: 该方法结合了VLM在视觉理解和规划方面的广泛泛化能力与动作专家在动作级别的细粒度泛化能力。

Conclusion: 提出的框架有效解决了VLA模型在物理世界中的泛化问题，通过中间表示和通用动作专家实现了更好的跨任务训练和环境适应性。

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [73] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 提出了一种将零样本细粒度图像分类转化为视觉问答框架的新方法，利用大视觉语言模型的综合理解能力，通过注意力干预技术提升性能，并在多个基准测试中超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在视觉语言推理任务中表现出色，但它们在零样本细粒度图像分类方面的潜力尚未充分探索，该任务需要在视觉相似类别间进行精确区分。

Method: 将零样本细粒度图像分类转化为视觉问答框架，利用LVLMs的综合理解能力而非直接生成类别名称，并采用新颖的注意力干预技术来增强模型性能。

Result: 在多个细粒度图像分类基准测试中进行了广泛实验，所提出的方法始终优于当前最先进方法。

Conclusion: 该方法不仅证明了其有效性，还展示了LVLMs在零样本细粒度分类任务中的更广泛潜力。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [74] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: 本文系统评估了多种去雾方法在自动驾驶感知中的效果，包括传统滤波器、去雾网络、级联方法和视觉语言模型，发现图像质量的提升并不总是转化为下游检测和分割性能的改善。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知系统在雾天条件下特别脆弱，现有去雾方法虽然能提高图像保真度，但这种改善并不总能转化为更好的下游检测和分割性能，且先前评估多依赖合成数据，对真实世界适用性存在疑问。

Method: 使用Foggy Cityscapes数据集，系统评估了四类去雾流程：传统滤波器、现代去雾网络、级联变体（滤波器→模型、模型→滤波器）以及直接应用于雾图的提示驱动视觉语言图像编辑模型。

Result: 分析揭示了去雾何时有效、级联何时产生协同或退化效应，以及VLM编辑器与专用方法的比较。VLM评判的定性评分与任务指标（特别是mAP）显示出强相关性。

Conclusion: 研究为去雾方法建立了透明、面向任务的基准，并明确了在何种条件下预处理能真正改善恶劣天气中的自动驾驶感知性能。

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [75] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: CAMEO是一个用于通用人体运动视频生成的分层框架，通过连接文本到运动(T2M)模型和条件视频扩散模型(VDM)，解决了训练和推理过程中的次优因素，实现了从文本到人体运动视频的生成。


<details>
  <summary>Details</summary>
Motivation: 尽管视频扩散模型(VDM)发展迅速，但在通用人体视频生成方面的应用仍未被充分探索，大多数工作局限于图像到视频设置或狭窄领域(如舞蹈视频)。

Method: 提出CAMEO分层框架，通过精心设计的组件连接T2M模型和条件VDM，包括分析准备文本提示和视觉条件来有效训练VDM，以及引入相机感知条件模块自动选择与输入文本对齐的视角。

Result: 在MovieGen基准和新引入的T2M-VDM组合基准上证明了方法的有效性，展示了其在多样化用例中的多功能性。

Conclusion: CAMEO框架成功实现了通用人体运动视频生成，通过连接T2M和VDM模型并解决对齐问题，在多个基准测试中表现出色，具有广泛的应用潜力。

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [76] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: OpenFLAME是一个联邦化的视觉定位系统后端，允许多个组织独立维护各自的3D扫描和VPS服务，解决集中式VPS在隐私、监管和维护方面的限制。


<details>
  <summary>Details</summary>
Motivation: 集中式VPS无法覆盖私人室内空间，存在隐私担忧、监管限制和维护瓶颈。需要一种分布式解决方案来扩大覆盖范围并保护隐私。

Method: 提出联邦化图像定位概念，采用分布式VPS服务架构，各组织独立维护自己的3D扫描和VPS服务，通过参考解决方案管理和合并地图数据而不共享私有数据。

Result: 实现了访问控制、分布式维护和更大覆盖范围，解决了跨空间定位结果一致性、服务质量控制和位置服务选择等挑战。

Conclusion: 联邦化VPS架构能够有效解决集中式系统的局限性，为大规模AR应用提供更广泛、更隐私保护的定位服务。

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [77] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: 开发了一个结合CNN-LSTM模型提取生物力学特征和大型语言模型生成可操作反馈的框架，用于网球击球分析。


<details>
  <summary>Details</summary>
Motivation: 现有系统未能将生物力学洞察转化为对球员和教练有意义且可操作的语言反馈，需要填补这一空白。

Method: 使用CNN-LSTM模型从运动数据中提取关键生物力学特征（关节角度、肢体速度、动力链模式），然后通过大型语言模型分析这些特征与击球效果和受伤风险的关系，生成反馈。

Result: 基于THETIS数据集和特征提取技术，该方法能够产生技术上准确、生物力学基础扎实且对终端用户可操作的反馈。

Conclusion: 该框架在分类性能和可解释性方面进行评估，弥合了可解释AI与运动生物力学之间的差距。

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [78] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: TimeWarp方法通过创建针对性的合成时间数据集来改进视频大语言模型的细粒度时间理解能力，在七个基准测试中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在需要细粒度时间理解的任务上表现不佳，原因是微调数据集缺乏视觉复杂性和时间细节，导致模型过度依赖语言推理而非真正理解视频动态。

Method: 提出TimeWarp方法，系统性地创建目标合成时间数据集来微调模型响应，使其专注于输入视频内容。引入大规模偏好数据集捕捉常被忽略的复杂时间动态，将模型响应基于视觉和时间信息。

Result: 将该方法应用于现有模型后，在时间理解基准测试中性能显著提升，在七个基准测试中实现了绝对性能改进。

Conclusion: TimeWarp方法通过针对性数据集有效推进了视频大语言模型的时间理解能力，证明了所提出数据集在提升时间理解方面的有效性。

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [79] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 本文研究了在生物医学视觉语言模型中扩展文本编码器上下文长度的影响，发现更长的上下文能带来更好的检索和分类性能。作者提出了BIOMEDICA-LongCAP数据集和BMC-LongCLIP模型，将上下文容量扩展6.6倍，在长文本检索基准上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型通常使用短文本窗口（<77个token）进行预训练，这导致长格式的生物医学描述被截断。然而，大规模开源文献显示大部分生物医学描述远超77个token，因此需要研究长上下文预训练的影响。

Method: 通过扩展视觉语言模型中文本编码器的上下文长度，使用BIOMEDICA-LongCAP数据集（包含100万图像-描述对，具有来自全文文章的情境感知描述）训练BMC-LongCLIP模型，支持最多512个token的文本窗口。

Result: BMC-LongCLIP将上下文容量扩展6.6倍，token浪费从55%降至2.2%。在长描述检索基准上，Recall@1获得高达30%的绝对提升，分类平均提升2%，且收敛速度比短上下文模型更快。

Conclusion: 长上下文建模是推进生物医学视觉语言模型发展的有前景方向，长格式描述提供的额外监督能显著提升模型性能。

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [80] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: 提出CPG框架解决长尾半监督学习中未标记数据分布未知的问题，通过可控伪标签生成和动态过滤机制，确保更新后的标记数据集遵循已知分布，并在多个基准数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有长尾半监督学习方法假设未标记数据遵循预定义分布，但实际上未标记数据的分布通常是未知且任意的，这限制了方法的实际应用效果。

Method: 采用可控自增强优化循环：动态可控过滤机制选择可靠伪标签，构建贝叶斯最优分类器，结合类别感知自适应增强模块和辅助分支最大化数据利用率。

Result: 在多个常用基准数据集上的综合评估显示，CPG实现了持续改进，在准确率上比最先进方法高出最多15.97%。

Conclusion: CPG框架通过可控伪标签生成有效解决了未标记数据分布未知的问题，理论证明能显著降低泛化误差，实验验证了其优越性能。

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [81] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: 提出基于PaddleOCRv5的微调方法，显著提升越南汉喃古籍文字识别准确率，从37.5%提升至50%，特别是在噪声图像条件下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有OCR系统在处理越南汉喃古籍时面临扫描质量差、字形不标准和手写变体等挑战，需要专门优化以支持历史文献数字化和跨语言语义研究。

Method: 使用精选的越南汉喃手稿子集对PaddleOCRv5的文本识别模块进行微调，构建完整的训练流程包括预处理、LMDB转换、评估和可视化。

Result: 微调后模型准确率显著提升，从37.5%提高到50.0%，在噪声图像条件下表现尤为突出。开发了交互式演示系统对比微调前后识别效果。

Conclusion: 该方法有效提升了汉喃文字识别性能，支持汉越语义对齐、机器翻译和历史语言学研究等下游应用，提供了公开可用的演示系统。

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [82] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: MetaSeg是一个基于元学习的隐式神经表示框架，用于医学图像分割，能够在保持高精度的同时减少90%的参数数量。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示在信号表示方面表现出色，但难以直接应用于需要学习语义结构的预测任务（如分割）。

Method: 使用一个同时预测像素强度和类别标签的隐式神经表示，通过元学习找到最优初始参数，使得模型只需微调即可适应新图像。

Result: 在2D和3D脑部MRI分割任务中，Dice得分与常用的U-Net模型相当，但参数数量减少了90%。

Conclusion: MetaSeg为医学图像分割提供了一个轻量级、可扩展的替代方案，相比传统的U-Net和视觉变换器架构更加资源高效。

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [83] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: ViTL是一个两阶段长视频问答框架，通过低帧率浏览定位问题相关片段，然后在固定token预算下重新分配视觉token进行回答，实现计算高效的视频理解。


<details>
  <summary>Details</summary>
Motivation: 解决长视频问答中的计算效率问题，传统方法在固定token预算下难以同时处理时间定位和答案生成。

Method: 两阶段框架：1) 低帧率浏览定位问题相关时间片段；2) 在固定token预算下重新分配视觉token进行问答，使用交错组相对目标联合训练定位和答案生成。

Result: 在固定token预算下，ViTL在长视频问答和时间定位任务上达到8.6%的性能提升，同时减少50%的帧输入；span感知的token重分配始终优于均匀采样。

Conclusion: ViTL和配套数据集提供了一个可解释、计算高效的解决方案，可用于可扩展的长视频问答系统。

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [84] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: 本文研究了如何通过优化超参数来提高基于提示的图像编辑框架的精度和可靠性，提出了注意力重加权方法和CL P2P框架来解决现有方法中的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于稳定扩散模型的图像编辑方法虽然简化了编辑过程，但存在结果可变性问题，如头发颜色变化不一致。研究旨在提高提示到提示图像编辑的精确性和可靠性。

Method: 研究了"词语替换"方法，开发了"注意力重加权方法"以提高适应性，并提出了"CL P2P"框架来解决循环不一致等现有限制。

Result: 通过探索和优化超参数，增强了图像编辑框架的精度和可靠性，改善了注意力机制与神经网络架构选择之间的交互作用。

Conclusion: 这项工作有助于理解和改进超参数设置与神经网络模型架构选择（特别是注意力机制）之间的相互作用，这些因素显著影响生成图像的构图和质量。

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [85] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight是一个多模态大语言模型，通过动态调用专用工具迭代聚焦屏幕相关区域，显著提升视觉定位准确性，在ScreenSpot-Pro基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在实际GUI系统中视觉定位可靠性不足的问题，无法准确执行指针级操作如点击或拖拽。

Method: 训练图像基础推理模型，动态调用多个专用工具迭代缩小屏幕关注区域，提高视觉定位精度。

Result: 在ScreenSpot-Pro基准测试中，仅使用18.5K训练样本达到52.8%准确率，超越V2P-7B（50.6%，9.6M样本）和GTA-1-7B（50.1%，1.56M样本）。

Conclusion: GUI-Spotlight通过工具调用和迭代聚焦策略，有效提升了GUI系统中的视觉定位能力，为实际应用提供了更可靠的解决方案。

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [86] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: 提出一种用于后训练量化的范围估计方法，通过层间局部最小值优化来最小化量化误差，在图像分类任务中显著提升了低比特量化的精度表现。


<details>
  <summary>Details</summary>
Motivation: 后训练量化虽然能有效减少深度神经网络模型的存储需求，但在保持模型精度的同时进行低比特量化仍然是一个具有挑战性的问题。

Method: 将范围估计建模为通过层间局部最小值最小化量化误差的优化问题，证明该问题具有局部凸性，并提出高效的搜索算法来寻找最优解。在实际应用中，将搜索算法应用于变换后的权重空间以进一步改进性能。

Result: 在ResNet系列模型和Inception-v3模型上的实验表明，该方法在图像分类任务的top-1准确率上普遍优于现有技术。在8位和6位设置下几乎无精度损失，4位量化的准确率也得到显著提升。

Conclusion: 提出的范围估计方法能够有效提升后训练量化的性能，特别是在低比特设置下保持模型精度方面表现优异。

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [87] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind是一个面向元宇宙场景生成的三模态组合检索框架，通过从大规模存储库中检索3D资产来解决资产检索不一致和缺乏标准化检索范式的问题。


<details>
  <summary>Details</summary>
Motivation: 解决当前3D资产检索中的两个核心挑战：(i)忽视空间、语义和风格约束的不一致资产检索；(ii)缺乏专门为3D资产检索设计的标准化检索范式，现有方法主要依赖通用3D形状表示模型。

Method: 提出灵活的检索机制，支持文本、图像和3D模态的任意组合作为查询，通过联合建模对象级特征（包括外观）和场景级布局结构来增强空间推理和风格一致性。引入即插即用的等变布局编码器ESSGNN来捕获空间关系和对象外观特征。

Result: 经验评估表明，MetaFind在各种检索任务中相比基线方法具有改进的空间和风格一致性。

Conclusion: MetaFind框架通过三模态组合检索和等变布局编码，有效提升了3D资产检索的空间和风格一致性，支持迭代式场景构建。

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [88] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: 提出一种改进的损失函数，将耀斑子类间的序数信息整合到二元交叉熵损失中，通过序数加权增强模型对阈值附近事件的区分能力。


<details>
  <summary>Details</summary>
Motivation: 传统二元分类框架忽略了耀斑子类间的序数关系，导致模型在预测阈值附近频繁出现误分类。

Method: 在传统二元交叉熵损失基础上整合序数信息，作为序数感知的数据驱动正则化方法，对阈值附近的错误预测施加更重惩罚。

Result: 该方法通过利用数据的序数特性增强模型学习过程，有望提升整体性能。

Conclusion: 整合序数信息的损失函数能够改善太阳耀斑预测模型在阈值附近的分类准确性。

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [89] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出了QuantDemoire，一种专门针对去摩尔纹任务的训练后量化框架，通过异常值感知量化器和频率感知校准策略，在保持质量的同时大幅减少参数和计算量。


<details>
  <summary>Details</summary>
Motivation: 现有的去摩尔纹深度学习方法需要大量计算资源，难以在边缘设备上部署。直接应用现有量化方法会导致严重的性能下降，主要原因是分布异常值和平滑区域表示弱化。

Method: 包含两个关键组件：1) 异常值感知量化器，使用基于采样的范围估计减少激活异常值，并将少量极端权重保留为FP16；2) 频率感知校准策略，在微调过程中强调低频和中频分量。

Result: 在W4A4配置下比现有量化方法性能提升超过4dB，同时大幅减少了参数和计算量。

Conclusion: QuantDemoire框架有效解决了去摩尔纹模型量化中的性能退化问题，实现了高效部署。

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [90] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA是一种结合扩散生成先验和多正则化约束的低剂量稀疏视图CT重建方法，在ADMM框架下实现高效3D重建。


<details>
  <summary>Details</summary>
Motivation: 解决极稀疏视图下CT重建的病态问题和纹理丢失问题，结合生成先验和物理约束提高重建质量。

Method: 结合扩散生成先验(NCSN++ + SDE建模)和各向异性TV、核范数(LoRA)正则化，采用ADMM框架，使用2D切片策略并利用FFT加速和并行优化。

Result: 在AAPM-2016、CTHD和LIDC数据集上，TV-LoRA在SSIM、纹理恢复、边缘清晰度和伪影抑制方面均优于基准方法，表现出强鲁棒性和泛化能力。

Conclusion: TV-LoRA实现了高保真、高效的3D CT重建，在低剂量稀疏采样场景下具有广泛的临床应用前景。

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [91] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 该论文提出了拓扑映射的标准化评估协议，包括拓扑一致性作为核心指标、数据集模糊度量化方法，并发布了基准数据集和基线系统以促进可复现研究。


<details>
  <summary>Details</summary>
Motivation: 拓扑映射领域缺乏标准化的评估指标、数据集和协议，现有系统在不同环境和标准下评估，无法进行公平可复现的比较，且感知混淆问题的影响未被充分量化。

Method: 形式化拓扑一致性作为拓扑映射的基本属性，提出定位精度作为替代指标；提出数据集模糊度的定量度量方法；构建具有校准模糊度水平的多样化基准数据集；实现并发布深度学习基线系统。

Result: 实验和分析揭示了当前方法在感知混淆情况下的局限性，为拓扑映射研究提供了新的见解。

Conclusion: 所有数据集、基线系统和评估工具都已开源，旨在促进拓扑映射领域的一致性和可复现性研究。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [92] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出了事件相机的网格流估计新任务，创建了HREM高分辨率事件网格流数据集，开发了轻量级EEMFlow网络进行快速准确的网格流估计，并扩展支持稠密光流。还提出了自适应密度模块ADM来提升模型在不同事件数据密度下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 事件相机网格流估计领域存在两个关键问题：缺乏专门的网格流数据集和方法，以及事件数据密度挑战未被充分探索。现有方法未能完全处理高分辨率、动态对象和复杂运动模式。

Method: 1) 创建HREM高分辨率事件网格流数据集；2) 提出轻量级EEMFlow网络，采用编码器-解码器架构；3) 扩展支持稠密光流，引入置信度诱导细节补全模块；4) 提出自适应密度模块ADM优化输入事件数据密度。

Result: EEMFlow模型相比现有最优方法性能优异且运行速度快30倍。ADM模块显著提升EEMFlow和EEMFlow+性能，分别提高8%和10%。HREM+数据集支持多密度事件研究。

Conclusion: 本文成功解决了事件相机网格流估计的关键挑战，通过新数据集、高效网络架构和自适应密度处理，显著推进了该领域的发展，为实际应用提供了实用解决方案。

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [93] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出一种新的扩散模型方法用于6D物体姿态估计，通过预训练编码器和联合学习策略加速训练收敛，并引入采样指导机制消除额外评估网络需求，在多个基准测试中达到最先进精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的6D姿态估计方法存在训练收敛慢、需要端到端学习编码器、以及需要额外网络来筛选姿态候选的问题。

Method: 1) 预训练编码器并使用直接姿态回归头，联合学习回归头和去噪扩散头；2) 提出时间依赖的分数缩放采样指导机制，有效平衡探索-利用权衡。

Result: 在REAL275、HouseCat6D和ROPE等多个基准测试中达到最先进精度，即使使用单姿态推理也能实现高效训练和推理。

Conclusion: 该方法简单有效，解决了现有扩散模型在6D姿态估计中的关键限制，在精度和效率方面都有显著提升。

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [94] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: 本文提出了一种解决多模态大语言模型蒸馏中概念漂移问题的新方法，通过自主偏好优化(APO)来对齐教师模型推理轨迹中的概念漂移，提升学生模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多教师蒸馏中，不同教师模型产生的推理轨迹存在概念漂移问题，导致推理分布不可预测地演化，并将偏见传递给学生模型，最终损害其性能。

Method: 提出"学习、比较、批判"范式，通过自主偏好优化(APO)让学生模型在教师指导下学习偏好思维，并对教师推理漂移进行批判性反思，实现概念对齐。

Result: 实验表明该方法在知识蒸馏中具有优越的一致性、鲁棒性和泛化性能，并贡献了包含170,982条蒸馏推理轨迹的大规模数据集CXR-MAX。

Conclusion: 通过理论连接概念漂移与知识蒸馏，提出的APO方法能有效解决多教师蒸馏中的概念漂移问题，产生鲁棒、一致且可泛化的模型。

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [95] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: BLADE是一种无需先验偏差知识或偏差冲突样本的生成式去偏框架，通过跨偏差域图像翻译和自适应精炼来缓解神经网络中的隐式偏差问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络容易学习训练数据中的虚假相关性，现有方法需要偏差先验知识或偏差冲突样本，这在现实场景中往往不切实际。

Method: 首先训练生成模型进行跨偏差域图像翻译，保持任务相关特征；然后基于图像对偏差的敏感性进行自适应精炼；通过对齐任务相关特征但偏差不同的图像对，以及错开相同偏差的样本来鼓励鲁棒表示。

Result: 在多个基准数据集上显著优于现有最先进方法，在损坏CIFAR-10数据集的最差组设置下比最接近的基线绝对提升约18%。

Conclusion: BLADE为无需显式监督开发更鲁棒的深度学习模型建立了新基准，展示了在偏差缓解方面的潜力。

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [96] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: 提出SEG-MIL-CBM框架，将概念引导的图像分割与注意力多示例学习结合，通过语义区域推理实现透明、空间定位的概念级解释，无需概念标注。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络缺乏可解释性，在安全关键应用中限制信任。现有概念瓶颈模型需要昂贵概念标注且缺乏空间定位，无法识别支持每个概念的区域。

Method: 集成概念引导图像分割到注意力多示例学习框架，将分割区域作为实例，学习跨区域证据聚合，通过语义区域推理突出任务相关证据。

Result: 在涉及虚假相关性、输入损坏和大规模基准测试中实现鲁棒性能，同时提供透明概念级解释。

Conclusion: SEG-MIL-CBM通过语义区域推理实现了无需概念标注的透明、空间定位概念级解释，在多种设置下保持鲁棒性能。

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [97] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: HyCa是一个基于混合ODE求解器的缓存框架，通过维度级缓存策略加速扩散变换器的采样过程，在多个模型上实现5-6倍加速且几乎无损。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器在图像和视频合成中具有最先进的保真度，但其迭代采样过程由于每个时间步都需要昂贵的变换器前向传播而成为主要瓶颈。现有特征缓存方法对所有特征维度采用统一策略，忽略了它们异质的动态行为。

Method: 将隐藏特征演化建模为跨维度的ODE混合，引入HyCa框架应用维度级缓存策略，基于混合ODE求解器设计缓存机制。

Result: 在多个模型上实现显著加速：FLUX 5.55倍、HunyuanVideo 5.56倍、Qwen-Image和Qwen-Image-Edit 6.24倍，且几乎无损。

Conclusion: HyCa通过维度级缓存策略有效解决了扩散变换器采样瓶颈，实现了训练免费的高效加速。

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [98] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: World-To-Image框架通过代理驱动的世界知识增强文本到图像生成，解决模型在处理新颖或分布外实体时的性能下降问题，在语义对齐和视觉美学方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在处理新颖或分布外实体时性能显著下降，因为模型存在固有的知识截止限制，无法准确合成未知概念。

Method: 设计一个代理动态搜索网络获取基础模型未知概念的图像，然后进行多模态提示优化，引导强大的生成主干实现准确合成。

Result: 在NICE基准测试中，准确度比提示提高+8.1%，在语义对齐和视觉美学方面显著优于最先进方法，且效率高，在不到三次迭代内实现。

Conclusion: 该框架为文本到图像系统铺平了道路，使其能够更好地反映不断变化的现实世界，实现了高效率和显著的性能提升。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [99] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC提出了一种层次化语义树框架，通过几何感知距离度量和密度驱动的聚合构建，将平坦的视觉标记预测任务转化为结构化层次任务，显著提升自回归模型的训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型使用平坦、非结构化的视觉标记词汇表，忽视了标记嵌入空间的内在结构，导致预测任务复杂，限制了训练效率和最终生成质量。

Method: 提出MASC框架，使用几何感知距离度量和密度驱动的聚合构建方法，直接从码本的内在结构中构建层次化语义树，将高维预测任务转化为结构化层次任务。

Result: 训练速度提升高达57%，LlamaGen-XL的FID从2.87降低到2.58，显著改善了生成质量，使现有AR框架与最先进方法具有竞争力。

Conclusion: 结构化预测空间对于可扩展生成建模与架构创新同等重要，MASC通过引入有益的归纳偏置简化了AR模型的学习问题。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [100] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: ZoomIn是一个两阶段取证框架，通过模仿人类视觉检查，先扫描图像定位可疑区域，再对这些放大区域进行聚焦分析，提高AI生成图像检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的快速增长模糊了真实与合成内容的边界，引发了数字完整性的严重关切。视觉语言模型虽然提供可解释性，但在检测高质量合成图像的细微伪影方面往往失败。

Method: 提出ZoomIn两阶段取证框架：第一阶段扫描图像定位可疑区域，第二阶段对放大区域进行聚焦分析。创建MagniFake数据集（20,000张真实和高质量合成图像，带有边界框和取证解释），通过自动化VLM流程生成训练数据。

Result: 该方法达到96.39%的准确率，具有强大的泛化能力，同时提供基于视觉证据的人类可理解解释。

Conclusion: ZoomIn框架在AI生成图像检测方面实现了高准确性和可解释性的平衡，为数字完整性保护提供了有效的解决方案。

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [101] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: 提出了一种简单、端到端可训练的图像配准算法，只需少量Python代码即可实现，在训练数据和训练时间有限的情况下仍能获得准确结果。


<details>
  <summary>Details</summary>
Motivation: 图像配准问题需要找到对齐两个图像的变换，使对应点位于相同位置。传统方法通常复杂且需要大量训练数据，本文旨在开发简单高效的解决方案。

Method: 采用端到端可训练的算法，仅需少量Python代码实现。在立体视觉应用中，使用74张图像在19x15输入窗口上进行训练。

Result: 算法在训练数据和训练时间有限的情况下表现出色，能够获得准确的配准结果，代码简洁（仅需十几行Python）。

Conclusion: 该算法在代码简洁性、训练数据需求和训练时间方面具有优势，为相关场景提供了良好的起点。

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [102] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 提出了一种名为ArConv的新型卷积层，通过重新设计和优化卷积层来创建轻量级神经网络模型，该模型仅含130万参数，在RfMiD数据集上表现优于MobileNetV2（220万参数），准确率达到0.9328。


<details>
  <summary>Details</summary>
Motivation: 提高深度神经网络的可访问性，特别是在移动设备上应用，用于早期诊断眼部疾病。现有方法计算复杂，限制了在资源受限设备上的部署。

Method: 在基础层面重新设计和优化卷积层，开发了新型ArConv卷积层，构建了轻量级通用模型。

Result: 最终模型仅含130万参数，在RfMiD数据集测试集上准确率达到0.9328，优于MobileNetV2的0.9266。

Conclusion: 通过优化卷积层设计，成功创建了适用于移动设备的轻量级神经网络模型，在眼部疾病诊断任务中实现了高准确率和更好的可访问性。

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [103] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido是一个用于照片级真实感神经渲染的生成模型，将3D视为视频的特殊子域，通过序列到序列的图像合成实现生成式视图合成，无需显式3D表示。


<details>
  <summary>Details</summary>
Motivation: 传统3D渲染方法依赖显式3D表示和大量相机标注数据，限制了模型的泛化能力和可扩展性。

Method: 采用序列到序列的生成神经渲染框架，结合掩码自回归架构和整流流变换器，统一3D和视频建模，利用大规模视频数据进行预训练。

Result: 在多个视图合成基准测试中达到最先进水平，零样本性能在少视图设置中显著优于其他生成方法，在多视图设置中首次达到逐场景优化方法的质量。

Conclusion: Kaleido通过将3D建模统一到视频生成框架中，显著减少了对外部3D数据的依赖，实现了高质量的生成式神经渲染。

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [104] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: 提出了CoSSeg-TTA框架，用于增强MRI肝脏分割，通过半监督学习和域适应技术解决标注数据有限和跨域差异问题。


<details>
  <summary>Details</summary>
Motivation: 对比增强MRI肝脏分割面临标注数据有限、增强协议异质性和跨扫描仪/机构的域偏移挑战。传统图像翻译方法存在结构扭曲和训练不稳定等问题，不适合单模态场景。

Method: 基于nnU-Netv2构建紧凑分割框架，采用半监督均值教师方案利用未标注数据，结合随机直方图风格外观转换和可训练对比感知网络的域适应模块，并使用持续测试时适应策略。

Result: 在广泛实验中，该框架持续优于nnU-Netv2基线，获得更高的Dice分数和Hausdorff距离，在低标注条件下对未见域表现出强泛化能力。

Conclusion: CoSSeg-TTA框架有效解决了肝脏MRI分割中的域适应和标注稀缺问题，实现了优越的分割性能和跨域泛化能力。

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [105] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 提出了一种基于概念解释的补丁无关防御方法，通过抑制最具影响力的概念激活向量来中和对抗补丁效果，无需显式检测补丁位置或大小。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗补丁防御方法通常需要预先知道补丁大小或位置，这限制了它们的实际应用。

Method: 利用基于概念的解释来识别和抑制最具影响力的概念激活向量，从而在不显式检测补丁的情况下中和补丁效果。

Result: 在Imagenette数据集和ResNet-50模型上的评估显示，该方法在鲁棒性和干净准确率方面均优于最先进的PatchCleanser方法，并在不同补丁大小和位置下保持强劲性能。

Conclusion: 将可解释性与鲁棒性相结合具有前景，概念驱动的防御策略是保护机器学习模型免受对抗补丁攻击的可扩展方法。

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [106] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: 提出Adapt-STformer，一种基于循环可变形Transformer编码器的Seq-VPR方法，解决了现有方法在灵活性、推理速度和内存使用方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的Seq-VPR方法在追求性能的同时牺牲了灵活性和效率，无法满足实际应用中可变序列长度、快速推理和低内存使用的需求。

Method: 采用循环可变形Transformer编码器（Recurrent-DTE），通过迭代循环机制融合多帧序列信息，支持可变序列长度。

Result: 在Nordland、Oxford和NuScenes数据集上，召回率提升高达17%，序列提取时间减少36%，内存使用降低35%。

Conclusion: Adapt-STformer在保持高性能的同时，实现了灵活性、快速推理和低内存使用，满足了实时Seq-VPR应用的需求。

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [107] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit将图像编辑重构为视频生成问题，利用预训练视频生成模型来确保物理一致性，通过时间推理阶段生成合理的编辑轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有大型生成模型在图像编辑和上下文图像生成方面取得进展，但缺乏物理一致性保证，这在世界模拟相关任务中尤为重要。

Method: 将输入和编辑图像视为视频的首尾帧，利用预训练视频模型学习的时间一致性；引入时间推理阶段，在推理时联合去噪目标帧和推理标记，想象合理的编辑轨迹。

Result: 在PBench-Edit基准测试中，ChronoEdit在视觉保真度和物理合理性方面超越了最先进的基线方法。

Conclusion: ChronoEdit通过将图像编辑重构为视频生成问题，有效解决了物理一致性问题，为世界模拟任务提供了重要能力。

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [108] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: CARE-PD是最大的公开帕金森病3D步态数据集，包含9个队列的匿名SMPL网格数据，支持临床评分预测和运动预训练任务。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的客观步态评估受限于缺乏大规模、多样化且临床标注的运动数据集。

Method: 将RGB视频或运动捕捉数据通过统一预处理流程转换为匿名SMPL网格，支持监督临床评分预测和无监督运动预训练任务。

Result: 运动编码器持续优于手工特征，在CARE-PD上预训练将MPJPE从60.8mm降至7.5mm，PD严重程度macro-F1提高17个百分点。

Conclusion: CARE-PD展示了临床策划的多样化训练数据的价值，所有数据和基准代码已公开发布用于非商业研究。

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [109] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: GenAR是一个多尺度自回归框架，通过从粗到细的方式预测空间转录组学数据，将基因聚类为层次组来建模基因间依赖关系，直接预测原始计数作为离散标记生成，并在融合的组织学和空间嵌入条件下进行解码。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学(ST)成本高昂，而H&E染色图像广泛可用。现有计算方法独立预测每个基因，忽略了共表达结构，且将任务视为连续回归，而表达实际上是离散计数，这导致生物学上不可信的输出并复杂化下游分析。

Method: GenAR框架：1) 将基因聚类为层次组以揭示跨基因依赖关系；2) 将表达建模为无码本的离散标记生成，直接预测原始计数；3) 在融合的组织学和空间嵌入条件下进行解码；4) 采用从粗到细的自回归预测方式。

Result: 在四种不同组织类型的空间转录组学数据集上的广泛实验结果表明，GenAR实现了最先进的性能。

Conclusion: GenAR通过离散建模和从粗到细的因子分解，避免了log诱导的偏差，为精准医学和成本效益高的分子分析提供了潜在应用。代码已公开。

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [110] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: 提出RAP方法，使用轻量级3D栅格化替代昂贵的渲染技术，通过特征空间对齐实现模拟到现实的迁移，显著提升端到端驾驶规划器的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 模仿学习训练的端到端驾驶策略缺乏恢复数据，小错误会累积导致失败。现有方法使用神经渲染或游戏引擎生成数字孪生，但成本高、速度慢，主要用于评估而非训练。

Method: 提出3D栅格化技术，栅格化标注的几何基元而非渲染；引入栅格到现实的特征空间对齐方法，弥合模拟到现实的差距；构建RAP数据增强流水线。

Result: 在NAVSIM v1/v2、Waymo开放数据集视觉端到端驾驶、Bench2Drive四个主要基准测试中排名第一，实现了最先进的闭环鲁棒性和长尾泛化能力。

Conclusion: 轻量级栅格化配合特征对齐足以扩展端到端训练，为逼真渲染提供了实用替代方案，证明了语义保真度比照片级真实感对驾驶规划更重要。

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [111] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: 提出Diffusion^2框架用于瞬时轨迹预测，通过两个连接的扩散模型分别生成未观测的历史轨迹和预测未来轨迹，解决了盲区行人突然出现等极端场景下的预测挑战。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和人机交互中，当行人从盲区突然出现时，往往缺乏足够的观测数据（瞬时轨迹），这使得准确预测变得困难，增加了交通事故风险。因此需要研究极端场景下的行人轨迹预测。

Method: Diffusion^2包含两个顺序连接的扩散模型：一个用于向后预测生成未观测的历史轨迹，另一个用于向前预测未来轨迹。提出了双头参数化机制估计不确定性，并设计了时间自适应噪声模块动态调节前向扩散过程中的噪声尺度。

Result: 在ETH/UCY和Stanford Drone数据集上，Diffusion^2在瞬时轨迹预测方面达到了新的最先进水平。

Conclusion: 该工作为极端场景下的行人轨迹预测提供了有效解决方案，通过扩散模型和不确定性建模显著提升了预测性能，有助于提高交通安全。

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [112] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim是一个语言引导的4D场景生成框架，能够创建多视角一致且支持对象级控制的动态环境，通过轨迹引导生成和特征场蒸馏技术实现交互式编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频模型局限于2D视图且交互性有限，需要能够支持可控和可编辑时空环境的世界模型，用于机器人技术中的可扩展训练数据、可重复评估和灵活任务设计。

Method: 集成轨迹引导生成与特征场蒸馏，从自然语言指令生成具有多视角一致性和对象级控制的4D场景，支持对象导向、重新着色或移除等交互式编辑操作。

Result: 实验表明MorphoSim在保持高场景保真度的同时实现了可控性和可编辑性。

Conclusion: MorphoSim框架成功解决了现有方法的局限性，为机器人应用提供了更灵活和可控的动态环境生成能力。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [113] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: 该论文提出了VLMCountBench基准测试，专门评估视觉语言模型在简单几何形状计数任务中的表现，发现当前模型在组合计数方面存在显著失败。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在各种任务上表现出色，但作者质疑这些模型是否能正确计数物体，特别是在组合场景中。

Method: 设计了极简的基准测试VLMCountBench，仅使用基本几何形状及其组合，严格控制变量，系统研究颜色、大小和提示词优化等因素的影响。

Result: 实验结果显示，当只有单一形状类型时，VLMs能够可靠计数，但在多种形状类型组合时表现出显著的失败。

Conclusion: 这揭示了当前视觉语言模型的基本经验局限性，为未来研究指明了重要方向。

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [114] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: CodeFormer++是一个新颖的盲人脸恢复框架，通过分解任务为身份保护恢复、高质量生成和动态融合，解决了现有方法在视觉质量和身份保真度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有盲人脸恢复方法在集成生成先验时，往往面临视觉质量与身份保真度之间的权衡，导致身份失真或降质去除不理想。

Method: 将BFR分解为三个子任务：身份保护恢复、高质量生成和动态融合；提出基于学习的可变形人脸配准模块、纹理引导恢复网络，并集成深度度量学习。

Result: 在真实世界和合成数据集上的广泛实验表明，CodeFormer++在视觉保真度和身份一致性方面均取得优越性能。

Conclusion: CodeFormer++框架成功最大化生成先验的效用，实现了高质量人脸恢复同时保持身份信息，解决了现有方法的局限性。

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [115] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: 提出A.I.R.方法，通过自适应、迭代和基于推理的帧选择策略，在视频问答任务中平衡计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有帧选择方法面临关键权衡：轻量级相似性模型无法捕捉复杂查询的细微差别，而使用VLM进行深度分析的方法计算成本过高。

Method: 利用强大的VLM对复杂查询进行深度语义分析，在成本效益高的迭代循环中每次只处理一小批最有潜力的帧。

Result: 在多个VideoQA基准测试中，该方法优于现有帧选择方法，显著提升了基础VLM的性能，并大幅提高了计算效率。

Conclusion: A.I.R.方法有效解决了帧选择中的准确性与计算效率之间的权衡问题，为视频问答任务提供了实用的解决方案。

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [116] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: 提出reAR训练策略，通过token-wise正则化解决视觉自回归生成中的生成器-分词器不一致问题，显著提升性能，仅用177M参数即可达到与675M扩散模型相当的效果。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成性能不如扩散模型，主要原因是生成器与分词器之间的不一致性——自回归生成的token可能无法被分词器良好解码。

Method: 提出reAR训练策略，在预测下一个token时，因果变换器同时学习恢复当前token的视觉嵌入，并在噪声上下文中预测目标token的嵌入，无需改变分词器、生成顺序或推理流程。

Result: 在ImageNet上，gFID从3.02降至1.86，IS提升至316.9；应用于先进分词器后，仅用177M参数就达到gFID 1.42，与675M扩散模型性能相当。

Conclusion: reAR通过简单的正则化策略有效解决了视觉自回归生成中的核心瓶颈，显著提升了性能，为统一视觉和语言模型提供了有前景的路径。

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [117] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: SPEGNet是一个用于伪装目标检测的统一架构，通过通道校准和空间增强整合多尺度特征，在保持语义-空间对齐的同时实现边界精确性和区域一致性的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前伪装目标检测方法依赖复杂的组件积累（如边界模块、注意力机制、多尺度处理器），这造成了计算负担却没有相应收益，且通常需要在降低分辨率下处理，丢失了伪装检测所需的精细细节。

Method: 提出SPEGNet统一架构，通过通道校准和空间增强整合多尺度特征，边界直接从上下文丰富的表示中产生，采用渐进式精炼实现尺度自适应边缘调制，在中间分辨率处达到峰值影响。

Result: 在CAMO数据集上达到0.887 $S_\alpha$，COD10K上0.890，NC4K上0.895，同时具有实时推理速度，能够处理从微小复杂物体到大型模式相似物体的各种尺度。

Conclusion: SPEGNet通过统一设计有效解决了现有方法的碎片化问题，在边界精度和区域一致性之间取得了良好平衡，能够处理遮挡和模糊边界，并在多个数据集上取得了优异性能。

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [118] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM是一个将检测数据集转换为大规模医学视觉问答数据的自动化流程，通过链接病变框与器官分割和结构化推理，使医学视觉语言模型能够生成具有逐步推理的问题-答案对。


<details>
  <summary>Details</summary>
Motivation: 将临床诊断推理与AI结合在医学影像中仍然是一个核心挑战，需要开发能够生成逐步推理的医学视觉语言模型。

Method: 提出MedCLM自动化管道，将检测数据集转换为带有思维链推理的大规模医学VQA数据；采用集成CoT-课程策略，包含三个难度阶段：简单阶段（显式病变框）、中等阶段（隐式定位）和困难阶段（弱监督推理）。

Result: MedCLM在多个医学VQA基准测试中达到了最先进的性能，为开发临床对齐的医学视觉语言模型提供了可扩展框架。

Conclusion: MedCLM通过将检测数据转换为带有思维链推理的VQA数据，成功开发了能够进行临床推理的医学视觉语言模型，在多个基准测试中表现优异。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [119] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: 提出了VaseVQA-3D数据集和VaseVLM模型，解决视觉语言模型在文化遗产3D花瓶分析中的领域适应问题，在R@1指标上提升12.8%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在文化遗产等专业领域面临数据稀缺和领域知识不足的问题，特别是在3D花瓶文物分析任务上表现不佳。

Method: 构建首个古希腊陶器3D视觉问答数据集VaseVQA-3D（包含664个3D模型），并开发领域自适应训练的VaseVLM模型。

Result: 在VaseVQA-3D数据集上，R@1指标提升12.8%，词汇相似度提升6.6%，显著优于现有最优方法。

Conclusion: 该方法有效提升了3D花瓶文物的识别和理解能力，为数字文化遗产保护研究提供了新技术路径。

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [120] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: TBStar-Edit是一个专门为电商领域设计的图像编辑模型，通过数据工程、模型架构设计和两阶段训练策略，在保持产品外观和布局完整性的同时实现精确高保真的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的通用图像生成和编辑模型在电商场景中存在一致性限制，无法很好地保持产品外观和布局的完整性。

Method: 1) 建立全面的数据构建流程；2) 设计分层模型框架（基础模型、模式转换模块、一致性增强模块）；3) 采用两阶段训练策略（模式转换阶段和一致性增强阶段）。

Result: 在自建的电商基准测试中，TBStar-Edit在客观指标（VIE Score）和主观用户偏好方面均优于现有的通用领域编辑模型。

Conclusion: TBStar-Edit成功解决了电商领域图像编辑的一致性问题，为电商场景提供了专门的图像编辑解决方案。

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [121] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: 提出异步扩散模型，通过为不同像素分配不同时间步长，让提示相关区域比无关区域更渐进地去噪，从而利用更清晰的像素间上下文，显著改善文本到图像的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型采用同步去噪，所有像素同时从噪声演化到清晰图像，导致提示相关区域只能参考相同噪声水平的无关区域，无法获得清晰上下文，最终影响文本到图像的对齐质量。

Method: 提出异步扩散模型框架，为不同像素分配不同的时间步长，重新制定逐像素去噪过程。通过动态调节单个像素的时间步长调度，让提示相关区域比无关区域更渐进地去噪。

Result: 广泛实验表明，异步扩散模型能够显著改善各种提示下的文本到图像对齐效果。

Conclusion: 异步扩散模型通过异步去噪机制，让提示相关区域能够利用更清晰的像素间上下文，从而在最终图像中实现更好的对齐效果。

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [122] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出TAG方法，通过放大估计分数的切向分量来修正采样轨迹，提高扩散模型生成质量，无需修改底层模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型存在语义不一致或幻觉问题，现有推理时引导方法依赖外部信号或架构修改，计算开销大。

Method: 利用中间样本作为投影基，放大估计分数相对于该基的切向分量来修正采样轨迹，基于一阶泰勒展开形式化引导过程。

Result: TAG作为即插即用模块，以最小计算成本提高扩散采样保真度，提供新的扩散引导视角。

Conclusion: TAG是一种更高效直接的引导方法，仅基于轨迹信号操作，能有效减少不一致性并提升样本质量。

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [123] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: 提出条件表示学习(CRL)方法，通过用户指定的条件生成定制化特征空间，解决通用表示学习与下游任务需求不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统表示学习方法学习的是通用表示，主要捕捉主导语义，可能与定制化下游任务需求不一致。例如在动物栖息地分析中，研究者关注场景相关特征，而通用嵌入强调类别语义，导致次优结果。现有监督微调方法计算和标注成本高。

Method: CRL通过大语言模型(LLM)根据用户指定条件生成描述性文本来构建语义基，然后利用视觉语言模型(VLM)将图像表示投影到这个条件特征空间中。

Result: 在分类和检索任务上的大量实验证明了CRL的优越性和通用性。

Conclusion: 条件表示学习能够为特定标准更好地捕捉语义，可用于多个定制化任务，避免了监督微调的高成本。

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [124] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: 提出了AI Session Recorder框架，通过记录病理学家在WSI查看器中的日常导航行为，构建Pathology-CoT数据集，并开发了Pathologist-o3智能代理系统，在胃肠道淋巴结转移检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有病理学基础模型虽然强大，但缺乏能够决定查看区域、调整放大倍数并提供可解释诊断的实用智能代理系统，主要障碍是缺乏可扩展的临床对齐监督数据。

Method: 开发AI Session Recorder记录病理学家在WSI查看器中的导航行为，通过轻量级人工审核将行为日志转化为标准化的行为指令和边界框，构建Pathology-CoT数据集，并基于此开发两阶段的Pathologist-o3代理系统。

Result: 在胃肠道淋巴结转移检测中达到84.5%精确率、100.0%召回率和75.4%准确率，超过最先进的OpenAI o3模型，并在不同骨干网络上具有良好泛化能力。

Conclusion: 该框架将日常查看器日志转化为可扩展的专家验证监督，使病理学智能代理系统实用化，并为人类对齐、可升级的临床AI建立了路径。

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [125] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出空间-光谱-频率交互网络(S²Fin)，通过高频稀疏增强变换器和两级空间-频率融合策略，解决多模态遥感图像分类中结构特征和细节特征提取困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态遥感图像分类方法难以从异构冗余的多模态图像中有效提取结构和细节特征，需要引入频域学习来建模关键稀疏细节特征。

Method: 使用高频稀疏增强变换器优化高频滤波器参数，采用两级空间-频率融合策略（自适应频率通道模块和高频共振掩码），并加入空间-光谱注意力融合模块增强中间层特征提取。

Result: 在四个基准多模态数据集上，S²Fin在有限标注数据下实现了优越的分类性能，超越了现有最先进方法。

Conclusion: S²Fin通过空间-光谱-频率域的多层次交互融合，有效提升了多模态遥感图像的分类精度，证明了频域学习在特征提取中的重要性。

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [126] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: 本文研究了不同图像分割方法对森林砍伐检测分类器训练的影响，发现通过分类器融合方法可以显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 在ForestEyes项目中，传统使用SLIC算法进行图像分割，但研究表明其他超像素方法在遥感图像分割中表现更好，需要评估这些方法对森林砍伐检测任务的影响。

Method: 比较了四种最佳分割方法与SLIC算法，使用PyCaret AutoML库选择前五名分类器，并应用分类器融合（集成学习）方法。

Result: 初始结果显示不同分割方法间性能差异不大，但通过分类器融合方法后，平衡准确率有明显提升。

Conclusion: 分割方法的选择和机器学习模型的组合对于森林砍伐检测任务都很重要，分类器融合方法能显著改善检测性能。

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [127] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduPersona是一个面向教育领域的大规模基准测试，专注于评估语言模型在课堂环境中的主观能力，包含多语言、多学科和多种人格类型的数据集和评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育中的集成日益增多，虚拟学生代理在课堂模拟和教师培训中变得至关重要，但其课堂导向的主观能力尚未得到充分评估，限制了模型边界理解和可信部署。

Method: 构建包含1,308个真实课堂对话轮次的大规模数据集，基于大五人格理论扩展至约128k轮次；将主观性能分解为三个渐进任务：基本连贯性、学生真实性和长期人格一致性；对三个代表性LLM进行系统实验，比较原始版本与在EduPersona上微调的变体。

Result: 在所有任务上均观察到一致且显著的改进：TASK1 +33.6%、TASK2 +30.6%、TASK3 +14.9%，证明了数据集的有效性和研究价值，同时揭示了人格建模的异质难度。

Conclusion: EduPersona提供了首个专注于主观能力的课堂基准，建立了可解耦和可验证的研究范式，将开源数据集和框架以支持教育领域可信和类人AI的发展。

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [128] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: 提出了一种分层多阶段运动专家混合架构，用于从步态序列中预测心理特征，在17个心理特征上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 步态编码了丰富的生物特征和行为信息，但利用行走方式来推断心理特征仍然是一个具有挑战性且未被充分探索的问题。

Method: 采用分层多阶段运动专家混合架构，通过四个运动复杂度阶段处理行走周期，使用轻量级专家模型提取时空特征，并通过任务特定门控模块自适应加权专家。

Result: 在PsyMo基准测试中，该方法在运行级别达到37.47%加权F1分数，在主题级别达到44.6%，优于现有步态分析模型。

Conclusion: 研究表明多任务步态学习在心理特征估计方面是可行的，为未来基于运动信息的心理推断研究奠定了基础。

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [129] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: ConceptSplit是一个解决多概念个性化中概念混合问题的新框架，包含ToVA训练方法和LODA推理优化，能有效分离多个概念避免干扰。


<details>
  <summary>Details</summary>
Motivation: 多概念个性化中多个学习的概念在输出图像中会相互干扰或混合，这是主要挑战。

Method: 提出ConceptSplit框架：1) Token-wise Value Adaptation (ToVA) - 仅调整交叉注意力中的值投影；2) Latent Optimization for Disentangled Attention (LODA) - 在推理时优化输入潜变量来解耦注意力。

Result: 通过广泛的定性和定量实验证明，ConceptSplit实现了鲁棒的多概念个性化，减轻了意外的概念干扰。

Conclusion: ConceptSplit通过训练和推理阶段的创新方法有效解决了多概念个性化中的概念混合问题。

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [130] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出一种标签高效的肝脏分割方法，通过基础模型微调、交叉伪监督协同训练和无空间配准预处理，在多模态多厂商MRI中实现跨模态泛化。


<details>
  <summary>Details</summary>
Motivation: 多期相MRI中肝脏分割对肝纤维化评估至关重要，但标注数据稀缺且在不同成像模态和厂商系统中分布不均，存在空间错位和缺失期相等现实问题。

Method: 整合基础级3D分割主干网络微调、交叉伪监督协同训练利用未标注数据、标准化预处理流程，无需空间配准。

Result: 模型在标注和未标注域均表现出鲁棒的分割性能，能够跨MRI期相和厂商系统泛化。

Conclusion: 该方法展示了结合基础模型适应与协同训练在现实临床成像任务中的潜力，为多期相多厂商MRI肝脏分割提供了有效的标签高效基线。

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [131] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的人脸表情生成框架，能够在保持身份一致性的同时实现精细的表情控制，支持从基础情感到微妙微表情的生成。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的人物生成模型在保持身份一致性和实现精细表情控制方面存在挑战，特别是在不损害身份一致性的前提下实现细粒度表情控制。

Method: 基于身份一致的人脸基础模型，采用组合式设计，包含由FLAME blendshape参数引导的表情交叉注意力模块，并在丰富的图像和视频数据上进行训练。

Result: 模型在定制化和身份一致的表情生成方面优于现有方法，能够生成从基础情感到微妙微表情的各种表情。

Conclusion: 该框架成功解决了身份一致性和精细表情控制之间的平衡问题，为AI驱动的故事讲述提供了更高质量的人物生成能力。

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [132] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: 提出了一种用于3D语义场景图预测的新方法，通过设计高判别性的物体特征编码器和对比预训练策略，显著提升了物体分类和关系预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖图神经网络但缺乏足够的判别能力，未能充分优化物体和关系特征的表示能力。研究发现物体特征质量对整体场景图准确性至关重要。

Method: 设计高判别性物体特征编码器，采用对比预训练策略将物体表示学习与场景图预测解耦，并有效结合几何和语义特征进行关系预测。

Result: 在3DSSG数据集上的综合实验表明，该方法显著优于现有最先进方法，将预训练编码器插入现有框架后所有评估指标均有大幅提升。

Conclusion: 通过优化物体特征表示并有效整合关系信息，该方法在3D语义场景图预测任务中取得了突破性进展，为机器人和AR/VR应用提供了更可靠的技术支持。

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [133] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: 首个野生动物监测单目深度估计基准，评估4种先进方法在93张相机陷阱图像上的性能，发现Depth Anything V2表现最佳（MAE 0.454m），中值深度提取优于均值方法。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱广泛用于野生动物监测，但单目图像缺乏深度信息，现有单目深度估计方法在自然野生动物环境中的性能尚未系统评估。

Method: 建立首个野生动物监测单目度量深度估计基准，评估Depth Anything V2、ML Depth Pro、ZoeDepth和Metric3D四种先进方法，使用93张带有校准ChARUCO图案地面真实距离的相机陷阱图像。

Result: Depth Anything V2表现最佳，平均绝对误差0.454m，相关性0.962；ZoeDepth在户外自然环境中性能显著下降（MAE 3.087m）；中值深度提取在所有深度学习方法中均优于均值方法；ZoeDepth最快但精度最低，Depth Anything V2在精度和速度间达到最佳平衡。

Conclusion: 该基准为野生动物应用建立了性能基线，为在保护监测系统中实施深度估计提供了实用指导。

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [134] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: 提出AA-YOLO方法，将统计异常检测集成到YOLO检测头中，有效控制红外小目标检测中的误报率，并在多种YOLO骨干网络中展现良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在国防应用中面临复杂背景和小目标尺寸的挑战，传统目标检测器会产生大量误报。

Method: 在YOLO检测头中集成统计异常检测测试，将小目标视为背景中的异常模式。

Result: 在多个IRSTD基准测试中取得竞争性性能，在训练数据有限、噪声和域偏移场景下表现出显著鲁棒性。

Conclusion: AA-YOLO设计通用性强，可应用于各种YOLO骨干网络，包括轻量级模型，是资源受限实际部署的有吸引力的解决方案。

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [135] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: 本文研究了基于Transformer的架构在自然面对面对话场景中的人物识别性能，通过双流框架分别建模空间配置和时间运动模式，在CANDOR对话语料库上取得了98.03%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer架构在自然对话场景中的人物识别潜力，特别是在面对面交流环境下如何有效利用姿态和动态信息进行身份识别。

Method: 采用双流框架：空间Transformer处理133个COCO WholeBody关键点的空间配置，多尺度时间Transformer建模层次化运动模式；比较了预训练与从头训练、速度特征的使用等策略。

Result: 领域特定训练显著优于迁移学习；空间配置比时间动态更具判别性（空间Transformer 95.74%，多尺度时间Transformer 93.90%）；特征级融合达到98.03%准确率。

Conclusion: Transformer架构在自然交互中的人物识别具有巨大潜力，姿态和动态信息具有互补性，为未来多模态和跨文化研究提供了见解。

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [136] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: PG-Occ是一个渐进式高斯变换器框架，用于开放词汇的3D占用预测，通过渐进在线稠密化和各向异性感知采样策略，在保持计算效率的同时提升对小物体的检测能力。


<details>
  <summary>Details</summary>
Motivation: 解决文本对齐场景建模中的权衡问题：稀疏高斯表示难以捕捉小物体，而稠密表示计算开销大。

Method: 采用渐进在线稠密化策略逐步增强3D高斯表示，结合各向异性感知采样和时空融合技术自适应分配感受野。

Result: 在3D占用预测任务中实现了最先进性能，相比之前最佳方法相对提升了14.3%的mIoU。

Conclusion: PG-Occ框架有效平衡了表示能力和计算效率，为开放词汇3D场景理解提供了有力解决方案。

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [137] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: 提出了一种新的开放词汇学习方法，通过生成未见类数据来估计开放环境中的分布，从而提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用已见类数据估计开放环境分布，但未见类的缺失使得估计误差无法识别。学习超越已见类对于边界分布估计误差至关重要。

Method: 包含类域级数据生成流程和分布对齐算法。数据生成流程在分层语义树和从已见类数据推断的域信息指导下生成未见类数据；分布对齐算法估计并最大化后验概率。

Result: 在11个数据集上的广泛实验表明，该方法比基线方法性能提升高达14%。

Conclusion: 通过生成未见类数据可以有效估计开放环境中的分布，理论证明估计误差有上界，实验验证了方法的有效性和优越性。

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [138] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg挑战赛评估了联邦学习在外科视频分类中的表现，重点关注模型在未见临床中心的泛化能力和本地微调适应性。结果显示ViViT模型表现最佳，但泛化能力有限且排名稳定性低。


<details>
  <summary>Details</summary>
Motivation: 建立外科视频分类中联邦学习的基准，评估当前方法在未见临床中心的泛化能力和本地微调适应性，同时实现不共享患者数据的协作模型开发。

Method: 参与者使用Appendix300视频数据集开发策略分类阑尾炎炎症阶段。评估两个任务：未见中心的泛化和中心特定适应性微调。方法包括基础模型线性探测、度量学习、联邦学习聚合方案等，使用F1分数和期望成本评估性能。

Result: 泛化任务中跨中心性能有限；适应任务中所有团队微调后都有改善但排名稳定性低。ViViT模型表现最强，挑战凸显了泛化限制、类别不平衡敏感性和去中心化训练中超参数调优困难。

Conclusion: FedSurg挑战赛建立了外科视频分类中联邦学习策略评估的首个基准，揭示了本地个性化与全局鲁棒性之间的权衡，强调了架构选择、预处理和损失设计的重要性，为未来开发不平衡感知、自适应和鲁棒的临床外科AI联邦学习方法提供了参考。

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [139] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: 提出了一种自动化双机器人3D扫描系统，通过协调机器人操作和高分辨率扫描，无需手持或半自动工作流程，显著提高了文化遗产数字化的几何精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统3D扫描方法需要专业知识和手动干预来维持最佳扫描条件和覆盖范围，这限制了文化遗产数字化过程的效率和可访问性。

Method: 将扫描空间参数化为不同区域，使用配备扫描仪的机器人和托盘处理机器人进行协调运动规划，通过优化的轨迹规划和路径点分布确保全面表面覆盖。

Result: 实验结果显示，与基线方法相比，该方法实现了显著更低的Chamfer Distance和更高的F-score，提供卓越的几何精度和数字化效率。

Conclusion: 该系统通过自动化双机器人协调扫描，显著减少了对外部操作人员的依赖，为文化遗产保护提供了高效、准确的数字化解决方案。

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [140] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: 该研究系统比较了ViT和大规模CNN在几何估计任务中的表现，发现在大数据场景下ViT表现更好，但在小数据场景下CNN的归纳偏置使其能与ViT匹敌，ViT在跨域评估中表现出更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索ViT和CNN在几何估计任务（如图像刚性变换和基础矩阵估计）中的效率，特别是在低数据环境下的表现，这些任务需要平衡局部和全局特征。

Method: 系统比较了大规模CNN（ResNet、EfficientNet、CLIP-ResNet）与ViT基础模型（CLIP-ViT变体和DINO）在不同数据规模设置下的表现，包括少样本场景。

Result: 在大数据场景下ViT优于CNN，但在小数据场景下CNN的归纳偏置使其性能与ViT相当，ViT在跨域评估中表现出更强的泛化能力。

Conclusion: 需要根据数据规模仔细选择模型架构进行微调，未来研究应关注平衡局部和全局表示的混合架构。

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [141] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 提出DiT-VTON框架，利用扩散变换器进行虚拟试穿，通过多种配置探索最佳图像条件设置，在扩展数据集上训练增强鲁棒性，支持多种产品类别和图像编辑功能。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿模型在细节保留、鲁棒性、采样效率、图像编辑能力和跨类别泛化方面存在挑战，需要更强大的解决方案。

Method: 采用扩散变换器架构，探索上下文标记拼接、通道拼接和ControlNet集成等多种配置，在扩展数据集上进行训练。

Result: 在VITON-HD数据集上超越现有方法，实现更好的细节保留和鲁棒性，无需额外条件编码器；在多样化数据集上优于具有VTA和图像编辑能力的模型。

Conclusion: DiT-VTON不仅重新定义了虚拟试穿任务，还提供了通用的虚拟试穿解决方案，支持多种产品类别和高级图像编辑功能。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [142] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: EgoSurg是一个从固定摄像头视频重建手术室人员第一人称视角回放的框架，无需干扰临床工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统手术观察方法依赖固定视角或回忆，无法记录指导临床决策的第一人称视觉视角，限制了手术安全、培训和流程优化的洞察。

Method: 结合几何驱动的神经渲染和基于扩散的视图增强技术，从壁挂固定摄像头视频合成任意时刻的任意第一人称视角。

Result: 在多地点手术案例和对照研究中，EgoSurg能够以高视觉质量和保真度重建人员特定的视觉场和任意视点。

Conclusion: EgoSurg将现有手术室摄像头基础设施转变为可导航的动态3D记录，为沉浸式手术数据科学奠定新基础，使手术实践可以从每个角度可视化、体验和分析。

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [143] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: 该论文研究了多模态语言模型在感知任务上的表现不佳问题，发现视觉值令牌包含足够信息执行多种感知任务，但语言模型处理过程中存在信息损失和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 理解为什么多模态语言模型在感知密集型任务上表现不佳，特别是研究视觉键值令牌在模型中的处理机制。

Method: 分析流行多模态语言模型（LLaVA-OneVision、Qwen2.5-VL、Llama-3-LLaVA-NeXT）中视觉键值令牌的信息流动，通过零样本任务测试视觉信息的编码能力。

Result: 发现图像值令牌编码了足够的感知信息，但语言模型处理会减少视觉信息；视觉键令牌在后期层产生伪影；添加文本前缀能改善感知能力；33.3%的BLINK基准问题中，语言模型内的感知信息未输出。

Conclusion: 揭示了键值令牌在多模态系统中的关键作用，为MLMs的机制解释性提供了新见解，并提出了改进视觉编码器和语言模型组件训练的新方向。

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [144] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON是首个4D虚拟试穿框架，能从单张服装图片生成逼真的试穿效果，支持自由姿态控制、新视角渲染和多样化服装选择。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要多视角服装捕捉或物理先验的限制，实现单视角监督下的动态服装交互。

Method: 包含两个关键模块：无先验光流校正策略（Reciprocal Flow Rectifier）确保时间一致性；非线性变形器（Non-Linear Deformer）将高斯图分解为视角姿态不变和特定分量，实现自适应服装变形。

Result: 建立了4D虚拟试穿基准，扩展现有基线进行公平比较。实验表明AvatarVTON在保真度、多样性和动态服装真实性方面表现优异。

Conclusion: 该框架适用于AR/VR、游戏和数字人应用，实现了高质量的4D虚拟试穿效果。

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [145] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D流匹配(Flow Matching)框架的合成CT生成方法，可以从MRI或CBCT生成sCT，在SynthRAD2025挑战赛基准上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 从MRI或CBCT生成合成CT对于实现仅MRI和基于CBCT的自适应放射治疗至关重要，可以提高治疗精度同时减少患者辐射暴露。

Method: 采用完全3D流匹配框架，将高斯噪声体积通过学习到的流匹配速度场转换为sCT图像，该速度场由轻量级3D编码器从输入MRI或CBCT中提取的特征进行条件化。

Result: 在SynthRAD2025挑战赛基准上评估，方法能够准确重建全局解剖结构，但由于内存和运行时间限制导致的训练分辨率较低，细部细节保留有限。

Conclusion: 该方法在全局结构重建方面表现良好，但需要进一步探索基于补丁的训练和潜在空间流模型来提高分辨率和局部结构保真度。

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [146] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: 提出AT-BPTT框架，通过动态调整截断位置和窗口大小来改进数据集蒸馏中的内循环优化，显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法使用随机截断策略缺乏灵活性且效果欠佳，神经网络在不同训练阶段具有不同的学习动态。

Method: AT-BPTT框架包含三个关键组件：阶段感知时间步选择概率机制、基于梯度变化的自适应窗口大小策略、以及降低计算开销的低秩Hessian近似。

Result: 在多个数据集上实现最先进性能，平均准确率提升6.16%，内循环优化加速3.9倍，节省63%内存成本。

Conclusion: AT-BPTT通过动态适应梯度行为有效解决了随机截断的局限性，在数据集蒸馏中实现了性能与效率的双重提升。

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [147] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: 提出了一种基于航拍图像的光伏电站映射新方法，能够自动完成详细建模，精确定位到单个光伏模块级别。


<details>
  <summary>Details</summary>
Motivation: 光伏电站的准确模型对其优化运维至关重要，但现有模型不易获取，且依赖第三方数据。

Method: 利用航拍图像进行视觉分割，识别光伏模块和结构布局关键点，通过多图像检测融合保持结构完整性，推断模块在长凳、行、列中的位置。

Result: 在两个不同电站上进行了实验验证和评估，成功生成包含3D位置和语义结构的紧凑地理参考模型。

Conclusion: 该方法能够自动化光伏电站映射过程，摆脱对第三方数据的依赖，为电站维护提供合适的模型。

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [148] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: 提出了一个基于3D骨架数据的动觉识别框架，通过ST-GCN和CNN结合迁移学习，从人体动作推断心理状态，实现隐私保护的人类行为建模。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖理论模型或问卷调查，存在范围有限、静态和劳动密集的问题，需要一种既能保护隐私又能捕捉人类心理状态的通用方法。

Method: 结合空间-时间图卷积网络和卷积神经网络，利用迁移学习避免手动定义物理动作与心理类别之间的映射，直接从3D骨架关节数据推断动觉功能。

Result: 在DUET数据集上的结果表明，该方法能够实现可扩展、准确且以人为中心的行为建模。

Conclusion: 该方法为增强强化学习驱动的人类-环境交互模拟提供了新途径，同时保护用户匿名性并揭示反映认知和情感状态的潜在身体运动结构。

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [149] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: 该论文比较了五种基于骨架的交互识别算法，用于识别12种二元人类交互，旨在解决CPS系统中忽视社会效益的问题，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 传统网络物理系统(CPS)主要关注经济目标如性能和安全，但忽视了人类中心的社会效益。网络物理社会基础设施系统(CPSIS)旨在通过将CPS与社会目标对齐来解决这一问题，需要理解人类互动及其社会意义。

Method: 使用深度传感器分析骨骼运动来识别二元人类交互，避免了RGB摄像头的隐私问题。比较了五种基于骨架的交互识别算法，在包含12种二元交互的数据集上进行测试，这些交互按沟通类型分类，如象征性动作和情感表达。

Result: 研究结果表明，基于骨架的方法能够有效识别不同类型的二元人类交互，为理解人类互动的文化情感层面提供了基础。

Conclusion: 基于骨架的交互识别方法为网络物理社会基础设施系统提供了隐私保护的人类互动测量方案，为促进积极社会成果奠定了基础。

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [150] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: 该论文提出了一种结合早期退出和知识蒸馏的神经网络压缩方法，通过引入新的基于熵的损失函数来优化学生模型训练，在保持分类性能的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在图像分类中表现出色但计算成本高，难以应用于实时和边缘场景。需要开发动态压缩技术来在运行时调节压缩级别。

Method: 将早期退出和知识蒸馏相结合，训练简化的学生早期退出模型。创新性地在教师分类错误的图像上使用基于熵的损失函数，而非传统的知识蒸馏损失。

Result: 在CIFAR10、CIFAR100和SVHN数据集上的实验验证了该方法的有效性，实现了计算复杂度的显著降低而不影响分类性能。

Conclusion: 该方法有效平衡了准确性和效率的权衡，为知识蒸馏在其他场景中的应用开辟了新的研究视角。

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [151] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: μDeepIQA是一种基于深度学习的图像质量评估方法，专门针对光学显微镜图像设计，能够提供快速、稳定的质量预测，并能可视化单张图像中空间变化的图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统图像质量评估方法在处理大规模数据集时计算成本高，且对超出理想域的图像不稳定。深度学习IQA方法能提供更优性能、更好的泛化能力和快速预测。

Method: 将自然图像IQA的深度卷积神经网络架构重新训练，用于预测光学显微镜数据的个体质量指标和全局质量分数，并提供基于图像块的质量预测。

Result: μDeepIQA能够提供快速稳定的图像质量预测，即使在标准方法理想范围之外也能泛化质量估计，并能可视化单张图像中空间变化的质量。

Conclusion: 深度学习模型因其在异常值存在时的稳定性能、评估小图像块的能力和快速预测，使光学显微镜研究受益于其泛化能力。

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [152] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 开发了一个端到端的物联网机器人系统，用于葡萄园中葡萄产量和质量的非破坏性实时空间映射。系统包含葡萄串检测与重量估计模块，以及基于光照不变光谱自编码器(LISA)的质量评估模块，能够克服野外光照变化带来的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 解决葡萄园精准农业中的关键挑战：如何在野外光照变化条件下实现葡萄产量和质量的实时、非破坏性空间映射。传统方法面临光照变化导致的领域偏移问题，限制了质量评估的准确性。

Method: 系统集成两个关键模块：高性能葡萄串检测和重量估计模型，以及基于LISA（光照不变光谱自编码器）的深度学习质量评估框架。LISA采用领域对抗学习从未校准数据中学习光照不变特征。

Result: 在包含实验室人工光照、早晨和下午自然光照三个不同光照域的数据集上验证，系统实现葡萄串检测召回率0.82，重量预测R²为0.76。LISA模块相比基线方法将质量预测泛化能力提升超过20%。

Conclusion: 该系统成功生成了高分辨率的地理参考数据，为精准葡萄栽培提供了可操作的数据驱动洞察，通过结合稳健的模块解决了野外光照变化带来的挑战。

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [153] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: 本文介绍了一个用于海底栖息地测绘的多模态数据集，包含约100万个侧扫声纳图块、测深地图和光学图像，其中36000个声纳图块带有手动标注的分割掩码，旨在为机器学习模型开发提供标准化基准。


<details>
  <summary>Details</summary>
Motivation: 海底栖息地测绘对于理解海洋生态系统至关重要，但缺乏大规模标注数据集限制了机器学习模型的发展。

Method: 收集加泰罗尼亚海岸的侧扫声纳数据、测深地图和AUV光学图像，手动标注部分声纳图块，开发数据融合和预处理工具。

Result: 创建了包含约100万声纳图块的多模态数据集，其中36000个带有标注，提供了开源预处理和标注工具。

Conclusion: 该资源为水下栖息地测绘建立了标准化基准，将促进自主海底分类和多传感器集成技术的发展。

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [154] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: 在卢旺达基加利比较四种目标检测模型（YOLOv5、Faster R-CNN、SSD、RetinaNet）用于摩托车检测，评估其在资源受限环境下的实时导航适用性。


<details>
  <summary>Details</summary>
Motivation: 基加利的摩托车出租车经常不遵守交通规则且行驶不可预测，给自动驾驶系统带来重大挑战，需要研究适合发展中国家的实时检测方案。

Method: 使用在基加利收集的198张图像自定义数据集，在PyTorch中实现四种目标检测模型并采用迁移学习，评估准确性、定位精度和推理速度。

Result: 识别了数据集限制和模型复杂性等实施挑战，建议为未来工作采用简化架构。

Conclusion: 推荐简化架构以增强发展中国家自动驾驶系统的可访问性，特别是在资源受限环境中。

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [155] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种语义感知层次共识（SAHC）方法，通过整合专门处理不同类别粒度的层次特定分类头，在深度网络架构中学习层次特征和关系。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感图像分类中日益重要，但预定义的标签层次结构经常被忽视，大多数方法只关注细粒度分类方案。

Method: 使用可训练的层次矩阵以自监督方式指导网络学习层次结构，并引入层次共识机制确保不同层次级别概率分布的一致性。

Result: 在三个具有不同层次复杂度的基准数据集上评估，使用不同骨干架构验证了方法的适应性，实验结果显示该方法在指导网络学习和层次共识的鲁棒性方面都有效。

Conclusion: SAHC方法能够有效利用层次分类任务的固有结构，在遥感图像分类任务中表现出良好的效果和适应性。

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [156] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出了首个针对医学图像分类的解剖学引导混合专家框架REN，通过区域特异性专家网络和放射组学引导的门控机制，在间质性肺病分类中显著优于传统深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统混合专家架构缺乏医学成像所需的解剖学约束，而肺部解剖结构和区域疾病异质性对病理模式有重要影响，需要专门针对医学图像设计的解剖学引导框架。

Method: 利用解剖学先验训练7个专门针对不同肺叶和双侧肺组合的专家网络；采用多模态门控机制动态整合放射组学生物标志物和深度学习特征（CNN、ViT、Mamba）来优化专家贡献权重。

Result: 在间质性肺病分类中，放射组学引导的集成方法达到平均AUC 0.8646±0.0467，比SwinUNETR基线提升12.5%；区域特异性专家显示下叶模型AUC达0.88-0.90，优于深度学习方法。

Conclusion: REN展示了强大的泛化能力和临床可解释性，为结构化医学成像应用提供了一个可扩展的解剖学引导方法。

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [157] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: 提出了NFPF无监督主动学习框架，通过特定特征学习机量化样本重要性，显著超越现有无监督方法，达到与监督方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统主动学习需要大量人工标注的问题，无监督主动学习虽然减少了标注负担，但现有方法性能不足，存在对噪声数据敏感、无法充分代表数据分布等问题。

Method: 使用自然特征渐进框架(NFPF)，核心是特定特征学习机(SFLM)来量化样本对模型性能的贡献，并定义重建差异度量进行初始样本选择。

Result: 在视觉数据集上显著优于所有现有无监督主动学习方法，性能与监督主动学习方法相当，具有更强的鲁棒性和更好的数据分布覆盖。

Conclusion: NFPF通过创新的样本重要性度量方法，为无监督主动学习提供了有效解决方案，在减少标注负担的同时保持了高性能。

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [158] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 提出CA3D-Diff框架，通过列感知交叉注意力和隐式3D结构重建解决乳腺X光片双视图转换中的结构不对齐问题


<details>
  <summary>Details</summary>
Motivation: 临床实践中乳腺X光片的一个视图可能缺失或损坏，影响诊断效果。视图间转换可以帮助恢复缺失视图，但由于X射线投影中的大变形和组织重叠，该任务极具挑战性

Method: 基于条件扩散模型，设计列感知交叉注意力机制利用解剖对应区域在列位置相似的几何特性，并引入隐式3D结构重建模块将2D潜在空间反投影到3D特征体积中

Result: CA3D-Diff在双向任务中表现优异，在视觉保真度和结构一致性方面优于现有方法，合成的视图有效改善了单视图恶性分类性能

Conclusion: 该方法在真实世界诊断中具有实用价值，能够有效恢复缺失的乳腺X光片视图并提升诊断准确性

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [159] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: SSDD是一种新的像素扩散解码器架构，通过蒸馏技术实现单步重建，在无对抗损失的情况下达到比KL-VAE更高的重建质量和更快采样速度。


<details>
  <summary>Details</summary>
Motivation: 当前基于KL-VAE的tokenizer需要对抗损失，而扩散解码器虽然更理论化但需要迭代采样导致解码时间较长。需要解决这些限制。

Method: 提出新的像素扩散解码器架构，利用transformer组件和无GAN训练，通过蒸馏技术将扩散解码器性能复制到高效的单步解码器中。

Result: SSDD将重建FID从0.87提升到0.50，吞吐量提高1.4倍，DiTs生成质量保持的同时采样速度加快3.8倍。

Conclusion: SSDD可作为KL-VAE的直接替代品，用于构建更高质量和更快的生成模型。

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [160] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: 提出一种视觉基础模型所有权验证方法，通过在模型内部表示中嵌入数字水印来保护知识产权，防止非法再分发。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型训练成本高昂，需要保护知识产权。现有方法难以区分受保护模型的再分发副本和独立模型，因此需要可靠的所有权验证工具。

Method: 通过微调视觉基础模型的一小部分表达层，结合小型编码器-解码器网络，在输入图像的内部表示中嵌入数字水印。水印在模型的功能副本中仍可检测。

Result: 理论和实验证明，该方法在非水印模型的误检概率和水印模型的漏检概率方面表现良好。

Conclusion: 所提出的方法为视觉基础模型提供了有效的所有权验证机制，能够可靠地区分受保护模型和独立模型。

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [161] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: 提出了一种新的潜在不确定性表示方法LUR和RLUR，用于深度神经网络的不确定性估计和分布外检测，在视频驾驶行为识别任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在安全关键任务中应用广泛，但现有最后一层概率深度学习方法在分布外检测方面表现不一，需要更有效的不确定性估计方法。

Method: 在预训练DNN基础上添加变换层生成多个潜在表示来估计不确定性，提出LUR和排斥训练的RLUR方法。

Result: LUR和RLUR在分类性能上与现有方法相当，在不确定性分布外检测方面表现优异，且训练更高效、调参更简单。

Conclusion: LUR方法在保持分类性能的同时，提供了更高效的不确定性估计和分布外检测能力，适合资源受限环境。

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [162] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: 使用手绘螺旋和波浪图像作为生物标志物，通过机器学习方法检测帕金森病，结合CNN、迁移学习和注意力机制，在数据集增强后达到93.3%的整体准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断至关重要，但传统诊断方法繁琐且昂贵，需要开发非侵入性、成本效益高的解决方案。

Method: 采用卷积神经网络、迁移学习和注意力机制，通过数据增强增加图像多样性，架构包含预训练CNN、自定义卷积层和集成投票三个阶段。

Result: 螺旋图像精度、召回率和F1分数为90%，波浪图像为96.67%，通过硬投票集成后整体准确率达到93.3%。

Conclusion: 机器学习在帕金森病早期诊断中具有巨大潜力，提供了一种非侵入性且成本效益高的解决方案。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [163] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: 该论文首次全面调查了视频大语言模型的后训练方法，包括监督微调、强化学习和测试时扩展三大支柱，为提升视频理解能力提供了统一框架。


<details>
  <summary>Details</summary>
Motivation: 视频理解是计算机视觉中最具挑战性的前沿领域，虽然视频大语言模型已展现出强大能力，但将其从基础感知系统转变为复杂推理引擎的关键后训练阶段在文献中仍然零散。

Method: 提出了结构化分类法，涵盖三大后训练方法：带思维链的监督微调、基于可验证目标的强化学习、通过增强推理计算的测试时扩展，并针对视频特有的时间定位、时空基础、长视频效率等挑战进行适配。

Result: 通过系统分析代表性方法，综合了关键设计原则、见解和评估协议，同时识别了奖励设计、可扩展性和成本性能优化等关键开放挑战。

Conclusion: 该调查为研究人员和从业者提供了推进视频大语言模型能力的统一框架，并整理了必要的基准、数据集和指标以促进后训练有效性的严格评估。

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [164] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: 利用3D基础模型的空间理解能力来解决宽基线分割匹配问题，在极端视角变化下实现分割区域的跨图像匹配


<details>
  <summary>Details</summary>
Motivation: 分割匹配比关键点匹配更能捕捉结构化区域，对遮挡、光照变化和视角变化具有更强的鲁棒性。宽基线分割匹配在极端视角变化下具有挑战性

Method: 提出一种架构，利用3D基础模型的归纳偏置来匹配具有高达180度视角变化的图像对中的分割区域

Result: 在ScanNet++和Replica数据集上，我们的方法在AUPRC指标上比最先进方法（包括SAM2视频传播器和局部特征匹配方法）高出30%

Conclusion: 该方法在相关下游任务（包括3D实例分割和图像目标导航）中展现出优势，证明了其有效性

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [165] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: 提出了一种针对对比度失真图像的无参考图像质量评估方法，通过选择最合适的对比度增强算法生成伪参考图像，将NR-IQA问题转化为FR-IQA问题来提高评估准确性。


<details>
  <summary>Details</summary>
Motivation: 对比度变化是影响图像质量的重要因素，但在图像质量评估中常被忽视。现有方法主要关注模糊和噪声等失真，而对比度失真的视觉影响和特性与其他失真类型不同。

Method: 使用对比度增强算法生成视觉上接近真实参考图像的伪参考图像，训练分类网络根据图像内容和失真选择最合适的对比度增强算法，最终以全参考方式评估对比度增强图像与退化图像之间的质量差异。

Result: 在三个包含对比度失真的数据库（CCID2014、TID2013和CSIQ）上进行性能评估，结果表明该方法具有有前景的性能。

Conclusion: 该方法成功地将无参考图像质量评估问题转化为全参考评估问题，通过伪参考图像生成提高了对比度失真图像质量评估的准确性。

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [166] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: 提出Neuroplastic Modular Classifier，一种结合ResNet-50、Vision Transformer和FAISS相似性检索的混合架构，通过神经可塑性模块设计实现动态扩展，在垃圾分类和工业表面缺陷检测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 需要高效准确的垃圾分类和工业表面缺陷分类方法，以支持可持续废物管理和高质量控制标准。传统静态模型在动态环境中适应性不足。

Method: 结合ResNet-50进行局部特征提取，Vision Transformer捕捉全局语义上下文，FAISS相似性检索提供记忆参考。关键创新是神经可塑性模块设计，包含可扩展的学习块，在训练性能停滞时动态增长。

Result: 在KolektorSDD2工业表面缺陷数据集等多个领域实验中，该架构在准确性和适应性方面均优于传统静态模型。

Conclusion: Neuroplastic Modular Classifier为现实世界图像分类提供了可扩展的高性能解决方案，在环境和工业领域都具有强适用性。

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [167] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 该论文提出了首个针对结构化视觉内容生成和编辑的全面解决方案，包括大规模数据集构建、统一模型训练和评估基准，解决了现有视觉生成模型在图表、图表和数学图形等结构化内容上的不足。


<details>
  <summary>Details</summary>
Motivation: 现代视觉生成模型在生成自然图像方面表现出色，但在处理需要组合规划、文本渲染和多模态推理的结构化视觉内容（如图表、图表和数学图形）时存在困难，需要确保事实准确性。

Method: 构建了130万高质量结构化图像对数据集，基于可执行绘图程序生成并带有思维链推理标注；训练了集成VLM和FLUX.1 Kontext的统一模型，采用三阶段训练课程进行渐进特征对齐、知识注入和推理增强生成，在推理时使用外部推理器增强性能。

Result: 评估了15个模型，发现即使是领先的闭源系统也远未达到满意水平；提出的模型在编辑任务上表现强劲，推理时推理在不同架构中带来一致性能提升。

Conclusion: 通过发布数据集、模型和基准测试，旨在推进结构化视觉内容的统一多模态基础，为这一重要但被忽视的领域提供全面解决方案。

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [168] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: 提出了一个跨角色交互的视频生成框架，通过跨角色嵌入和跨角色增强技术，让不同世界中的角色能够自然互动而不丢失各自风格特征


<details>
  <summary>Details</summary>
Motivation: 解决文本到视频生成中跨角色交互的挑战，即如何在保持角色身份和行为特征的同时，实现不同世界角色之间的自然互动，避免风格失真问题

Method: 使用跨角色嵌入(CCE)从多模态源中学习角色身份和行为逻辑，以及跨角色增强(CCA)通过合成共存和混合风格数据来丰富训练

Result: 在包含10个角色的卡通和真人系列基准测试中，在身份保持、交互质量和风格失真鲁棒性方面都显示出明显改进

Conclusion: 该框架能够实现先前不共存角色之间的自然互动，同时保持风格保真度，为生成式故事讲述开辟了新形式

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [169] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain是一个推理时视觉思维链框架，通过多模态模型生成关键帧来指导视频生成，提升复杂动态场景的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在合成复杂动态和连贯因果链方面存在困难，而多模态模型具有强大的视觉状态推理能力，需要将两者优势结合。

Method: 利用大型多模态模型生成稀疏关键帧作为快照，然后在这些关键时刻对预训练视频生成器进行稀疏推理时微调。

Result: 在复杂多步骤场景上的广泛实验表明，VChain显著提升了生成视频的质量。

Conclusion: VChain通过注入多模态模型的视觉推理信号，实现了高效、低开销的视频生成质量提升。

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [170] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 通过分析GPT2-Small第一层中注意力模式分散且对内容依赖较弱的注意力头，发现其softmax分母在固定token分布下稳定。利用校准文本采样softmax分母，可近似这些稳定头的组合输出为周围文本的线性摘要，从而仅从权重和单一校准文本就能识别数百个对周围文本高级上下文属性敏感的第一层神经元。


<details>
  <summary>Details</summary>
Motivation: 研究transformer语言模型中注意力模式分散的注意力头，探索其softmax分母的稳定性特性，旨在开发仅从模型权重和少量校准数据就能识别神经元功能的方法。

Method: 分析GPT2-Small第一层中注意力模式分散的注意力头，利用校准文本采样softmax分母，将这些稳定头的组合输出近似为周围文本的线性摘要，从而识别对高级上下文属性敏感的神经元。

Result: 成功识别出GPT2-Small第一层中数百个对周围文本高级上下文属性敏感的神经元，包括在校准文本上未激活的神经元，仅需模型权重和单一校准文本即可实现。

Conclusion: 注意力模式分散的注意力头具有稳定的softmax分母特性，这一发现为仅从模型权重和少量数据就能分析神经元功能提供了有效方法，有助于理解transformer的内部工作机制。

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [171] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: Graph-S³是一个基于LLM的文本图推理框架，通过合成逐步监督训练检索器，解决了图检索中信息充分性和上下文紧凑性的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据多以文本图形式存在，但现有图检索方法要么依赖浅层嵌入相似性，要么需要大量标注数据和训练成本，导致性能不佳。

Method: 提出基于LLM的检索器，使用合成逐步监督进行训练；通过数据合成管道提取黄金子图生成奖励，采用两阶段训练方案学习交互式图探索策略。

Result: 在三个常用数据集上与七个强基线对比，平均准确率提升8.1%，F1分数提升9.7%；在多跳推理任务中优势更明显。

Conclusion: Graph-S³框架通过合成监督和逐步评估有效解决了图检索问题，在复杂推理任务中表现优异，代码将开源。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [172] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 审计6个流行LLM在30个日常任务中的表现，发现LLM在隐含价值观（如环保主义、慈善、多样性）方面与人类和其他LLM存在显著不一致


<details>
  <summary>Details</summary>
Motivation: 了解AI助手在完成主观日常任务时展现的隐含价值观，以及这些价值观与人类的比较

Method: 通过审计6个流行LLM完成30个日常任务，并与100名美国众包工作者进行比较

Result: LLM在隐含价值观表现上经常与人类不一致，不同LLM之间也存在差异

Conclusion: LLM在价值观对齐方面存在挑战，需要进一步研究以确保AI助手与人类价值观一致

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [173] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: CSAR算法用于从平行语料库中诱导语素，通过互信息加权、选择、移除和重复的贪婪过程来提取语素。


<details>
  <summary>Details</summary>
Motivation: 开发一种从涌现语言数据中自动识别语素的算法，以理解语言形成的基本单元。

Method: 基于互信息的贪婪算法：计算形式与意义的互信息权重，选择最高权重对，从语料库中移除，重复此过程。

Result: 在程序生成数据集上验证有效性，在人类语言数据上表现合理，并能量化涌现语言的同义词和多义词特征。

Conclusion: CSAR算法能有效从涌现语言中诱导语素，为语言特征分析提供量化工具。

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [174] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: Omni-Embed-Nemotron是一个统一的多模态检索嵌入模型，支持文本、图像、音频和视频的跨模态和联合模态检索。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的检索器在处理现实世界中视觉和语义丰富的内容（如PDF、幻灯片或视频）时存在困难，需要支持更多模态的统一检索模型。

Method: 基于ColPali和Qwen2.5-Omni等工作的启发，开发了支持文本、图像、音频和视频的统一多模态检索架构和训练设置。

Result: 模型在文本、图像和视频检索方面表现出有效性，能够实现跨模态和联合模态检索。

Conclusion: Omni-Embed-Nemotron提供了一个统一的解决方案来处理现实世界中复杂的信息检索需求，支持多种模态的检索能力。

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [175] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 通过基于信号博弈的涌现通信环境和超参数优化，生成与人类语言相似度最高的涌现语言，使用XferBench作为目标函数来量化统计相似性。


<details>
  <summary>Details</summary>
Motivation: 设计一个能够生成与人类语言高度相似的涌现语言的环境，以推动涌现通信研究的发展。

Method: 使用基于信号博弈的涌现通信环境，结合超参数优化和XferBench目标函数来生成涌现语言。

Result: 证明了熵对涌现语言迁移学习性能的预测能力，验证了涌现通信系统的熵最小化特性，并确定了产生更真实涌现语言的超参数规律。

Conclusion: 成功生成了与人类语言高度相似的涌现语言，揭示了熵在预测语言质量中的重要作用，并为生成更真实涌现语言提供了具体指导。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [176] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: SEER基准测试LLMs识别文本中表达情感的具体片段的能力，包含单句和跨句情感证据检测任务，评估发现模型在长文本中性能下降。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别只给整句分配单一标签，而实际应用需要知道情感如何表达，因此需要开发能精确定位情感表达片段的方法。

Method: 创建包含1200个真实句子的SEER基准，标注情感和情感证据，评估14个开源LLMs在单句和跨句任务上的表现。

Result: 部分模型在单句任务上接近人类平均水平，但在长文本中准确性下降；错误分析显示模型过度依赖情感关键词且在中文文本中产生误报。

Conclusion: SEER基准揭示了LLMs在细粒度情感证据检测中的局限性，特别是在处理长文本时，需要改进模型对情感表达的深层理解能力。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [177] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: LSD是一种基于几何框架的幻觉检测方法，通过分析transformer层间隐藏状态语义的动态变化来检测大语言模型的幻觉，无需多次采样或外部验证，只需单次前向传播即可实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常产生流畅但事实错误的陈述（幻觉现象），这在高风险领域构成严重威胁，需要开发高效的幻觉检测方法。

Method: 使用基于边际的对比学习，将隐藏激活与事实编码器生成的ground-truth嵌入对齐，分析语义轨迹的分离：事实性响应保持稳定对齐，而幻觉在深度上表现出明显的语义漂移。

Result: 在TruthfulQA和合成事实-幻觉数据集上评估，LSD达到F1分数0.92、AUROC 0.96和聚类准确率0.89，优于SelfCheckGPT和语义熵基线方法，速度比基于采样的方法快5-20倍。

Conclusion: LSD提供了一个可扩展、模型无关的实时幻觉监测机制，并为理解大语言模型中事实一致性的几何特性提供了新见解。

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [178] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: ALHD是首个专为区分阿拉伯语人类生成文本和LLM生成文本的大规模综合数据集，涵盖新闻、社交媒体和评论三种文体，包含超过40万个平衡样本，支持阿拉伯语LLM文本检测的可泛化性研究。


<details>
  <summary>Details</summary>
Motivation: 建立专门针对阿拉伯语的LLM生成文本检测数据集，以应对错误信息、学术不端和网络威胁等风险，填补现有研究的空白。

Method: 构建包含MSA和方言阿拉伯语的平衡数据集，涵盖三种文体，使用三个主流LLM生成文本，并进行严格预处理、丰富标注和标准化分割。使用传统分类器、BERT模型和LLM进行基准实验。

Result: 微调的BERT模型表现最佳，优于基于LLM的模型。但在跨文体泛化方面存在挑战，特别是在新闻文章中，LLM生成文本与人类文本风格相似，导致检测困难。

Conclusion: ALHD为阿拉伯语LLM检测研究奠定了基础，揭示了跨文体泛化的挑战，为未来研究指明了方向。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [179] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: 研究发现不礼貌的提示词比礼貌提示词在LLM中表现更好，准确率从非常礼貌的80.8%提升到非常粗鲁的84.8%。


<details>
  <summary>Details</summary>
Motivation: 探索自然语言提示中礼貌程度和语调对大型语言模型性能的影响，特别是对多选问题准确率的影响。

Method: 创建包含50个基础问题的数据集，每个问题重写为5种语调变体（非常礼貌、礼貌、中性、粗鲁、非常粗鲁），共250个独特提示，使用ChatGPT 4o评估响应并进行配对样本t检验。

Result: 与预期相反，不礼貌提示持续优于礼貌提示，准确率从非常礼貌的80.8%到非常粗鲁的84.8%。

Conclusion: 新LLM对语调变化的响应可能与早期研究不同，强调研究提示语用学的重要性，并引发关于人机交互社会维度的更广泛问题。

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [180] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: TS-Reasoner通过将时间序列基础模型的潜在表示与大型语言模型的文本输入对齐，解决了时间序列推理任务中数值理解和语义推理的融合问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型能捕捉动态模式但缺乏推理能力，而大型语言模型具备推理能力但不擅长数值理解。需要有效整合两种模型以实现时间序列推理。

Method: 提出两阶段训练方法：首先使用合成的时间序列-文本对进行对齐预训练，然后进行指令微调。冻结预训练的时间序列基础模型，仅对齐其表示与语言模型。

Result: 在多个基准测试中，TS-Reasoner超越了主流LLM、VLM和时间序列LLM，且具有显著的数据效率（使用不到一半的训练数据）。

Conclusion: 该方法成功实现了时间序列与文本模态的有效对齐，为时间序列推理任务提供了高效解决方案。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [181] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 在专业领域推理中，传统RAG方法只能提取事实信息但缺乏比较性推理。本文提出在RAG之上增加对比性推理层，通过检索相似案例进行对比分析，在金融领域生成更具体而非通用的风险评估。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在专业领域推理中存在局限性，虽然能提取上下文相关信息，但无法检索可比案例或相关问题，导致输出结果过于通用化，缺乏针对特定情境的深入洞察。

Method: 在RAG基础上增加对比性推理层，采用对比方法检索相似案例进行对比分析，而非孤立分析信息。

Result: 对比方法在文本生成指标（如ROUGE和BERTScore）上优于基线RAG，与人工生成的股票研究和风险评估结果更接近。

Conclusion: 在专业领域推理任务中，增加对比性推理层能够显著提升RAG的性能，生成更具针对性和洞察力的分析结果。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [182] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 提出了Consensus Graphs (ConGrs)数据结构，通过采样多个语言模型响应并构建有向无环图来捕捉共享信息和语义变化，从而提高事实精度和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效合成不同长文本响应中的丰富认知信号，需要一种能捕捉语言模型响应变化并利用这些认知信号的方法。

Method: 使用生物信息学中的轻量级词汇序列对齐算法构建ConGrs数据结构，辅以辅助语言模型判断，并设计任务相关的解码方法从ConGr合成最终响应。

Result: 在传记生成任务中事实精度提升31%，减少对语言模型判断的依赖80%以上；在拒绝任务中弃权率提升56%；在数学推理任务中准确率提升6个百分点。

Conclusion: ConGrs提供了一种灵活的方法来捕捉语言模型响应变化，并利用响应变化提供的认知信号来合成更有效的响应。

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [183] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 研究表明，在指令微调数据中引入扰动（如删除停用词或打乱词序）可以增强大语言模型对噪声指令的抵抗能力，在某些情况下还能提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型对指令表述的微小变化很敏感，这影响了其在真实场景中的实用性。本文旨在探索通过指令微调数据扰动来增强模型对噪声指令的鲁棒性。

Method: 在指令微调数据中引入扰动（删除停用词、打乱词序等），然后在多个基准测试（MMLU、BBH、GSM8K）上评估模型在原始和扰动版本上的表现，并分析学习动态和模型行为变化。

Result: 令人惊讶的是，在某些情况下，对扰动指令进行指令微调反而能提升下游任务性能，这表明扰动指令训练能使模型对噪声用户输入更具弹性。

Conclusion: 在指令微调中包含扰动指令非常重要，这能使大语言模型对噪声用户输入更具弹性，提高实际应用中的鲁棒性。

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [184] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文介绍了在EvalLLM 2025挑战赛中，针对法语生物医学命名实体识别和健康事件抽取的三种方法：基于GPT-4.1的上下文学习、GLiNER系统微调以及LLaMA-3.1-8B-Instruct微调。


<details>
  <summary>Details</summary>
Motivation: 解决法语生物医学领域在少样本设置下的命名实体识别和健康事件抽取问题，探索在资源匮乏场景下的有效方法。

Method: 1. GPT-4.1上下文学习：自动选择10个示例并将标注指南摘要融入提示；2. GLiNER系统：在合成语料上微调，后经LLM验证；3. LLaMA-3.1-8B-Instruct：在相同合成语料上微调。事件抽取采用相同的GPT-4.1上下文学习策略。

Result: GPT-4.1表现最佳，NER的宏F1为61.53%，事件抽取为15.02%，表明精心设计的提示在低资源场景中至关重要。

Conclusion: 在极低资源场景下，精心设计的提示策略能够最大化LLM性能，GPT-4.1在法语生物医学NER和事件抽取任务中表现最优。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [185] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: TriMediQ框架通过将患者回答转换为三元组结构并构建知识图谱，解决了LLM在多轮医疗对话中推理能力下降的问题，在iMedQA数据集上比基线方法准确率提升10.4%。


<details>
  <summary>Details</summary>
Motivation: LLM在静态单轮医疗问答中表现良好，但在实际临床咨询的多轮交互式信息收集过程中可靠性显著下降，因为临床事实在对话日志中缺乏明确关联。

Method: 提出TriMediQ框架：1）使用冻结的三元组生成器提取临床相关三元组；2）通过可训练的投影模块（图编码器和投影器）从知识图谱中捕获关系信息；3）分两步操作：先微调投影模块（LLM权重冻结），然后在推理时使用微调模块指导多跳推理。

Result: 在两个交互式QA基准测试中，TriMediQ在iMedQA数据集上比五个基线方法准确率提升高达10.4%。

Conclusion: 将患者回答转换为结构化三元组图谱能够在多轮设置中实现更准确的临床推理，为基于LLM的医疗助手部署提供了解决方案。

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [186] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: 论文指出在计算社会科学中使用大语言模型进行文本分类时，概念化步骤常被忽视，这会导致概念化诱导偏差，且无法通过提高模型准确性或后处理偏差校正来纠正。


<details>
  <summary>Details</summary>
Motivation: 当前计算社会科学中广泛使用生成式大语言模型进行文本分类，但模型使用前后的关键步骤——概念化和将预测结果用于下游统计推断——往往被忽视，这可能导致严重的估计偏差。

Method: 通过模拟实验分析概念化诱导偏差的影响，并评估提高LLM准确性和后处理偏差校正方法的有效性。

Result: 研究发现概念化诱导偏差无法仅通过提高LLM准确性或后处理偏差校正方法来纠正，这种偏差会严重影响下游估计的准确性。

Conclusion: 在LLM时代，概念化仍然是计算社会科学中的首要关注点，作者提供了获取低成本、无偏、低方差下游估计的具体建议。

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [187] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型生成文本的同质化问题，提出了衡量认知多样性的新方法，发现虽然新模型生成更多样化的主张，但几乎所有模型都比基础网络搜索的认知多样性低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型倾向于生成词汇、语义和风格同质的文本，这带来了知识崩溃的风险，即同质化的LLM随着时间的推移导致可获取信息范围的缩小。现有研究局限于封闭式选择题设置或模糊语义特征，且未考察跨时间和文化背景的趋势。

Method: 提出了一种衡量认知多样性的新方法，即LLM输出中真实世界主张的变异性。测试了27个LLM、155个涵盖12个国家的主题，以及200个来自真实用户聊天的提示变体。

Result: 研究发现：较新的模型倾向于生成更多样化的主张；几乎所有模型的认知多样性都低于基础网络搜索；模型大小对认知多样性有负面影响；检索增强生成(RAG)有积极影响，但其改善程度因文化背景而异；与维基百科相比，特定国家的主张更多反映英语而非当地语言。

Conclusion: LLM存在认知多样性不足的问题，特别是在文化代表性方面。模型大小越大，认知多样性越低，而RAG技术可以改善这一问题，但改善效果受文化背景影响。需要关注LLM在跨文化知识表示方面的差距。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [188] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: CCD-Bench是一个评估大语言模型在跨文化价值冲突中决策能力的基准，包含2,182个开放困境，覆盖7个领域，评估17个非推理LLM。结果显示模型偏好北欧和日耳曼欧洲文化，而东欧和中东文化选项代表性不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注文化知识、价值预测或单轴偏见诊断，但缺乏评估LLM在多种文化价值直接冲突时的裁决能力。

Method: 使用2,182个开放困境，每个困境配10个对应GLOBE文化集群的匿名响应选项，采用分层拉丁方设计来减轻顺序效应。

Result: 模型明显偏好北欧欧洲(20.2%)和日耳曼欧洲(12.4%)，东欧和中东选项代表性不足(5.6-5.8%)。87.9%的理据引用多个GLOBE维度，但主要是表面多元主义。

Conclusion: 当前对齐流程促进共识导向世界观，但无法处理需要权力协商、权利推理或性别意识分析的场景。CCD-Bench将评估从孤立偏见检测转向多元决策，强调需要实质性参与多元世界观的对齐策略。

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [189] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: RxT是一种新型Transformer架构，通过事件驱动范式解决传统Transformer在对话AI中的状态保持和计算复杂度问题，将对话成本从二次方降低到线性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在对话AI中存在无状态特性和二次计算复杂度的限制，导致长对话中成本高昂和延迟严重。

Method: RxT采用事件驱动架构，将每个对话轮次作为离散事件处理，通过集成固定大小的短期记忆系统，使用生成器-解码器生成响应，并通过记忆编码器和专用记忆注意力网络异步更新记忆。

Result: RxT将对话总成本从O(N²·T)降低到O(N·T)，实现了低延迟、实时状态保持，在合成数据实验中表现出优越性能和恒定时间推理延迟。

Conclusion: RxT架构为长形式对话提供了经济可行的解决方案，实现了真正实时、有状态且成本可控的对话AI系统。

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [190] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: GRACE是一个新颖的框架，将对比信号重新构想为奖励而非损失函数，通过策略梯度优化训练LLM生成可解释的推理过程，同时产生高质量嵌入表示。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM文本编码器训练方法依赖对比损失，将模型视为黑盒函数，丢弃了其生成和推理能力。

Method: 将LLM作为策略生成可解释的推理过程，通过多组件奖励函数进行策略梯度优化，最大化正样本对相似度并最小化负样本相似度。

Result: 在MTEB基准测试中，监督设置比基础模型总体得分提高11.5%，无监督变体提高6.9%，同时保持通用能力。

Conclusion: 将对比目标作为推理过程的奖励，统一了表示学习和生成，产生更强的嵌入和透明的推理过程。

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [191] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: 提出ALC辅助学习策略，通过细粒度嵌入提升产品推荐的覆盖率，结合最难负样本构建判别性训练信号，在极端多标签分类任务中实现最先进的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现实生产系统对推荐覆盖率有严格要求，需要高比例自动化推荐，但现有模型在真实系统集成中常被忽视这一需求。

Method: ALC辅助学习策略，引入两个训练目标：利用批次中最难负样本构建正负样本间的判别性训练信号，结合阈值一致性边界损失。

Result: 在两个产品推荐数据集（LF-AmazonTitles-131K和Tech and Durables）上验证，使用三种极端多标签分类方法，展示了最先进的覆盖率。

Conclusion: ALC策略能有效提升产品推荐系统的覆盖率，满足生产系统对自动化推荐的高要求。

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [192] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: Deco-G是一个解码框架，通过将格式遵循与任务解决解耦，使用单独的概率模型处理格式合规性，从而提高LLM在复杂指令下的性能。


<details>
  <summary>Details</summary>
Motivation: 随着提示变得越来越复杂，LLM往往难以同时遵循所有指令，特别是在推理指令和严格格式要求交织时，这种纠缠会为模型创造竞争目标。

Method: Deco-G使用单独的可处理概率模型处理格式合规性，同时仅向LLM提供任务指令，在每个解码步骤中结合LLM的下一个标记概率和TPM计算的格式合规似然。

Result: 在数学推理、LLM-as-a-judge和事件参数提取等任务中，Deco-G相比常规提示方法获得了1.0%到6.0%的相对性能提升，并保证了格式合规性。

Conclusion: 通过明确分离格式遵循和任务解决，Deco-G框架有效提高了LLM在复杂指令下的性能，同时确保格式合规性。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [193] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: SciNCL方法应用于流程工业领域，通过知识图谱增强语言模型，在文本嵌入任务上显著优于现有方法且模型更小。


<details>
  <summary>Details</summary>
Motivation: 利用知识图谱增强预训练语言模型，为流程工业领域提供更好的文本理解能力，该领域的文本日志通常结构化为稀疏知识图谱。

Method: 采用SciNCL图感知邻域对比学习方法，从流程工业的知识图谱中提取三元组来微调语言模型。

Result: 在专有流程工业文本嵌入基准(PITEB)上，该方法比最先进的mE5-large文本编码器性能提升9.8-14.3%，同时模型尺寸小3-5倍。

Conclusion: 图感知对比学习方法能有效提升流程工业领域的文本嵌入性能，同时实现模型压缩。

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [194] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 现有评估基准在衡量LLMs的有效上下文长度和遗忘倾向方面存在不足，本文提出通过更复杂的关系推理任务来评估模型性能，发现LLMs在关系推理中比现有基准显示更早出现记忆漂移和上下文遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准主要依赖简单的检索或续写任务，不能准确反映LLMs在信息密集场景中的真实表现，特别是处理需要从文本中提取结构化关系知识的复杂推理任务。

Method: 使用需要从潜在嘈杂的自然语言内容中归纳图结构关系的复杂推理任务进行评估，这些任务要求模型从分布式文本线索中推导连接关系。

Result: 研究发现LLMs在进行关系推理时，在比现有基准建议的更短有效长度下就开始出现记忆漂移和上下文遗忘，即使是专门用于推理的模型如OpenAI o1也容易受到早期记忆漂移的影响。

Conclusion: 这些结果揭示了模型从非结构化输入中抽象结构化知识能力的显著局限性，强调了需要架构改进来提升长距离推理能力。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [195] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 提出基于音节级别的无监督语音识别框架，使用掩码语言建模替代传统的音素方法和GAN方法，在LibriSpeech上实现40%相对字符错误率降低，并能有效泛化到中文。


<details>
  <summary>Details</summary>
Motivation: 解决无监督语音识别中依赖昂贵G2P资源、训练不稳定以及难以处理音素边界模糊语言的问题。

Method: 基于掩码语言建模的音节级别无监督语音识别框架，避免使用G2P和GAN方法。

Result: 在LibriSpeech上实现40%相对字符错误率降低，在中文上表现良好，解决了先前方法的泛化难题。

Conclusion: 提出的音节级别UASR框架有效解决了传统方法的局限性，为低资源语言和多模态学习提供了可行方案。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [196] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 提出了UniDoc-Bench，首个大规模真实的多模态检索增强生成基准，包含来自8个领域70k PDF页面的1600个多模态QA对，支持四种范式的公平比较。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检索增强生成评估存在碎片化问题，要么单独评估文本或图像，要么使用简化的多模态设置，无法捕捉文档中心的多模态用例。

Method: 从真实PDF页面提取并链接文本、表格和图像证据，生成涵盖事实检索、比较、摘要和逻辑推理的多模态QA对，其中20%经过多标注者验证和专家裁决。

Result: 多模态文本-图像融合RAG系统始终优于单模态和联合多模态嵌入检索，表明单独文本或图像都不足够，当前多模态嵌入仍不充分。

Conclusion: 分析揭示了视觉上下文何时以及如何补充文本证据，发现了系统性失败模式，并为开发更稳健的多模态RAG管道提供了可行指导。

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [197] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 提出基于QLoRA的微调框架，用于检测罗马乌尔都语-英语混合文本中的冒犯性语言，通过翻译处理低资源数据，Meta LLaMA 3 8B模型取得最佳F1分数91.45。


<details>
  <summary>Details</summary>
Motivation: 罗马乌尔都语等代码混合语言中的贬义词汇检测面临语法不明确、拼写不一致和标注数据稀缺的挑战，需要开发有效的自然语言处理方法。

Method: 使用Google Translate将罗马乌尔都语-英语混合数据集翻译成英语，利用QLoRA对多个Transformer和大型语言模型进行内存高效的微调，包括Meta LLaMA 3 8B、Mistral 7B等模型。

Result: Meta LLaMA 3 8B模型获得最高F1分数91.45，Mistral 7B达到89.66，均优于传统Transformer基线模型。

Conclusion: QLoRA在低资源环境下的代码混合冒犯性语言检测中表现出色，证实了LLMs在此任务中的潜力，为基于LLMs的多语言冒犯性检测系统奠定了基础。

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [198] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: 提出MedReflect框架，通过模拟医生反思思维模式，让大语言模型在医学问题解决中进行自我反思和自我验证，无需外部检索或大量标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部知识检索或大量标注数据，存在检索开销大、标注成本高的问题，在医学领域性能有限。

Method: MedReflect生成单次反思链，包括初始假设生成、自我提问、自我回答和决策优化，实现自我验证和反思。

Result: 仅用2000个训练样本和轻量微调，在多个医学基准测试中取得显著准确率提升，大幅减少标注需求。

Conclusion: 大语言模型可以通过自我反思学习解决专业医学问题，减少对外部监督和大量任务特定微调数据的依赖。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [199] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 提出了TreePrompt方法，通过树状结构框架学习LLM偏好来选择高质量、上下文相关的翻译示例，结合K-NN和AFSP方法平衡相似性和质量，在英-波斯和英-德语翻译任务中取得改进


<details>
  <summary>Details</summary>
Motivation: 现有的少样本提示方法主要关注查询与示例的相似性，但忽略了示例质量对翻译性能的影响

Method: TreePrompt方法在树状结构框架中学习LLM偏好，识别高质量且上下文相关的示例，并与K-NN和AFSP方法结合

Result: 在MIZAN（英-波斯）和WMT19（英-德）数据集上的评估显示，TreePrompt与AFSP或随机选择结合能提升翻译性能

Conclusion: TreePrompt通过考虑示例质量而不仅仅是相似性，有效改进了少样本提示的翻译效果

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [200] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: 提出了一种基于语音细粒度分析的帕金森病检测方法，通过分析音素、音节和单词级别特征，在多种语言数据集上实现了93.78%的AUROC性能，发现音素级分析效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前帕金森病语音检测系统分析整个话语，可能忽略了特定语音元素的诊断价值。帕金森病影响全球超1000万人，其中89%患者存在语音障碍。

Method: 开发了细粒度感知方法，使用自动化流程从录音中提取时间对齐的音素、音节和单词。在意大利语、西班牙语和英语数据集上使用双向LSTM和多头注意力机制，比较不同粒度级别的诊断性能。

Result: 音素级分析表现最佳，AUROC达93.78% ± 2.34%，准确率达92.17% ± 2.43%。注意力分析显示最有信息的语音特征与临床协议一致：音素级的持续元音、音节级的交替运动音节、单词级的/pataka/序列。

Conclusion: 该方法展示了跨语言帕金森病检测的增强诊断能力，细粒度语音分析可提供更准确的检测结果。

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [201] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: 研究探讨了少样本提示策略对词义消歧任务的影响，特别关注样本分布不平衡带来的偏见问题。研究发现不平衡的少样本示例会导致多语言词义消歧错误，但英语中未出现此问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，少样本提示因其实用性和有效性受到广泛关注。本研究旨在调查少样本提示策略如何影响词义消歧任务，特别关注样本分布不平衡引入的偏见。

Method: 使用GLOSSGPT提示方法，在英语、德语、西班牙语、法语和意大利语五种语言上测试其有效性。评估了GPT-4o和LLaMA-3.1-70B模型的行为。

Result: 结果显示，不平衡的少样本示例会导致多语言词义消歧错误预测，但英语中未出现此问题。两种模型的结果都突显了多语言词义消歧对少样本设置中样本分布的敏感性。

Conclusion: 研究强调了在多语言词义消歧任务中需要平衡和具有代表性的提示策略，以确保模型性能的稳定性和准确性。

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [202] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: Rezwan是一个包含120万条圣训的大型AI辅助语料库，通过全自动流水线从数字资源中提取和结构化圣训文本，并进行多语言翻译、智能标注、摘要生成等丰富处理。


<details>
  <summary>Details</summary>
Motivation: 传统圣训整理依赖人工，耗时耗力且难以大规模处理。本研究旨在利用AI技术自动化圣训处理流程，实现大规模、多语言、语义丰富的伊斯兰文本数字化。

Method: 使用大型语言模型构建全自动流水线，包括文本分割、传述链-正文分离、验证和多层增强处理（机器翻译、智能标注、摘要生成、主题标记、跨文本语义分析）。

Result: 在1,213条随机样本的专家评估中，传述链-正文分离和摘要生成接近人类水平（9.33/10），整体质量得分8.46/10，显著优于手动整理的Noor语料库（3.66/10）。AI方法在几个月内完成了原本需要229,000小时人工的工作。

Conclusion: AI技术能够有效增强人类专业知识，为伊斯兰研究提供大规模、多语言、语义丰富的文本处理新范式，证明了AI在宗教文本处理中的可行性和经济性。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [203] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: 大语言模型能够生成和识别深层认知框架，特别是在社会政治语境中。研究发现LLMs能流畅生成唤起特定框架的文本，并在零样本设置下识别这些框架。通过机制可解释性研究，确定了模型隐藏表示中与'严格父亲'和'培育父母'框架强相关的单一维度。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型捕捉和表达有意义的认知框架的能力，特别是在社会政治语境中，了解LLMs如何内化和表示人类概念。

Method: 采用机制可解释性研究方法，分析模型隐藏表示中的特定维度，识别与'严格父亲'和'培育父母'框架相关的单一维度。测试LLMs生成框架化文本和零样本框架识别的能力。

Result: LLMs能够高度流畅地生成唤起特定认知框架的文本，并在零样本设置下成功识别这些框架。在模型隐藏表示中发现了与特定框架强相关的单一维度。

Conclusion: 大语言模型能够有效捕捉和表达有意义的认知框架，这有助于理解LLMs如何表示人类概念，并为模型可解释性研究提供了新的视角。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [204] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 提出Step Pruner框架，通过强化学习惩罚冗余推理步骤而非单纯减少token数量，解决大推理模型过度思考问题，在保持准确性的同时显著减少响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的解决方案通过惩罚生成token来促进简洁性，但存在两个问题：token少不一定对应推理步骤少，模型可能在训练后期出现丢弃推理步骤的作弊行为。

Method: 提出Step Pruner框架，包含步骤感知奖励函数（优先正确性、惩罚冗余步骤、错误响应不奖励）和动态停止机制（当输出步骤长度超过上限时停止更新防止作弊）。

Result: 在四个推理基准测试中达到最先进准确性，同时显著减少响应长度。在AIME24上减少69.7%的token使用量。

Conclusion: Step Pruner框架能有效引导大推理模型进行更高效的推理，在保持准确性的同时显著减少响应冗长度。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [205] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: 比较使用INCEpTION工具手动标注修辞关系与基于BERT、DistilBERT和逻辑回归的自动方法在板球新闻中的表现，发现DistilBERT准确率最高。


<details>
  <summary>Details</summary>
Motivation: 探索修辞关系标注在语篇分析中的应用，比较手动标注与基于大语言模型的自动方法在体育报道中的效果。

Method: 使用INCEpTION工具进行手动标注，并评估BERT、DistilBERT和逻辑回归模型在分类修辞关系（如阐述、对比、背景、因果）上的性能。

Result: DistilBERT模型在修辞关系分类中取得了最高的准确率，显示出其在语篇关系预测中的高效潜力。

Conclusion: 这项工作促进了语篇解析与基于Transformer的自然语言处理方法的交叉研究，证明了DistilBERT在修辞关系自动标注中的有效性。

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [206] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: 本文介绍了首个孟加拉语政治立场检测基准数据集，包含200篇新闻文章，标注了亲政府、批评政府和中性立场。评估了28个大语言模型，发现模型在检测批评政府内容上表现良好，但在识别中性文章和避免过度预测亲政府立场方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语政治立场检测研究缺乏标注数据集和计算研究，需要理解语言线索、文化背景、微妙偏见、修辞策略、语码转换、隐含情感和社会政治背景。

Method: 创建了包含200篇孟加拉语新闻文章的基准数据集，标注三种政治立场（亲政府、批评政府、中性），并对28个专有和开源大语言模型进行全面评估。

Result: 模型在检测批评政府内容上表现强劲（F1最高达0.83），但在识别中性文章上存在显著困难（F1低至0.00）。模型倾向于过度预测亲政府立场，经常误解模糊叙述。

Conclusion: 该数据集及其相关诊断为推进孟加拉语媒体立场检测研究奠定了基础，并为改进低资源语言中大语言模型的性能提供了见解。

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [207] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: PsychoLexTherapy是一个用于波斯语心理治疗推理模拟的框架，使用小型语言模型，注重文化适应性和隐私保护，支持设备端部署。


<details>
  <summary>Details</summary>
Motivation: 开发针对波斯语等代表性不足语言的文化基础、治疗连贯的对话系统，解决多轮交互中的结构化记忆挑战，同时确保隐私和可行性。

Method: 三阶段开发过程：评估SLMs心理知识、设计推理导向框架、构建评估数据集；比较简单提示、多代理辩论和结构化治疗推理路径。

Result: 在单轮偏好研究中获得最高人类评价，在多轮测试中完整框架在共情、连贯性、文化适应性和个性化方面获得最高评分，长期记忆模块至关重要。

Conclusion: PsychoLexTherapy为波斯语心理治疗模拟建立了实用、隐私保护和文化对齐的基础，贡献了新颖数据集、可复现评估流程和结构化记忆的实证见解。

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [208] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: 使用大型语言模型从410万条患者评价中提取医生的五大性格特质和患者主观判断，揭示了性别差异、专科模式及医生原型分类，验证了自动化特质提取在医疗质量评估中的有效性。


<details>
  <summary>Details</summary>
Motivation: 了解患者对医生的感知对于改善信任、沟通和满意度至关重要，需要大规模分析患者评价来理解医患关系。

Method: 基于大型语言模型的流水线方法，从410万条患者评价中推断五大性格特质和五项患者主观判断，通过多模型比较和人类专家基准进行验证。

Result: 人类与LLM评估高度一致（相关系数0.72-0.89），与患者满意度显著相关（r=0.41-0.81）；发现男性医生在所有特质上评分更高，儿科和精神病学中同理心特质占主导；识别出四种医生原型。

Conclusion: 从患者叙述中自动提取特质可以提供可解释、经过验证的指标，用于大规模理解医患关系，对医疗质量测量、偏见检测和劳动力发展具有重要意义。

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [209] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 该研究提出了首个用于在长序列交互任务中探测和评估大语言模型欺骗行为的模拟框架，发现欺骗行为具有模型依赖性，会随事件压力增加而加剧，并持续削弱监督者信任。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多局限于单轮提示下的欺骗检测，无法捕捉欺骗策略在长期交互中的动态发展过程，需要开发能够评估长序列任务中欺骗行为的框架。

Method: 建立多智能体系统：执行者智能体完成任务，监督者智能体评估进展并提供反馈，独立欺骗审计员审查完整轨迹以识别欺骗行为。在11个前沿模型上进行广泛实验。

Result: 欺骗行为具有模型依赖性，随事件压力增加而加剧，持续削弱监督者信任。定性分析揭示了隐瞒、模棱两可和伪造等不同欺骗策略。

Conclusion: 欺骗是长序列交互中出现的风险，为评估未来LLM在现实世界信任敏感环境中的表现提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [210] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 提出了一种用于COVID-19命名实体识别的实体知识增强方法，该方法通过增强领域特定知识来解决社交媒体文本中命名实体识别的挑战。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情期间社交媒体上产生了大量非正式文本，但相关标注数据稀缺且不足以训练鲁棒的识别模型，同时COVID-19命名实体识别需要广泛的领域特定知识。

Method: 提出了一种新颖的实体知识增强方法，该方法可应用于一般生物医学命名实体识别，适用于非正式和正式文本格式。

Result: 在COVID-19推文数据集和PubMed数据集上的实验表明，该方法在完全监督和少样本设置下都能提高NER性能。

Conclusion: 所提出的实体知识增强方法有效解决了COVID-19命名实体识别中的挑战，并在不同设置下均表现出性能提升。

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [211] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: 提出了AgriGPT-VL套件，包含农业领域最大的视觉语言语料库Agri-3M-VL、专门训练的视觉语言模型AgriGPT-VL和评估基准AgriBench-VL-4K，在农业任务上优于通用VLMs。


<details>
  <summary>Details</summary>
Motivation: 解决农业应用中缺乏领域定制模型、精心策划的视觉语言语料库和严格评估的问题。

Method: 使用可扩展多智能体数据生成器构建Agri-3M-VL语料库；通过渐进式课程训练AgriGPT-VL模型，包括文本基础、多模态浅层/深层对齐和GRPO细化；建立AgriBench-VL-4K评估套件。

Result: AgriGPT-VL在AgriBench-VL-4K上优于领先的通用VLMs，在LLM-as-a-judge评估中获得更高的成对胜率，同时在纯文本AgriBench-13K上保持竞争力。

Conclusion: 该框架在农业任务上表现出色，同时保持了语言能力，消融研究证实了对齐和GRPO细化阶段的一致收益。

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [212] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: 利用大语言模型的内部激活信号来预测输出正确性和评估外部上下文有效性，通过简单分类器实现早期审计和防止错误上下文污染。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常以高置信度生成错误信息，需要解决如何识别查询是否需要检索上下文以及如何评估上下文有效性的挑战。

Method: 使用可解释性方法，基于模型中间层激活训练简单分类器，考虑正确、错误和不相关上下文，并引入指标进行区分。

Result: 在六个不同模型上的实验表明，基于第一个输出token的中间层激活的分类器能以约75%的准确率预测输出正确性；基于模型内部指标的评估方法显著优于提示基准。

Conclusion: 这些发现为更好理解大语言模型的底层决策过程提供了视角，能够实现早期审计并防止被污染上下文引入的不准确性。

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [213] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 本文系统研究了泰语文本端到端(EOT)检测，比较了零样本/少样本提示紧凑LLM与轻量级transformer监督微调的方法，建立了泰语EOT检测基准。


<details>
  <summary>Details</summary>
Motivation: 传统音频静音端点检测延迟高且对犹豫或语言特定现象不敏感，需要可靠低延迟的语音交互端点检测方法。

Method: 使用YODAS语料库转录字幕和泰语特定语言线索，将EOT建模为token边界上的二元决策，比较零样本/少样本提示与监督微调方法。

Result: 发现了明确的准确率-延迟权衡，微调后的小模型能提供近乎即时的EOT决策，适合设备端代理使用。

Conclusion: 建立了泰语EOT检测基准，证明小型微调模型可实现适合实时设备的低延迟端点检测。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [214] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 该论文研究了在LLM分类任务中，通过引入反事实推理来识别对分类决策贡献最大的关键词，并提出了决策变化率框架来量化关键词重要性。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs通常是黑盒模型且调用成本高昂，需要解释其分类决策，特别是识别影响分类的关键词。

Method: 提出决策变化率框架，通过引入反事实来评估LLM分类决策中关键词的重要性。

Result: 实验结果表明使用反事实方法有助于识别对分类决策贡献最大的关键词。

Conclusion: 反事实推理可以有效帮助理解和解释LLMs的分类决策过程。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [215] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 研究发现通用领域的小型语言模型在急诊科决策支持任务中表现优于医学微调的模型，表明急诊科场景下可能不需要专门的医学微调。


<details>
  <summary>Details</summary>
Motivation: 急诊科环境快节奏、高风险，小型语言模型因其推理能力和高效性能具有显著潜力，可以支持医生提供及时准确的信息合成，改善临床决策和工作流程效率。

Method: 构建综合基准测试，评估在通用领域和医学语料混合训练的小型语言模型，使用MedMCQA、MedQA-4Options、PubMedQA和医学摘要数据集来模拟急诊科医生的日常任务。

Result: 实验结果显示通用领域的小型语言模型在多样化急诊科基准测试中意外地优于医学微调的对应模型。

Conclusion: 对于急诊科应用，专门对模型进行医学微调可能不是必需的，通用领域的小型语言模型已经能够提供足够的支持能力。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [216] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 研究探索了如何使用思维链推理技术构建可引导的多元化大语言模型，发现强化学习与可验证奖励方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型通常反映统一价值观，限制了其在需要理解细微人类观点任务中的应用，需要支持可引导的多元化能力。

Method: 研究了多种方法：思维链提示、基于人工思维链的微调、基于合成解释的微调、以及强化学习与可验证奖励方法。

Result: 强化学习与可验证奖励方法在所有方法中表现最优，并展现出强大的训练样本效率。

Conclusion: 思维链推理技术可以有效构建可引导的多元化模型，其中强化学习与可验证奖励是最有前景的方法。

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [217] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 扩散语言模型在有限数据条件下表现出显著的数据效率，研究发现随机掩码输入标记是主要原因，类似效果可通过MLP dropout和权重衰减实现，表明随机正则化在多轮训练中广泛提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 研究扩散语言模型在有限数据约束下实现卓越数据效率的潜在机制，目前这些机制尚不明确。

Method: 进行广泛的消融实验，分离不同因素对数据效率的贡献，特别关注随机掩码、MLP dropout和权重衰减的作用。

Result: 随机掩码输入标记在提升数据效率中起主导作用，类似效果可通过MLP dropout和权重衰减获得，证明随机正则化是多轮训练中数据效率提升的通用机制。

Conclusion: 随机正则化是扩散语言模型在有限数据条件下实现高数据效率的关键机制，这一发现为优化数据效率提供了新的视角和方法。

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [218] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: PoLi-RL是一个新颖的点对列表强化学习框架，通过两阶段课程学习成功将强化学习应用于条件语义文本相似性任务，在C-STS基准上创造了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有C-STS方法主要局限于判别模型，未能充分利用LLM和RL的最新进展。RL特别适合此任务，因为它可以直接优化不可微的Spearman排序指标并指导推理过程。

Method: 提出PoLi-RL框架，采用两阶段课程学习：先用简单的点对奖励训练基础评分能力，然后转向结合点对、配对和列表目标的混合奖励。关键创新是并行切片排序奖励机制，在并行切片中计算排序奖励。

Result: 在官方C-STS基准上，PoLi-RL实现了48.18的Spearman相关系数，为交叉编码器架构建立了新的SOTA。

Conclusion: 这是首个成功将RL应用于C-STS的工作，为在复杂、基于排序的条件判断任务上训练LLM引入了强大而精确的范式。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [219] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Caco是一个通过代码驱动增强来自动合成高质量、可验证、多样化推理数据的框架，它使用代码执行验证和规则过滤来确保逻辑正确性，并将验证后的代码推理反向转换为自然语言指令，从而提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理方法存在生成不可控、质量不足和推理路径多样性有限的问题，而基于代码的推理方法通常局限于预定义的数学问题，缺乏可扩展性和泛化性。

Method: Caco首先在统一的代码格式上微调代码推理生成器，然后通过代码执行验证和规则过滤确保逻辑正确性和结构多样性，最后将验证后的代码推理反向转换为自然语言指令和语言推理路径。

Result: 在创建的Caco-1.3M数据集上的实验表明，Caco训练的模型在数学推理基准测试中表现优异，超越了现有强基线方法。

Conclusion: Caco建立了一种无需人工干预、构建自持续可信推理系统的范式，其代码锚定验证和指令多样性有助于在未见任务上实现优越的泛化能力。

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [220] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本研究从概念映射、隐喻-字面知识库和句法敏感性三个角度分析LLMs的隐喻处理能力，发现LLMs存在15%-25%的概念无关解释，依赖训练数据中的隐喻指示器而非上下文线索，对句法不规则性比结构理解更敏感。


<details>
  <summary>Details</summary>
Motivation: 隐喻分析是受语境和外部因素影响的复杂语言现象。虽然LLMs在知识整合、上下文推理和创造性生成方面表现出先进能力，但其隐喻理解机制仍未被充分探索。

Method: 从三个角度研究LLMs的隐喻处理能力：(1)概念映射：使用嵌入空间投影评估LLMs如何在目标域中映射概念；(2)隐喻-字面知识库：分析隐喻词及其字面对应词以识别内在隐喻知识；(3)句法敏感性：评估隐喻句法结构如何影响LLMs性能。

Result: LLMs生成15%-25%概念无关的解释，依赖训练数据中的隐喻指示器而非上下文线索，对句法不规则性比结构理解更敏感。

Conclusion: 这些发现突显了LLMs在隐喻分析中的局限性，并呼吁开发更强大的计算方法。

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [221] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: 本文介绍了一个包含斯里兰卡议会记录、法律判决、政府出版物、新闻和旅游统计的开放机器可读文档数据集集合，涵盖僧伽罗语、泰米尔语和英语，支持计算语言学、法律分析等研究。


<details>
  <summary>Details</summary>
Motivation: 为支持计算语言学、法律分析、社会政治研究和多语言自然语言处理的研究，提供斯里兰卡多语言文档资源。

Method: 构建数据收集管道，从多个来源收集文档数据，以机器可读格式整理，并在GitHub和Hugging Face上每日更新和镜像。

Result: 截至v20251005，该集合包含13个数据集，共215,670个文档（60.3 GB），涵盖僧伽罗语、泰米尔语和英语。

Conclusion: 这些资源为研究社区提供了宝贵的多语言文档数据集，同时讨论了许可和伦理考虑。

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [222] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 开发了一种通用方法，通过准备文化相关数据集和后训练Gemma 2模型，提升该模型在代表性不足语言上的性能，以促进生成式AI在不同国家的应用和文化保护。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要基于英语文本和文化训练，导致在其他语言和文化背景下表现不佳，缺乏文化包容性。

Method: 开发了准备文化相关数据集的通用方法，并对Gemma 2模型进行后训练。

Result: 提高了Gemma 2在代表性不足语言上的性能表现。

Conclusion: 该方法可帮助其他国家解锁生成式AI的潜力并保护其文化遗产。

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [223] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 为僧伽罗语成人阅读障碍者开发的多模态辅助系统，整合语音转文本、错误识别、文本校正和语音合成功能


<details>
  <summary>Details</summary>
Motivation: 解决成人阅读障碍在非英语环境（特别是低资源语言僧伽罗语）中研究不足和服务缺乏的问题

Method: 集成Whisper进行语音转文本，使用SinBERT识别常见阅读障碍错误，结合mT5和Mistral模型生成校正文本，最后用gTTS转回语音

Result: 在僧伽罗语数据集有限的情况下，系统达到0.66的转录准确率、0.7的校正准确率和0.65的整体系统准确率

Conclusion: 证明了该方法在低资源语言环境下的可行性和有效性，强调了包容性NLP技术对少数语言的重要性

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [224] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出SSD（自推测解码）方法，利用扩散大语言模型自身作为推测解码的起草器和验证器，实现无损推理加速，在单次前向传播中验证多个token，达到3.46倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 当前并行解码方法的生成结果与逐步解码存在偏差，导致性能下降，限制了实际部署。需要解决扩散大语言模型在并行生成时的性能退化问题。

Method: SSD方法引入自起草机制，模型为多个位置生成预测，然后通过分层验证树在单次前向传播中进行验证，无需辅助模块，利用dLLM固有的多位置并行预测能力。

Result: 实验显示SSD在开源模型LLaDA和Dream上实现了最高3.46倍的加速，同时保持输出与逐步解码完全相同。

Conclusion: SSD是一种有效的无损推理加速方法，通过自推测解码机制解决了并行解码的性能偏差问题，为扩散大语言模型的实用部署提供了可行方案。

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [225] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: LTPO是一个无需参数更新的框架，通过将中间潜在思想向量作为动态参数进行优化，使用基于置信度的内在奖励信号来增强LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法在具有挑战性的分布外任务中表现脆弱，而稳健推理在这些任务中最为关键

Method: LTPO将中间潜在思想向量视为动态参数，采用在线策略梯度方法，使用基于冻结LLM输出分布计算的内在置信度奖励信号

Result: 在五个推理基准测试中，LTPO不仅匹配或超越强基线，在标准任务上表现优异，在极具挑战性的AIME基准上，现有潜在推理方法准确率接近零时，LTPO仍能提供显著改进

Conclusion: LTPO展示了在复杂推理任务上的独特能力，特别是在其他方法失效的情况下仍能保持稳健性能

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [226] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: CALM框架通过渐进式修正提示来优化大型推理模型在优化建模任务中的表现，生成的STORM模型在5个基准测试中达到68.9%的平均准确率，匹配671B参数模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有领域适应方法无法充分利用现代大型推理模型的高级推理能力，直接微调传统非反思数据集效果有限。

Method: 提出CALM框架：专家识别推理缺陷并提供修正提示，模型据此改进推理轨迹，修改少于2.6%的生成标记，通过监督微调和强化学习进行软适应。

Result: 基于CALM开发的4B参数STORM模型在5个优化建模基准测试中达到68.9%的平均准确率，与671B参数模型性能相当。

Conclusion: 基于提示的动态数据合成能够保持并增强现代大型推理模型的原生推理模式，为挑战性优化建模任务提供更有效和可扩展的路径。

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [227] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 提出了REPO强化学习框架，通过整合偏好奖励模型、说服行为奖励和程序化奖励函数，优化LLM在在线旅行社价格谈判中的表现，显著提升对话质量和约束遵守率。


<details>
  <summary>Details</summary>
Motivation: 传统后训练方法在部署LLM作为商务谈判代理时存在过度拟合脚本、忽视微妙说服风格、无法强制执行可验证业务约束的问题。

Method: REPO框架结合三种奖励：偏好训练的奖励模型用于密集人类对齐，奖励法官用于高级说服行为和SOP合规性，程序化奖励函数用于数值、格式和防护栏的确定性检查。

Result: 在真实对话和精选坏案例对话评估中，REPO将平均对话评分提升至4.63，比基准高1.20，比DPO高0.83；将至少有一个优秀回复的对话比例提升至66.67%，坏案例修复率达到93.33%。

Conclusion: REPO框架在商务谈判任务中显著优于SFT、DPO、PPO和GRPO等现有方法，并展现出超越黄金标注的新兴能力。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [228] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 该论文提出了语言混合思维链（Language-Mixed CoT）方法，在英语和目标语言之间切换推理，以英语为锚点提升推理能力。针对韩语进行了案例研究，构建了Yi-Sang数据集，训练了多个模型，其中KO-REAson-35B在9个基准测试中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语的推理能力蒸馏，对语言特定推理了解甚少。需要填补这一研究空白，探索多语言环境下的推理能力提升。

Method: 提出语言混合思维链推理模式，在英语和目标语言间切换；构建韩语数据集Yi-Sang（包含579万条韩语提示和370万条长推理轨迹）；训练9个不同规模的模型（4B-35B）。

Result: 最佳模型KO-REAson-35B在9个基准测试中取得最高平均分（64.0±25），在5/9个测试中排名第一，其余排名第二。中小型模型平均提升18.6分，语言混合CoT比单语CoT更有效。

Conclusion: 语言混合思维链方法能有效提升语言特定推理能力，并带来跨语言和多模态性能增益。发布了数据管道、评估系统、数据集和模型以推动相关研究。

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [229] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 提出了LongTail-Swap基准测试，专注于评估语言模型在罕见词上的学习能力，揭示了不同架构在长尾分布上的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有BabyLM挑战主要关注词分布头部，而儿童学习语言时特别擅长从少量数据中学习新词，因此需要评估模型在罕见词上的学习能力。

Method: 构建了LT-Swap基准测试集，包含可接受与不可接受的句子对，专门针对罕见词的语义和句法使用进行测试，采用零样本方式评估模型。

Result: 评估了16个BabyLM排行榜模型，发现语言模型在罕见词上表现较差，且不同架构在长尾分布上的性能差异比头部更明显。

Conclusion: LT-Swap基准提供了评估模型罕见词泛化能力的新视角，揭示了哪些架构更擅长处理长尾分布。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [230] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [231] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: SliceMoE是一种改进的混合专家架构，通过将token的隐藏向量分割成切片，并在切片级别进行路由，解决了传统token级路由的容量瓶颈、负载均衡问题和有限专业化问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE层在token级别进行路由，将整个语义谱分配给每个专家，导致容量瓶颈、负载均衡问题和有限的专业化。需要更细粒度的路由机制来改善这些问题。

Method: 将d维嵌入分割成S个切片，每个切片通过轻量级共享路由器预测top-k专家。专家独立处理分配的切片，输出重新组装，保持每个token的FLOP效率。采用切片级容量损失、跨切片dropout和高效融合批量GEMM内核。

Result: 在WikiText-103语言建模、WMT En-De翻译和三个文本分类数据集上的实验显示，SliceMoE比密集基线推理速度快1.7倍，比参数匹配的token-MoE困惑度低12-18%，专家平衡性更好，并在句法与语义子空间上具有可解释的专业知识。

Conclusion: SliceMoE通过切片级路由有效解决了传统MoE的局限性，实现了更好的性能、效率和专家专业化。

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [232] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 提出了一种结合机器学习和深度学习的混合方法，用于波斯语方面级情感分析，通过整合多语言BERT的极性得分特征，在Pars-ABSA数据集上达到93.34%的准确率，超越了现有基准。


<details>
  <summary>Details</summary>
Motivation: 波斯语情感分析面临标注数据集稀缺、预处理工具有限、高质量嵌入和特征提取方法缺乏等挑战，需要开发有效的解决方案来提升低资源语言的情感分析性能。

Method: 采用混合方法整合机器学习和深度学习技术，利用多语言BERT的极性得分作为额外特征，结合决策树分类器，并引入波斯语同义词和实体词典进行文本增强。

Result: 在Pars-ABSA数据集上实现了93.34%的准确率，超越了现有基准，证明了混合建模和特征增强在低资源语言情感分析中的有效性。

Conclusion: 混合建模方法和特征增强策略能够有效提升波斯语等低资源语言的情感分析性能，为类似语言的NLP任务提供了有价值的参考。

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [233] [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
*Lingnan Xu,Chong Feng,Kaiyuan Zhang,Liu Zhengyong,Wenqiang Xu,Fanqing Meng*

Main category: cs.CL

TL;DR: 提出了RDR2框架，通过显式整合文档结构信息来增强检索增强生成系统，利用LLM路由器动态导航文档结构树，实现最优化证据组合。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法将检索到的段落视为孤立块，忽略了文档组织结构这一关键信息，导致在复杂场景下知识获取和利用能力受限。

Method: RDR2框架包含三个核心组件：检索、文档路由和阅读。使用LLM路由器动态导航文档结构树，联合评估内容相关性和层次关系，将文档路由制定为可训练任务。

Result: 在五个具有挑战性的数据集上进行了全面评估，RDR2实现了最先进的性能，特别是在需要多文档合成的复杂场景中表现优异。

Conclusion: 显式结构感知显著提升了RAG系统获取和利用知识的能力，证明了文档结构信息在检索增强生成中的重要性。

Abstract: While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

</details>


### [234] [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
*Thomas F Burns*

Main category: cs.CL

TL;DR: 提出了Distributional Correctness Score (DCS)评估指标，通过考虑模型在答案选择上的完整概率分布来区分有害的过度自信和通过弃权表达的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估范式只关注单个响应的准确性评分，无法捕捉模型完整的信念状态，导致模型因优化二元评分方案而产生幻觉。

Method: 引入DCS评估指标，通过理论分析和示例展示其区分过度自信和不确定性的能力，并将12个现有评估基准适配到DCS变体，在6个语言模型上测量性能。

Result: 半数测试基准在所有模型上都显示负分，表明存在显著的幻觉倾向。DCS提供了更细致和对齐的评估范式。

Conclusion: DCS能够激励模型表达真正的不确定性而非猜测，提供比传统评估方法更丰富的模型信念状态分析。

Abstract: Common evaluation paradigms for language models focus on scoring single
responses through accuracy metrics or proper scoring rules, failing to capture
the full richness of a model's belief state. Recent work illustrates that
language models hallucinate in-part because they are optimised to be good
test-takers under binary scoring schemes that reward any answer over
abstention. While this insight naturally leads to penalty-based approaches,
they ignore crucial distinctions in how models distribute uncertainty, for
example between hedging toward incorrect answers versus hedging toward "I don't
know" responses. A novel evaluation metric, the Distributional Correctness
Score (DCS), is introduced to solve this problem, i.e., of not considering a
model's entire probability distribution over answer choices. DCS naturally
distinguishes between harmful overconfidence in wrong answers and uncertainty
expressed through abstention, providing scores in an interpretable default
range. Through theoretical analysis and illustrative examples, DCS is
demonstrated to offer a more nuanced and aligned evaluation paradigm that
incentivises models to express genuine uncertainty rather than guessing.
Adapting 12 existing evaluation benchmarks to DCS's variants and measuring
performance on six language models reveals that for half of the tested
benchmarks scores are negative across all tested models, indicating significant
tendencies towards hallucination.

</details>


### [235] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: 论文发现安全对齐的大语言模型存在后果盲视问题，即模型无法正确推理行动与后果的关联，过度依赖表面形式信号。作者构建了CB-Bench基准来评估这一问题，并提出了CS-Chain-4k数据集来缓解后果盲视。


<details>
  <summary>Details</summary>
Motivation: 当前安全对齐的LLMs存在两种主要失败模式：容易被越狱攻击，以及对无害输入过度拒绝。作者认为这两种问题都源于模型对行动与后果关联的推理能力不足，过度依赖表面形式信号。

Method: 构建了CB-Bench基准，涵盖四种风险场景，评估模型在语义风险与后果风险匹配和不匹配情况下的表现。同时开发了CS-Chain-4k数据集，用于训练模型进行后果推理。

Result: 主流模型在CB-Bench上表现不佳，无法区分语义风险和后果风险，显示出系统性的后果盲视问题。使用CS-Chain-4k微调的模型在语义伪装越狱攻击上表现更好，减少了对无害输入的过度拒绝，同时在其他基准上保持了效用和泛化能力。

Conclusion: 后果盲视是当前对齐方法的核心限制，后果感知推理应成为对齐的核心目标。该研究为更实用和可复现的安全评估提供了路径。

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [236] [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
*Mathieu Laï-king,Patrick Paroubek*

Main category: cs.CL

TL;DR: 本文测试了大型语言模型使用CONSORT标准评估临床试验研究报告质量的能力，创建了CONSORT-QA评估语料库，最佳模型组合达到85%准确率。


<details>
  <summary>Details</summary>
Motivation: 临床试验研究报告质量会影响临床决策，需要评估大型语言模型使用CONSORT标准评估这类文章报告质量的能力。

Method: 创建CONSORT-QA评估语料库，评估不同大型生成语言模型（通用领域或生物医学领域适应）使用不同提示方法（包括思维链）正确评估CONSORT标准的能力。

Result: 最佳模型和提示方法组合达到85%的准确率。使用思维链方法为模型完成任务提供了有价值的推理信息。

Conclusion: 大型语言模型能够有效评估临床试验研究报告质量，思维链提示方法有助于理解模型的推理过程。

Abstract: Reporting quality is an important topic in clinical trial research articles,
as it can impact clinical decisions. In this article, we test the ability of
large language models to assess the reporting quality of this type of article
using the Consolidated Standards of Reporting Trials (CONSORT). We create
CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality
with CONSORT-abstract standards. We then evaluate the ability of different
large generative language models (from the general domain or adapted to the
biomedical domain) to correctly assess CONSORT criteria with different known
prompting methods, including Chain-of-thought. Our best combination of model
and prompting method achieves 85% accuracy. Using Chain-of-thought adds
valuable information on the model's reasoning for completing the task.

</details>


### [237] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: 提出接种提示方法，通过在微调数据前添加简短系统提示指令来选择性抑制不良特征，测试时不使用该指令，有效降低不良特征表达。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调时常会同时学习到期望和不期望的特征，需要一种方法能够选择性地抑制不期望特征的学习。

Method: 在微调数据前添加系统提示指令，故意引发不期望特征，使模型在学习过程中降低对该特征的依赖。

Result: 接种方法在多个场景中有效：减少任务特定微调中的突发错位、防御后门注入、缓解潜意识学习中的特征传递。

Conclusion: 接种提示是一种简单有效的选择性学习方法，通过降低特征的意外性来减少优化压力，从而限制不良特征的泛化。

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [238] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: 该论文提出了一种基于注意力机制和梯度信息的推理时防御方法，用于检测预训练语言模型中的后门攻击，通过结合token级别的注意力和梯度信息构建异常分数，显著降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型虽然在各种NLP任务中表现出色，但容易受到后门攻击的威胁。攻击者通过在训练数据中嵌入触发模式，使模型在正常使用时表现正常，但在触发激活时产生定向错误分类。

Method: 提出了一种推理时防御方法，通过分析后门模型在处理中毒输入时的内部行为特征，结合token级别的注意力分布和梯度信息构建异常分数来检测后门攻击。

Result: 在多种文本分类任务和不同后门攻击场景下的广泛实验表明，该方法相比现有基线方法显著降低了攻击成功率。

Conclusion: 该方法不仅有效防御后门攻击，还通过可解释性分析揭示了触发定位机制和防御方法的鲁棒性，为理解后门攻击的内部机制提供了新的视角。

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [239] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: 提出Con-RAG系统，通过PS-GRPO强化学习方法解决RAG系统在语义等价查询下输出不一致的问题，显著提升一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: RAG系统在高风险领域部署时，用户期望语义等价查询的输出保持一致，但现有系统因检索器和生成器的变异性导致显著不一致，影响可信度和可靠性。

Method: 引入评估框架分解RAG一致性为检索器级、生成器级和端到端组件；提出PS-GRPO强化学习方法，利用释义集合的群体相似性奖励训练生成器；开发可扩展的近似奖励计算方法。

Result: 在短格式、多跳和长格式QA基准测试中，Con-RAG相比强基线显著提升了一致性和准确性，即使在没有显式真值监督的情况下也表现良好。

Conclusion: 为安全关键部署提供了评估和构建可靠RAG系统的实用解决方案，通过一致性优化增强了系统的可信度。

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [240] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: 提出了第一个大规模的后编辑时间标注数据集，并引入PEET评分器来评估语法纠错工具的用户友好性，通过估计后编辑时间来量化工具节省的用户工作量。


<details>
  <summary>Details</summary>
Motivation: 在文本编辑过程中，高效的语法纠错工具可以显著影响后续人工编辑工作和最终文本质量，需要量化GEC工具的用户友好性。

Method: 创建了包含后编辑时间标注和修正的大规模数据集，引入PEET评分器来估计后编辑时间，并分析不同编辑类型对后编辑时间的影响。

Result: 量化了GEC工具在文本编辑中节省的时间，发现判断句子是否需要修正以及释义和标点符号更改等编辑类型对后编辑时间影响最大。

Conclusion: PEET与人工技术努力判断相关性良好，为评估GEC工具用户友好性提供了新的人本中心方向。

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [241] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: SECA提出了一种通过语义等价且连贯的提示修改来引发LLM幻觉的对抗攻击方法，相比现有方法产生更现实的攻击提示。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法通常产生不现实的提示（如插入乱码或改变原意），无法反映实际中幻觉如何发生。需要探索在保持语义等价和连贯性的前提下引发LLM幻觉的现实攻击方法。

Method: 将寻找现实攻击制定为输入提示空间上的约束优化问题，引入保持约束的零阶优化方法搜索对抗性提示，确保语义等价和连贯性约束。

Result: 在开放式多选题任务上，SECA相比现有方法获得更高的攻击成功率，同时几乎不违反约束条件。

Conclusion: SECA揭示了开源和商业LLM对现实且合理的提示变异的敏感性，为理解LLM在实际部署中的可靠性提供了重要见解。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [242] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: 研究探索了大型语言模型生成文本是否保留语义同位素，通过故事续写实验发现LLM在给定token范围内能够保持语义同位素。


<details>
  <summary>Details</summary>
Motivation: 扩展先前关于分布语义学和结构语义学联系的研究，探究LLM生成文本是否保持语义同位素。

Method: 设计故事续写实验，使用10,000个ROCStories提示由5个LLM完成，验证GPT-4o提取同位素能力并分析结构和语义属性。

Result: LLM在给定token范围内完成文本时，能够跨多个属性保留语义同位素。

Conclusion: 大型语言模型在限定范围内生成文本时能够有效保持语义同位素结构。

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [243] [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
*Grace LeFevre,Qingcheng Zeng,Adam Leif,Jason Jewell,Denis Peskoff,Rob Voigt*

Main category: cs.CL

TL;DR: 本研究从作者和会议层面分析NLP4SG（自然语言处理促进社会公益）领域，发现ACL作者在非ACL会议上更可能从事社会公益研究，且大多数NLP4SG工作由非ACL作者在非ACL会议上发表。


<details>
  <summary>Details</summary>
Motivation: 随着NLP社会影响日益重要，NLP4SG研究快速增长（ACL文集近20%论文涉及联合国可持续发展目标），需要了解该领域的整体格局和ACL社区的角色。

Method: 采用作者和会议层面的视角，量化ACL社区内外、ACL核心贡献者和非ACL作者在NLP4SG工作上的比例分布。

Result: 发现两个令人惊讶的事实：1）ACL作者在非ACL会议上发表时更可能从事社会公益研究；2）大多数NLP4SG工作由非ACL作者在非ACL会议上完成。

Conclusion: 这些发现对ACL社区在NLP4SG议程设置方面具有重要启示意义，需要重新思考社区定位和参与策略。

Abstract: The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

</details>


### [244] [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
*Lucie Kunitomo-Jacquin,Edison Marrese-Taylor,Ken Fukuda*

Main category: cs.CL

TL;DR: 本文主张在大型语言模型的不确定性量化中考虑未观测序列的概率，这对提升不确定性量化方法至关重要。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，准确量化大型语言模型的不确定性对于识别错误答案（幻觉）非常重要。现有基于熵估计的方法主要依赖观测到的输出序列，但忽略了未观测序列的概率。

Method: 通过实验证明未观测序列概率在不确定性量化中的关键作用，建议未来研究将其整合到LLM不确定性量化方法中。

Result: 实验结果表明未观测序列概率对提升不确定性量化方法的准确性具有重要作用。

Conclusion: 未来研究应该将未观测序列的概率整合到大型语言模型的不确定性量化方法中，以提高其准确性和可靠性。

Abstract: Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

</details>


### [245] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: 提出了一种动态整合监督微调(SFT)和强化学习(RL)的即插即用框架，通过选择挑战性样本进行SFT，显著减少数据需求并避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有方法结合SFT和RL面临数据效率低、算法特定设计和灾难性遗忘三大挑战，需要一种更高效的整合方案。

Method: 动态选择挑战性样本进行SFT，选择高熵token计算损失，并冻结对RL重要的参数。

Result: 仅使用先前SOTA方法1.5%的SFT数据和20.4%的RL数据，就实现了最先进的推理性能。

Conclusion: 该方法为推理后训练中结合SFT和RL提供了高效且即插即用的解决方案。

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [246] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: CCA是一种新的注意力机制，通过在共享潜在空间中进行注意力操作，显著减少参数、KV缓存和FLOPs。结合GQA形成CCGQA，在保持质量的同时实现更好的计算-带宽权衡。


<details>
  <summary>Details</summary>
Motivation: 多头注意力(MHA)的二次计算复杂度和线性增长的KV缓存使得长上下文transformer训练和服务成本高昂。现有方法如GQA和MLA主要优化缓存，但对计算效率提升有限。

Method: 引入压缩卷积注意力(CCA)，将查询、键和值降维投影到共享潜在空间，并在该空间内执行完整的注意力操作。结合GQA形成CCGQA，实现计算和内存的双重优化。

Result: CCGQA在相同KV缓存压缩下优于GQA和MLA，在MoE模型上以GQA和MLA一半的KV缓存实现8倍压缩且性能不下降。在H100 GPU上，预填充延迟减少约1.7倍，反向传播加速约1.3倍。

Conclusion: CCA和CCGQA通过降低注意力计算成本，显著加速训练和预填充，为长上下文transformer提供了高效的计算和内存解决方案。

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [247] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: PsySET是一个心理学基准，用于评估LLM在情感和人格领域的引导效果和可信度，发现不同引导策略各有优劣，情感引导可能带来意想不到的副作用。


<details>
  <summary>Details</summary>
Motivation: 为了在社交交互环境中实现丰富的人本交互，需要控制LLM的情感状态和人格特质。

Method: 使用PsySET基准评估四种不同LLM模型，结合提示、微调和表示工程等引导策略。

Result: 提示方法持续有效但强度控制有限，向量注入实现更精细控制但略微降低输出质量。情感引导可能降低对抗性事实的鲁棒性、隐私意识和增加偏好偏见。

Conclusion: 该框架首次建立了情感和人格引导的整体评估，为社交交互应用的可解释性和可靠性提供了见解。

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [248] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest是一个基于大语言模型的生成式文字冒险游戏，通过沉浸式互动故事促进第二语言学习。系统为EFL学习者提供协作式"选择你自己的冒险"叙事，根据学习者选择动态生成内容，包含分支决策点和故事里程碑等游戏机制。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型创建沉浸式语言学习环境，通过互动故事游戏提高EFL学习者的语言学习效果和参与度。

Method: 开发基于LLM的生成式文字冒险游戏，包含分支决策、故事里程碑、根据学习者水平定制内容、词汇助手提供上下文解释等功能。

Result: 对中国大学生EFL学习者的试点研究表明，该系统在词汇增长方面表现良好，用户反馈积极，但参与者建议改进叙事长度和质量，并增加多模态内容如图像。

Conclusion: GenQuest展示了利用生成式AI创建沉浸式语言学习环境的潜力，未来可改进叙事质量和增加多模态内容以增强学习体验。

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [249] [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
*Dang Anh,Rick Nouwen,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究LLMs在歧义和非歧义语境中如何表示和解释复数指称，发现LLMs有时能识别歧义代词的潜在指称对象，但在选择解释时并不总是遵循人类偏好，且难以在没有直接指令的情况下识别歧义。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否展现类似人类的复数指称偏好，以及能否检测复数照应表达的歧义性并识别可能的指称对象。

Method: 设计实验：使用下一个词预测任务进行代词生成、代词解释，以及使用不同提示策略进行歧义检测。

Result: LLMs有时能识别歧义代词的潜在指称对象，但在选择解释时不总是遵循人类偏好（特别是当可能解释未被明确提及时），且难以在没有直接指令的情况下识别歧义。不同实验类型的结果存在不一致性。

Conclusion: LLMs在复数指称的理解和生成方面与人类存在差异，特别是在歧义识别和解释偏好方面，且不同实验方法的结果不一致。

Abstract: Our goal is to study how LLMs represent and interpret plural reference in
ambiguous and unambiguous contexts. We ask the following research questions:
(1) Do LLMs exhibit human-like preferences in representing plural reference?
(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and
identify possible referents? To address these questions, we design a set of
experiments, examining pronoun production using next-token prediction tasks,
pronoun interpretation, and ambiguity detection using different prompting
strategies. We then assess how comparable LLMs are to humans in formulating and
interpreting plural reference. We find that LLMs are sometimes aware of
possible referents of ambiguous pronouns. However, they do not always follow
human reference when choosing between interpretations, especially when the
possible interpretation is not explicitly mentioned. In addition, they struggle
to identify ambiguity without direct instruction. Our findings also reveal
inconsistencies in the results across different types of experiments.

</details>


### [250] [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
*Fernando López,Santosh Kesiraju,Jordi Luque*

Main category: cs.CL

TL;DR: 研究发现大型音频语言模型在多项选择题评估中对选项顺序、问题表述和选项改写高度敏感，提出了更稳健的评估协议和指标。


<details>
  <summary>Details</summary>
Motivation: 现有MCQA评估框架仅报告单一准确率，但细微变化（如选项顺序调整）会导致结果显著差异，需要更全面的评估方法。

Method: 系统研究三个基准（MMAU、MMAR、MMSU）和四个模型，分析模型对选项顺序、问题改写和选项改写的敏感性。

Result: 模型对选项顺序、问题表述和选项改写高度敏感，现有评估方法无法捕捉这种变异性。

Conclusion: 提出了考虑细微变化的更简单评估协议和指标，为MCQA框架下的LALMs提供更详细的评估报告。

Abstract: Recent advances in large audio language models (LALMs) have primarily been
assessed using a multiple-choice question answering (MCQA) framework. However,
subtle changes, such as shifting the order of choices, result in substantially
different results. Existing MCQA frameworks do not account for this variability
and report a single accuracy number per benchmark or category. We dive into the
MCQA evaluation framework and conduct a systematic study spanning three
benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio
Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings
indicate that models are sensitive not only to the ordering of choices, but
also to the paraphrasing of the question and the choices. Finally, we propose a
simpler evaluation protocol and metric that account for subtle variations and
provide a more detailed evaluation report of LALMs within the MCQA framework.

</details>


### [251] [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
*Guochen Yan,Luyuan Xie,Qingni Shen,Yuejian Fang,Zhonghai Wu*

Main category: cs.CL

TL;DR: FedSRD是一个通信高效的联邦学习框架，通过稀疏化-重构-分解方法显著降低LoRA参数传输开销，在异构客户端数据上减少90%通信成本的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前在公开网络数据上训练大语言模型的方式不可持续，高质量专业领域数据接近枯竭。联邦学习虽然能利用分布式私有数据进行隐私保护的协同微调，但LoRA在联邦设置中的通信开销仍然是关键瓶颈。

Method: 提出FedSRD框架：1)重要性感知稀疏化方法减少上传参数数量；2)服务器在全秩空间重构和聚合更新以缓解冲突；3)将全局更新分解为稀疏低秩格式进行广播。还提出了计算开销更低的变体FedSRD-e。

Result: 在10个基准测试上的实验结果表明，该框架显著降低通信成本达90%，同时在异构客户端数据上甚至提升了模型性能。

Conclusion: FedSRD有效解决了联邦学习中LoRA参数的结构冗余和通信瓶颈问题，为下一代去中心化网络上的AI提供了实用的通信高效解决方案。

Abstract: The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

</details>


### [252] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: 本文提出了一个评估框架，用于评估大语言模型在检测英语文本中针对人口统计的社会偏见的能力，发现微调的小型模型在可扩展检测方面有潜力，但在多人口统计偏见检测方面仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 大规模网络爬取的文本语料库通常包含有害的人口统计目标社会偏见，需要数据审计和可扩展的偏见检测方法。现有研究范围狭窄，缺乏对LLMs自动偏见检测能力的全面理解。

Method: 构建了一个针对英语文本的全面评估框架，将偏见检测定义为多标签任务，使用人口统计聚焦的分类法，系统评估了不同规模和技术的模型，包括提示、上下文学习和微调。

Result: 使用12个涵盖不同内容类型和人口统计的数据集，研究表明微调的小型模型在可扩展检测方面具有潜力，但分析也揭示了在人口统计轴和多人口统计目标偏见方面的持续差距。

Conclusion: 需要更有效和可扩展的审计框架来解决多人口统计偏见检测的挑战。

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [253] [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
*Yuheng Li,Jiechao Gao,Wei Han,Wenwen Ouyang,Wei Zhu,Hui Yi Leong*

Main category: cs.CL

TL;DR: 提出了PI-LoRA方法，通过集成梯度路径信息自动从临床指南中提取医疗决策树，显著优于现有参数高效微调方法


<details>
  <summary>Details</summary>
Motivation: 当前医疗决策树构建方法依赖耗时的人工标注，需要自动化解决方案来支持临床决策系统

Method: PI-LoRA（路径集成LoRA）方法，集成梯度路径信息捕获模块间协同效应，实现更有效的秩分配，关键模块获得适当秩分配，次要模块被剪枝

Result: 在医疗指南数据集上的实验表明，PI-LoRA在Text2MDT任务中显著优于现有参数高效微调方法，准确率更高且模型复杂度大幅降低

Conclusion: 该方法实现了最先进的结果，同时保持轻量级架构，特别适合计算资源有限的临床决策支持系统

Abstract: Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

</details>


### [254] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文提出基于核心焦点引导的优化框架，通过提取忠实于原文的核心焦点、构建微调数据集和多维度质量评估，显著提升医疗问题摘要任务中焦点识别能力并减少幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 在线医疗平台快速发展，消费者健康问题存在信息冗余和非专业术语，导致诊断效率低下。现有医疗问题摘要方法在问题焦点识别和模型幻觉方面仍面临挑战。

Method: 提出核心焦点引导优化框架：1)设计提示模板驱动LLMs提取忠实原文的核心焦点；2)结合原始CHQ-FAQ对构建微调数据集；3)提出多维度质量评估与选择机制。

Result: 在两个广泛采用的MQS数据集上使用三个评估指标进行实验，所提框架在所有指标上均达到最先进性能，显著提升了模型识别问题关键焦点的能力并明显缓解了幻觉问题。

Conclusion: 基于核心焦点引导的优化框架有效解决了医疗问题摘要任务中的焦点识别偏差和幻觉问题，为LLMs在该领域的应用提供了有效解决方案。

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [255] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: 提出MATPO方法，在单个LLM中实现多智能体工具集成规划，通过强化学习训练规划者和工作者角色，提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法存在上下文长度限制和工具响应噪声问题，而多智能体框架缺乏有效的强化学习训练方法。

Method: MATPO通过角色特定提示在单个LLM实例中训练规划者和工作者角色，采用基于原则的信用分配机制。

Result: 在GAIA-text、WebWalkerQA和FRAMES数据集上，MATPO相比单智能体基线平均性能提升18.38%，对噪声工具输出更具鲁棒性。

Conclusion: 在单个LLM中统一多智能体角色是有效的，为稳定高效的多智能体强化学习训练提供了实用见解。

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [256] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: TiTok是一个新的框架，通过token级知识转移实现有效的LoRA移植，无需额外模型即可在不同骨干模型间传输适配参数。


<details>
  <summary>Details</summary>
Motivation: 解决PEFT方法如LoRA中适配参数依赖于基础模型且无法跨不同骨干模型传输的问题，避免知识蒸馏对训练数据的依赖和TransLoRA需要额外判别器模型的复杂性。

Method: 通过对比源模型在有和没有LoRA时的差异，捕获任务相关信息，突出信息丰富的token并选择性过滤合成数据。

Result: 在三个基准测试的多个传输设置中，该方法始终有效，相比基线平均性能提升4~8%。

Conclusion: TiTok框架能够有效实现LoRA移植，无需额外模型开销，在不同骨干模型间实现参数高效传输。

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [257] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: 分析了MoE架构在多语言数据中的稀疏路由动态，发现在中间层存在跨语言路由对齐现象，并提出了通过干预路由器来提升多语言性能的方法。


<details>
  <summary>Details</summary>
Motivation: 理解MoE架构在多语言数据中的稀疏路由动态，探索如何通过路由干预来提升多语言性能。

Method: 使用并行多语言数据集分析专家路由模式，提出在推理时通过促进中间层任务专家来引导路由器的方法。

Result: 干预方法在多个评估任务、模型和15+语言上实现了1-2%的稳定性能提升，而其他干预策略则导致性能下降。

Conclusion: MoE模型处理非英语文本的能力受限于其利用语言通用专家的能力，中间层的跨语言路由对齐对多语言泛化至关重要。

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [258] [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
*Sarel Duanis,Asnat Greenstein-Messica,Eliya Habba*

Main category: cs.CL

TL;DR: JSON Whisperer框架让LLM生成RFC 6902差异补丁而非完整文档，通过EASE编码解决数组索引问题，减少31%令牌使用且保持编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法为每个编辑重新生成整个JSON结构，计算效率低下。

Method: 提出JSON Whisperer框架生成差异补丁，引入EASE将数组转换为带稳定键的字典以消除索引算术复杂性。

Result: 补丁生成与EASE减少31%令牌使用，编辑质量保持在完整重生成的5%以内，复杂指令和列表操作效果显著。

Conclusion: JSON Whisperer通过差异补丁和EASE编码有效解决了LLM编辑JSON时的计算效率和数组索引问题。

Abstract: Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

</details>


### [259] [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
*Eduardo Martínez Rivera,Filippo Menolascina*

Main category: cs.CL

TL;DR: 提出一种两阶段检索架构，结合ModernBERT进行高效初始检索和ColBERTv2进行精细化重排序，在生物医学领域RAG系统中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中检索模块性能限制问题，通用密集检索器在专业领域语言理解不足，而领域专用模型计算成本过高，需要平衡效率与准确性。

Method: 使用ModernBERT双向编码器进行初始候选检索，结合ColBERTv2延迟交互模型进行细粒度重排序，在PubMedQA数据集上对IR模块进行微调。

Result: ColBERT重排序器将Recall@3提升4.2个百分点，在MIRAGE问答基准的五个任务中达到0.4448的平均准确率，优于MedCPT基线(0.4436)。

Conclusion: 两阶段检索架构在生物医学RAG中表现优异，但性能关键依赖于检索器和重排序器的联合微调过程，否则重排序器可能降低性能。

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

</details>


### [260] [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
*Raha Askari,Sina Zarrieß,Özge Alacam,Judith Sieker*

Main category: cs.CL

TL;DR: 本文提出了一个新颖的基准测试，用于评估在少于1000万和1亿token上预训练的语言模型是否能区分遵守和违反Grice会话准则的话语，并与儿童和大型语言模型的表现进行比较。


<details>
  <summary>Details</summary>
Motivation: 隐含意义是人类交流的重要组成部分，需要语言模型具备识别和解释这些意义的能力。基于Grice的会话准则理论，说话者可能故意违反这些原则来表达字面之外的含义，而听者通过识别这些违反来进行语用推理。

Method: 基于Surian等人对儿童违反Grice准则敏感性的研究，引入新基准测试，比较在少于1000万和1亿token上预训练的BabyLMs在五个准则上的表现，并与儿童和基于3万亿token预训练的LLM进行对比。

Result: 总体而言，在少于1亿token上训练的模型优于在少于1000万token上训练的模型，但仍未达到儿童水平和LLM的能力。适度的数据增加改善了语用行为的某些方面，导致对语用维度更细粒度的区分。

Conclusion: 研究表明，虽然数据量的增加能提升语言模型对Grice准则的理解能力，但小规模预训练模型在语用推理方面仍落后于儿童和大型语言模型，需要进一步改进。

Abstract: Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

</details>


### [261] [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
*Sangmin Bae,Bilge Acun,Haroun Habeeb,Seungyeon Kim,Chien-Yu Lin,Liang Luo,Junjie Wang,Carole-Jean Wu*

Main category: cs.CL

TL;DR: 本文系统评估了基于自注意力机制与Mamba等结构化状态空间模型混合的架构，比较了层间（顺序）和层内（并行）融合策略，分析了影响混合模型性能的关键因素，并提出了最优设计方法。


<details>
  <summary>Details</summary>
Motivation: 虽然混合架构在长上下文任务中展现出良好的建模质量和计算效率平衡，但缺乏对混合策略的系统比较和有效性关键因素的分析，需要为社区提供清晰的指导。

Method: 通过层间（顺序）和层内（并行）融合两种混合策略，从语言建模性能、长上下文能力、扩展分析、训练和推理效率等多个角度进行综合评估，并识别计算原语的核心特征。

Result: 识别了每种混合策略的最关键元素，提出了两种混合模型的最优设计方法，为开发混合语言模型提供了实用指导和宝贵见解。

Conclusion: 综合分析为混合语言模型的开发提供了实践指导和有价值的见解，促进了架构配置的优化。

Abstract: Recent progress in large language models demonstrates that hybrid
architectures--combining self-attention mechanisms with structured state space
models like Mamba--can achieve a compelling balance between modeling quality
and computational efficiency, particularly for long-context tasks. While these
hybrid models show promising performance, systematic comparisons of
hybridization strategies and analyses on the key factors behind their
effectiveness have not been clearly shared to the community. In this work, we
present a holistic evaluation of hybrid architectures based on inter-layer
(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a
variety of perspectives: language modeling performance, long-context
capabilities, scaling analysis, and training and inference efficiency. By
investigating the core characteristics of their computational primitive, we
identify the most critical elements for each hybridization strategy and further
propose optimal design recipes for both hybrid models. Our comprehensive
analysis provides practical guidance and valuable insights for developing
hybrid language models, facilitating the optimization of architectural
configurations.

</details>


### [262] [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
*Christopher Bartley,Anton Ragni*

Main category: cs.CL

TL;DR: 研究表明，濒危语言构建语音识别系统的数据需求远低于预期，仅需40分钟的发音数据即可实现可用ASR（词错误率<50%），为资源有限的濒危语言社区提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 全球近半数语言濒临灭绝，传统语音识别系统需要大量标注数据，而濒危语言往往缺乏这种资源。本文探索构建濒危语言ASR所需的最小数据量和数据形式。

Method: 使用短格式发音资源作为替代方案，在曼克斯盖尔语（约2200名使用者）和康沃尔语（约600名使用者）上进行实验，验证小数据量ASR的可行性。

Result: 仅需40分钟发音数据即可为曼克斯盖尔语构建可用的ASR系统（词错误率<50%），并在康沃尔语上成功复现该方法。

Conclusion: 构建濒危语言ASR系统的数据门槛（数量和质量）远低于传统认知，为无法满足传统数据要求的濒危语言社区带来了希望。

Abstract: Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

</details>


### [263] [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
*Yuto Nishida,Masaru Isonuma,Yusuke Oda*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型训练过程中下游任务性能的不稳定性，并提出了两种后处理检查点集成方法（平均和集成）来提高性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练时，下游任务性能存在显著波动，难以确定真正的最佳检查点，这影响了模型选择的质量。

Method: 研究了下游任务性能的稳定性，并提出了两种检查点集成方法：检查点平均和检查点集成，通过聚合相邻检查点来减少性能波动。

Result: 经验证明和理论分析都表明，这些方法能够提高下游性能的稳定性，且无需改变训练过程。

Conclusion: 检查点集成方法能有效解决LLM训练中下游任务性能波动问题，提升模型选择的可靠性。

Abstract: When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

</details>


### [264] [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
*Elisei Rykov,Kseniia Petrushina,Maksim Savkin,Valerii Olisov,Artem Vazhentsev,Kseniia Titova,Alexander Panchenko,Vasily Konovalov,Julia Belikova*

Main category: cs.CL

TL;DR: 提出了PsiloQA数据集，这是一个大规模多语言数据集，包含14种语言的span级幻觉标注，用于评估大语言模型的幻觉检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测基准主要在序列级别且仅限于英语，缺乏细粒度的多语言监督，无法全面评估大语言模型的事实准确性。

Method: 通过自动化的三阶段流程构建数据集：使用GPT-4o从维基百科生成问答对，在无上下文设置下从不同LLM获取可能包含幻觉的答案，使用GPT-4o通过与标准答案和检索上下文的比较自动标注幻觉片段。

Result: 评估了多种幻觉检测方法，发现基于编码器的模型在所有语言中表现最强。PsiloQA展示了有效的跨语言泛化能力，并能稳健地迁移到其他基准，同时比人工标注数据集成本效益更高。

Conclusion: PsiloQA数据集和结果推动了在多语言环境中可扩展、细粒度幻觉检测的发展。

Abstract: Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

</details>


### [265] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: 提出了一种名为Token Probability Deviation (TBD)的新方法，用于检测推理蒸馏过程中可能导致的基准污染问题。该方法通过分析生成令牌的概率模式来区分已见过和未见过的蒸馏数据。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏虽然能有效提升大语言模型的推理能力，但可能导致基准污染问题——蒸馏数据中的评估数据会人为提高蒸馏模型的性能指标。由于蒸馏数据通常只能部分获取，检测这类污染具有独特挑战性。

Method: 提出Token Probability Deviation (TBD)方法，基于蒸馏模型对已见过问题倾向于生成接近确定性的令牌，而对未见过问题会产生更多低概率令牌的观察，量化生成令牌概率与高参考概率的偏离程度。

Result: 在S1数据集上取得了0.918的AUC和0.470的TPR@1% FPR，表现出优异的检测性能。

Conclusion: TBD方法能有效检测推理蒸馏过程中的基准污染问题，为评估蒸馏模型的真实性能提供了可靠工具。

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [266] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: SocialHarmBench是一个包含585个提示的数据集，涵盖7个社会政治类别和34个国家，用于评估LLM在政治敏感情境下的脆弱性。研究发现开源模型在历史修正主义、宣传和政治操纵等领域存在高达97-98%的攻击成功率，且对21世纪和前20世纪情境以及拉美、美国、英国等地区特别脆弱。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准很少测试政治操纵、宣传和虚假信息生成、监控和信息控制等领域的漏洞，而LLM在这些领域的失败可能产生直接的社会政治后果。

Method: 构建SocialHarmBench数据集，包含585个提示，涵盖7个社会政治类别和34个国家，评估LLM在政治敏感情境下的表现。

Result: 开源模型表现出高度脆弱性，Mistral-7B在历史修正主义、宣传和政治操纵等领域的攻击成功率高达97-98%。时间分析显示LLM对21世纪和前20世纪情境最脆弱，地理分析显示对拉美、美国、英国等地区最敏感。

Conclusion: 当前的安全措施无法泛化到高风险的社会政治环境中，暴露了系统性偏见，引发了对LLM在保护人权和民主价值观方面可靠性的担忧。

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [267] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: 本文研究了NL2SQL任务中训练数据与目标查询的结构对齐问题，发现结构对齐度是预测微调成功的重要指标。


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)是适应大语言模型的有效方法，但训练数据的变异性会阻碍模型的跨领域泛化能力。本文旨在研究NL2SQL任务中数据集对齐问题。

Method: 通过比较训练集、目标数据和模型预测中SQL结构特征的分布来估计对齐度，并在三个大型跨领域NL2SQL基准和多个模型家族上进行综合实验。

Result: 实验表明结构对齐是微调成功的强预测因子：对齐度高时SFT带来准确率和SQL生成质量的显著提升；对齐度低时改进微乎其微或没有改进。

Conclusion: 这些发现强调了在NL2SQL任务中采用对齐感知的数据选择对于有效微调和泛化的重要性。

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [268] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: 为Nawatl语言构建上下文无关语法，生成大量语法正确的句子以扩充语料库，用于训练语言模型。


<details>
  <summary>Details</summary>
Motivation: Nawatl是一种数字资源稀少的π语言，缺乏可用于机器学习的语料库，需要人工生成句子来扩充训练数据。

Method: 使用上下文无关语法生成人工句子，扩充π-yalli语料库，并用FastText等算法进行训练和语义任务评估。

Result: 通过使用语法，相比某些大语言模型取得了比较性改进，但需要更有效的语法建模才能获得更显著的提升。

Conclusion: 语法方法可以有效扩充低资源语言的语料库，但要获得更大改进需要开发更精确的语法模型。

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [269] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: AWARE框架通过提升Transformer模型在领域、上下文和类别重叠三个维度的感知能力，显著改善了从学生反思中识别文化资本主题的性能。


<details>
  <summary>Details</summary>
Motivation: 学生反思中的文化资本主题（如志向目标、家庭支持）通常以叙事方式表达而非直接关键词，标准NLP模型因缺乏领域特定语言和叙事上下文感知而难以检测。

Method: 提出AWARE框架，包含三个核心组件：1）领域感知-调整模型词汇以适应学生反思的语言风格；2）上下文感知-生成考虑全文上下文的句子嵌入；3）类别重叠感知-采用多标签策略识别同一句子中多个主题的共存。

Result: AWARE在Macro-F1指标上比强基线提升2.1个百分点，在所有主题上均显示出显著改进。

Conclusion: 该工作为任何依赖叙事上下文的文本分类任务提供了稳健且可推广的方法论。

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [270] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: 提出了一种资源高效的LLaMA-3.2-3B微调方法，使用LoRA和QLoRA技术在有限GPU和内存条件下增强医学链式推理能力，内存使用减少60%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然推理能力强，但全参数微调需要大量计算资源，限制了在资源受限环境下的应用

Method: 使用参数高效微调技术（LoRA和QLoRA）在公开医学推理数据集上对LLaMA-3.2-3B进行适配

Result: 模型在保持强推理能力的同时，推理连贯性和事实准确性得到提升，内存使用比标准全微调减少60%

Conclusion: 轻量级适配可以在医学问答任务中保持强推理能力，为低资源研究环境部署LLM提供了实用策略

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [271] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出了一种利用Unicode变体选择器实现不可察觉的越狱攻击方法，通过在恶意问题后附加不可见的变体选择器字符，使提示在视觉上与原问题相同但分词被秘密改变，从而诱导模型产生有害响应。


<details>
  <summary>Details</summary>
Motivation: 视觉模态的越狱攻击通常依赖不可察觉的对抗扰动，而文本模态攻击通常需要可见修改。本文旨在探索在文本模态上实现不可察觉越狱攻击的可能性。

Method: 利用Unicode变体选择器字符，提出链式搜索流水线生成对抗后缀，这些后缀在屏幕上不可见但会改变文本的分词方式。

Result: 实验表明该方法对四个对齐的LLM实现了高攻击成功率，并能泛化到提示注入攻击，且不产生任何可见修改。

Conclusion: 成功证明了在文本模态上实现不可察觉越狱攻击的可行性，揭示了当前LLM安全防护的潜在漏洞。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [272] [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 提出了两个新的魁北克法语方言理解基准数据集QFrCoRE和QFrCoRT，用于测试模型对区域习语的掌握能力。


<details>
  <summary>Details</summary>
Motivation: 将习语理解和方言理解任务相结合，通过区域习语来测试方言理解能力。

Method: 构建了包含4,633个习语短语的QFrCoRE数据集和171个区域习语词的QFrCoRT数据集，并提供了可复现的方法论。

Result: 通过对94个LLM的实验表明，区域习语基准能够可靠地衡量模型在特定方言上的熟练程度。

Conclusion: 区域习语基准是评估模型方言理解能力的有效工具，该方法可推广到其他方言。

Abstract: The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

</details>


### [273] [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
*Omri Uzan,Asaf Yehudai,Roi pony,Eyal Shnarch,Ariel Gera*

Main category: cs.CL

TL;DR: 提出了一种名为GQR的测试时优化方法，通过使用互补检索器的分数来优化主检索器的查询嵌入，从而在视觉文档检索中实现性能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前多模态编码器在视觉文档检索中虽然性能优越，但存在表示规模过大、部署困难的问题，且纯视觉中心方法受到模态差距的限制。

Method: 引入引导查询优化(GQR)方法，在测试时使用互补检索器的分数来优化主检索器的查询嵌入，实现细粒度的表示空间交互。

Result: 实验表明GQR能让视觉中心模型达到更大表示模型的性能，同时速度提升14倍，内存需求减少54倍。

Conclusion: GQR有效推动了多模态检索在性能与效率方面的帕累托前沿，为实际部署提供了可行方案。

Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

</details>


### [274] [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: COLE是一个新的法语自然语言理解基准测试，包含23个多样化任务，评估了94个大语言模型，揭示了闭源与开源模型之间的性能差距，并识别了当前LLM面临的挑战领域。


<details>
  <summary>Details</summary>
Motivation: 为了解决法语自然语言理解评估不够全面的问题，需要创建一个涵盖广泛NLU能力的基准测试，特别关注法语特有的语言现象。

Method: 构建了包含23个多样化任务的COLE基准测试，涵盖情感分析、复述检测、语法判断和推理等能力，并对94个大语言模型进行了基准测试。

Result: 结果显示闭源模型与开源模型之间存在显著性能差距，识别出零样本抽取式问答、细粒度词义消歧和区域语言变体理解等关键挑战领域。

Conclusion: COLE作为公共资源发布，旨在促进法语语言建模的进一步发展，为法语NLU研究提供了全面的评估框架。

Abstract: To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

</details>


### [275] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: SwiReasoning是一个无需训练的大语言模型推理框架，通过动态切换显式和潜在推理，基于熵趋势的置信度指导，平衡探索与利用，限制思考块切换次数来抑制过度思考，提高准确性和token效率。


<details>
  <summary>Details</summary>
Motivation: 潜在推理虽然提高了token效率，但面临两个挑战：纯粹潜在推理会扩大搜索分布，分散概率质量，引入噪声，阻碍收敛到高置信度解；过度思考问题依然存在，浪费token并降低效率。

Method: 提出SwiReasoning框架，包含两个关键创新：1) 基于下一个token分布的熵趋势估计块级置信度，动态切换显式和潜在推理；2) 限制思考块最大切换次数来抑制过度思考。

Result: 在数学和STEM基准测试中，SwiReasoning在不同模型系列和规模的推理LLM上平均准确率提高1.5%-2.8%；在受限预算下，平均token效率提高56%-79%，预算越紧增益越大。

Conclusion: SwiReasoning通过动态切换显式和潜在推理，有效解决了潜在推理的收敛问题和过度思考问题，显著提高了推理准确性和效率。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [276] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: 提出了一种名为SLM-MUX的三阶段方法来协调多个小语言模型(SLMs)，通过模型选择搜索和测试时缩放策略，在多个基准测试中显著优于现有协调方法。


<details>
  <summary>Details</summary>
Motivation: 随着小语言模型数量的快速增长，虽然它们无法达到最先进精度，但在特定任务上表现出色且更高效。现有协调方法主要针对前沿模型，在SLMs上表现不佳，因此需要专门针对SLMs的协调方法。

Method: 采用三阶段方法：1) SLM-MUX多模型架构协调多个SLMs；2) 模型选择搜索从候选池中识别最具互补性的SLMs；3) 针对SLM-MUX的测试时缩放策略。

Result: 在MATH上提升13.4%，GPQA上提升8.8%，GSM8K上提升7.0%。仅使用两个SLMs，SLM-MUX在GPQA和GSM8K上超越Qwen 2.5 72B，在MATH上与之持平。

Conclusion: 通过所提出的方法，小语言模型可以被有效协调成更准确和高效的系统，并通过理论分析验证了该方法的优势。

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [277] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: TeachLM通过参数高效微调优化LLM用于教学，利用真实学生-导师对话数据生成高质量合成对话，显著提升教学对话能力


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在教育领域的局限性，特别是缺乏反映真实学生学习过程的高质量训练数据，以及提示工程在编码复杂教学策略方面的不足

Method: 使用100,000小时真实学生-导师互动数据进行参数高效微调，开发真实学生模型以生成高质量合成对话，并提出基于合成对话的多轮评估协议

Result: 微调显著提升对话和教学性能：学生发言时间翻倍、提问风格改善、对话轮次增加50%、教学个性化程度提高

Conclusion: 基于真实学习数据的微调能有效提升LLM的教学对话能力，为教育AI发展提供了新途径

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [278] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: 提出了Tolerator解码策略，通过令牌级交叉验证改进扩散大语言模型的解码过程，解决了传统扩散解码中令牌一旦接受就无法修改的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型作为自回归模型的替代方案具有并行解码和双向上下文建模的优势，但传统离散扩散解码存在关键限制：一旦令牌被接受就无法在后续步骤中修正，导致早期错误持续影响最终输出质量。

Method: Tolerator采用两阶段过程：序列填充和迭代精炼。通过重新掩码和解码令牌子集，同时将剩余令牌作为上下文，使先前接受的令牌能够被重新考虑和修正。

Result: 在五个标准基准测试（语言理解、代码生成和数学）上的实验表明，该方法在相同计算预算下相比基线方法实现了持续改进。

Conclusion: 解码算法对于充分发挥扩散大语言模型的潜力至关重要，Tolerator通过令牌级交叉验证实现了更可靠的扩散解码输出。

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>


### [279] [ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization](https://arxiv.org/abs/2505.02819)
*Dmitriy Shopkhoev,Ammar Ali,Magauiya Zhussip,Valentin Malykh,Stamatios Lefkimmiatis,Nikos Komodakis,Sergey Zagoruyko*

Main category: cs.CL

TL;DR: ReplaceMe是一种无需训练即可进行深度剪枝的方法，通过用线性变换替换Transformer块，在保持高性能的同时实现模型压缩。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法需要额外训练或微调，计算成本高。ReplaceMe旨在开发一种无需训练的高效剪枝方法，仅需少量校准数据即可实现模型压缩。

Method: 使用小规模校准数据集估计线性变换来近似被剪枝的Transformer块，该线性映射可与剩余块无缝合并，无需额外网络参数。

Result: 在多个大语言模型上实现高达25%的剪枝率，同时保留约90%的原始模型性能，计算开销极小，优于其他无需训练的方法，与需要大量重训练的方法竞争力强。

Conclusion: ReplaceMe提供了一种高效、无需训练的深度剪枝解决方案，在保持模型性能的同时显著减少计算需求，并提供了开源实现。

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation, which approximates the pruned blocks.
The estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.

</details>


### [280] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出MASA框架，通过跨层共享矩阵原子来减少Transformer注意力模块参数，在保持性能的同时减少66.7%参数。


<details>
  <summary>Details</summary>
Motivation: 现有压缩技术主要关注块内优化，而Transformer的重复层结构存在显著的块间冗余，这一维度尚未充分探索。

Method: 将注意力投影矩阵分解为共享字典原子，每层权重表示为共享矩阵原子的线性组合，作为即插即用方案使用标准优化器训练。

Result: 在100M-700M参数规模实验中，MASA在相同参数预算下优于分组查询注意力、低秩基线和最近提出的重复共享方法。

Conclusion: MASA为参数高效模型提供了可扩展蓝图，结合字典学习策略与Transformer效率，在不牺牲性能的情况下减少参数。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [281] [FinCall-Surprise: A Large Scale Multi-modal Benchmark for Earning Surprise Prediction](https://arxiv.org/abs/2510.03965)
*Dong Shu,Yanguang Liu,Huopu Zhang,Mengnan Du*

Main category: cs.MM

TL;DR: 提出了FinCall-Surprise数据集，这是首个用于预测企业盈利意外的大规模、开源、多模态数据集，包含2,688个企业电话会议的文字转录、音频和演示文稿，并评估了26个先进模型。


<details>
  <summary>Details</summary>
Motivation: 企业盈利意外预测具有重要投资价值，但现有研究受限于昂贵、专有和纯文本数据，阻碍了先进模型的发展。

Method: 构建包含文字转录、音频和演示文稿的多模态数据集，并系统评估26个单模态和多模态大语言模型。

Result: 发现许多模型的高准确率是类别不平衡造成的假象；部分专业金融模型在指令遵循和语言生成方面表现不佳；多模态融合效果有限。

Conclusion: 现有大语言模型在金融推理能力上存在明显局限，为未来研究设立了具有挑战性的新基准。

Abstract: Predicting corporate earnings surprises is a profitable yet challenging task,
as accurate forecasts can inform significant investment decisions. However,
progress in this domain has been constrained by a reliance on expensive,
proprietary, and text-only data, limiting the development of advanced models.
To address this gap, we introduce \textbf{FinCall-Surprise} (Financial
Conference Call for Earning Surprise Prediction), the first large-scale,
open-source, and multi-modal dataset for earnings surprise prediction.
Comprising 2,688 unique corporate conference calls from 2019 to 2021, our
dataset features word-to-word conference call textual transcripts, full audio
recordings, and corresponding presentation slides. We establish a comprehensive
benchmark by evaluating 26 state-of-the-art unimodal and multi-modal LLMs. Our
findings reveal that (1) while many models achieve high accuracy, this
performance is often an illusion caused by significant class imbalance in the
real-world data. (2) Some specialized financial models demonstrate unexpected
weaknesses in instruction-following and language generation. (3) Although
incorporating audio and visual modalities provides some performance gains,
current models still struggle to leverage these signals effectively. These
results highlight critical limitations in the financial reasoning capabilities
of existing LLMs and establish a challenging new baseline for future research.

</details>


### [282] [Evaluating Keyframe Layouts for Visual Known-Item Search in Homogeneous Collections](https://arxiv.org/abs/2510.04396)
*Bastian Jäckl,Jiří Kruchina,Lucas Joos,Daniel A. Keim,Ladislav Peška,Jakub Lokoč*

Main category: cs.MM

TL;DR: 该研究评估了7种关键帧布局在视频检索任务中的表现，发现视频分组布局效率最高，而四列排名保持网格准确率最高。排序网格存在权衡，既能快速扫描无关区域，但也会将相关目标降级到不显眼位置。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态深度学习模型在视频检索方面取得进展，用户仍需手动浏览排序结果。关键帧在搜索网格中的排列方式影响浏览效果和用户效率，但这一领域仍未被充分探索。

Method: 对49名参与者进行实验，评估7种关键帧布局在视觉已知项目搜索任务中的表现，除了效率和准确性外，还分析了浏览现象（如遗漏）与布局特征的关系。

Result: 视频分组布局效率最高，四列排名保持网格准确率最高。排序网格能快速扫描无关区域，但会将相关目标降级到不显眼位置，延迟首次到达时间并增加遗漏。

Conclusion: 研究结果支持混合设计方法，即保留排名靠前项目的位置，同时对剩余项目进行排序或分组，并为视频检索之外的网格搜索提供指导。

Abstract: Multimodal deep-learning models power interactive video retrieval by ranking
keyframes in response to textual queries. Despite these advances, users must
still browse ranked candidates manually to locate a target. Keyframe
arrangement within the search grid highly affects browsing effectiveness and
user efficiency, yet remains underexplored. We report a study with 49
participants evaluating seven keyframe layouts for the Visual Known-Item Search
task. Beyond efficiency and accuracy, we relate browsing phenomena, such as
overlooks, to layout characteristics. Our results show that a video-grouped
layout is the most efficient, while a four-column, rank-preserving grid
achieves the highest accuracy. Sorted grids reveal potentials and trade-offs,
enabling rapid scanning of uninteresting regions but down-ranking relevant
targets to less prominent positions, delaying first arrival times and
increasing overlooks.
  These findings motivate hybrid designs that preserve positions of top-ranked
items while sorting or grouping the remainder, and offer guidance for searching
in grids beyond video retrieval.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [283] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: 使用LLM引导的进化程序合成方法，自动发现高质量准蒙特卡洛构造，包括低差异点集和Sobol'方向数，在多个维度上创造了新的最佳基准。


<details>
  <summary>Details</summary>
Motivation: 解决准蒙特卡洛方法中长期存在的设计问题：构造有限2D/3D低星差异点集和选择最小化随机化QMC误差的Sobol'方向数。

Method: 采用两阶段程序：结合构造性代码提议和迭代数值优化，使用LLM引导的进化循环在任务特定适应度下突变和选择代码。

Result: 在2D情况下重新发现已知最优解并创造N≥40的新最佳基准；在3D中匹配已知最优解并报告改进的基准；在Sobol'参数进化中，相比广泛使用的Joe-Kuo参数，在32维期权定价任务中持续降低rQMC均方误差。

Conclusion: LLM驱动的进化程序合成能够自动化发现高质量QMC构造，在经典设计最优时恢复它们，在有限N结构重要时改进它们。

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [284] [HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting](https://arxiv.org/abs/2510.03744)
*Qianfei Fan,Jiayu Wei,Peijun Zhu,Wensheng Ye,Meie Fang*

Main category: cs.LG

TL;DR: HydroFusion-LMF是一个用于小流域十年尺度日径流预测的统一框架，通过可学习的趋势-季节-残差分解、异构专家集合和半监督多任务学习，在非平稳条件下实现了比现有方法更好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 小流域的十年尺度日径流预测面临信号混合（漂移趋势、多尺度季节周期、状态转移、稀疏极端值）的挑战，现有深度模型通常只针对单一方面且未充分利用未标记数据，限制了状态适应性。

Method: 提出HydroFusion-LMF框架：(1)可学习的趋势-季节-残差分解降低非平稳性；(2)残差通过紧凑异构专家集合（线性细化、频率核、补丁Transformer、循环记忆、动态归一化注意力）；(3)基于水文上下文感知门融合专家输出；(4)半监督多任务目标增强监督。

Result: 在约10年日数据上，HydroFusion-LMF达到MSE 1.0128/MAE 0.5818，比最强基线（DLinear）提升10.2%/10.3%，比平均基线提升24.6%/17.1%，同时实现了MSE和MAE的降低。

Conclusion: 该框架在可解释性（显式组件、稀疏门控）与性能之间取得平衡，推进了非平稳条件下标签高效的水文预测。

Abstract: Accurate decade-scale daily runoff forecasting in small watersheds is
difficult because signals blend drifting trends, multi-scale seasonal cycles,
regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,
PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single
facets and under-utilize unlabeled spans, limiting regime adaptivity. We
propose HydroFusion-LMF, a unified framework that (i) performs a learnable
trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes
residuals through a compact heterogeneous expert set (linear refinement,
frequency kernel, patch Transformer, recurrent memory, dynamically normalized
attention), (iii) fuses expert outputs via a hydrologic context-aware gate
conditioned on day-of-year phase, antecedent precipitation, local variance,
flood indicators, and static basin attributes, and (iv) augments supervision
with a semi-supervised multi-task objective (composite MSE/MAE + extreme
emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,
augmentation consistency, variance-filtered pseudo-labeling). Optional adapter
/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a
~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,
improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean
baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions
relative to baselines. The framework balances interpretability (explicit
components, sparse gating) with performance, advancing label-efficient
hydrologic forecasting under non-stationarity.

</details>


### [285] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: PARS是一种提示感知的LLM任务调度器，通过近似最短作业优先调度来减少头部阻塞，提高推理服务的效率和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 传统FCFS调度策略存在头部阻塞问题，长任务会延迟短任务，影响LLM推理服务的延迟和吞吐量。

Method: 使用成对排序和边界排序损失来近似SJF调度，预测基于响应长度的任务排序，并集成到vLLM系统中。

Result: 在多个LLM和真实推理数据集上的实验表明，PARS显著提升了性能，包括推理工作负载，且设计具有良好的泛化能力。

Conclusion: PARS通过智能的任务调度有效解决了LLM推理中的头部阻塞问题，实现了低延迟和高吞吐量的服务。

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [286] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: VIFO是一个跨模态时间序列预测模型，通过将多元时间序列转换为图像，利用预训练的大型视觉模型提取跨通道依赖关系，并与时间序列模态特征对齐融合，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模时间序列基础模型通常采用通道独立架构，忽略了关键的跨通道依赖关系；同时，现有的多模态方法未能充分利用大型视觉模型来解释时空数据，不同模态信息提取的潜力尚未被充分挖掘。

Method: 将多元时间序列渲染成图像，利用预训练的大型视觉模型提取复杂的跨通道模式，然后将视觉特征与时间序列模态的表征进行对齐和融合，仅训练7.45%的参数。

Result: VIFO在多个基准测试中实现了具有竞争力的性能，为捕捉跨变量关系提供了高效有效的解决方案。

Conclusion: VIFO通过跨模态方法成功解决了通道独立模型的局限性，利用视觉模型提取不可见的跨通道模式，在参数效率高的前提下显著提升了时间序列预测性能。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [287] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 提出了一种可转移的频率感知对抗攻击方法，并基于此开发了频率感知模型参数探索器(FAMPE)来提升深度神经网络的可解释性，相比现有最佳方法AttEXplore在插入得分上平均提升13.02%。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在真实世界噪声和故意扰动下的可靠性问题，现有归因方法效果欠佳需要进一步改进。

Method: 提出可转移的频率感知对抗攻击，通过高低频组件进行频率感知探索，并基于此开发FAMPE归因方法。

Result: FAMPE相比最先进的AttEXplore方法在插入得分上平均提升13.02%，通过消融研究验证了高低频组件在可解释性中的作用。

Conclusion: 频率感知方法能有效提升深度神经网络的可解释性，高低频组件在模型解释中都发挥着重要作用。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [288] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: 提出STRUPRUNE框架，结合结构化剪枝的硬件效率与局部剪枝的内存效率，通过分治策略将全局剪枝分解为协调的子问题，显著降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 全局剪枝性能好但内存需求高，局部剪枝内存效率高但忽略层间依赖导致性能下降。结构化剪枝硬件友好但通常依赖全局方法。需要同时实现结构化剪枝和内存效率。

Method: 采用分治策略将全局剪枝分解为跨模块的协调子问题，基于ADMM框架整合结构化稀疏性，推导结构化剪枝掩码的闭式解析解和基于能量的渐近分配方案。

Result: STRUPRUNE在保持与全局结构化剪枝相同困惑度的同时，将内存成本从O(N)降低到O(√N)，支持十亿参数规模的部署。

Conclusion: STRUPRUNE成功实现了结构化剪枝与内存效率的结合，为大规模语言模型的实用部署提供了可行方案。

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [289] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: 提出了首个针对未对齐多模态数据的主动学习框架，通过主动获取跨模态对齐而非标签来降低标注成本，在保持性能的同时将标注需求减少高达40%。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习算法主要关注单模态数据，忽视了多模态学习中标注对齐的显著负担，特别是在CLIP和SigLIP等现代多模态管道中，高质量的对齐标注成本很高。

Method: 开发了一种结合不确定性和多样性原则的模态感知算法，实现线性时间获取，并适用于基于池和基于流的设置。

Result: 在基准数据集上的广泛实验表明，该方法能持续减少多模态标注成本同时保持性能，在ColorSwap数据集上可将标注需求减少高达40%且不损失准确率。

Conclusion: 该框架为多模态主动学习提供了有效的解决方案，显著降低了跨模态对齐的标注负担。

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [290] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 该研究评估了四种神经算子架构，用于快速预测创伤性脑损伤中的脑位移场，将计算时间从数小时缩短到毫秒级，同时保持解剖真实性。


<details>
  <summary>Details</summary>
Motivation: 创伤性脑损伤是全球重大公共卫生问题，传统有限元模型计算成本高，限制了临床快速决策应用。需要开发快速、患者特定的脑变形预测方法。

Method: 将TBI建模为算子学习问题，使用四种神经算子架构（FNO、F-FNO、MG-FNO、DeepONet）从患者特定的解剖MRI、MRE刚度图和人口统计学特征映射到全场3D脑位移预测。

Result: MG-FNO达到最高精度（MSE = 0.0023，94.3%空间保真度），F-FNO收敛速度快2倍，DeepONet推理速度最快（14.5次/秒），比MG-FNO快7倍。所有神经算子将计算时间从小时级缩短到毫秒级。

Conclusion: 神经算子为预测脑变形提供了高效、分辨率不变的方法，为实现实时、患者特定的TBI风险评估、临床分诊支持和防护装备优化开辟了道路，展示了基于神经算子的人脑数字孪生潜力。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [291] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出了一种新的可微分逻辑门网络参数化方法，解决了原有方法存在的梯度消失、离散化误差和高训练成本问题，同时显著减少了参数规模并加速了训练。


<details>
  <summary>Details</summary>
Motivation: 可微分逻辑门网络在推理时效率极高且保持竞争力精度，但存在梯度消失、离散化误差和高训练成本问题，即使使用专门的参数初始化方案，增加网络深度仍会损害精度。

Method: 重新参数化逻辑门神经元，将每个门的参数规模按输入数量对数级缩小。对于二进制输入，这已经将模型大小减少了4倍。

Result: 新参数化方法使反向传播速度提升达1.86倍，训练步数减少8.5倍收敛，在CIFAR-100上的精度保持稳定甚至优于原始参数化方法。

Conclusion: 通过重新参数化逻辑门神经元，成功解决了可微分逻辑门网络的扩展问题，显著提升了训练效率和模型性能。

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [292] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: 提出Numerion模型，利用超复数空间自然分解时间序列，通过多维度RHR-MLP架构实现高效预测，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分解方法受限于计算复杂性和假设鲁棒性，研究发现超复数空间中时间序列特征频率自然降低，可利用此特性改进预测。

Method: 将线性层和激活函数推广到任意2的幂次维超复数空间，提出RHR-MLP架构，使用多个RHR-MLP在不同维度超复数空间中分解建模时间序列，并通过动态融合机制自适应整合模式。

Result: 在多个公共数据集上验证模型性能，达到最先进结果，可视化分析显示高维超复数空间能捕获低频特征。

Conclusion: Numerion模型通过超复数空间自然分解时间序列，有效提升预测性能，揭示了高维超复数空间与低频特征的关系。

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [293] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 提出了通用多域翻译(UMDT)框架，使用扩散路由器(DR)方法，只需K-1个配对数据集就能实现K个域之间的任意翻译，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有多域翻译方法需要完全对齐的元组或只能处理训练中见过的域对，限制了实用性并排除了许多跨域映射。

Method: 提出扩散路由器(DR)框架，使用单一噪声预测器建模所有中心域与非中心域之间的翻译，通过中心域路由实现间接非中心翻译，并引入变分边界目标和Tweedie精炼程序支持直接映射。

Result: 在三个大规模UMDT基准测试中，DR在间接和直接翻译方面都达到了最先进的结果，同时降低了采样成本，并解锁了草图↔分割等新任务。

Conclusion: DR被证明是一个可扩展且通用的多域翻译框架，能够实现跨多个域的通用翻译。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [294] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: HPL是一个分层偏好学习框架，通过多粒度偏好信号优化LLM智能体，解决了轨迹级DPO信号过粗和步骤级DPO过于短视的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的离线方法（如DPO）存在粒度不匹配问题：轨迹级DPO信号太粗无法精确分配信用，步骤级DPO又过于短视无法捕捉多步行为价值。

Method: HPL结合轨迹级和步骤级DPO实现全局和局部策略稳定性，核心创新是组级偏好优化和双层课程调度。首先将专家轨迹分解为语义连贯的动作组，生成对比次优组进行细粒度偏好学习；然后通过课程调度器按组长度（子任务复杂度）和样本难度（奖励差距）组织从简单到复杂的学习过程。

Result: 在三个具有挑战性的智能体基准测试中，HPL优于现有最先进方法。分析表明分层DPO损失有效整合了多粒度偏好信号，双层课程对于智能体解决从简单行为到复杂多步序列的广泛任务至关重要。

Conclusion: HPL通过分层偏好学习和双层课程调度，成功解决了LLM智能体在多粒度偏好学习中的挑战，为复杂长视野问题的解决提供了有效框架。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [295] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 该论文提出了一种约束悲观双层优化模型，通过限制对手的行动范围来改进对抗性机器学习中的分类器训练，使其更贴近现实场景。


<details>
  <summary>Details</summary>
Motivation: 现有悲观双层优化方法中的对手不受限制，可能导致模型过于悲观和不切实际，无法在真实数据上取得良好性能。

Method: 构建约束悲观双层优化模型，限制对手的数据修改范围，确保生成的恶意数据保持其本质特征。

Result: 实验表明，该模型在平均性能上优于现有方法。

Conclusion: 通过约束对手行动，约束悲观双层优化能够产生更现实的对抗场景，从而提高分类器在真实世界数据上的性能。

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [296] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: 提出了SciTS基准测试和TimeOmni框架，用于评估和改进大语言模型在科学时间序列数据上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在科学时间序列处理上存在不足，要么将数值序列编码为文本，要么转换为图像，这些方法无法全面理解科学时间序列。现有统一时间序列模型要么专注于预测要么专注于分析，对非周期性、异质性科学信号的有效性不明确。

Method: 引入SciTS基准测试，涵盖12个科学领域和43个任务，包含超过5万个实例；提出TimeOmni框架，使大语言模型能够理解和生成时间序列，同时保持与通用大语言模型训练的兼容性。

Result: 基准测试了17个模型，发现通用大语言模型比专用时间序列模型具有更强的泛化能力；将时间序列表示为文本或图像会因序列过长或数值精度损失而限制性能。

Conclusion: 这项工作填补了科学时间序列专用基准测试和建模框架的空白，为大语言模型理解和生成复杂时间科学数据铺平了道路。

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [297] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: 提出了Triple-BERT方法，一种基于TD3变体的集中式单智能体强化学习方法，用于解决网约车平台的大规模订单调度问题，通过动作分解策略和BERT网络处理大规模动作和观测空间。


<details>
  <summary>Details</summary>
Motivation: 网约车平台面临复杂的实时订单匹配挑战，现有MARL方法无法有效捕获全局信息且存在维度灾难问题，需要新的解决方案来处理大规模司机和订单的调度问题。

Method: 基于TD3变体的集中式单智能体强化学习，采用动作分解策略将联合动作概率分解为单个司机动作概率，使用BERT网络通过参数重用和注意力机制处理大规模观测空间。

Result: 在曼哈顿真实网约车数据集上验证，相比现有最优方法提升约11.95%，服务订单数增加4.26%，接驾时间减少22.25%。

Conclusion: Triple-BERT方法有效解决了大规模订单调度问题，在服务效率和接驾时间方面均有显著提升，为网约车平台提供了可行的集中式调度解决方案。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [298] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: POEM提出了一种测试时自适应方法，通过探索先前未被利用的可靠样本来改进模型适应，同时引入Adapt Branch网络平衡领域无关表示提取和目标数据性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法依赖熵作为置信度指标，对预定义熵阈值敏感，导致许多潜在可靠的目标样本被忽视和未充分利用。

Method: 提出POEM方法探索先前未被利用的可靠样本，并引入额外的Adapt Branch网络来平衡领域无关表示提取和目标数据性能。

Result: 在多种架构上的综合实验表明，POEM在挑战性场景和真实世界领域偏移中持续优于现有TTA方法，同时保持计算效率。

Conclusion: POEM的核心思想可以作为增强策略来提升现有TTA方法的性能，该方法在多个基准测试中表现出色。

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [299] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: 本文提出MASA方法，通过自我对齐增强语言模型的元认知能力，证明元认知对齐能显著提升推理性能，在多个数学和推理基准上实现显著准确率提升和训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型缺乏元认知能力，即模型不知道如何思考自身推理过程。研究发现真实推理过程与预测的元信息之间存在严重不对齐，假设对齐元预测与真实推理过程能带来显著性能提升。

Method: 设计MASA训练框架，通过自我对齐增强元认知能力。该方法无需外部训练数据，利用自生成信号训练元认知，并通过过滤零方差提示和截断不可能得到正确答案的长推理过程来提高训练效率。

Result: 方法在领域内任务上显著提升准确率和训练效率，在AIME25上获得19.3%准确率提升，在六个数学基准上平均提升6.2%，GRPO训练加速1.28倍。在领域外泛化方面，GPQA-Diamond提升3.87%，13个基准平均提升2.08%。

Conclusion: 训练元认知指导能增强模型的领域外泛化能力，MASA方法通过自我对齐有效提升推理模型的元认知能力，从而显著改善推理性能和训练效率。

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [300] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: 该论文提出了一种在零样本学习中改进语义空间的方法，通过分区方案模拟未见条件，并研究了两种特征选择策略来减少语义属性中的冗余和噪声。


<details>
  <summary>Details</summary>
Motivation: 零样本学习中的语义空间通常包含噪声、冗余或不相关属性，这会阻碍性能表现。需要一种方法在不访问未见类语义信息的情况下评估属性相关性。

Method: 提出了一个分区方案来模拟未见条件，并研究了两种特征选择策略：基于嵌入的特征选择（RFS）和基于进化计算的特征选择（GA）。

Result: 在五个基准数据集上的实验表明，两种方法都能通过减少冗余一致提高未见类的准确率，RFS高效但依赖超参数，GA成本更高但搜索更广泛且不依赖超参数。

Conclusion: 语义空间本质上是冗余的，提出的分区方案是在归纳条件下精炼语义空间的有效工具。

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [301] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: 提出了一种新的热误差补偿范式，使用神经网络预测机床内部的高保真温度和热通量场，而不是直接预测热误差或补偿值，提高了方法的通用性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统热误差补偿方法依赖于特定的温度-变形场或传递函数，现有数据驱动方法通常直接预测热误差或补偿值，这些方法受限于特定误差类型、空间位置或机床配置，缺乏通用性和适应性。

Method: 使用有限元方法获取数据训练神经网络预测温度和热通量场，采用基于相关性的测量点选择策略减少推理时的硬件需求，并比较了多种时间序列神经网络架构（RNN、GRU、LSTM、双向LSTM、Transformer、TCN）。

Result: 能够准确且低成本地预测温度和热通量场，为机床环境中的灵活和通用热误差校正奠定了基础。

Conclusion: 提出的新范式通过预测物理场而非直接补偿值，实现了更灵活和通用的热误差校正，为机床热误差补偿提供了新的解决方案。

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [302] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出正交蒙特卡洛dropout方法，在合并LoRA模块时强制正交性以避免语义向量干扰，但实证发现正交性本身不足以实现语义组合性。


<details>
  <summary>Details</summary>
Motivation: LoRA微调方法训练特定概念的模块，但合并多个LoRA时语义向量会相互干扰，影响生成质量。

Method: 正交蒙特卡洛dropout机制，在不增加时间复杂度的前提下强制语义向量正交性。

Result: 理论上和运行时都能保证合并LoRA的正交性，但实证分析显示正交性并不能实现语义解缠或组合性。

Conclusion: 仅靠LoRA间的正交性可能不足以实现真正的语义组合性，需要重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [303] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [304] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 该研究系统分析了推理数据在不同训练阶段（预训练vs微调）对LLM性能的影响，发现预训练阶段引入推理数据效果更好（平均提升19%），且预训练需要多样化的推理模式，而微调更依赖数据质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM增强推理能力主要依赖微调阶段的高质量推理数据，但预训练阶段引入推理数据的效果尚不明确。研究旨在探索推理数据在不同训练阶段的作用差异，以及早期引入是否会带来过拟合风险。

Method: 通过系统研究不同规模、多样性和质量的推理数据在预训练和微调阶段的影响，分析数据分配策略对模型性能的影响。

Result: 预训练阶段引入推理数据效果显著（平均提升19%），预训练受益于推理模式多样性（平均提升11%），微调更依赖数据质量（平均提升15%）。过早扩展微调数据可能削弱早期推理注入的益处。

Conclusion: 研究挑战了语言建模与推理分离的传统观念，为在整个训练流程中战略性分配数据提供了原则性指导，证明预训练阶段引入推理数据能建立无法通过后期微调完全复现的基础能力。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [305] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: 提出了MindCraft框架和概念树方法，通过谱分解和概念路径重建概念层次结构，揭示深度模型中概念如何从共享表示中分离出来。


<details>
  <summary>Details</summary>
Motivation: 理解大规模基础模型内部如何组织和稳定概念结构，但目前这方面的机制仍不清楚。

Method: 基于因果推理构建MindCraft框架，使用概念树方法，在每一层应用谱分解并将主方向链接成分支概念路径。

Result: 在医疗诊断、物理推理和政治决策等多个领域的实证评估表明，概念树能够恢复语义层次结构、解缠潜在概念，并具有跨领域适用性。

Conclusion: 概念树建立了一个广泛适用且强大的框架，能够深入分析深度模型中的概念表示，是迈向可解释AI的重要一步。

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [306] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: 本研究提出了一种基于变分自编码器(VAE)的新方法，用于检测陆地碳循环中的极端事件，并与传统的奇异谱分析(SSA)方法进行比较。结果表明VAE方法在性能上与SSA相当，但具有计算优势并能更好地捕捉非线性时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 气候异常显著影响陆地碳循环动态，需要稳健的方法来检测和分析植物生产力的异常行为。传统方法如奇异谱分析需要预先定义信号的周期性，而VAE方法能够直接从数据中发现这些特征。

Method: 使用变分自编码器(VAE)架构，包含三个密集层和潜在空间，输入序列长度为12个月，在归一化的GPP时间序列上进行训练以重建GPP，并根据重建误差识别异常。极端事件使用第5百分位数阈值定义。

Result: VAE和SSA方法在极端事件频率的空间模式上表现出强烈的一致性，但VAE产生更高的阈值值(179-756 GgC vs 100-784 GgC)。两种方法都显示到2050-80年，北美西部和中部的负碳循环极端事件幅度和频率增加。

Conclusion: VAE方法在性能上与成熟的SSA技术相当，同时提供计算优势并增强了对碳循环变异中非线性时间依赖性的捕捉能力。VAE方法不需要预先定义数据中信号的周期性，而是从数据中发现它们。

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [307] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: PT²-LLM是一个针对大语言模型的后训练三值化框架，通过非对称三值量化器和两阶段优化流程，在保持竞争力的同时显著降低内存需求和加速推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能强大，但巨大的内存和计算需求限制了部署。三值化作为一种有前景的压缩技术，在后训练量化场景下的潜力尚未充分探索，主要面临训练自由参数优化困难以及异常值和分散权重带来的量化挑战。

Method: 提出PT²-LLM框架，核心是非对称三值量化器配合两阶段优化：1）迭代三值拟合，交替进行最优三值网格构建和灵活舍入；2）激活感知网格对齐，进一步优化三值网格匹配全精度输出。还提出基于结构相似性的重排序策略来缓解异常值影响。

Result: 大量实验表明，PT²-LLM在保持与最先进2位后训练量化方法竞争力的同时，具有更低的内存成本，并能加速预填充和解码过程，实现端到端加速。

Conclusion: PT²-LLM为LLM部署提供了一种高效的后训练三值化解决方案，在性能、内存效率和推理速度方面取得了良好平衡。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [308] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: 本文提出了首个理论框架分析多模态对比学习的最优表示和模态对齐，证明维度坍缩是模态间隙的根本原因，并提出了两种实现完美对齐的方法。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习中存在模态间隙现象，即不同模态的表示占据嵌入空间的不同区域，且关于模态间隙对下游性能影响的研究结果不一致，需要理论分析其成因和影响。

Method: 建立理论框架分析多模态对比学习的最优表示收敛性，在不同约束条件下（无约束、锥约束、子空间约束）分析模态间隙的收敛行为。

Result: 证明在无约束或锥约束下模态间隙收敛到零；在子空间约束下（维度坍缩）模态间隙收敛到两个超平面间的最小角度，维度坍缩是模态间隙的根本原因。

Conclusion: 模态间隙通过影响样本对对齐来影响下游性能，提出了通过超平面旋转和共享空间投影两种方法实现完美对齐。

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [309] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: 本文提出了通用探索奖励（GEB）框架，解决了现有KL或α-散度正则化方法在强化学习人类反馈中无意偏向高概率区域的问题，实现了真正的乐观探索。


<details>
  <summary>Details</summary>
Motivation: 现有探索奖励方法在KL或α-散度正则化下会无意中将探索偏向参考模型的高概率区域，强化保守行为而非促进不确定区域的发现，这违背了乐观探索的核心原则。

Method: 引入通用探索奖励（GEB）理论框架，通过参考依赖的奖励调节来抵消散度诱导的偏差，将先前的启发式奖励统一为特例，并自然扩展到完整的α-散度族。

Result: 在多个散度设置和大语言模型骨干上的对齐任务中，GEB始终优于基线方法。

Conclusion: GEB为RLHF中的乐观探索提供了既有理论原则又实用的解决方案。

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [310] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: CoDA是一个1.7B参数的扩散语言模型，专为代码生成设计，通过大规模扩散预训练、代码中心化中训练和指令微调，在保持竞争性推理延迟的同时，在多个代码评估基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有双向上下文和填充能力，但现有系统通常过于笨重。作者旨在开发轻量级但性能优异的扩散编码器，以加速基于扩散的轻量级代码助手研究。

Method: 采用1.7B参数规模的扩散模型，结合大规模扩散预训练、代码中心化中训练和指令微调，使用置信度引导采样来保持推理延迟的竞争力。

Result: 在Humaneval、MBPP和EvalPlus等基准测试中，CoDA-1.7B-Instruct匹配或超越了参数规模达7B的扩散模型。

Conclusion: CoDA展示了轻量级扩散模型在代码生成任务上的潜力，作者开源了模型检查点、评估工具和TPU训练流程，以促进相关研究。

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [311] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 提出了决策势能面（DPS）作为分析LLM决策边界的新概念，通过有限次序列采样近似构建决策边界，解决了传统方法计算不可行的问题。


<details>
  <summary>Details</summary>
Motivation: 由于LLM词汇序列规模巨大和自回归特性，构建其决策边界在计算上不可行，需要新的分析方法。

Method: 定义DPS概念，基于区分不同采样序列的置信度来捕捉决策边界潜力，提出K-DPS算法通过K次有限采样近似决策边界。

Result: 理论推导了K-DPS与理想DPS之间的误差上界，证明误差可通过采样次数进行权衡，实验验证了方法在各种LLM和语料上的有效性。

Conclusion: DPS为LLM决策边界分析提供了可行的解决方案，K-DPS算法能以可接受的误差高效近似决策边界。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [312] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: 将Transformer架构重新概念化为由主偏微分方程（PDE）控制的连续时空动力系统，揭示残差连接和层归一化是稳定该系统的必要数学机制。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然革命性，但其内部机制的理论理解仍然缺乏。本文旨在通过连续动力系统的视角，为Transformer的设计提供第一性原理的解释。

Method: 引入分析框架，将Transformer的离散分层结构映射为连续PDE系统：自注意力对应非局部交互，前馈网络对应局部反应，残差连接和层归一化对应稳定机制。通过比较标准Transformer与缺乏明确稳定器的PDE模拟器进行实验验证。

Result: 实验证明，没有残差连接会导致灾难性的表示漂移，而没有层归一化会导致不稳定的爆炸性训练动态。这些看似启发式的技巧实际上是稳定强大但固有不稳定连续系统的基本数学稳定器。

Conclusion: 这项工作为Transformer的设计提供了第一性原理解释，并建立了通过连续动力学视角分析深度神经网络的新范式。

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [313] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: 提出Synergistic Information Distillation (SID)框架，通过将深度学习重构为局部协同精炼问题，解决反向传播的更新锁定和高内存消耗问题，实现并行训练并保持前向推理不变。


<details>
  <summary>Details</summary>
Motivation: 解决反向传播的两个关键可扩展性瓶颈：更新锁定（网络模块需等待整个反向传播完成）和高内存消耗（存储激活值用于梯度计算）。

Method: 将深度网络构建为模块管道，每个模块具有局部目标来精炼对真实目标的概率信念，平衡目标保真度和与前序模块信念的一致性，从而解耦模块间的反向依赖。

Result: 理论证明SID保证网络深度增加时性能单调提升，实验显示其分类精度与反向传播相当或更优，具有更好的可扩展性和对标签噪声的鲁棒性。

Conclusion: SID作为反向传播的即插即用替代方案，消除了更新锁定，大幅减少内存需求，同时保持标准前向推理过程。

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [314] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: 提出了Quant-dLLM框架，专门针对扩散大语言模型进行超低位后训练量化，解决了标准PTQ方法在2位量化时性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型具有双向上下文和灵活掩码去噪生成的优势，但模型规模持续增长，需要权重压缩部署。标准后训练量化方法直接应用到dLLMs在2位量化时性能不理想。

Method: 提出三个关键技术：掩码校准模拟（MCS）对齐时间步相关的掩码校准；数据感知任意顺序量化器（DAQ）通过优化算法学习超低位权重表示；自适应分块混合精度（ABMP）基于敏感度分配位宽。

Result: 在严格的2位预算下，Quant-dLLM在dLLMs上始终比最先进的AR迁移PTQ方法获得更高准确率。

Conclusion: Quant-dLLM是针对扩散大语言模型的专用超低位量化框架，有效解决了标准PTQ方法在dLLMs上的性能问题。

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [315] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: SDQ-LLM是一种用于1位LLM的Sigma-Delta量化框架，通过过采样率和Hadamard权重平滑实现极低比特量化，同时保持语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临显著的计算和内存挑战，需要极低比特量化来高效部署。

Method: 使用上采样结合Sigma-Delta量化器将权重二值化或三值化，采用Hadamard权重平滑减少精度损失，并提出细粒度的MultiOSR分配策略。

Result: 在OPT和LLaMA模型系列上的广泛实验表明，SDQ-LLM在极低过采样率设置下实现了更高效和高精度的性能。

Conclusion: SDQ-LLM框架能够有效实现LLM的极低比特量化，在保持性能的同时显著提升推理效率。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [316] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种轻量级二次增强器，通过在神经网络中引入二次变换来增强非线性能力，同时使用低秩、权重共享和稀疏化技术来减少参数和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络主要基于线性变换和非线性激活函数，但非线性能力有限。本文旨在通过引入二次变换来进一步增强神经网络的非线性表达能力，提升现有架构的性能。

Method: 提出轻量级二次增强器，采用低秩分解、权重共享和稀疏化技术，在每层特征之间引入二次交互，同时只增加少量额外参数和计算量。

Result: 在图像分类、文本分类和大语言模型微调三个任务上的实验表明，该方法在所有任务中都带来了明显且显著的性能提升。

Conclusion: 通过引入轻量级二次变换增强器，可以在不显著增加参数和计算成本的情况下，有效提升神经网络在各种任务上的性能表现。

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [317] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: 提出了一个可扩展的矩阵自由拉普拉斯框架，用于分解贝叶斯物理信息神经网络的后验Hessian矩阵，量化各物理约束对损失曲面的相对影响。


<details>
  <summary>Details</summary>
Motivation: 需要澄清单个物理约束如何塑造贝叶斯物理信息神经网络，因为物理约束可能导致过度自信，这可能是约束强制带来的合理精度而非校准错误。

Method: 引入可扩展的矩阵自由拉普拉斯框架，将后验Hessian矩阵分解为每个约束的贡献，并提供量化指标来衡量它们在损失曲面上的相对影响。

Result: 应用于Van der Pol方程，该方法跟踪约束如何塑造网络几何结构，并通过Hessian矩阵显示改变单个损失权重如何非平凡地重新分布曲率和有效主导地位。

Conclusion: 该方法能够直接通过Hessian矩阵分析物理约束对贝叶斯物理信息神经网络的影响，为理解约束如何塑造网络提供了新工具。

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [318] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: 提出了MemMamba框架，通过状态汇总机制和跨层跨token注意力，解决了Mamba模型长程记忆指数衰减问题，在保持线性复杂度的同时显著提升了长序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有长序列建模方法存在效率与内存的权衡：RNN有梯度消失问题，Transformer有二次复杂度限制，Mamba虽然高效但长程记忆会指数衰减。需要解决Mamba的长程遗忘问题。

Method: 通过数学推导和信息论分析揭示Mamba记忆衰减机制，提出水平-垂直记忆保真度指标，并设计MemMamba框架，集成状态汇总机制和跨层跨token注意力。

Result: 在PG19和Passkey Retrieval等长序列基准测试中显著优于现有Mamba变体和Transformer，推理效率提升48%。

Conclusion: MemMamba在复杂度-内存权衡上取得突破，为超长序列建模提供了新范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [319] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: LogAction是一个基于主动领域适应的日志异常检测模型，结合了迁移学习和主动学习技术，仅需2%的人工标注就能达到93.01%的平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法严重依赖标注，但大规模日志标注极具挑战性。迁移学习和主动学习方法存在源系统与目标系统数据分布差异和冷启动问题。

Method: 使用成熟系统的标注数据训练基础模型解决冷启动问题，采用自由能采样和不确定性采样选择分布边界处的日志进行人工标注，以最小化标注成本解决数据分布差异。

Result: 在六个不同数据集组合上的实验表明，LogAction仅需2%的人工标注就能达到平均93.01%的F1分数，优于现有最先进方法26.28%。

Conclusion: LogAction通过主动领域适应有效解决了日志异常检测中的标注依赖和数据分布差异问题，在最小化人工标注成本的同时实现了优异的检测性能。

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [320] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka是第一个针对扩散语言模型的系统性扩展定律，涵盖计算约束和数据约束两种场景，研究关键建模和优化设计。


<details>
  <summary>Details</summary>
Motivation: 为扩散语言模型的训练提供短期实践指导和长期AI社区启发，扩展Chinchilla定律的适用范围。

Method: 建立系统性扩展定律，分析计算约束和数据约束下的关键建模和优化设计。

Result: 提出了Quokka扩展定律，为扩散语言模型训练提供指导框架。

Conclusion: Quokka是Chinchilla的好朋友，提供更广泛的研究范围，有望为AI社区带来实践指导和灵感。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [321] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出了一种混合归因和剪枝（HAP）框架，用于快速发现语言模型中的忠实电路，比基线算法快46%且保持电路忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现算法面临基本权衡：归因修补速度快但不忠实于完整模型，而边缘剪枝忠实但计算成本高。需要一种平衡速度和忠实性的方法。

Method: 使用归因修补识别高潜力子图，然后应用边缘剪枝从中提取忠实电路。

Result: HAP比基线算法快46%，且不牺牲电路忠实性。在间接对象识别任务中，该方法保留了归因修补方法在高稀疏度下会剪枝的合作电路组件。

Conclusion: HAP可能是提高机制可解释性研究扩展到更大模型的有效方法。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [322] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: MACE是一个混合LLM系统，通过在边缘服务器上协同部署推理和微调任务，实现迭代级调度来平衡推理延迟和模型准确性。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器上的LLM应用需要频繁重训练以适应非平稳用户数据，但现有策略无法在有限GPU资源下同时保证推理延迟和模型准确性。

Method: 提出MACE系统，协同部署推理（预填充、解码）和微调任务，采用智能内存管理和迭代级调度，根据模型更新对输出对齐的影响分配GPU周期。

Result: 在跟踪驱动评估中，MACE在保持吞吐量的同时将推理延迟降低高达63%，GPU利用率维持在85%以上，优于连续重训练和周期性重训练。

Conclusion: 迭代级混合调度是在边缘平台上部署具有持续学习能力的LLM的有前景方向。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [323] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: Edge-FIT是一个用于在边缘设备上联邦指令微调大语言模型的可扩展框架，通过结合联邦学习和4位量化低秩适应来解决传统联邦学习方法在LLM上的通信和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法如FedAvg在面对大语言模型的巨大参数量时会失效，需要解决通信和计算开销问题以实现边缘设备的LLM部署。

Method: 结合联邦学习和4位量化低秩适应(QLORA)，在物联网领域过滤通用数据集进行指令微调。

Result: Edge-FIT微调的Llama 2(7B)模型达到0.89的F1分数，使用Phi-3-mini(3.8B)模型展示了可行的权衡方案。

Conclusion: Edge-FIT被验证为在家庭计算网关上部署去中心化LLM的可扩展框架。

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [324] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散语言模型的局限性，指出其在实现并行生成和双向注意力方面存在固有困难，并提出了最有效的训练和推理策略。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型相比自回归模型具有并行生成和双向注意力的优势，但现有的开源掩码扩散模型基于吸收扩散变体，存在实现这些优势的固有困难。

Method: 分析了掩码扩散模型在并行生成和双向注意力方面的技术挑战，并提出了优化的训练和推理策略。

Result: 证明了掩码扩散在实现并行生成和双向注意力方面存在固有困难，但通过提出的策略可以改善其性能。

Conclusion: 掩码扩散语言模型在实现并行生成和双向注意力方面存在技术挑战，需要采用特定的训练和推理策略来优化性能。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [325] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: 本文分析了Clifford卷积层的内部计算结构，提出了多种优化方法来加速推理过程，同时保持正确性。通过理论分析和系统优化，最终实现了21.35倍的平均加速。


<details>
  <summary>Details</summary>
Motivation: 随着物理科学中对具有等变性网络兴趣的增长，Clifford神经网络层作为实现E(n)和O(n)等变性的一种方法而备受关注。本文旨在优化其推理过程的性能。

Method: 首先分析Clifford代数的理论基础以消除冗余的矩阵分配和计算，然后系统性地应用已建立的优化技术来进一步提升性能。

Result: 在11个函数上实现了相对于基线实现21.35倍的平均加速，在6个案例中达到了与原始PyTorch实现相当甚至更快的运行时间，其余案例中性能与原始库处于同一数量级。

Conclusion: 通过理论分析和系统优化，成功提升了Clifford卷积层的推理性能，为等变性网络的实际应用提供了更高效的实现方案。

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [326] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: UniPruning是一个统一的后训练剪枝框架，结合了局部显著性度量的速度和全局协调的稳定性，通过镜像下降优化实现，无需更新模型权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临高昂的计算和内存成本，现有剪枝方法难以平衡效率和鲁棒性：局部方法在高稀疏度下容易崩溃，全局方法需要昂贵的权重更新或限制性半结构化格式。

Method: 使用快速层间评分和轻量级全局控制器分配单一稀疏度预算，支持非结构化和半结构化N:M剪枝，通过镜像下降优化实现全局协调。

Result: 在多个预训练LLM家族和标准基准测试中，UniPruning始终提供有竞争力或更优的困惑度和零样本准确率。

Conclusion: UniPruning为大规模LLM稀疏化提供了一个高效、原理性和可扩展的解决方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [327] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: LASER是一种即插即用的推理时路由算法，通过平衡专家负载来提升MoE模型的推理性能，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: MoE模型通过条件路由减少训练成本，但推理时专家参数和激活占用内存，导致专家负载不均衡，影响系统延迟、吞吐量和成本。

Method: LASER根据门函数得分分布自适应路由：当得分差异明显时选择最强专家；当得分均匀时扩大可行专家集并选择负载最轻的专家。无需重新训练或微调。

Result: 在Mixtral-8x7B和DeepSeek-MoE-16b-chat模型上测试，LASER改善了负载均衡，降低了延迟并提高了吞吐量，同时准确率变化可忽略不计。

Conclusion: LASER是一种有效的推理时负载均衡解决方案，能够直接集成到现有MoE推理流程中，显著提升系统性能而不影响模型精度。

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [328] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: CAFL-L是FedAvg的扩展方法，通过拉格朗日对偶优化显式处理设备资源约束（能量、通信、内存、热预算），动态调整训练超参数，在保持训练稳定性的同时实现更好的约束满足。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在资源受限边缘设备上的部署问题，传统FedAvg方法未充分考虑设备级资源约束，导致在实际部署中可能违反设备资源限制。

Method: 使用拉格朗日对偶优化动态调整训练超参数（冻结深度、本地步数、批量大小、通信压缩），通过梯度累积保持token预算来维持训练稳定性。

Result: 在字符级语言模型实验中，相比标准FedAvg，内存使用减少20%，通信减少95%，同时保持竞争力的验证性能。

Conclusion: CAFL-L方法在满足资源约束方面表现优异，适合在资源受限的边缘设备上实际部署。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [329] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: 提出一种结合XGBoost和神经网络的自适应集成框架，通过元学习实现模型动态选择和组合，提升预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 开发更智能灵活的机器学习系统，通过集成不同模型的优势来提升整体性能。

Method: 使用元学习协同结合XGBoost和神经网络，采用先进的不确定性量化技术和特征重要性集成，动态编排模型选择和组合。

Result: 实验结果表明在多个数据集上实现了优越的预测性能和增强的可解释性。

Conclusion: 该方法为开发更智能灵活的机器学习系统做出了贡献，证明了自适应集成框架的有效性。

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [330] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 论文揭示了概念擦除技术只是制造了"遗忘"的假象，实际上是通过偏置采样轨迹来避开目标概念，这种擦除是可逆的。作者提出了RevAm框架，通过RL轨迹优化来复活被擦除的概念。


<details>
  <summary>Details</summary>
Motivation: 随着模型架构演进到Flux等新一代架构，现有的概念擦除方法效果下降。研究发现这些方法并非真正删除概念，而是通过轨迹偏置制造遗忘假象，这种擦除本质上是可逆的。

Method: 提出RevAm框架，基于RL的轨迹优化方法，通过动态引导去噪过程来复活被擦除的概念，无需修改模型权重。采用Group Relative Policy Optimization (GRPO)来探索多样化的恢复轨迹。

Result: 实验表明RevAm在概念复活保真度方面表现优异，同时将计算时间减少了10倍，暴露了当前安全机制的关键漏洞。

Conclusion: 当前基于轨迹操纵的安全机制存在根本性漏洞，需要开发超越轨迹操纵的更强健擦除技术。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [331] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: 该论文分析机器学习在气候建模中的应用案例，重点关注工作流设计模式，包括替代建模、ML参数化、概率编程等，旨在为科学机器学习提供严谨性框架。


<details>
  <summary>Details</summary>
Motivation: 解决气候建模中机器学习应用面临的物理一致性、多尺度耦合、数据稀疏性等挑战，促进数据科学与气候建模的跨学科合作。

Method: 分析一系列应用机器学习研究案例，综合不同项目中的工作流设计模式，包括替代建模、ML参数化、概率编程、仿真推理和物理信息迁移学习。

Result: 提供了一个基于物理知识、仿真数据和观测集成的科学机器学习工作流框架。

Conclusion: 该研究为科学机器学习提供了确保严谨性的框架，包括透明模型开发、关键评估、知情适应和可重复性，有助于降低跨学科合作的门槛。

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [332] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的对比桥接方法，通过冻结的单模态编码器和简单的投影头，实现了化学分子指纹与生物医学文本嵌入的跨模态对齐，避免了大规模多模态预训练的需求。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基础模型通常需要大量预训练或多模态语料库，计算成本高昂。本文旨在探索是否可以通过轻量级的投影头在冻结的单模态编码器上实现化学和文本表示的对齐。

Method: 使用ChEMBL中的配对机制，通过双线性投影将ECFP4分子指纹与生物医学句子嵌入对齐，采用对比学习目标，并引入硬负样本加权和边界损失来处理共享相同治疗靶点的药物。

Result: 在基于支架的分割评估中，该方法实现了非平凡的跨模态对齐，相比冻结基线显著提高了靶点内区分能力。

Conclusion: 薄桥接方法为大规模多模态预训练提供了一种计算高效的替代方案，能够在精准医学中实现支架感知的药物文本对齐和靶点特异性检索。

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [333] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: LLMs能够复现行为运营管理中的大部分假设检验结果，但响应分布与人类数据存在差异；轻量级干预措施可以改善对齐效果


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在运营管理中模拟人类行为的能力，作为实验室实验、实地研究和调查的低成本补充工具

Method: 使用9个已发表的行为运营实验，评估两个标准：假设检验结果复现和通过Wasserstein距离的分布对齐；测试两种轻量级干预措施——思维链提示和超参数调优

Result: LLMs复现了大部分假设级效应，捕捉了关键决策偏差，但响应分布与人类数据存在差异；轻量级干预措施减少了不对齐，有时能让较小或开源模型匹配或超越较大系统

Conclusion: LLMs在行为运营管理中能有效复现假设检验结果，但需要干预措施来改善分布对齐；较小模型通过适当干预可以达到与大型商业模型相当的性能

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [334] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: 本文提出了一个包含数据质量参数的扩展缩放定律，将Chinchilla框架扩展到同时考虑模型大小、数据量和数据质量来预测损失。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型缩放定律主要关注模型大小和数据集规模，但缺乏对数据质量影响的系统化理论框架。

Method: 引入无量纲数据质量参数Q，基于有效样本量和信息论视角，提出两种Q的实用估计器：腐败率代理和缺陷度量。通过神经机器翻译和自回归建模中的合成实验系统控制数据质量。

Result: 损失随数据质量可预测地缩放，高质量数据可显著减小模型规模和计算需求。数据有效性随质量呈亚线性衰减，对适度数据腐败具有鲁棒性。

Conclusion: 建立了数据质量的显式、可推广定律，为大规模预训练中数据整理工作与模型规模的平衡提供了具体指导。

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [335] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: 提出一种神经网络方法，能够在约10毫秒内从正弦信号中重建数百赫兹的频率，比传统傅里叶方法快得多，精度提高2倍。同时开发了自动分类框架来识别信号中的物理干扰。


<details>
  <summary>Details</summary>
Motivation: 环形激光陀螺仪等设备需要从正弦信号中快速重建频率，传统方法需要数秒数据，无法满足实时应用需求。

Method: 使用神经网络方法进行频率重建，并开发自动分类框架来识别物理干扰（如激光不稳定性和地震事件）。

Result: 在10毫秒内重建数百赫兹频率，频率估计精度比标准傅里叶技术提高2倍；物理干扰分类在独立测试数据集上达到99%-100%的准确率。

Conclusion: 该方法在信号分析中整合人工智能，为地球物理应用提供了重要进展，实现了快速频率估计和自动干扰识别。

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [336] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: 提出CIC框架来增强强化学习算法的稳定性，通过维护代表性策略和当前策略，选择性更新代表性策略，并使用自适应调整机制共同促进critic训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练过程经常存在严重震荡，导致不稳定和性能下降，需要提高算法稳定性。

Method: CIC框架维护代表性策略和当前策略，只在当前策略表现更优时更新代表性策略，并采用自适应调整机制让两个策略共同促进critic训练。

Result: 在五个MuJoCo环境上的评估显示，CIC提高了传统算法的性能，且没有增加额外计算成本。

Conclusion: CIC框架能有效增强强化学习算法的稳定性，提升性能而不增加计算开销。

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [337] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: SchedMate是一个通过从源代码、运行时日志和历史作业中提取语义信息来增强深度学习调度器的框架，能够将作业完成时间减少高达1.91倍。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习调度器缺乏对作业语义上下文的理解，只能依赖有限的元数据，导致高分析开销、不可靠的持续时间估计、不充分的故障处理和差的可观测性。

Method: SchedMate通过三个基于LLM的组件非侵入式地增强现有调度器，系统地从源代码、运行时日志和历史作业等非结构化数据源提取深度洞察。

Result: 在128-GPU物理集群上的评估和生产跟踪的广泛模拟显示，SchedMate将平均作业完成时间减少了高达1.91倍，显著提升了调度性能。

Conclusion: 语义感知在现代深度学习调度中扮演关键角色，SchedMate通过弥合语义鸿沟证明了其有效性。

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [338] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: 本文研究了扩散模型中旋转对齐步骤的理论基础，发现对齐操作对应于矩阵Fisher分布的采样模式，并推导了在小噪声水平下的更好近似方法。


<details>
  <summary>Details</summary>
Motivation: 当训练点云（如分子和蛋白质）的扩散模型时，通常没有规范的方向定义。为了捕捉这种对称性，真实数据样本通过随机旋转进行增强，然后通过Kabsch-Umeyama算法进行旋转对齐来计算损失。然而，这种对齐步骤的效果尚未得到充分研究。

Method: 通过将最优去噪器表示为SO(3)上的矩阵Fisher分布，证明对齐操作对应于该分布的采样模式。在此基础上推导了小噪声水平下对最优去噪器的更好近似方法。

Result: 研究表明对齐操作是矩阵Fisher分布的采样模式，并且是小噪声水平下的零阶近似。实验表明对于扩散模型训练中最重要的噪声水平，对齐通常是一个'足够好'的近似。

Conclusion: 对齐操作在扩散模型训练中具有坚实的理论基础，对于实际训练中关键的噪声水平而言，它通常是一个有效的近似方法。

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [339] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: 该论文系统分析了Transformer模型中池化操作的理论表达能力和实际影响，为不同任务选择合适的池化机制提供了理论和实证指导。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型中的注意力机制已被广泛研究，但池化操作作为将token表示聚合为固定大小向量的关键步骤，其作用却未被充分探索，尽管它对模型行为有重要影响。

Method: 提出了一个理论框架，通过推导闭式边界来严格表征配备常用池化方法的Transformer模型的表达能力，分析扩展到不同的注意力变体，并在计算机视觉、自然语言处理和时间序列分析三个主要模态上进行实证评估。

Result: 研究揭示了池化选择对准确性、敏感性和优化行为的一致影响趋势，为特定任务选择或设计池化机制提供了实用指导。

Conclusion: 这项工作将池化定位为Transformer模型中的关键架构组件，并为超越仅关注注意力的更原则性模型设计奠定了基础。

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [340] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 开发了一个基于多目标强化学习和随机微分方程模拟器的疾病防控框架，能够平衡流行病控制与社会经济稳定，并在COVID-19、小儿麻痹症、流感等多种传染病上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行凸显了在疾病控制与社会经济稳定之间寻求平衡的迫切需求，需要开发能够同时考虑多个竞争目标的干预策略评估框架。

Method: 结合多目标强化学习（MORL）和新的随机微分方程（SDE）大流行模拟器，使用Pareto条件网络（PCN）代理进行训练，模拟器经过全球COVID-19数据校准和验证。

Result: 模拟器比其他常用RL模型具有更高保真度，能够量化流行病控制与经济稳定之间的政策权衡，并展示了不同病原体需要根本不同的干预策略。

Conclusion: 该框架为缓解公共卫生危机提供了稳健且适应性强的工具，支持透明、基于证据的政策制定，特别是在疫苗接种覆盖率下降时能评估更严格干预措施的必要性。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [341] [Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models](https://arxiv.org/abs/2510.03345)
*Luoma Ke,Guangpeng Zhang,Jibo He,Yajing Li,Yan Li,Xufeng Liu,Peng Fang*

Main category: cs.LG

TL;DR: 提出了一种结合机器学习和VR技术的飞行员选拔新方法，使用SVM和MIC特征选择算法，在准确率、AUC和F1分数上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着航空业快速发展，需要大量飞行员，如何高效、低成本地选拔合适飞行员成为重要研究问题。

Method: 招募23名中国东方航空飞行员和23名清华大学新手，使用VR模拟平台采集眼动和飞行动态数据，应用SVM分类器结合MIC特征选择方法。

Result: SVM+MIC方法在所有指标上表现最佳：准确率0.93、AUC 0.96、F1分数0.93，优于其他四种分类器和两种特征选择方法。

Conclusion: 该研究的VR模拟平台和算法可用于飞行员选拔和培训，SVM+MIC算法是基于眼动和飞行动态数据的首个实现，性能优于现有所有飞行员选拔算法。

Abstract: With the rapid growth of the aviation industry, there is a need for a large
number of flight crew. How to select the right pilots in a cost-efficient
manner has become an important research question. In the current study,
twenty-three pilots were recruited from China Eastern Airlines, and 23 novices
were from the community of Tsinghua University. A novel approach incorporating
machine learning and virtual reality technology was applied to distinguish
features between these participants with different flight skills. Results
indicate that SVM with the MIC feature selection method consistently achieved
the highest prediction performance on all metrics with an Accuracy of 0.93, an
AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier
algorithms and two other feature selection methods. From the perspective of
feature selection methods, the MIC method can select features with a nonlinear
relationship to sampling labels, instead of a simple filter-out. Our new
implementation of the SVM + MIC algorithm outperforms all existing pilot
selection algorithms and perhaps provides the first implementation based on eye
tracking and flight dynamics data. This study's VR simulation platforms and
algorithms can be used for pilot selection and training.

</details>


### [342] [KVComm: Enabling Efficient LLM Communication through Selective KV Sharing](https://arxiv.org/abs/2510.03346)
*Xiangyu Shi,Marco Chiesa,Gerald Q. Maguire Jr.,Dejan Kostic*

Main category: cs.LG

TL;DR: KVComm是一种新的LLM间通信框架，通过选择性共享KV对实现高效通信，仅传输30%层的KV对就能达到与直接合并输入方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统的通信协议存在自然语言通信推理成本高、信息丢失，或隐藏状态通信存在信息集中偏差和效率低下的问题。

Method: 提出KVComm框架，基于注意力重要性分数和高斯先验的KV层选择策略，识别最具信息量的KV对进行通信。

Result: 在多种任务和模型对上实验表明，KVComm仅传输30%层的KV对就能达到与直接合并输入方法相当的性能。

Conclusion: KV对作为LLM间通信媒介具有巨大潜力，为可扩展和高效的多智能体系统铺平了道路。

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent
systems, where effective inter-model communication is crucial. Existing
communication protocols either rely on natural language, incurring high
inference costs and information loss, or on hidden states, which suffer from
information concentration bias and inefficiency. To address these limitations,
we propose KVComm, a novel communication framework that enables efficient
communication between LLMs through selective sharing of KV pairs. KVComm
leverages the rich information encoded in the KV pairs while avoiding the
pitfalls of hidden states. We introduce a KV layer-wise selection strategy
based on attention importance scores with a Gaussian prior to identify the most
informative KV pairs for communication. Extensive experiments across diverse
tasks and model pairs demonstrate that KVComm achieves comparable performance
to the upper-bound method, which directly merges inputs to one model without
any communication, while transmitting as few as 30\% of layers' KV pairs. Our
study highlights the potential of KV pairs as an effective medium for inter-LLM
communication, paving the way for scalable and efficient multi-agent systems.

</details>


### [343] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster是一个用于龙卷风预测的框架，使用多模态大语言模型处理异构时空数据，评估了模型在40天历史数据上的表现，发现人类专家显著优于最先进模型。


<details>
  <summary>Details</summary>
Motivation: 需要评估大语言模型在复杂、高影响的真实世界任务中的表现，以检验其作为推理智能体的真实准备程度。

Method: 使用多模态大语言模型端到端处理高分辨率对流允许预报档案中的异构时空数据，从3625个预报图和40125个预报探空数据中交互查询，进行12-36小时预报。

Result: 人类专家显著优于最先进模型，模型倾向于产生幻觉和过度预测风险强度，在精确地理定位方面表现不佳，在复杂动态演化系统中的时空推理能力差。

Conclusion: AgentCaster旨在推进改进大语言模型智能体在关键领域挑战性推理任务方面的研究。

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [344] [Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks](https://arxiv.org/abs/2510.03351)
*Song Wang,Zhenyu Lei,Zhen Tan,Jundong Li,Javier Rasero,Aiying Zhang,Chirag Agarwal*

Main category: cs.LG

TL;DR: CONCEPTNEURO是一个基于概念的精神疾病诊断框架，利用大语言模型和神经生物学知识自动生成、筛选和编码可解释的功能连接概念，通过概念分类器实现透明且高性能的诊断。


<details>
  <summary>Details</summary>
Motivation: 近五分之一的青少年患有精神或行为健康问题，迫切需要开发准确且可解释的诊断工具。现有的图神经网络方法虽然有效，但缺乏可解释性，限制了临床应用的可靠性。

Method: 结合大语言模型和神经生物学领域知识，自动生成功能连接概念，每个概念表示为连接特定脑区的结构化子图，然后通过概念分类器进行预测。

Result: 在多个精神疾病数据集上的实验表明，CONCEPTNEURO增强的图神经网络始终优于原始版本，在提高准确性的同时提供透明、临床对齐的解释。

Conclusion: CONCEPTNEURO建立了基于概念的可解释框架，概念分析揭示了与专家知识一致的疾病特异性连接模式，并为未来研究提出了新假设。

Abstract: Nearly one in five adolescents currently live with a diagnosed mental or
behavioral health condition, such as anxiety, depression, or conduct disorder,
underscoring the urgency of developing accurate and interpretable diagnostic
tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a
powerful lens into large-scale functional connectivity, where brain regions are
modeled as nodes and inter-regional synchrony as edges, offering clinically
relevant biomarkers for psychiatric disorders. While prior works use graph
neural network (GNN) approaches for disorder prediction, they remain complex
black-boxes, limiting their reliability and clinical translation. In this work,
we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages
large language models (LLMs) and neurobiological domain knowledge to
automatically generate, filter, and encode interpretable functional
connectivity concepts. Each concept is represented as a structured subgraph
linking specific brain regions, which are then passed through a concept
classifier. Our design ensures predictions through clinically meaningful
connectivity patterns, enabling both interpretability and strong predictive
performance. Extensive experiments across multiple psychiatric disorder
datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform
their vanilla counterparts, improving accuracy while providing transparent,
clinically aligned explanations. Furthermore, concept analyses highlight
disorder-specific connectivity patterns that align with expert knowledge and
suggest new hypotheses for future investigation, establishing CONCEPTNEURO as
an interpretable, domain-informed framework for psychiatric disorder diagnosis.

</details>


### [345] [High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)](https://arxiv.org/abs/2510.03355)
*Aryan Patel*

Main category: cs.LG

TL;DR: 开发了基于LSTM的迁移学习框架，利用轴向疲劳数据预测铝合金的高周扭转S-N曲线，显著降低疲劳性能测试成本。


<details>
  <summary>Details</summary>
Motivation: 铝合金疲劳性能测试耗时且昂贵，特别是高周疲劳数据。需要一种方法来减少测试成本和时间。

Method: 使用LSTM网络构建迁移学习框架，先用铝合金7075-T6的纯轴向疲劳数据训练源模型，然后迁移到预测高周扭转S-N曲线。

Result: 该框架能够准确预测铝合金在高周范围内的扭转S-N曲线。

Conclusion: 该框架可大幅降低获取不同材料疲劳特性的成本，帮助在更好的成本和时间约束下优先安排测试。

Abstract: Aluminum is a widely used alloy, which is susceptible to fatigue failure.
Characterizing fatigue performance for materials is extremely time and cost
demanding, especially for high cycle data. To help mitigate this, a transfer
learning based framework has been developed using Long short-term memory
networks (LSTMs) in which a source LSTM model is trained based on pure axial
fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict
high cycle torsional S-N curves. The framework was able to accurately predict
Al torsional S-N curves for a much higher cycle range. It is the belief that
this framework will help to drastically mitigate the cost of gathering fatigue
characteristics for different materials and help prioritize tests with better
cost and time constraints.

</details>


### [346] [Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility](https://arxiv.org/abs/2510.03358)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 本文通过秩结构分析时间序列Transformer，发现其嵌入具有快速衰减的奇异值谱，导致注意力层可压缩。基于此压缩了Chronos模型，在保持精度下显著减少推理时间和内存。


<details>
  <summary>Details</summary>
Motivation: 文本模型的原理不能很好地迁移到其他模态的Transformer模型，特别是时间序列数据具有与文本或视觉显著不同的结构特性。

Method: 通过秩结构分析时间序列嵌入的奇异值谱，证明Q/K/V投影可低秩近似，注意力层可压缩。引入秩流概念解释非线性混合导致秩随深度增长。

Result: 成功压缩Chronos大型时间序列基础模型，推理时间减少65%，内存减少81%，且无精度损失。

Conclusion: 为时间序列基础模型的宽度、深度和注意力头分配提供了原则性指导，并利用其固有可压缩性实现高效压缩。

Abstract: Transformers are widely used across data modalities, and yet the principles
distilled from text models often transfer imperfectly to models trained to
other modalities. In this paper, we analyze Transformers through the lens of
rank structure. Our focus is on the time series setting, where the structural
properties of the data differ remarkably from those of text or vision. We show
that time-series embeddings, unlike text or vision, exhibit sharply decaying
singular value spectra: small patch sizes and smooth continuous mappings
concentrate the data into low-rank subspaces. From this, we prove that the
associated $Q/K/V$ projections admit accurate low-rank approximations, and that
attention layers become compressible in proportion to the decay of the
embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by
which nonlinear mixing across depth inflates the rank, explaining why early
layers are most amenable to compression and why ranks grow with depth. Guided
by these theoretical and empirical results, we use these insights to compress
Chronos, a large time series foundation model, achieving a reduction of $65\%$
in inference time and $81\%$ in memory, without loss of accuracy. Our findings
provide principled guidance for allocating width, depth, and heads in time
series foundation models, and for exploiting their inherent compressibility.

</details>


### [347] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息神经算子(PINO)的深度强化学习框架PINO-PC，用于湍流建模和控制，在未见过的高雷诺数流动中实现了39.0%的减阻效果。


<details>
  <summary>Details</summary>
Motivation: 数值评估湍流控制对壁面摩擦的影响需要昂贵的湍流流体动力学模拟，因此需要开发更高效的建模和控制方法。

Method: 使用基于模型的强化学习进行预测控制，通过物理信息神经算子(PINO)联合学习湍流控制的策略和观测器模型，PINO具有离散不变性并能准确捕捉湍流中的精细尺度。

Result: PINO-PC在各种具有挑战性的高雷诺数未见过流动场景中优于先前的无模型强化学习方法，在雷诺数为15,000时实现了39.0%的减阻，比之前的流体控制方法提高了32%以上。

Conclusion: PINO-PC框架为湍流建模和控制提供了一种高效且有效的方法，特别适用于高雷诺数未见过流动的控制任务。

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [348] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: 提出一种基于开源数据的车辆排放估算框架，集成MOVES模型、GPS轨迹、OSM路网等数据，通过神经网络预测操作模式分布，在波士顿地区验证显示排放估算精度提升50%以上。


<details>
  <summary>Details</summary>
Motivation: 利用开源数据为城市区域车辆活动和排放估算提供可扩展且透明的解决方案，解决传统方法成本高、数据获取难的问题。

Method: 集成MOVES模型、开源GPS轨迹数据、OpenStreetMap路网、区域交通数据集和卫星影像特征向量，训练神经网络模型预测MOVES定义的操作模式分布。

Result: 在波士顿大都会区45个城市应用，相比MOVES基准，关键污染物（CO、NOx、CO2、PM2.5）的区域尺度交通排放RMSE降低超过50%。

Conclusion: 证明了使用完全开源数据进行低成本、可复制、数据驱动的排放估算的可行性。

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [349] [Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds](https://arxiv.org/abs/2510.03364)
*Xiaolong Ma,Xu Dong,Ashley Tarrant,Lei Yang,Rao Kotamarthi,Jiali Wang,Feng Yan,Rajkumar Kettimuthu*

Main category: cs.LG

TL;DR: WindSR是一个用于轮毂高度风速超分辨率降尺度的扩散模型，通过数据同化将稀疏观测数据与模拟场整合，结合地形信息，在降尺度效率和准确性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 高质量的轮毂高度风观测数据在时空上稀疏，而模拟数据虽然广泛可用但存在偏差且分辨率不足，无法满足风电场选址或极端天气风险评估的需求。

Method: 开发了WindSR扩散模型，采用动态半径融合方法将观测数据与模拟场结合，在训练和推理过程中融入地形信息，使用数据同化技术进行超分辨率降尺度。

Result: 与卷积神经网络和生成对抗网络基线相比，WindSR在降尺度效率和准确性上表现更优，数据同化使模型偏差相对于独立观测降低了约20%。

Conclusion: WindSR通过整合观测和模拟数据，结合地形信息，成功实现了高质量的轮毂高度风速超分辨率降尺度，为风能应用提供了更准确的风速数据。

Abstract: High-quality observations of hub-height winds are valuable but sparse in
space and time. Simulations are widely available on regular grids but are
generally biased and too coarse to inform wind-farm siting or to assess
extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully
utilize both data types for generating high-quality, high-resolution hub-height
wind speeds (tens to ~100m above ground), this study introduces WindSR, a
diffusion model with data assimilation for super-resolution downscaling of
hub-height winds. WindSR integrates sparse observational data with simulation
fields during downscaling using state-of-the-art diffusion models. A
dynamic-radius blending method is introduced to merge observations with
simulations, providing conditioning for the diffusion process. Terrain
information is incorporated during both training and inference to account for
its role as a key driver of winds. Evaluated against
convolutional-neural-network and generative-adversarial-network baselines,
WindSR outperforms them in both downscaling efficiency and accuracy. Our data
assimilation reduces WindSR's model bias by approximately 20% relative to
independent observations.

</details>


### [350] [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
*Harshwardhan Fartale,Ashish Kattamuri,Rahul Raja,Arpita Vats,Ishita Prasad,Akshata Kishore Moharir*

Main category: cs.LG

TL;DR: 该论文通过机械可解释性方法，发现Transformer语言模型中的记忆和推理能力依赖于可分离但相互作用的神经回路，为模型功能专业化提供了因果证据。


<details>
  <summary>Details</summary>
Motivation: 区分记忆和推理能力对于预测模型泛化、设计针对性评估以及构建更安全的干预措施至关重要，但目前尚不清楚这两种能力是否依赖不同的内部机制。

Method: 使用机械可解释性方法，结合激活修补和结构化消融，在受控的合成语言谜题数据集上探测Transformer模型的层、头和神经元级别贡献。

Result: 在Qwen和LLaMA模型家族中发现，干预不同层和注意力头会导致选择性损伤：禁用"记忆回路"使事实检索准确率下降达15%但推理能力完好，禁用"推理回路"使多步推理能力下降类似幅度。

Conclusion: 研究首次提供了因果证据表明记忆和推理在Transformer模型中依赖于可分离但相互作用的回路，这些发现推进了机械可解释性研究，并为大型语言模型的安全部署提供了机制性见解。

Abstract: Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

</details>


### [351] [Distributed Low-Communication Training with Decoupled Momentum Optimization](https://arxiv.org/abs/2510.03371)
*Sasho Nedelkoski,Alexander Acker,Odej Kao,Soeren Becker,Dominik Scheinert*

Main category: cs.LG

TL;DR: 提出了一种结合低频同步和梯度动量压缩的方法，通过离散余弦变换将Nesterov动量分解为高低频分量，仅同步高频分量，显著减少分布式训练中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 减少对高带宽互联的依赖，使分布式计算资源能够替代集中式数据中心训练大型模型。

Method: 将优化器动量视为信号，通过离散余弦变换分解Nesterov动量为高低频分量，每H步仅同步高频分量。

Result: 相比基线DiLoCo实现了16倍的通信减少，在transformer语言模型和卷积神经网络上均有效。

Conclusion: 该方法推进了在低带宽互联的分布式节点上训练大型模型的可行性。

Abstract: The training of large models demands substantial computational resources,
typically available only in data centers with high-bandwidth interconnects.
However, reducing the reliance on high-bandwidth interconnects between nodes
enables the use of distributed compute resources as an alternative to
centralized data center training. Building on recent advances in distributed
model training, we propose an approach that further reduces communication by
combining infrequent synchronizations across distributed model replicas with
gradient momentum compression. In particular, we treat the optimizer momentum
as a signal and decompose the Nesterov momentum into high- and low-frequency
components via the discrete cosine transform (DCT). Only the high-frequency
components are synchronized across model replicas every $H$ steps. Empirically,
our method achieves up to a $16\times$ reduction in communication compared to
the baseline DiLoCo, and it generalizes across architectures, including
transformer-based language models and convolutional neural networks for images.
Overall, this work advances the feasibility of training large models on
distributed nodes with low-bandwidth interconnects.

</details>


### [352] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: 提出了一种新的数据自由知识蒸馏方法CPSC-DFKD，通过条件生成对抗网络合成类别特定的多样化图像，改进生成器模块以区分不同类别的分布，并引入伪监督对比学习来增强多样性。


<details>
  <summary>Details</summary>
Motivation: 现有数据自由知识蒸馏方法存在三个主要问题：缺乏伪监督学习范式、无法区分不同类别样本分布导致生成模糊样本、无法优化类别多样性样本。

Method: 使用条件生成对抗网络合成类别特定的多样化图像，改进生成器模块以更好区分不同类别分布，并基于教师和学生视图提出伪监督对比学习来增强多样性。

Result: 在三个常用数据集上的综合实验验证了CPSC-DFKD在提升学生模型和生成器性能方面的有效性。

Conclusion: CPSC-DFKD通过引入条件生成对抗网络和伪监督对比学习，有效解决了当前数据自由知识蒸馏方法的局限性，显著提升了学生模型的性能。

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [353] [A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew](https://arxiv.org/abs/2510.03380)
*Michael Ben Ali,Imen Megdiche,André Peninou,Olivier Teste*

Main category: cs.LG

TL;DR: 本文评估了现有聚类联邦学习算法在数量倾斜问题下的表现，并提出了一种新的迭代算法CORNFLQS，该算法在准确性和聚类质量方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的非独立同分布数据是一个关键挑战，特别是数量倾斜问题。现有的聚类联邦学习方法缺乏在数量倾斜设置下的系统性评估，且面临显著挑战。

Method: 提出了CORNFLQS算法，该算法在两种CFL操作策略之间实现最优协调：客户端选择最小化本地训练损失的集群，以及服务器基于本地模型相似性对客户端进行分组。

Result: 在6个图像分类数据集上的270个非独立同分布配置实验中，CORNFLQS在准确性和聚类质量方面获得了最高平均排名，并对数量倾斜扰动表现出强鲁棒性。

Conclusion: CORNFLQS算法优于现有的CFL算法，能够有效应对数量倾斜问题，在联邦学习环境中提供更好的模型性能和聚类效果。

Abstract: Federated Learning (FL) is a decentralized paradigm that enables a
client-server architecture to collaboratively train a global Artificial
Intelligence model without sharing raw data, thereby preserving privacy. A key
challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of
Non-IID, where clients hold highly heterogeneous data volumes. Clustered
Federated Learning (CFL) is an emergent variant of FL that presents a promising
solution to Non-IID problem. It improves models' performance by grouping
clients with similar data distributions into clusters. CFL methods generally
fall into two operating strategies. In the first strategy, clients select the
cluster that minimizes the local training loss. In the second strategy, the
server groups clients based on local model similarities. However, most CFL
methods lack systematic evaluation under QS but present significant challenges
because of it. In this paper, we present two main contributions. The first one
is an evaluation of state-of-the-art CFL algorithms under various Non-IID
settings, applying multiple QS scenarios to assess their robustness. Our second
contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes
an optimal coordination between both operating strategies of CFL. Our approach
is robust against the different variations of QS settings. We conducted
intensive experiments on six image classification datasets, resulting in 270
Non-IID configurations. The results show that CORNFLQS achieves the highest
average ranking in both accuracy and clustering quality, as well as strong
robustness to QS perturbations. Overall, our approach outperforms actual CFL
algorithms.

</details>


### [354] [Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges](https://arxiv.org/abs/2510.03381)
*Yongchao Li,Jun Chen,Zhuoxuan Li,Chao Gao,Yang Li,Chu Zhang,Changyin Dong*

Main category: cs.LG

TL;DR: 提出STDAE框架，通过跨模态重建预训练从主线数据重建历史匝道流量，解决匝道检测器缺失导致的交通预测盲区问题。


<details>
  <summary>Details</summary>
Motivation: 高速公路互通立交是车辆转换的关键节点，但缺乏实时匝道检测器导致交通预测存在盲区。

Method: 使用解耦的时空自编码器两阶段框架：第一阶段从主线数据重建历史匝道流量，第二阶段将学习到的表示与GWNet等模型集成进行预测。

Result: 在三个真实世界互通立交数据集上的实验表明，STDAE-GWNET持续优于13个最先进的基线方法，性能接近使用历史匝道数据的模型。

Conclusion: 该方法有效克服检测器稀缺问题，具有即插即用潜力，适用于多种预测流程。

Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet
the lack of real-time ramp detectors creates blind spots in traffic prediction.
To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a
two-stage framework that leverages cross-modal reconstruction pretraining. In
the first stage, STDAE reconstructs historical ramp flows from mainline data,
forcing the model to capture intrinsic spatio-temporal relations. Its decoupled
architecture with parallel spatial and temporal autoencoders efficiently
extracts heterogeneous features. In the prediction stage, the learned
representations are integrated with models such as GWNet to enhance accuracy.
Experiments on three real-world interchange datasets show that STDAE-GWNET
consistently outperforms thirteen state-of-the-art baselines and achieves
performance comparable to models using historical ramp data. This demonstrates
its effectiveness in overcoming detector scarcity and its plug-and-play
potential for diverse forecasting pipelines.

</details>


### [355] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: 使用可验证奖励的强化学习(RLVR)训练韩语接龙游戏，通过课程学习缓解规则奖励冲突


<details>
  <summary>Details</summary>
Motivation: 研究RLVR在多语言逻辑谜题中的应用，特别是韩语接龙游戏中规则奖励的自然冲突问题

Method: 在韩语接龙游戏中应用RLVR，采用课程学习方案来缓解规则奖励之间的冲突

Result: 实验证明课程学习能够有效缓解规则奖励冲突

Conclusion: 研究结果鼓励在多样化语言环境中进一步研究谜题任务

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [356] [Training Variation of Physically-Informed Deep Learning Models](https://arxiv.org/abs/2510.03416)
*Ashley Lenau,Dennis Dimiduk,Stephen R. Niezgoda*

Main category: cs.LG

TL;DR: 该研究探讨了深度学习训练算法的可靠性和可复现性，特别关注物理信息损失函数在强制边界条件方面的表现。通过Pix2Pix网络预测高弹性对比复合材料应力场的案例研究，评估了不同损失函数的训练变异性。


<details>
  <summary>Details</summary>
Motivation: 随着物理信息损失函数的流行，需要评估损失函数在强制特定边界条件方面的可靠性。报告模型变异性对于评估损失函数一致训练网络的能力和不同方法间的公平比较至关重要。

Method: 使用Pix2Pix网络预测高弹性对比复合材料的应力场作为案例研究，实施了几种不同的强制应力平衡的损失函数，并在多次训练会话中评估其收敛性、准确性和强制应力平衡的变异性。

Result: 不同的损失函数在收敛性、准确性和强制应力平衡方面表现出不同程度的变异性，某些损失函数比其他函数更可靠。

Conclusion: 报告模型变异性对于评估训练算法的可靠性和可复现性至关重要，研究分享了报告模型变异性的建议实践，为不同方法提供了更公平的比较基础。

Abstract: A successful deep learning network is highly dependent not only on the
training dataset, but the training algorithm used to condition the network for
a given task. The loss function, dataset, and tuning of hyperparameters all
play an essential role in training a network, yet there is not much discussion
on the reliability or reproducibility of a training algorithm. With the rise in
popularity of physics-informed loss functions, this raises the question of how
reliable one's loss function is in conditioning a network to enforce a
particular boundary condition. Reporting the model variation is needed to
assess a loss function's ability to consistently train a network to obey a
given boundary condition, and provides a fairer comparison among different
methods. In this work, a Pix2Pix network predicting the stress fields of high
elastic contrast composites is used as a case study. Several different loss
functions enforcing stress equilibrium are implemented, with each displaying
different levels of variation in convergence, accuracy, and enforcing stress
equilibrium across many training sessions. Suggested practices in reporting
model variation are also shared.

</details>


### [357] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: 提出了多任务神经扩散过程（MT-NDP）框架用于风电功率预测，通过任务编码器捕捉跨风机相关性，并在真实SCADA数据上首次实证评估NDPs。


<details>
  <summary>Details</summary>
Motivation: 不确定性感知的风电功率预测对于电网集成和风电场可靠运行至关重要，需要能够提供校准且可信的预测区间。

Method: 扩展神经扩散过程（NDPs）到多任务框架（MT-NDP），引入任务编码器捕捉跨风机相关性，支持对未见风机的少样本适应。

Result: MT-NDP在点精度和校准方面优于单任务NDPs和高斯过程，特别对于偏离平均行为的风机表现更好，提供更尖锐但可信的预测区间。

Conclusion: 基于NDP的模型提供校准且可扩展的预测，适用于实际部署，能够支持现代风电场的调度和维护决策。

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [358] [Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices](https://arxiv.org/abs/2510.03425)
*Congzheng Song,Xinyu Tang*

Main category: cs.LG

TL;DR: 提出了一种内存高效的移动设备反向传播实现方法MeBP，能够在内存受限的设备上微调大语言模型，在内存使用和计算时间之间提供更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播方法在移动设备上内存消耗大，而零阶优化方法虽然内存占用小但收敛速度慢，需要寻找更好的平衡方案。

Method: 开发了内存高效的反向传播实现MeBP，优化内存使用同时保持计算效率，支持在移动设备上微调0.5B到4B参数的LLM。

Result: 在iPhone 15 Pro Max上验证，各种LLM可以使用少于1GB内存进行微调，比零阶优化基线收敛更快且性能更好。

Conclusion: MeBP为移动设备上的LLM微调提供了实用的解决方案，在内存使用和计算效率之间实现了更好的平衡。

Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even
for a subset of parameters such as LoRA\textemdash can be much more
memory-consuming than inference and is often deemed impractical for
resource-constrained mobile devices. Alternative methods, such as zeroth-order
optimization (ZO), can greatly reduce the memory footprint but come at the cost
of significantly slower model convergence (10$\times$ to 100$\times$ more steps
than backpropagation). We propose a memory-efficient implementation of
backpropagation (MeBP) on mobile devices that provides better trade-off between
memory usage and compute time, while converging faster and achieving better
performance than the ZO baseline. We verify the effectiveness of MeBP on an
iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B
parameters, can be fine-tuned using less than 1GB of memory. We release an
example of the MeBP implementation at https://github.com/apple/ml-mebp.

</details>


### [359] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 提出了广义数量级(GOOMs)方法，扩展传统数量级概念以包含浮点数，通过并行前缀扫描实现在GPU上的高效计算，解决了长序列计算中的数值下溢/上溢问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习、金融等领域需要在长序列上进行实数复合计算，传统方法容易导致灾难性的数值下溢或上溢，需要一种更稳定的计算方法。

Method: 引入广义数量级(GOOMs)概念，实现自定义并行前缀扫描算法，支持在GPU等并行硬件上的原生执行。

Result: GOOMs方法在三个代表性实验中表现优异：超越标准浮点数限制的实数矩阵乘积复合计算；并行估计Lyapunov指数谱，速度比之前方法快几个数量级；在深度循环神经网络中捕获长程依赖关系，无需任何稳定化处理。

Conclusion: GOOMs结合高效并行扫描为高动态范围应用提供了可扩展且数值鲁棒的替代方案，使之前被认为不切实际或不可能的计算变得可行和实用。

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [360] [LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation](https://arxiv.org/abs/2510.03432)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: 提出了LHGEL框架，通过集成学习解决大规模异构图学习问题，包含批量视图聚合、残差注意力和多样性正则化三个关键组件，在五个真实异构网络上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模异构图的挑战包括网络规模大、节点和边类型异构、节点特征变化以及复杂的局部邻域结构，集成学习能自然解决这些问题。

Method: LHGEL框架包含三个关键组件：批量视图聚合（采样子图形成多个图视图）、残差注意力（自适应加权视图贡献）、多样性正则化（鼓励不同视图的表示差异）。

Result: 在五个真实异构网络上的实验结果表明，LHGEL方法始终以显著优势优于最先进的竞争对手。

Conclusion: LHGEL通过集成学习和三个关键组件有效解决了大规模异构图学习问题，理论分析表明残差注意力缓解了集成学习中的梯度消失问题。

Abstract: Learning from large heterogeneous graphs presents significant challenges due
to the scale of networks, heterogeneity in node and edge types, variations in
nodal features, and complex local neighborhood structures. This paper advocates
for ensemble learning as a natural solution to this problem, whereby training
multiple graph learners under distinct sampling conditions, the ensemble
inherently captures different aspects of graph heterogeneity. Yet, the crux
lies in combining these learners to meet global optimization objective while
maintaining computational efficiency on large-scale graphs. In response, we
propose LHGEL, an ensemble framework that addresses these challenges through
batch sampling with three key components, namely batch view aggregation,
residual attention, and diversity regularization. Specifically, batch view
aggregation samples subgraphs and forms multiple graph views, while residual
attention adaptively weights the contributions of these views to guide node
embeddings toward informative subgraphs, thereby improving the accuracy of base
learners. Diversity regularization encourages representational disparity across
embedding matrices derived from different views, promoting model diversity and
ensemble robustness. Our theoretical study demonstrates that residual attention
mitigates gradient vanishing issues commonly faced in ensemble learning.
Empirical results on five real heterogeneous networks validate that our LHGEL
approach consistently outperforms its state-of-the-art competitors by
substantial margin. Codes and datasets are available at
https://github.com/Chrisshen12/LHGEL.

</details>


### [361] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: 该论文研究了核变点检测在文本分割中的应用，建立了m-依赖数据下的理论保证，并通过LLM模拟和实证研究验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的核变点检测理论主要基于独立性假设，而真实文本数据存在强依赖性，需要建立更符合实际情况的理论保证。

Method: 建立了m-依赖数据下的理论保证，使用LLM生成合成m-依赖文本来验证渐近性质，并在多个文本数据集上进行了全面的实证研究。

Result: 证明了在m-依赖数据下变点检测数量和位置的弱一致性，实证研究表明使用文本嵌入的KCPD在标准文本分割指标上优于基线方法。

Conclusion: KCPD不仅具有理论和模拟可靠性，在实际文本分割任务中也表现出良好的有效性，特别是在使用现代文本嵌入的情况下。

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [362] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: 本文提出使用结构化论证方法为AI系统提供可验证的解释，替代传统的可解释性和LLM生成解释方法。该方法在论证关系分类任务上达到94.44宏F1分数，并在多智能体风险评估中实现透明协作和自动幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 人类是黑盒子，无法观察其神经过程，但社会通过评估可验证的论证来运作。AI可解释性应遵循这一原则：利益相关者需要可验证的推理链，而非机制透明度。

Method: 使用结构化论证方法，将LLM文本转换为论证图，在推理的每个步骤实现验证。采用双极假设基础论证捕捉支持/攻击关系，通过事实节点攻击论证实现自动幻觉检测。

Result: 在AAEC数据集上达到94.44宏F1分数（比先前工作高5.7分），在Argumentative MicroTexts关系分类上达到0.81宏F1分数（比可比数据设置下的先前结果高约0.07）。

Conclusion: 结构化论证方法提供了一种既非可解释性也非LLM生成解释所能提供的解释和验证水平，支持无需重新训练即可通过测试时反馈进行迭代优化。

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [363] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: 本文通过残差展开定理证明了深度残差网络实际上表现为浅层网络的隐式集成，网络深度的增加等价于集成规模的扩大，这解释了归一化层的历史必要性。


<details>
  <summary>Details</summary>
Motivation: 理解为什么深度残差网络如此有效，特别是验证残差网络是否真的像Veit等人提出的那样表现为浅层网络的集成。

Method: 提出了残差展开定理，通过解析公式证明深度残差网络的数学等价性，并分析其分层集成结构。

Result: 发现网络深度的增加等价于隐式集成规模的扩大，计算路径的组合增长导致输出信号爆炸，这解释了归一化层的必要性。

Conclusion: 残差模块的缩放提供了控制组合爆炸的原则性解决方案，同时作为容量控制机制隐式正则化模型复杂度。

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [364] [How to Set $β_1, β_2$ in Adam: An Online Learning Perspective](https://arxiv.org/abs/2510.03478)
*Quan Nguyen*

Main category: cs.LG

TL;DR: 本文对Adam优化器的动量参数β₁和β₂进行了理论分析，突破了以往要求β₁=√β₂的限制，提出了适用于β₁≥√β₂和β₁≤√β₂的新分析框架，证明了新界限的紧致性，并揭示了不同对抗环境下最优参数设置的差异。


<details>
  <summary>Details</summary>
Motivation: Adam优化器在实践中被广泛使用，但其动量参数β₁和β₂的最优设置缺乏完整的理论指导。现有分析要求β₁=√β₂，这限制了理论结果在实际应用中的适用性。

Method: 将Adam视为Follow-the-Regularized-Leader(FTRL)算法的实例，推导出适用于β₁≥√β₂和β₁≤√β₂两种情况的新的、更一般的理论分析框架。

Result: 新分析框架严格推广了现有界限，并在最坏情况下证明界限是紧致的。同时发现β₁=√β₂对于无意识对手是最优的，但对于有意识对手是次优的。

Conclusion: 本文提供了更全面的Adam优化器理论分析，为不同对抗环境下动量参数的最优设置提供了理论依据，拓展了Adam优化器的理论理解。

Abstract: While Adam is one of the most effective optimizer for training large-scale
machine learning models, a theoretical understanding of how to optimally set
its momentum factors, $\beta_1$ and $\beta_2$, remains largely incomplete.
  Prior works have shown that Adam can be seen as an instance of
Follow-the-Regularized-Leader (FTRL), one of the most important class of
algorithms in online learning.
  The prior analyses in these works required setting $\beta_1 =
\sqrt{\beta_2}$, which does not cover the more practical cases with $\beta_1
\neq \sqrt{\beta_2}$.
  We derive novel, more general analyses that hold for both $\beta_1 \geq
\sqrt{\beta_2}$ and $\beta_1 \leq \sqrt{\beta_2}$.
  In both cases, our results strictly generalize the existing bounds.
  Furthermore, we show that our bounds are tight in the worst case.
  We also prove that setting $\beta_1 = \sqrt{\beta_2}$ is optimal for an
oblivious adversary, but sub-optimal for an non-oblivious adversary.

</details>


### [365] [Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains](https://arxiv.org/abs/2510.03486)
*Anupam Panwar,Himadri Pal,Jiali Chen,Kyle Cho,Riddick Jiang,Miao Zhao,Rajiv Krishnamurthy*

Main category: cs.LG

TL;DR: RADF是一个统一框架，解决大规模分布式系统中异常检测的三个主要挑战：数据量大、数据集异质性和根因分析困难。该框架采用mSelect技术自动选择算法和调参，并在检测后提供快速分类和根因分析能力。


<details>
  <summary>Details</summary>
Motivation: 解决大规模分布式系统中异常检测面临的三大挑战：处理海量数据、适应异构时间序列数据集、以及快速确定异常根因。传统方法难以同时应对这些挑战，需要手动调参且缺乏有效的根因分析能力。

Method: 提出推理式异常检测框架(RADF)，包含mSelect技术来自动化算法选择和超参数调优，支持实时大规模异常检测，并提供检测后的快速分类和根因分析能力。

Result: 在9个公共基准数据集中，RADF在5个数据集上的AUC性能超过最先进的异常检测模型。其中7个数据集的AUC超过0.85，这是其他最先进模型无法达到的成就。

Conclusion: RADF框架通过mSelect技术成功解决了大规模异常检测的关键挑战，在多个数据集上表现出优于现有方法的性能，为实际部署提供了有效解决方案。

Abstract: Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.

</details>


### [366] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: 本文提出了在轨迹数据和qπ可实现性假设下的高效离线强化学习策略评估方法，并改进了策略优化的样本复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，在仅有数据覆盖度和qπ可实现性假设下，统计高效学习是不可能的。最近有工作提出了轨迹数据假设下的策略优化方法，本文旨在解决策略评估问题并改进样本复杂度分析。

Method: 使用函数逼近的有限时域离线强化学习方法，假设数据以轨迹形式给出且状态-动作值函数线性可实现。

Result: 提出了统计高效的策略评估学习器，并证明了对策略优化学习器样本复杂度的改进。

Conclusion: 在轨迹数据和qπ可实现性假设下，可以实现统计高效的离线强化学习策略评估，并且策略优化的样本复杂度可以得到改进。

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [367] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: D2AC是一种新的无模型强化学习算法，通过稳定的学习过程和鲁棒的分布评论家，在18个困难RL任务上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够有效在线训练表达性扩散策略的强化学习算法，避免典型策略梯度的高方差和通过时间的反向传播的复杂性。

Method: 核心是避免高方差策略梯度的策略改进目标，结合分布RL和裁剪双Q学习的鲁棒分布评论家设计。

Result: 在18个困难RL任务（包括Humanoid、Dog和Shadow Hand领域）上实现最先进性能，涵盖密集奖励和目标条件RL场景，并在生物启发的捕食者-猎物任务中展示行为鲁棒性和泛化能力。

Conclusion: D2AC算法通过稳定的学习过程和鲁棒的分布评论家设计，在多种RL任务中表现出色，具有强大的泛化能力。

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [368] [Task-Level Contrastiveness for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2510.03509)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出任务级对比学习新方法，通过任务增强和任务表示的无监督聚类，解决小样本分类和元学习方法在跨域泛化中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有小样本分类和元学习方法通常在单一数据集上表现良好，但难以跨不同领域泛化，存在准确率低、计算成本高、依赖限制性假设等问题。

Method: 引入任务级对比性概念，定义任务增强的简单方式，并设计任务级对比损失函数来促进任务表示的无监督聚类。该方法轻量级，可轻松集成到现有算法中。

Result: 在MetaDataset基准测试中表现出色，实现了优越性能，无需额外复杂性即可改善泛化能力和计算效率。

Conclusion: 任务级对比学习方法有效解决了跨域小样本学习问题，无需任务领域先验知识，显著提升了泛化性能和计算效率。

Abstract: Few-shot classification and meta-learning methods typically struggle to
generalize across diverse domains, as most approaches focus on a single
dataset, failing to transfer knowledge across various seen and unseen domains.
Existing solutions often suffer from low accuracy, high computational costs,
and rely on restrictive assumptions. In this paper, we introduce the notion of
task-level contrastiveness, a novel approach designed to address issues of
existing methods. We start by introducing simple ways to define task
augmentations, and thereafter define a task-level contrastive loss that
encourages unsupervised clustering of task representations. Our method is
lightweight and can be easily integrated within existing few-shot/meta-learning
algorithms while providing significant benefits. Crucially, it leads to
improved generalization and computational efficiency without requiring prior
knowledge of task domains. We demonstrate the effectiveness of our approach
through different experiments on the MetaDataset benchmark, where it achieves
superior performance without additional complexity.

</details>


### [369] [A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT](https://arxiv.org/abs/2510.03513)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: cs.LG

TL;DR: 提出基于联邦学习的轻量级隐私保护物联网僵尸网络检测框架，在保持检测准确性的同时减少通信开销


<details>
  <summary>Details</summary>
Motivation: 物联网快速发展带来创新机会，但也增加了僵尸网络攻击风险。传统检测方法在资源受限的物联网环境中面临可扩展性、隐私和适应性挑战

Method: 采用联邦学习方法，使分布式设备能够协作训练模型而不交换原始数据，同时引入通信高效的聚合策略以减少开销

Result: 在基准物联网僵尸网络数据集上的实验表明，该框架实现了高检测精度，同时显著降低了通信成本

Conclusion: 联邦学习为物联网生态系统提供了可扩展、安全且隐私感知的入侵检测实用路径

Abstract: The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.

</details>


### [370] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: RAPID是一种新型强化学习算法，通过大批次推理和小组策略梯度更新，显著减少小语言模型微调的训练时间，在三个基准测试中运行时间减少11%-34%。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法在微调小语言模型时资源消耗大、训练时间长，主要原因是训练过程中需要同时进行推理和反向传播。

Method: 采用大批次推理和小组策略梯度更新策略，结合组优势估计和重要性加权估计器来修正离策略学习带来的偏差。

Result: 在三个基准测试中，RAPID相比最先进的RL算法运行时间减少11%-34%，同时保持相似或更好的准确性。

Conclusion: RAPID算法通过优化计算资源利用，有效解决了RL训练时间过长的问题，为小语言模型的强化学习微调提供了高效解决方案。

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [371] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: CS-RLHF提出了一种可认证的安全RLHF方法，通过基于惩罚的公式替代拉格朗日方法，确保安全约束的可行性，无需双变量更新，显著提高了LLM的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于CMDP的方法存在两个主要问题：1) 性能高度依赖评分机制，需要捕捉语义而非表面关键词；2) 双变量调优计算昂贵且无法提供可证明的安全保证，容易受到对抗性越狱攻击。

Method: 引入CS-RLHF，使用在大规模语料上训练的成本模型分配语义基础的安全分数，采用修正的基于惩罚的公式，基于约束优化中的精确惩罚函数理论，通过适当缩放的惩罚项直接强制执行约束满足。

Result: 实证评估显示CS-RLHF优于最先进的LLM模型响应，在正常和越狱提示下效率至少提高5倍。

Conclusion: CS-RLHF通过基于惩罚的方法有效解决了现有安全RL方法的局限性，提供了更好的安全保证和计算效率。

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [372] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出多阶段LaSDI框架，通过顺序学习额外解码器来修正残差误差，提高重构和预测精度


<details>
  <summary>Details</summary>
Motivation: 传统LaSDI在训练时强制实施潜在动力学可能会损害模型对仿真数据的重构精度

Method: 多阶段LaSDI框架，顺序学习额外解码器来修正前阶段的残差误差

Result: 应用于1D-1V Vlasov方程时，mLaSDI始终优于标准LaSDI，实现更低的预测误差和更短的训练时间

Conclusion: 多阶段LaSDI框架能有效提高数据驱动降阶模型的精度和效率

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [373] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: 提出CrossLag注意力机制，通过整合外生数据中滞后内生信号，显著提升登革热疫情预测性能


<details>
  <summary>Details</summary>
Motivation: 现有模型难以准确预测需要及时公共卫生预警的重大登革热疫情爆发

Method: 在TimeXer基础上引入CrossLag注意力机制，利用气候和海洋异常等外生数据中的滞后内生信号

Result: 在24周预测窗口内，新模型在新加坡登革热数据上的疫情检测和预测性能显著优于TimeXer基线

Conclusion: CrossLag机制能有效捕捉疫情滞后于环境变化的特征，为重大疫情预警提供更可靠工具

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [374] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: 本文提出了一种统一的方法，通过最小化对LLM权重的干预，同时实现敏感信息遗忘和抵御越狱攻击的目标，无需依赖oracle分类器。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，需要确保隐私保护和安全生成。本文从两个关键方面解决这一目标：敏感信息遗忘和对越狱攻击的鲁棒性。

Method: 采用约束优化方法，寻找对LLM权重的最小干预，使特定词汇集不可达或将部分权重转移到更安全区域以增强鲁棒性。提出点式约束干预方法。

Result: 发现简单的点式约束干预比最大最小干预性能更好且计算成本更低。与最先进防御方法相比，所提方法表现出优越性能。

Conclusion: 该方法统一处理敏感信息遗忘和鲁棒性，无需oracle分类器，计算效率高，性能优于现有方法。

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [375] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 提出了IMMFM框架，通过分段二次插值路径学习连续随机动力学，联合优化漂移和数据驱动扩散系数，能够处理稀疏采样和高维轨迹数据。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型在处理稀疏采样和高维轨迹数据时往往只能学习成对转移，难以捕捉多时间点的联合动态一致性。

Method: 使用分段二次插值路径作为流匹配的平滑目标，联合优化漂移和基于数据的扩散系数，并提供了稳定学习的理论条件。

Result: 在合成基准和真实世界纵向神经影像数据集上，IMMFM在预测准确性和下游任务性能方面优于现有方法。

Conclusion: IMMFM能够捕捉内在随机性，处理不规则稀疏采样，并生成特定主题的轨迹，为连续随机动力学建模提供了有效框架。

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [376] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: 本文系统性地比较了不同图神经网络架构在RNN+GNN管道模型中的表现，首次提出在故障诊断中使用GraphSAGE和Graph Attention网络，并展示了RGATv2在拓扑变化下具有最佳泛化能力。


<details>
  <summary>Details</summary>
Motivation: 电力配电网故障检测对系统可靠性至关重要，但现有方法需要适应不断变化的电网拓扑。当前最先进的数据驱动方法使用RNN+GNN管道，但主要采用GCN架构，而其他更先进的GNN架构尚未在电力系统故障诊断中得到充分探索。

Method: 采用RNN+GNN管道模型，系统性地比较了多种GNN架构（包括GraphSAGE、GAT、GATv2）与传统的RGCN和纯RNN模型（特别是GRU），特别关注模型在不同拓扑设置下的泛化能力。

Result: 在IEEE 123节点配电网上的实验结果显示，RGATv2具有最优的泛化能力，在不同拓扑设置下F1分数仅下降约12%。相比之下，纯RNN模型表现较差（F1分数下降高达60%），其他RGNN变体也出现显著性能下降（F1分数下降高达25%）。

Conclusion: RGATv2在电力系统故障诊断中表现出卓越的泛化能力，能够有效应对电网拓扑变化，为实际部署提供了有前景的解决方案。

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [377] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: 提出了两种高效的测试时扩展策略TTAug和TTAdapt，通过模型内部特征而非外部监督来提升小型视觉语言模型的性能，在保持计算效率的同时实现了跨多个基准的稳定改进。


<details>
  <summary>Details</summary>
Motivation: 小型视觉语言模型虽然计算效率高，但泛化能力和下游任务性能较弱。现有测试时扩展方法通常计算成本高，违背了小型模型的资源高效设计目标。

Method: 提出了两种策略：(1) TTAug：生成多个增强输入并在token级别聚合输出，无需参数更新；(2) TTAdapt：使用TTAug生成的基于共识的伪标签在推理时调整模型参数。

Result: 在九个基准测试中展示了一致的性能提升，同时保持了适合资源受限环境的计算效率。方法在不同规模的模型和不同VLM中都具有通用性，无需额外调优。

Conclusion: 提出的测试时扩展策略有效解决了小型VLM的性能局限，在保持计算效率的同时显著提升了模型性能，具有很好的通用性和实用性。

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [378] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [379] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出了Latent Mixture of Symmetries (Latent MoS)模型，通过捕捉复杂动态测量中的对称性混合潜在因子，提高动态学习的样本效率，并在各种物理系统中表现出优越的插值和外推性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设单一全局对称群，并将对称性发现和动态学习作为独立任务，导致表达能力有限和误差累积。需要一种能够捕捉混合对称性并同时学习动态的模型。

Method: 提出Latent MoS模型，通过局部且可证明地保持底层对称变换来捕捉对称性主导的潜在因子混合。采用分层架构堆叠MoS块以捕获长期等变性。

Result: 在多种物理系统的数值实验中，Latent MoS在插值和外推任务上优于最先进的基线方法，并提供适合未来几何和安全关键分析的可解释潜在表示。

Conclusion: Latent MoS通过混合对称性建模显著提高了动态学习的样本效率和表达能力，为模型基控制和强化学习提供了有效的解决方案。

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [380] [FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors](https://arxiv.org/abs/2510.03589)
*Ankit Bhardwaj,Ananth Balashankar,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: FieldFormer是一个基于Transformer的框架，用于从稀疏、噪声、不规则时空传感器数据中进行无网格场重建，结合数据驱动灵活性和物理约束，在三个基准测试中比基线方法性能提升40%以上。


<details>
  <summary>Details</summary>
Motivation: 时空传感器数据通常稀疏、噪声大且不规则，现有插值或学习方法要么忽略控制PDE，要么无法扩展，难以有效处理这类数据。

Method: 使用基于Transformer的框架，通过可学习的速度缩放距离度量收集局部邻域，实现各向异性适应不同传播机制，通过期望最大化风格优化邻域，局部Transformer编码器进行预测，并通过自动微分PDE残差和边界特定惩罚强制物理一致性。

Result: 在三个基准测试（标量各向异性热方程、矢量值浅水系统、现实平流扩散污染模拟）中，FieldFormer始终比强基线方法性能提升40%以上，能够从稀疏（0.4%-2%）和噪声（10%）数据中实现准确（RMSE<10^-2）、高效且物理一致的场重建。

Conclusion: FieldFormer框架成功结合了数据驱动灵活性和物理基础结构，为从稀疏噪声时空数据中进行场重建提供了准确高效的解决方案。

Abstract: Spatio-temporal sensor data is often sparse, noisy, and irregular, and
existing interpolation or learning methods struggle here because they either
ignore governing PDEs or do not scale. We introduce FieldFormer, a
transformer-based framework for mesh-free spatio-temporal field reconstruction
that combines data-driven flexibility with physics-based structure. For each
query, FieldFormer gathers a local neighborhood using a learnable
velocity-scaled distance metric, enabling anisotropic adaptation to different
propagation regimes. Neighborhoods are built efficiently via per-batch offset
recomputation, and refined in an expectation-maximization style as the velocity
scales evolve. Predictions are made by a local transformer encoder, and physics
consistency is enforced through autograd-based PDE residuals and
boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat
equation, a vector-valued shallow-water system, and a realistic
advection-diffusion pollution simulation--FieldFormer consistently outperforms
strong baselines by more than 40%. Our results demonstrate that FieldFormer
enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field
reconstruction from sparse (0.4%-2%) and noisy(10%) data.

</details>


### [381] [Deep Reinforcement Learning for Multi-Agent Coordination](https://arxiv.org/abs/2510.03592)
*Kehinde O. Aina,Sehoon Ha*

Main category: cs.LG

TL;DR: 提出基于虚拟信息素的S-MADRL框架，通过课程学习解决多机器人在狭窄环境中的协调问题，实现去中心化涌现协调。


<details>
  <summary>Details</summary>
Motivation: 解决狭窄拥挤环境中多机器人协调的挑战，避免拥堵和干扰影响集体任务性能，受昆虫群体通过信息素实现鲁棒协调的启发。

Method: 采用基于虚拟信息素的S-MADRL框架，利用课程学习将复杂任务分解为逐步困难的子问题，实现去中心化协调而无需显式通信。

Result: 模拟结果显示该框架能有效协调最多8个智能体，机器人自组织形成不对称工作负载分布，减少拥堵并调节群体性能。

Conclusion: 该框架展示了在通信受限的拥挤环境中实现可扩展去中心化多智能体协调的有效解决方案，涌现行为类似于自然界观察到的策略。

Abstract: We address the challenge of coordinating multiple robots in narrow and
confined environments, where congestion and interference often hinder
collective task performance. Drawing inspiration from insect colonies, which
achieve robust coordination through stigmergy -- modifying and interpreting
environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement
Learning (S-MADRL) framework that leverages virtual pheromones to model local
and social interactions, enabling decentralized emergent coordination without
explicit communication. To overcome the convergence and scalability limitations
of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum
learning, which decomposes complex tasks into progressively harder
sub-problems. Simulation results show that our framework achieves the most
effective coordination of up to eight agents, where robots self-organize into
asymmetric workload distributions that reduce congestion and modulate group
performance. This emergent behavior, analogous to strategies observed in
nature, demonstrates a scalable solution for decentralized multi-agent
coordination in crowded environments with communication constraints.

</details>


### [382] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: 提出多层移动边缘计算框架MLMEC用于跌倒检测，通过知识蒸馏方法提高前端检测精度，在降低延迟的同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 解决传统跌倒检测系统中边缘设备模型容量有限和数据传输延迟的问题，平衡检测精度与延迟

Method: 使用多层MEC架构将系统分为多个站点，每个站点配备神经网络模型；采用知识蒸馏方法让高功率后端站点为前端提供额外学习经验

Result: 在SisFall数据集上准确率提升11.65%，在FallAllD数据集上提升2.78%；延迟分别降低46.67%和54.15%

Conclusion: MLMEC跌倒检测系统在提高准确率的同时显著降低了延迟，为实时跌倒检测提供了有效解决方案

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [383] [Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends](https://arxiv.org/abs/2510.03604)
*Yucheng Wang,Mohamed Ragab,Yubo Hou,Zhenghua Chen,Min Wu,Xiaoli Li*

Main category: cs.LG

TL;DR: 本文对涡轮风扇发动机剩余使用寿命预测中的领域自适应技术进行了全面综述，提出了针对涡轮风扇发动机特性的新分类法，并评估了相关技术在实际数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 涡轮风扇发动机的RUL预测在预测性维护中至关重要，但面临数据有限和运行条件变化导致的分布偏移挑战。领域自适应技术能够从数据丰富的源域向数据稀缺的目标域转移知识，因此需要专门针对涡轮风扇发动机特性的综述研究。

Method: 提出了专门针对涡轮风扇发动机的新分类法：基于方法的分类（如何应用DA）、基于对齐的分类（运行变化导致的分布偏移位置）、基于问题的分类（解决特定挑战所需的适配）。在涡轮风扇发动机数据集上评估了选定的DA技术。

Result: 提供了多维度的视角，超越了传统分类方法，考虑了涡轮风扇发动机数据的独特特性和应用DA技术的标准流程。为从业者提供了实用见解并识别了关键挑战。

Conclusion: 识别了未来研究方向，以指导开发更有效的领域自适应技术，推动涡轮风扇发动机RUL预测技术的发展。

Abstract: Remaining Useful Life (RUL) prediction for turbofan engines plays a vital
role in predictive maintenance, ensuring operational safety and efficiency in
aviation. Although data-driven approaches using machine learning and deep
learning have shown potential, they face challenges such as limited data and
distribution shifts caused by varying operating conditions. Domain Adaptation
(DA) has emerged as a promising solution, enabling knowledge transfer from
source domains with abundant data to target domains with scarce data while
mitigating distributional shifts. Given the unique properties of turbofan
engines, such as complex operating conditions, high-dimensional sensor data,
and slower-changing signals, it is essential to conduct a focused review of DA
techniques specifically tailored to turbofan engines. To address this need,
this paper provides a comprehensive review of DA solutions for turbofan engine
RUL prediction, analyzing key methodologies, challenges, and recent
advancements. A novel taxonomy tailored to turbofan engines is introduced,
organizing approaches into methodology-based (how DA is applied),
alignment-based (where distributional shifts occur due to operational
variations), and problem-based (why certain adaptations are needed to address
specific challenges). This taxonomy offers a multidimensional view that goes
beyond traditional classifications by accounting for the distinctive
characteristics of turbofan engine data and the standard process of applying DA
techniques to this area. Additionally, we evaluate selected DA techniques on
turbofan engine datasets, providing practical insights for practitioners and
identifying key challenges. Future research directions are identified to guide
the development of more effective DA techniques, advancing the state of RUL
prediction for turbofan engines.

</details>


### [384] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: Hill-ADAM是一种优化器，专注于逃离预设损失函数中的局部最小值以寻找全局最小值。它通过确定性探索状态空间来逃离局部最小值，消除了随机梯度更新带来的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统ADAM优化器在逃离局部最小值方面存在局限性，作者希望开发一种能够更有效地探索损失空间并找到全局最小值的优化算法。

Method: 首先推导ADAM优化器在特定模型状态下的步长分析近似，然后定义确定ADAM在逃离局部最小值方面局限性的主要条件。Hill-ADAM在误差最小化和最大化之间交替进行：最大化以逃离局部最小值，然后再次最小化。

Result: Hill-ADAM在5个损失函数和12个琥珀色饱和到冷色调图像颜色校正实例上进行了测试。

Conclusion: Hill-ADAM通过交替最小化和最大化的策略，在整个损失空间中提供探索，从而能够推导出全局最小值的状态。

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [385] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: 提出了神经贝叶斯滤波(NBF)算法，用于在部分可观测系统中维护隐藏状态的分布。NBF结合了经典滤波器的计算效率和深度生成模型的表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决在部分可观测系统中有效跟踪隐藏状态分布的问题，特别是处理快速变化、多模态的信念分布，同时缓解粒子贫化风险。

Method: NBF训练找到任务诱导的信念的良好潜在表示，将信念映射到固定长度的嵌入向量，这些向量条件化生成模型进行采样。在滤波过程中，使用粒子式更新在嵌入空间中计算后验分布。

Result: 在三个部分可观测环境的状态估计任务中验证了NBF的有效性。

Conclusion: NBF成功结合了经典滤波器的计算效率和深度生成模型的表达能力，能够跟踪快速变化的多模态信念分布。

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [386] [Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis](https://arxiv.org/abs/2510.03633)
*An Vuong,Susan Gauch*

Main category: cs.LG

TL;DR: 使用Meta Llama 3.1-8B-Instruct预处理推文数据，结合三种情感分析方法提取情感特征，与历史股价数据一起训练LSTM模型，显著提高了股票价格大幅变动的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 准确预测短期股票价格变动具有挑战性，因为市场具有固有的波动性且对投资者情绪敏感。本文旨在通过整合社交媒体情感特征和历史股价信息来提高预测准确性。

Method: 使用Meta的Llama 3.1-8B-Instruct模型预处理推文数据，采用三种情感分析方法（基于Transformer的DistilRoBERTa分类器和两种基于词典的NRC方法）提取情感特征，将这些特征与前一日股价数据结合训练LSTM模型。

Result: 在TSLA、AAPL和AMZN股票上的实验结果显示，所有三种情感分析方法都比仅使用历史股价的基线模型（准确率13.5%）提高了平均准确率。基于DistilRoBERTa的股票预测模型表现最佳，使用LLaMA增强情感分析后准确率从23.6%提升至38.5%。

Conclusion: 使用大语言模型预处理推文内容能增强情感分析的有效性，进而提高预测股票价格大幅变动的准确性。

Abstract: Accurately predicting short-term stock price movement remains a challenging
task due to the market's inherent volatility and sensitivity to investor
sentiment. This paper discusses a deep learning framework that integrates
emotion features extracted from tweet data with historical stock price
information to forecast significant price changes on the following day. We
utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby
enhancing the quality of emotion features derived from three emotion analysis
approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face
library and two lexicon-based methods using National Research Council Canada
(NRC) resources. These features are combined with previous-day stock price data
to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,
AAPL, and AMZN stocks show that all three emotion analysis methods improve the
average accuracy for predicting significant price movements, compared to the
baseline model using only historical stock prices, which yields an accuracy of
13.5%. The DistilRoBERTa-based stock prediction model achieves the best
performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced
emotion analysis. These results demonstrate that using large language models to
preprocess tweet content enhances the effectiveness of emotion analysis which
in turn improves the accuracy of predicting significant stock price movements.

</details>


### [387] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 本文研究了在公共卫生情感分析中，通过数据投毒攻击破坏大语言模型的上下文学习能力。即使微小的对抗性扰动也能导致67%的情感标签翻转，但通过谱签名防御可以成功保护数据完整性。


<details>
  <summary>Details</summary>
Motivation: 将先前关于ICL投毒的理论研究扩展到公共卫生话语分析的实际高风险场景，揭示ICL在攻击下的脆弱性，并探索使AI系统在健康相关社交媒体监控中更可靠的防御方法。

Method: 在人类偏肺病毒推文的支持示例中引入同义词替换、否定插入和随机扰动等小型对抗性扰动，然后应用谱签名防御来过滤被投毒的示例。

Result: 轻微的数据操纵导致高达67%的情感标签翻转，但防御后ICL准确率稳定在46.7%左右，逻辑回归验证达到100%准确率，成功保持了数据集完整性。

Conclusion: 研究强调了ICL在攻击下的脆弱性，以及谱防御在使AI系统对健康相关社交媒体监控更可靠方面的价值，为稳健的LLM部署提供了风险认知和防御方案。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [388] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: 隐式模型通过迭代单个参数块到固定点来计算输出，形成无限深度、权重绑定的网络，以恒定内存训练。研究表明，通过增加测试时计算，简单的隐式算子可以渐进表达更复杂的映射，使模型表达能力随测试时计算量扩展，最终匹配更丰富的函数类。


<details>
  <summary>Details</summary>
Motivation: 隐式模型作为新兴模型类别，虽然经验上知道这些紧凑模型通过分配更多测试时计算可以匹配甚至超越更大的显式网络，但其底层机制仍不清楚。本文旨在通过非参数分析来理解这种表达能力差距。

Method: 采用非参数分析方法来研究隐式模型的表达能力。通过严格的数学表征，证明简单的隐式算子通过迭代可以渐进表达更复杂的映射。理论分析表明，对于广泛的隐式模型，其表达能力可以随测试时计算量扩展。

Result: 理论在三个领域得到验证：图像重建、科学计算和运筹学。实验表明，随着测试时迭代次数增加，学习映射的复杂性上升，同时解的质量提高并稳定。

Conclusion: 隐式模型通过测试时计算扩展表达能力的能力得到了严格的理论证明和实证验证，这为理解隐式模型超越显式模型的机制提供了理论基础。

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [389] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的深部脑刺激方法，使用可测量的脑活动来调整刺激参数，相比传统临床方法能更好地抑制帕金森病生物标志物。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的DBS方法依赖于无法在患者体内测量的生物标志物，仅存在于脑芯片模拟中，需要开发使用可测量脑活动的方法。

Method: 使用基于TD3的强化学习智能体，在基底神经节脑区模型上进行训练，根据可测量的脑活动自适应调整刺激频率和幅度。

Result: 该方法相比现代临床DBS实施方案，能更有效地抑制与帕金森病严重程度相关的生物标志物。

Conclusion: 该方法为训练针对个体患者需求的个性化强化学习智能体开辟了可能性，且依赖于可在真实环境中测量的信息。

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [390] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出SAFA-SNN方法，用于边缘设备上的少样本类增量学习，通过稀疏性调节神经元动态和子空间投影来缓解灾难性遗忘和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 边缘设备需要持续学习新类别以保护数据隐私和维持可靠性能，但在数据样本不足的情况下，现有基于人工神经网络的方法受限于设备资源，而脉冲神经网络具有低能耗和生物合理性优势。

Method: 使用稀疏性调节的神经元动态（大部分神经元保持稳定，部分保持活跃），采用零阶优化处理脉冲不可微性，通过子空间投影增强新类别的区分性。

Result: 在CIFAR100、Mini-ImageNet和三个神经形态数据集上的实验表明，SAFA-SNN优于基线方法，在Mini-ImageNet最后一个增量会话中至少提升4.01%，能耗比基线方法低20%。

Conclusion: SAFA-SNN为边缘设备上的少样本类增量学习提供了一种高效且节能的解决方案，在性能和能耗方面均优于现有方法。

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [391] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: 该研究探讨如何利用澳大利亚能源市场运营商(AEMO)的价格预测来开发可靠的电池储能系统(BESS)交易算法，通过分析预测准确性模式并创建基于预测的BESS交易模型来优化套利收益。


<details>
  <summary>Details</summary>
Motivation: 随着电网波动性增加，虽然预测数据丰富，但其在实际BESS交易决策中的价值尚未充分探索，需要填补这一研究空白。

Method: 分析AEMO价格预测准确性模式(基于时间、预测周期和区域差异)，创建基于预测的BESS交易模型，并与无预测知识的基本算法进行性能对比，同时探索机器学习技术增强预测能力。

Result: 开发了新颖的基于预测的BESS交易模型，能够优化套利财务回报，并评估了预测驱动算法的性能表现。

Conclusion: 研究成果将为能源市场交易模型的未来改进提供信息，并促进BESS更有效地整合到市场运营中。

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [392] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器(SAE)的可解释性与模型行为控制效用之间只有弱正相关(τb≈0.298)，表明可解释性不能作为控制性能的有效代理。提出Delta Token Confidence特征选择方法，将LLM控制性能提升52.52%，且选择后两者相关性消失甚至变为负相关。


<details>
  <summary>Details</summary>
Motivation: 验证稀疏自编码器(SAE)的可解释性是否确实意味着更好的模型行为控制效果，因为现有研究普遍假设可解释特征自然能有效控制模型行为。

Method: 在三个LLM(Gemma-2-2B、Qwen-2.5-3B、Gemma-2-9B)上训练90个SAE，涵盖5种架构和6个稀疏度水平，使用SAEBench和AxBench分别评估可解释性和控制效用，通过Kendall秩系数分析相关性。提出Delta Token Confidence特征选择标准来衡量特征放大对下一个token分布的影响。

Result: 可解释性与控制效用仅存在弱正相关(τb≈0.298)。Delta Token Confidence方法相比当前最佳输出得分标准将三个LLM的控制性能提升52.52%。选择高Delta Token Confidence特征后，可解释性与效用相关性消失(τb≈0)甚至变为负相关。

Conclusion: 可解释性不是模型控制效用的充分代理指标，两者存在显著差异。Delta Token Confidence是更有效的特征选择标准，能显著提升模型行为控制性能，且最佳控制特征的可解释性反而较低。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [393] [Operationalizing Data Minimization for Privacy-Preserving LLM Prompting](https://arxiv.org/abs/2510.03662)
*Jijie Zhou,Niloofar Mireshghallah,Tianshi Li*

Main category: cs.LG

TL;DR: 提出了一个数据最小化框架，通过优先级队列树搜索在隐私有序转换空间中寻找最优平衡点，量化在保持任务效用的前提下最少需要披露的隐私信息。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在消费应用中的快速部署，用户经常过度分享个人信息以获取有用响应，这增加了通过记忆化、基于上下文的个性化或安全漏洞带来的隐私风险。

Method: 提出了一个正式定义和操作化数据最小化的框架，使用优先级队列树搜索在隐私有序转换空间中定位最优平衡点，并在四个数据集上评估了九个LLM的可实现数据最小化程度。

Result: 结果显示前沿大型LLM（如GPT-5）在保持任务质量的同时可以容忍更强的数据最小化（85.7%的编辑），而较小的开源模型（如Qwen2.5-0.5B）只能容忍19.3%的编辑。LLM难以直接预测最优数据最小化，存在偏向抽象的偏见导致过度分享。

Conclusion: 这不仅是隐私差距，更是能力差距：模型可能缺乏对解决任务实际需要哪些信息的意识。

Abstract: The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.

</details>


### [394] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 提出了Token Hidden Reward (THR)指标，通过重加权GRPO算法的学习信号来显式控制强化学习中的探索与利用平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励强化学习虽然提升了语言模型的推理能力，但如何显式控制训练过程中的探索与利用平衡仍是一个开放问题。

Method: 引入THR指标量化每个token对正确响应可能性的影响，开发基于THR的重加权算法来调节GRPO的学习信号。

Result: 正向THR重加权提升贪婪解码准确率（利用），反向策略提升Pass@K准确率（探索），且与GSPO等其他RL目标兼容，适用于Llama等架构。

Conclusion: THR为RL调优的LLM提供了细粒度的探索-利用控制机制，为推理密集型应用的针对性微调提供了新工具。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [395] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 本文提出注意力采样器定义，利用流式设置中的重要性采样方法显著降低传统注意力机制的计算负担，并分析其理论有效性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模注意力模型的计算挑战，受经典ℓ2采样器和LLM中注意力机制进展的启发。

Method: 提出注意力采样器定义，在流式设置中使用重要性采样方法。

Result: 显著降低传统注意力机制的计算负担，分析显示在空间和更新时间方面具有理论有效性。

Conclusion: 该框架具有可扩展性，适用于各种模型架构和领域。

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [396] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: 提出Group Policy Gradient (GPG)，一种无critic的策略梯度估计器家族，用基于组的蒙特卡洛优势估计器替代学习的价值函数，在保持PPO剪裁目标结构的同时消除critic训练成本。


<details>
  <summary>Details</summary>
Motivation: 受GRPO在人类反馈强化学习中成功方法的启发，旨在消除训练critic所需的内存、计算和超参数成本，同时保持PPO的性能优势。

Method: 使用基于组的蒙特卡洛优势估计器替代学习的价值函数，保留PPO的剪裁目标结构，无需训练critic。

Result: 经验证明GPG在标准基准测试中匹配或优于PPO，能更好地利用并行模拟，计算资源使用效率更高。

Conclusion: GPG提供了一种高效的无critic策略梯度估计方法，在保持性能的同时显著降低计算成本。

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [397] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出了一个统一框架，将图数据建模为图子混合模型，利用图矩进行聚类，从而改进图对比学习和数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 现实图数据集通常包含来自不同分布的混合群体，但现有图表示学习方法往往忽略这种混合结构。

Method: 利用图矩（模体密度）对图进行聚类，识别不同的生成机制；提出图子混合感知的Mixup数据增强和模型自适应图对比学习。

Result: 在无监督学习中达到SOTA，在8个数据集中平均排名第一；在有监督学习中，在6/7个数据集上达到新的SOTA准确率。

Conclusion: 显式建模图的混合分布结构能显著提升图表示学习性能，为图对比学习和数据增强提供了更理论化的方法。

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [398] [REG: A Regularization Optimizer for Robust Training Dynamics](https://arxiv.org/abs/2510.03691)
*Zehua Liu,Han Wu,Xiaojin Fu,Shuqi Liu,Xiongwei Han,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 提出了REG优化器，通过使用行-列缩放(RACS)操作符替代Muon的矩阵符号函数，解决了Muon优化器在训练稳定性和与AdamW预训练模型兼容性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Muon优化器虽然能平衡梯度更新的各个方向，但由于依赖矩阵符号函数，会导致训练不稳定，且在微调AdamW预训练模型时存在兼容性问题。

Method: 使用行-列缩放(RACS)操作符替代Muon的矩阵符号函数，该操作符基于矩阵平衡理论，以更温和的方式正则化更新步骤。

Result: 在LLM训练中，REG优化器不仅性能优于AdamW且更稳定，还能与AdamW训练范式保持一致，在微调阶段避免了Muon观察到的性能下降。

Conclusion: REG优化器通过RACS操作符提供了更好的训练稳定性和与现有AdamW生态的兼容性，是大语言模型训练的有前景的替代优化器。

Abstract: Optimizers are crucial for the efficient training of Large Language Models
(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers
like Muon have emerged, which regularize gradient updates by operating on
entire weight matrices. The Muon optimizer balances the gradient updates along
all the directions. However, Muon's reliance on the matrix sign function can
lead to training instability, exhibits incompatibility when fine-tuning models
pre-trained with AdamW. To address these limitations, we propose \textbf{REG},
a novel optimizer that replaces Muon's aggressive matrix sign operator with the
Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a
matrix, the RACS operator regularizes the update steps in a less drastic
manner, making it simpler to implement and more compatible with established
training dynamics. Through extensive empirical experiments on LLM training, we
demonstrate that our REG optimizer not only achieves superior performance and
stability over AdamW, but also maintains consistency with the AdamW training
paradigm. This consistency is particularly evident during the fine-tuning
stage, where REG optimizer avoids the performance degradation observed with
Muon.

</details>


### [399] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种基于谱滤波的线性强化学习方法，通过自适应正则化参数选择策略，在保证可解释性的同时提升决策性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法主要关注性能，依赖事后解释来提供可解释性。本文旨在设计一种面向可解释性但性能增强的RL方法。

Method: 提出基于谱滤波的线性RL方法，扩展了岭回归方法，通过谱滤波函数控制估计误差，并设计基于偏差-方差权衡的自适应正则化参数选择策略。

Result: 理论分析建立了参数估计和泛化误差的近似最优界。在模拟环境和快手、淘宝真实数据集上的实验表明，该方法在决策质量上优于或匹配现有基线方法。

Conclusion: 该方法有潜力弥合RL理论与实际决策之间的差距，在管理场景中提供可解释性、准确性和适应性。

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [400] [Personalized federated prototype learning in mixed heterogeneous data scenarios](https://arxiv.org/abs/2510.03726)
*Jiahao Zeng,Wolong Xing,Liangtao Shi,Xin Huang,Jialin Wang,Zhile Cao,Zhenkui Shi*

Main category: cs.LG

TL;DR: 提出PFPL方法解决联邦学习中的数据异构问题，通过构建个性化无偏原型和一致性正则化来提升模型性能并降低通信成本


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在异构场景下存在特征分布或标签分布倾斜的问题，而数据异构实际上是提升模型性能的关键因素

Method: 在混合异构场景中构建个性化无偏原型，并在本地更新阶段引入一致性正则化来对齐本地实例与其个性化原型

Result: 在Digits和Office Caltech数据集上的实验验证了方法的有效性，并成功降低了通信成本

Conclusion: PFPL方法通过利用数据异构性来提升联邦学习性能，同时保护用户隐私

Abstract: Federated learning has received significant attention for its ability to
simultaneously protect customer privacy and leverage distributed data from
multiple devices for model training. However, conventional approaches often
focus on isolated heterogeneous scenarios, resulting in skewed feature
distributions or label distributions. Meanwhile, data heterogeneity is actually
a key factor in improving model performance. To address this issue, we propose
a new approach called PFPL in mixed heterogeneous scenarios. The method
provides richer domain knowledge and unbiased convergence targets by
constructing personalized, unbiased prototypes for each client. Moreover, in
the local update phase, we introduce consistent regularization to align local
instances with their personalized prototypes, which significantly improves the
convergence of the loss function. Experimental results on Digits and Office
Caltech datasets validate the effectiveness of our approach and successfully
reduce the communication cost.

</details>


### [401] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: 提出IniLoRA，一种新的初始化策略，通过将低秩矩阵初始化为接近原始模型权重来改进LoRA，解决了LoRA初始化限制性能的问题。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然流行，但其初始化两个低秩矩阵乘积为零的方法限制了有效激活和利用原始模型权重的能力，成为性能优化的瓶颈。

Method: 提出IniLoRA初始化策略，将低秩矩阵初始化为接近原始模型权重。还引入了两个变体IniLoRA-α和IniLoRA-β，采用不同的初始化方法进一步提升性能。

Result: 实验结果表明，IniLoRA在多个模型和任务上都比LoRA表现更好。

Conclusion: IniLoRA通过改进初始化策略有效解决了LoRA的性能瓶颈问题，提供了更好的参数高效微调方法。

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [402] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: 提出了一种在部分反馈下审计分类器公平性的新方法，通过设计更贴合实际场景的成本模型，开发了在两种设置下的高效审计算法，显著降低了审计成本。


<details>
  <summary>Details</summary>
Motivation: 解决在真实场景中审计分类器公平性的挑战，特别是当真实标签仅在正分类个体中可用时（如贷款审批场景），现有方法成本高昂且效率低下。

Method: 在两种设置下开发审计算法：黑盒模型（无数据分布假设）和混合模型（指数族分布混合）。利用截断样本学习和最大后验概率预言机，扩展球形高斯混合到指数族混合。

Result: 在黑盒设置下提出接近最优的审计算法，证明自然基线方法严格次优；在混合模型下设计的算法审计成本显著低于黑盒情况。在真实数据集上比基线方法降低约50%的审计成本。

Conclusion: 提出的公平性审计算法在部分反馈场景下显著提高了成本效益，适用于多种常用公平性指标，为实际应用中的公平性评估提供了高效解决方案。

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [403] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: 提出了RAPO算法来解决RLVR训练中反向KL散度正则化导致的探索受限问题，通过使用前向KL惩罚和重新加权参考策略来促进更广泛的探索。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR训练存在一个根本限制：随着采样预算增加，RLVR训练模型相对于预训练基模型的优势往往会减弱甚至消失，这归因于反向KL散度正则化的模式寻求行为限制了策略在基模型支持区域内的探索。

Method: RAPO算法：(i) 使用前向KL惩罚替代反向KL惩罚以支持分布外探索；(ii) 重新加权参考策略以促进自适应分布内探索。在8K SimpleRL-Zero数据集上训练Qwen2.5-3B和7B模型，无需监督微调。

Result: 在AIME2024和AIME2025上的评估显示，RAPO持续提升问题解决性能，使模型能够超越基模型的性能上限，并解决之前难以处理的问题。

Conclusion: RAPO算法通过促进更广泛而集中的探索，推进了RLVR在具有挑战性推理任务中的前沿应用。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [404] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: NeuroLDS是首个基于机器学习的低差异序列生成框架，通过神经网络将索引映射为点，使所有前缀序列都具有最小差异度，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决MPMC方法只能生成点集而无法扩展到低差异序列的限制，因为低差异序列在许多应用中至关重要。

Method: 采用两阶段学习过程：首先监督学习近似经典构造，然后无监督微调以最小化前缀差异度。

Result: NeuroLDS在所有差异度度量上都显著优于以往的低差异序列构造方法，并在数值积分、机器人运动规划和科学机器学习等应用中表现出色。

Conclusion: 神经低差异序列展现了广阔的应用前景和重要意义，为低差异序列生成提供了新的机器学习方法。

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [405] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: 提出了LLM Chemistry框架，用于量化多LLM协作中的协同或对抗行为，通过分析模型间交互依赖关系来推荐最优模型组合。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM协作方法依赖隐式选择和输出评估，缺乏分析协作模型是否真正互补或冲突的机制。

Method: 形式化LLM间的chemistry概念，提出量化算法分析交互依赖关系，据此推荐最优模型集成。

Result: 理论分析表明LLM间的chemistry在异构模型配置下最明显，其影响受任务类型、群体规模和复杂度影响。在分类、摘要和程序修复任务上的评估验证了这些任务依赖性效应。

Conclusion: LLM Chemistry可作为多LLM系统的诊断因素和集成推荐的基础。

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [406] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: EvoEngineer是一个基于LLM的系统化代码演化框架，专门用于CUDA内核优化，在91个真实CUDA内核上实现了2.72倍的平均加速和69.8%的代码有效性。


<details>
  <summary>Details</summary>
Motivation: CUDA内核优化是AI性能的关键瓶颈，但现有LLM方法存在生态系统碎片化、问题定义不清晰以及无法满足严格正确性要求的问题。

Method: 首先形式化CUDA内核优化任务，然后建立EvoEngineer框架，提供指导以平衡性能和正确性，最后基于该框架实现内核优化系统。

Result: 在91个真实CUDA内核上，EvoEngineer实现了2.72倍平均加速，代码有效性达69.8%，最大加速达36.75倍，在50个操作中有28个（56.0%）实现了超过2倍加速。

Conclusion: EvoEngineer在性能和正确性之间实现了原则性平衡，在两方面均优于现有方法，为CUDA内核优化提供了有效的系统化解决方案。

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [407] [Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation](https://arxiv.org/abs/2510.03782)
*Guofu Xie,Chen Zhang,Xiao Zhang,Yunsheng Shi,Ting Yao,Jun Xu*

Main category: cs.LG

TL;DR: 提出了MAGE框架，通过两阶段方法解决多目标生成控制问题：第一阶段动态构建鲁棒基础模型，第二阶段合并显式和隐式价值模型来指导解码。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可控多目标生成中存在不足：基于合并的方法提供间接的参数级控制，而基于解码的指导方法需要聚合多个专家模型的logits，产生显著空间开销。

Method: 两阶段框架：1）动态合并多个骨干模型构建鲁棒基础模型；2）合并显式和隐式价值模型形成统一指导代理，指导基础模型的解码过程。

Result: 实验验证了价值模型中的线性模式连通性，探索了模型合并与预测集成的关系，证明了方法的增强可控性。在广泛实验中优于现有方法。

Conclusion: MAGE框架实现了优越的可控性、帕累托最优性能和增强的适应性，解决了多目标生成中的关键挑战。

Abstract: Adapting to diverse user needs at test time is a key challenge in
controllable multi-objective generation. Existing methods are insufficient:
merging-based approaches provide indirect, suboptimal control at the parameter
level, often disregarding the impacts of multiple objectives. While
decoding-based guidance is more direct, it typically requires aggregating
logits from multiple expert models, incurring significant space overhead and
relying heavily on individual model capacity. To address these issues, we
introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model
merging for guided decoding. We first identify a critical compatibility problem
between the guidance and base models. In Stage 1, MAGE resolves this by
dynamically constructing a more robust base model, merging a series of backbone
models that account for multiple objectives. In Stage 2, we merge explicit and
implicit value models into a unified guidance proxy, which then steers the
decoding of the base model from Stage 1. Our analysis empirically validates
Linear Mode Connectivity (LMC) in value models, explores the relationship
between model merging and prediction ensembling, and demonstrates the enhanced
controllability afforded by our approach. Extensive experiments show that our
method outperforms existing approaches, achieving superior controllability,
Pareto-optimal performance, and enhanced adaptability.

</details>


### [408] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: 本文从理论角度分析了Transformer模型中注意力头和头维度在不同层的分配策略，揭示了softmax激活的饱和现象，并提出了平衡表达能力和效率的参数分配原则。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在各种应用中表现出色，但其模型效率的理论基础尚未充分探索。本文旨在研究如何在层间分配注意力头和头维度参数，以平衡模型的表达能力和计算效率。

Method: 首先从近似理论角度分析早期层在信息提取中的作用，理论刻画了固定参数预算下注意力头数量和头维度之间的权衡关系。同时发现并证明了softmax激活的饱和行为：持续增加头维度会导致学习误差的收益递减，尤其是在长序列中。

Result: 理论和实验均支持softmax激活的饱和模式，表明后续层可以用更少的参数实现更高效的操作。基于这些发现，提出了在Transformer各层间分配注意力头和维度的原则性策略。

Conclusion: 本文为Transformer架构的模型效率提供了理论依据，揭示了参数分配的关键原则，有助于设计更高效的Transformer模型。

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [409] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: 本文针对重尾奖励的批处理多臂老虎机问题，提出了鲁棒算法，发现在实例无关和Lipschitz设置下，重尾奖励需要更少的批次，而在实例相关设置中批次需求不变。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要假设轻尾奖励分布，但许多实际应用（如临床试验）具有重尾特征，需要填补这一研究空白。

Method: 提出了适用于重尾奖励的鲁棒批处理老虎机算法，涵盖有限臂和Lipschitz连续设置。

Result: 揭示了有趣现象：在实例无关和Lipschitz设置下，重尾奖励需要更少的批次达到近最优遗憾；在实例相关设置中，批次需求与尾重程度无关。

Conclusion: 重尾奖励在批处理老虎机中的影响因设置而异，为实际应用提供了重要指导。

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [410] [Curriculum-Augmented GFlowNets For mRNA Sequence Generation](https://arxiv.org/abs/2510.03811)
*Aya Laajil,Abduragim Shtanchaev,Sajan Muhammad,Eric Moulines,Salem Lahlou*

Main category: cs.LG

TL;DR: 提出Curriculum-Augmented GFlowNets (CAGFN)，通过集成课程学习和多目标GFlowNets来生成mRNA序列，解决了传统方法在稀疏奖励和长序列优化中的训练困难。


<details>
  <summary>Details</summary>
Motivation: mRNA序列设计面临巨大组合空间和多个序列属性优化的挑战，传统生成流网络训练受限于稀疏奖励和长序列优化问题。

Method: CAGFN集成基于长度的课程学习，逐步调整最大序列长度，从简单到困难子问题引导探索，并提供新的mRNA设计环境。

Result: 在不同mRNA设计任务中，CAGFN提高了Pareto性能和生物合理性，同时保持多样性，比无课程学习的GFlowNet更快达到更高质量解，并能泛化到分布外序列。

Conclusion: CAGFN为治疗性序列设计中的GFlowNets应用提供了生物动机设置，在mRNA设计中表现出优越性能。

Abstract: Designing mRNA sequences is a major challenge in developing next-generation
therapeutics, since it involves exploring a vast space of possible nucleotide
combinations while optimizing sequence properties like stability, translation
efficiency, and protein expression. While Generative Flow Networks are
promising for this task, their training is hindered by sparse, long-horizon
rewards and multi-objective trade-offs. We propose Curriculum-Augmented
GFlowNets (CAGFN), which integrate curriculum learning with multi-objective
GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based
curriculum that progressively adapts the maximum sequence length guiding
exploration from easier to harder subproblems. We also provide a new mRNA
design environment for GFlowNets which, given a target protein sequence and a
combination of biological objectives, allows for the training of models that
generate plausible mRNA candidates. This provides a biologically motivated
setting for applying and advancing GFlowNets in therapeutic sequence design. On
different mRNA design tasks, CAGFN improves Pareto performance and biological
plausibility, while maintaining diversity. Moreover, CAGFN reaches
higher-quality solutions faster than a GFlowNet trained with random sequence
sampling (no curriculum), and enables generalization to out-of-distribution
sequences.

</details>


### [411] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: 提出了AGRPO算法，这是首个专门为扩散大语言模型设计的理论严谨的在线强化学习算法，在数学推理任务上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型尚未受益于现代后训练技术（如强化学习），传统LLM的算法与扩散框架不兼容，现有dLLM的RL方法缺乏理论基础。

Method: 开发了AGRPO算法，使用蒙特卡洛采样计算无偏策略梯度估计，是首个可处理、忠实适配扩散LLM的策略梯度方法。

Result: 在GSM8K上获得+7.6%绝对增益，在Countdown任务上性能提升3.8倍，优于基线模型和可比RL方法，在不同采样步数下保持性能增益。

Conclusion: 在线RL算法可以以理论严谨且实际有效的方式扩展到扩散LLM，实现了计算与性能之间的更好权衡。

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [412] [Detecting Invariant Manifolds in ReLU-Based RNNs](https://arxiv.org/abs/2510.03814)
*Lukas Eisenmann,Alena Brändle,Zahra Monfared,Daniel Durstewitz*

Main category: cs.LG

TL;DR: 提出了一种新的算法来检测循环神经网络中的稳定和不稳定流形，特别针对使用ReLU激活函数的PLRNN，用于分析多稳定性和混沌动力学。


<details>
  <summary>Details</summary>
Motivation: 理解训练好的RNN如何产生其行为对于科学和医学应用以及可解释AI很重要，RNN的动态特性取决于其状态空间的拓扑和几何性质。

Method: 引入一种新颖的算法来检测稳定和不稳定流形，特别针对使用ReLU激活函数的PLRNN，该算法可以追踪不同吸引域之间的边界。

Result: 算法能够表征多稳定性，发现同宿点（稳定和不稳定流形的交点），从而证明PLRNN中存在混沌，并在实际电生理记录中获得了对底层动态的洞察。

Conclusion: 该方法为分析RNN的动态特性提供了有效工具，特别是在理解多稳定性和混沌动力学方面，有助于提高RNN的可解释性。

Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in
machine learning for time series prediction and dynamical systems
reconstruction, and experienced a recent renaissance with improved training
algorithms and architectural designs. Understanding why and how trained RNNs
produce their behavior is important for scientific and medical applications,
and explainable AI more generally. An RNN's dynamical repertoire depends on the
topological and geometrical properties of its state space. Stable and unstable
manifolds of periodic points play a particularly important role: They dissect a
dynamical system's state space into different basins of attraction, and their
intersections lead to chaotic dynamics with fractal geometry. Here we introduce
a novel algorithm for detecting these manifolds, with a focus on
piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as
their activation function. We demonstrate how the algorithm can be used to
trace the boundaries between different basins of attraction, and hence to
characterize multistability, a computationally important property. We further
show its utility in finding so-called homoclinic points, the intersections
between stable and unstable manifolds, and thus establish the existence of
chaos in PLRNNs. Finally we show for an empirical example, electrophysiological
recordings from a cortical neuron, how insights into the underlying dynamics
could be gained through our method.

</details>


### [413] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: 本文提出TROLL方法，用离散可微信任区域投影替代PPO的clip机制，为LLM奖励微调提供更稳定的训练和更好的性能。


<details>
  <summary>Details</summary>
Motivation: PPO的clip机制作为KL信任区域的近似存在不足，常导致训练不稳定和次优性能，需要更原则性的替代方案。

Method: 提出离散可微信任区域投影，在模型重要token的logits子集上施加token级KL约束，平衡计算成本和投影效果。

Result: 在多个数据集、模型家族和优势估计方法上，TROLL在训练速度、稳定性和最终成功率方面一致优于PPO-like clip。

Conclusion: TROLL可作为PPO-like clip的直接替代，不改变模型推理行为，提供更稳定有效的RL训练方案。

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [414] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 该论文发现交叉熵缩放定律在超大规模下失效，并提出误差熵缩放定律作为更准确的替代方案。通过将交叉熵分解为误差熵、自对齐和置信度三个部分，发现只有误差熵遵循稳健的幂律缩放。


<details>
  <summary>Details</summary>
Motivation: 传统交叉熵缩放定律在超大规模语言模型中出现偏差，损失下降速度比预期更慢，这给大语言模型开发带来显著困扰。需要探究其根本原因并找到更准确的缩放规律。

Method: 提出新颖的交叉熵分解方法，将其分为误差熵、自对齐和置信度三个组成部分。通过理论分析和在多个数据集上对32个模型（跨越五个数量级规模）进行广泛实验验证。

Result: 实验表明只有误差熵遵循稳健的幂律缩放规律，而其他两个项基本保持不变。误差熵在小模型中占主导地位，但随着模型规模增大其比例逐渐减小，这解释了交叉熵缩放定律在小规模下准确但在超大规模下失效的原因。

Conclusion: 误差熵缩放定律比传统交叉熵缩放定律更能准确描述模型行为，将在大型语言模型的训练、理解和未来发展中有广泛应用。

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [415] [Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03823)
*Adam Haroon,Tristan Schuler*

Main category: cs.LG

TL;DR: 本文首次将多智能体强化学习应用于高空气球协同区域覆盖任务，证明QMIX算法能达到与理论最优几何确定性方法相似的性能。


<details>
  <summary>Details</summary>
Motivation: 现有高空气球多智能体协调方法主要使用确定性方法，在小型团队和局部任务中表现不佳，而多智能体强化学习在此领域尚未被研究。

Method: 扩展RLHAB仿真环境支持多智能体协同学习，采用QMIX算法和集中训练分散执行框架，使用包含个体状态、环境信息和队友数据的观察空间，以及优先覆盖和鼓励空间分布的层次化奖励。

Result: QMIX在分布式区域覆盖任务中达到了与理论最优几何确定性方法相似的性能表现。

Conclusion: 验证了多智能体强化学习方法的有效性，为更复杂的自主多高空气球任务奠定了基础，特别是在确定性方法难以处理的场景中。

Abstract: High Altitude Balloons (HABs) can leverage stratospheric wind layers for
limited horizontal control, enabling applications in reconnaissance,
environmental monitoring, and communications networks. Existing multi-agent HAB
coordination approaches use deterministic methods like Voronoi partitioning and
extremum seeking control for large global constellations, which perform poorly
for smaller teams and localized missions. While single-agent HAB control using
reinforcement learning has been demonstrated on HABs, coordinated multi-agent
reinforcement learning (MARL) has not yet been investigated. This work presents
the first systematic application of multi-agent reinforcement learning (MARL)
to HAB coordination for distributed area coverage. We extend our previously
developed reinforcement learning simulation environment (RLHAB) to support
cooperative multi-agent learning, enabling multiple agents to operate
simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area
coverage coordination, leveraging Centralized Training with Decentralized
Execution to address atmospheric vehicle coordination challenges. Our approach
employs specialized observation spaces providing individual state,
environmental context, and teammate data, with hierarchical rewards
prioritizing coverage while encouraging spatial distribution. We demonstrate
that QMIX achieves similar performance to the theoretically optimal geometric
deterministic method for distributed area coverage, validating the MARL
approach and providing a foundation for more complex autonomous multi-HAB
missions where deterministic methods become intractable.

</details>


### [416] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO通过慢-快策略优化框架解决强化学习训练早期的不稳定问题，在数学推理基准上比GRPO提升2.80分，减少4.93倍rollouts，加速4.19倍收敛。


<details>
  <summary>Details</summary>
Motivation: 现有on-policy算法如GRPO在早期训练中因低质量rollouts产生的噪声梯度导致不稳定更新和低效探索。

Method: SFPO将每个训练步骤分解为三个阶段：在同一批次上进行短快轨迹内部步骤、控制off-policy漂移的重新定位机制、以及最终的慢速校正。

Result: 在数学推理基准上比GRPO平均提升2.80分，减少4.93倍rollouts，wall-clock时间减少4.19倍达到GRPO最佳精度。

Conclusion: SFPO通过重新定位-更新设计保持目标和rollout过程不变，与现有策略梯度管道兼容，能稳定提升推理RL训练效果。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [417] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 提出PDNS框架，通过路径测度上的近端点方法解决多模态目标分布采样中的模式崩溃问题，将学习过程分解为一系列逐步逼近目标分布的简单子问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散神经采样器在处理多模态分布时容易发生模式崩溃，特别是当模式之间存在显著势垒时，训练变得困难。

Method: 采用路径测度上的近端点方法，将学习分解为渐进逼近的系列子问题，每个近端步骤使用加权去噪交叉熵目标实现。

Result: 在连续和离散采样任务中表现出有效性和鲁棒性，特别是在分子动力学和统计物理等挑战性场景中。

Conclusion: PDNS框架通过渐进式路径构建有效解决了多模态分布采样中的模式崩溃问题，促进了跨模式的充分探索。

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [418] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: HOFLON是一种混合离线学习+在线优化方法，用于解决连续过程工厂启动和产品等级转换的自动化问题，通过结合离线学习的数据流形和Q值评估器，以及在线优化来超越人类专家操作水平。


<details>
  <summary>Details</summary>
Motivation: 传统工厂启动和产品等级转换依赖少数专家操作员的手动操作，但随着这些专家退休，工厂缺乏执行这些关键操作所需的隐性知识。离线强化学习虽然能挖掘历史数据，但面临分布偏移和价值高估问题。

Method: HOFLON方法包含离线学习和在线优化两个阶段：离线学习数据流形和长时域Q值评估器；在线求解单步优化问题，最大化Q值同时惩罚偏离学习流形和操纵变量变化率过大。

Result: 在两个工业案例研究（聚合反应器启动和造纸机等级转换）中，HOFLON不仅超越了领先的离线RL算法IQL，而且平均累积奖励超过了历史数据中观察到的最佳启动或等级转换。

Conclusion: HOFLON展示了超越当前专家能力的自动化转换操作潜力，为解决工厂操作中专家知识流失问题提供了有效解决方案。

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [419] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: 提出FIRE方法，通过Fisher信息来累积碎片化引起的协变量偏移，作为每个数据片段的损失惩罚项，实现可扩展的分布对齐，在联邦学习中提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当训练数据在批次间或联邦学习的地理位置间碎片化时，训练模型会出现性能下降，这主要是由于协变量偏移导致各数据片段的经验分布与假设的未碎片化训练分布不同。

Method: 提出FIRE方法，通过近似Fisher信息累积碎片化引起的协变量偏移差异，并将其作为每个数据片段的损失惩罚项，实现可扩展的分布对齐。

Result: FIRE在偏移验证集上比重要性加权基准方法最多提升5.1%，比联邦学习基准方法最多提升5.3%。

Conclusion: FIRE方法能有效处理联邦学习中的数据碎片化问题，通过Fisher信息估计协变量偏移，显著提升模型在分布偏移下的性能。

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [420] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: M-FISHER是一个用于流数据中序列分布偏移检测和稳定适应的理论框架，通过指数鞅和Ville不等式实现时间均匀的误报控制，并基于Fisher预处理的提示参数更新实现自然梯度下降。


<details>
  <summary>Details</summary>
Motivation: 解决流数据中序列分布偏移的检测和适应问题，确保统计有效性并实现几何稳定的参数更新。

Method: 使用非一致性分数构建指数鞅，应用Ville不等式控制误报；通过Fisher预处理的提示参数更新实现自然梯度下降。

Result: 在持续偏移下，检测延迟为O(log(1/δ)/Γ)，其中Γ反映偏移后信息增益；适应过程实现KL散度最小化并保持稳定性。

Conclusion: M-FISHER为协变量偏移下的序列决策提供了原则性的、鲁棒的检测和几何稳定适应方法。

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [421] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 本文对比分析了自回归语言模型(ARMs)和扩散语言模型(DLMs)的性能特征，发现DLMs虽然具有更高的算术强度但难以扩展到长上下文，而ARMs在批量推理中吞吐量更优。


<details>
  <summary>Details</summary>
Motivation: 理解DLMs相对于广泛部署的ARMs的性能影响，探索两种架构在并行性、上下文扩展和批量推理方面的权衡。

Method: 使用理论分析和性能剖析数据，比较ARMs和DLMs的性能特征，探索块级解码的DLMs方法。

Result: DLMs因序列长度并行化具有更高算术强度，但上下文扩展性差；ARMs在批量推理中吞吐量更优；减少采样步骤可改善DLMs延迟。

Conclusion: DLMs和ARMs各有优劣，DLMs在算术强度上有优势但需要优化采样步骤，ARMs在批量推理和长上下文处理上表现更好。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [422] [On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records](https://arxiv.org/abs/2510.03844)
*Sarah C. Lotspeich,Abbey Collins,Brian J. Wells,Ashish K. Khanna,Joseph Rigdon,Lucy D'Agostino McGowan*

Main category: cs.LG

TL;DR: 开发了一种基于ICD-10代码和大型语言模型增强的算法，用于模仿专家图表审查来恢复电子健康记录中的缺失值，其准确性与人工审查相当但更具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据存在缺失和错误问题，传统图表审查方法昂贵且耗时，限制了可审查的患者数量，需要开发更高效的数据恢复方法。

Method: 使用ICD-10代码构建辅助诊断路线图，结合大型语言模型迭代优化路线图内容，开发算法自动恢复缺失数据，并与专家图表审查结果进行对比验证。

Result: 算法恢复的缺失数据量与专家图表审查相当甚至更多，具体表现取决于使用的路线图版本。在1000名患者的大规模研究中，使用LLM增强的路线图算法表现良好。

Conclusion: 临床驱动的算法（通过LLM增强）能够以与图表审查相似的准确性恢复缺失的EHR数据，并且可以可行地应用于大规模样本。未来可扩展到监测数据质量的其他维度。

Abstract: Objective: Electronic health records (EHR) data are prone to missingness and
errors. Previously, we devised an "enriched" chart review protocol where a
"roadmap" of auxiliary diagnoses (anchors) was used to recover missing values
in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a
missing hemoglobin A1c value would be considered unhealthy). Still, chart
reviews are expensive and time-intensive, which limits the number of patients
whose data can be reviewed. Now, we investigate the accuracy and scalability of
a roadmap-driven algorithm, based on ICD-10 codes (International Classification
of Diseases, 10th revision), to mimic expert chart reviews and recover missing
values. Materials and Methods: In addition to the clinicians' original roadmap
from our previous work, we consider new versions that were iteratively refined
using large language models (LLM) in conjunction with clinical expertise to
expand the list of auxiliary diagnoses. Using chart reviews for 100 patients
from the EHR at an extensive learning health system, we examine algorithm
performance with different roadmaps. Using the larger study of $1000$ patients,
we applied the final algorithm, which used a roadmap with clinician-approved
additions from the LLM. Results: The algorithm recovered as much, if not more,
missing data as the expert chart reviewers, depending on the roadmap.
Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing
EHR data with similar accuracy to chart reviews and can feasibly be applied to
large samples. Extending them to monitor other dimensions of data quality
(e.g., plausability) is a promising future direction.

</details>


### [423] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文研究了Muon优化器在联邦学习中的应用，提出了FedMuon算法并建立了其在非凸问题上的收敛率，理论分析和实验验证了该算法的有效性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在多种应用中表现出色，但其在联邦学习中的有效性尚未被探索，因此需要填补这一研究空白。

Method: 提出了FedMuon算法，该算法利用正交化更新方向，使其学习率独立于问题特定参数，并能自然适应重尾噪声。

Result: 理论分析揭示了FedMuon的多个有利特性，并在多种神经网络架构上的广泛实验验证了该算法的有效性。

Conclusion: FedMuon算法在联邦学习环境中表现出色，具有独立于问题参数的学习率和适应重尾噪声的能力。

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [424] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Wave-PDE Nets是一种基于二阶波动方程模拟的神经网络架构，使用可训练的空间速度和阻尼参数，通过FFT实现高效传播，在语言和视觉任务上性能媲美Transformer但效率更高。


<details>
  <summary>Details</summary>
Motivation: 为信息传播提供一种振荡的全局机制，作为注意力和一阶状态空间模型的替代方案，利用物理先验偏置构建计算高效且鲁棒的架构。

Method: 使用基于FFT的辛谱求解器实现二阶波动方程的可微模拟，每层通过具有可训练空间速度c(x)和阻尼γ(x)的介质传播隐藏状态。

Result: 在语言和视觉基准测试中，Wave-PDE Nets性能匹配或超越Transformer，同时减少30%的运行时间和25%的峰值内存使用。

Conclusion: Wave-PDE Nets是一种计算效率高且鲁棒的架构，具有强大的物理归纳偏置，辛积分和谱拉普拉斯算子对稳定性和性能至关重要。

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [425] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: 研究发现模型和数据集规模联合最优缩放由输出层的算子范数这一单一不变量控制，称为范数转移现象。


<details>
  <summary>Details</summary>
Motivation: 尽管在模型和数据集缩放下的最优超参数转移方面已有进展，但缺乏统一的解释原理。

Method: 使用Scion优化器，在多达13亿参数模型和1380亿token数据集上，通过测量最优学习率/批次大小组合的算子范数不变性来研究缩放规律。

Result: 发现最优学习率/批次大小组合始终具有相同的算子范数值，且输出层对学习率最敏感，隐藏层受益于较低学习率。

Conclusion: 提供了基于范数指导的最优缩放实用见解，并发布了分布式Scion实现和超过两千次运行的日志以支持大规模LLM训练动态研究。

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [426] [BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty](https://arxiv.org/abs/2510.03893)
*Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: 提出了BONSAI框架，一种利用部分结构知识的鲁棒贝叶斯优化方法，用于处理不确定性下的网络系统优化问题


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒优化方法需要已知问题结构，限制了在高保真仿真环境中的应用；现有鲁棒贝叶斯优化方法忽略结构信息且难以扩展到高维场景

Method: 将目标函数表示为有向图连接的白色和黑色盒子组件，利用中间信息；提出基于Thompson采样的可扩展采集函数，使用基于梯度的优化方法

Result: 在合成和实际案例研究中，相比现有仿真鲁棒优化算法，BONSAI能提供更样本高效和更高质量的鲁棒解

Conclusion: BONSAI在复杂工程系统的不确定性感知设计中具有实际优势

Abstract: Optimal design under uncertainty remains a fundamental challenge in advancing
reliable, next-generation process systems. Robust optimization (RO) offers a
principled approach by safeguarding against worst-case scenarios across a range
of uncertain parameters. However, traditional RO methods typically require
known problem structure, which limits their applicability to high-fidelity
simulation environments. To overcome these limitations, recent work has
explored robust Bayesian optimization (RBO) as a flexible alternative that can
accommodate expensive, black-box objectives. Existing RBO methods, however,
generally ignore available structural information and struggle to scale to
high-dimensional settings. In this work, we introduce BONSAI (Bayesian
Optimization of Network Systems under uncertAInty), a new RBO framework that
leverages partial structural knowledge commonly available in simulation-based
models. Instead of treating the objective as a monolithic black box, BONSAI
represents it as a directed graph of interconnected white- and black-box
components, allowing the algorithm to utilize intermediate information within
the optimization process. We further propose a scalable Thompson sampling-based
acquisition function tailored to the structured RO setting, which can be
efficiently optimized using gradient-based methods. We evaluate BONSAI across a
diverse set of synthetic and real-world case studies, including applications in
process systems engineering. Compared to existing simulation-based RO
algorithms, BONSAI consistently delivers more sample-efficient and
higher-quality robust solutions, highlighting its practical advantages for
uncertainty-aware design in complex engineering systems.

</details>


### [427] [LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis](https://arxiv.org/abs/2510.03904)
*Hangting Ye,Jinmeng Li,He Zhao,Mingchen Zhuge,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: LLM-DAS是一个新颖框架，将大语言模型从"数据处理者"重新定位为"算法专家"，通过分析检测器的内在弱点生成难以检测的异常数据，从而增强检测器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的表格异常检测方法依赖对异常模式的假设，在真实场景中表现不一致。虽然大语言模型具有强大的推理能力，但直接应用于表格异常检测面临异构数据处理困难和隐私风险等挑战。

Method: LLM-DAS框架利用大语言模型分析检测器的高层描述，理解其内在弱点，然后生成检测器特定、数据无关的Python代码来合成"难以检测"的异常，通过程序化合成增强训练数据。

Result: 在36个表格异常检测基准测试上的广泛实验表明，LLM-DAS能够持续提升主流检测器的性能。

Conclusion: 通过程序化合成将大语言模型推理与经典异常检测算法相结合，LLM-DAS提供了一种可扩展、有效且保护隐私的方法来修补现有检测器的逻辑盲点。

Abstract: Existing anomaly detection (AD) methods for tabular data usually rely on some
assumptions about anomaly patterns, leading to inconsistent performance in
real-world scenarios. While Large Language Models (LLMs) show remarkable
reasoning capabilities, their direct application to tabular AD is impeded by
fundamental challenges, including difficulties in processing heterogeneous data
and significant privacy risks. To address these limitations, we propose
LLM-DAS, a novel framework that repositions the LLM from a ``data processor''
to an ``algorithmist''. Instead of being exposed to raw data, our framework
leverages the LLM's ability to reason about algorithms. It analyzes a
high-level description of a given detector to understand its intrinsic
weaknesses and then generates detector-specific, data-agnostic Python code to
synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.
This generated synthesis program, which is reusable across diverse datasets, is
then instantiated to augment training data, systematically enhancing the
detector's robustness by transforming the problem into a more discriminative
two-class classification task. Extensive experiments on 36 TAD benchmarks show
that LLM-DAS consistently boosts the performance of mainstream detectors. By
bridging LLM reasoning with classic AD algorithms via programmatic synthesis,
LLM-DAS offers a scalable, effective, and privacy-preserving approach to
patching the logical blind spots of existing detectors.

</details>


### [428] [THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series](https://arxiv.org/abs/2510.03911)
*Yadav Mahesh Lorik,Kaushik Sarveswaran,Nagaraj Sundaramahalingam,Aravindakumar Venugopalan*

Main category: cs.LG

TL;DR: THEMIS是一个基于预训练基础模型的时间序列异常检测框架，利用Chronos时间序列基础模型的编码器提取嵌入，并通过异常检测技术识别数据中的异常。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测面临诸多挑战，包括季节性、趋势、噪声、概念漂移、数据不平衡、高维性、实时检测需求等，需要强大、灵活且可解释的方法。

Method: 提取Chronos时间序列基础模型编码器的嵌入表示，应用局部异常因子和自相似矩阵的谱分解等异常检测技术来识别异常。

Result: 在MSL数据集上达到最先进水平，在SMAP和SWAT*数据集上表现具有竞争力，甚至超过了专门为异常检测训练的模型，具有超参数鲁棒性和默认可解释性。

Conclusion: 倡导使用基础模型的预训练表示来进行高效且适应性强的时间序列异常检测。

Abstract: Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.

</details>


### [429] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出了一种基于高斯分布的偏信息分解(GPID)方法，通过梯度优化算法提高计算效率，并利用信息保持编码器将非高斯数据转换为高斯分布，解决了传统PID方法在连续高维模态中的计算成本和精度问题。


<details>
  <summary>Details</summary>
Motivation: 传统偏信息分解(PID)方法依赖于优化受估计的成对概率分布约束的联合分布，对于连续和高维模态来说计算成本高且不准确。需要一种更高效准确的方法来量化多模态数据中信息的独立、冗余和协同传递。

Method: 1. 提出高斯PID(GPID)框架，假设成对分布为多元高斯分布；2. 开发基于梯度优化的新算法，提高GPID计算效率；3. 使用信息保持编码器将任意分布的随机变量转换为成对高斯随机变量，扩展方法适用性。

Result: 在多种合成示例中的实证验证表明，所提方法比现有基线提供更准确和高效的PID估计。在大规模多模态基准测试中展示了其在量化多模态数据集中的PID和选择高性能模型方面的实用性。

Conclusion: 该方法有效解决了传统PID在连续高维模态中的计算挑战，通过高斯假设和编码器转换实现了高效准确的偏信息分解，为多模态数据分析提供了实用工具。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [430] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出一种处理聚类数据的广义拟合Q迭代算法，通过引入广义估计方程来处理簇内相关性，在医疗健康应用中显著优于标准FQI。


<details>
  <summary>Details</summary>
Motivation: 医疗健康应用中常遇到聚类数据（如多个患者来自同一医院），标准强化学习方法忽略簇内相关性，导致估计偏差。

Method: 将广义估计方程整合到拟合Q迭代中，开发广义FQI算法，处理聚类数据中的簇内相关性。

Result: 理论证明：当相关结构正确指定时，Q函数和策略估计器具有最优性；当结构误指定时仍保持一致性。实证显示：相比标准FQI，广义FQI平均减少一半遗憾。

Conclusion: 广义FQI能有效处理聚类数据中的相关性，在医疗健康应用中显著提升强化学习性能。

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [431] [Transductive and Learning-Augmented Online Regression](https://arxiv.org/abs/2510.03917)
*Vinod Raman,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 本文研究了在线回归问题，当学习者能够获得未来样本的预测信息时。在极端情况下（转导在线学习），我们完全刻画了最小最大期望遗憾与fat-shattering维度的关系，并开发了一个在线学习器，其遗憾随预测质量平滑提升。


<details>
  <summary>Details</summary>
Motivation: 现实数据流具有可预测性，因此研究当学习者能够获得未来样本预测时的在线回归问题具有重要意义。

Method: 首先研究转导在线学习（序列完全已知），然后推广到不完美预测情况，利用转导在线学习的结果开发在线学习器。

Result: 完全刻画了转导在线回归的最小最大期望遗憾，建立了与对抗性在线回归的分离。开发的在线学习器在预测精确时能达到接近转导在线学习的性能。

Conclusion: 该方法使得在可预测样本下原本不可学习的类变得可学习，符合学习增强模型范式。

Abstract: Motivated by the predictable nature of real-life in data streams, we study
online regression when the learner has access to predictions about future
examples. In the extreme case, called transductive online learning, the
sequence of examples is revealed to the learner before the game begins. For
this setting, we fully characterize the minimax expected regret in terms of the
fat-shattering dimension, establishing a separation between transductive online
regression and (adversarial) online regression. Then, we generalize this
setting by allowing for noisy or \emph{imperfect} predictions about future
examples. Using our results for the transductive online setting, we develop an
online learner whose minimax expected regret matches the worst-case regret,
improves smoothly with prediction quality, and significantly outperforms the
worst-case regret when future example predictions are precise, achieving
performance similar to the transductive online learner. This enables
learnability for previously unlearnable classes under predictable examples,
aligning with the broader learning-augmented model paradigm.

</details>


### [432] [On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks](https://arxiv.org/abs/2510.03923)
*Mingsong Yan,Charles Kulick,Sui Tang*

Main category: cs.LG

TL;DR: 本文对具有时变参数的连续深度图神经网络（GNDEs）在无限节点极限下的收敛性进行了严格分析，建立了图神经微分方程到图神经微分方程的轨迹收敛性，并推导了确定性图采样机制下的显式收敛速率和尺寸可转移性边界。


<details>
  <summary>Details</summary>
Motivation: 结合图神经网络的结构归纳偏置和神经ODE的连续深度架构，为图上的动力学建模提供可扩展的理论框架，研究GNDEs在无限节点极限下的收敛性及其尺寸可转移性。

Method: 引入图神经微分方程（Graphon-NDEs）作为GNDEs的无限节点极限，利用图论和动力系统工具证明GNDE解到Graphon-NDE解的轨迹收敛性，并在两种确定性图采样机制下推导显式收敛速率。

Result: 建立了GNDEs在无限节点极限下的收敛理论，证明了轨迹收敛性，给出了平滑图子和不连续图子采样下的显式收敛速率，并建立了尺寸可转移性边界。

Conclusion: 为将中等规模图上训练的GNDE模型迁移到结构相似的大规模图而不需要重新训练提供了理论依据，数值实验验证了理论结果。

Abstract: Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.

</details>


### [433] [On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection](https://arxiv.org/abs/2510.03944)
*Weiqing He,Xiang Li,Tianqi Shang,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文系统评估了8种拟合优度检验在三种流行水印方案中的表现，发现通用GoF测试能显著提升水印检测能力和鲁棒性，特别是在低温度设置下的文本重复场景中具有独特优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的内容真实性和完整性引发担忧，文本水印提供可验证的内容来源证明。虽然拟合优度检验是水印检测的自然工具，但在此领域尚未得到充分探索。

Method: 使用三种开源LLM、两个数据集、多种生成温度和后编辑方法，系统评估八种GoF测试在三种流行水印方案中的表现。

Result: 通用GoF测试能提高水印检测能力和鲁棒性，特别是在低温度设置下文本重复场景中具有独特优势，这是现有方法未利用的。

Conclusion: 经典的拟合优度检验是LLM水印检测中简单而强大但未被充分利用的工具。

Abstract: Large language models (LLMs) raise concerns about content authenticity and
integrity because they can generate human-like text at scale. Text watermarks,
which embed detectable statistical signals into generated text, offer a
provable way to verify content origin. Many detection methods rely on pivotal
statistics that are i.i.d. under human-written text, making goodness-of-fit
(GoF) tests a natural tool for watermark detection. However, GoF tests remain
largely underexplored in this setting. In this paper, we systematically
evaluate eight GoF tests across three popular watermarking schemes, using three
open-source LLMs, two datasets, various generation temperatures, and multiple
post-editing methods. We find that general GoF tests can improve both the
detection power and robustness of watermark detectors. Notably, we observe that
text repetition, common in low-temperature settings, gives GoF tests a unique
advantage not exploited by existing methods. Our results highlight that classic
GoF tests are a simple yet powerful and underused tool for watermark detection
in LLMs.

</details>


### [434] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDiR是一个新颖的推理框架，通过将连续潜在表示与潜在扩散模型相结合，解决了LLM自回归解码在推理过程中的局限性，实现了更高效、多样化的推理轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 传统LLM的自回归解码限制了模型对早期推理步骤的重新审视和优化能力，同时也导致解决方案多样性探索效率低下。

Method: 使用变分自编码器构建结构化潜在推理空间，将文本推理步骤编码为思想标记块；然后利用潜在扩散模型学习去噪潜在思想标记块，通过块状双向注意力掩码实现长程迭代优化。

Result: 在数学推理和规划基准测试中，LaDiR在准确性、多样性和可解释性方面均优于现有的自回归、基于扩散和潜在推理方法。

Conclusion: LaDiR为文本推理提供了一个新的潜在扩散范式，展示了在推理任务中结合连续表示和迭代优化的有效性。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [435] [What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis](https://arxiv.org/abs/2510.03950)
*Shahriar Kabir Nahin,Wenxiao Xiao,Joshua Liu,Anshuman Chhabra,Hongfu Liu*

Main category: cs.LG

TL;DR: 本文提出类别影响力函数和影响向量，通过线性规划样本重加权框架实现模型在所有类别上的帕累托性能提升，突破了传统仅关注整体准确率的局限。


<details>
  <summary>Details</summary>
Motivation: 现有数据中心学习主要关注"什么数据有益于学习模型"，而本文研究更基础的问题"学习模型的性能上限是什么"，强调类别准确率和帕累托改进，确保每个类别都受益。

Method: 提出类别影响力函数和影响向量来量化每个训练样本对所有类别的影响，基于这些影响向量开发原则性标准判断模型是否可改进，并设计基于线性规划的样本重加权框架。

Result: 在合成数据集、视觉和文本基准上的广泛实验表明，该方法能有效估计和实现模型在多个感兴趣类别上的性能改进。

Conclusion: 该方法为模型性能改进提供了新的视角和工具，能够实现所有类别的帕累托改进，而不仅仅是整体准确率的提升。

Abstract: Data-centric learning seeks to improve model performance from the perspective
of data quality, and has been drawing increasing attention in the machine
learning community. Among its key tools, influence functions provide a powerful
framework to quantify the impact of individual training samples on model
predictions, enabling practitioners to identify detrimental samples and retrain
models on a cleaner dataset for improved performance. However, most existing
work focuses on the question: "what data benefits the learning model?" In this
paper, we take a step further and investigate a more fundamental question:
"what is the performance ceiling of the learning model?" Unlike prior studies
that primarily measure improvement through overall accuracy, we emphasize
category-wise accuracy and aim for Pareto improvements, ensuring that every
class benefits, rather than allowing tradeoffs where some classes improve at
the expense of others. To address this challenge, we propose category-wise
influence functions and introduce an influence vector that quantifies the
impact of each training sample across all categories. Leveraging these
influence vectors, we develop a principled criterion to determine whether a
model can still be improved, and further design a linear programming-based
sample reweighting framework to achieve Pareto performance improvements.
Through extensive experiments on synthetic datasets, vision, and text
benchmarks, we demonstrate the effectiveness of our approach in estimating and
achieving a model's performance improvement across multiple categories of
interest.

</details>


### [436] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE框架通过将上下文视为不断演化的操作手册，通过生成、反思和策划的模块化过程来积累、优化和组织策略，解决了现有方法中的简洁性偏见和上下文崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型应用在上下文适应方面存在简洁性偏见（丢失领域洞察）和上下文崩溃（迭代重写导致细节丢失）的问题，需要一种能够保持详细知识并随长上下文模型扩展的解决方案。

Method: ACE框架采用结构化、增量式更新的方式，通过生成、反思和策划三个模块化过程来处理上下文，使其能够积累、优化和组织策略，防止上下文崩溃。

Result: 在代理和领域特定基准测试中，ACE在离线（如系统提示）和在线（如代理记忆）上下文优化方面均优于强基线：代理任务提升10.6%，金融任务提升8.6%，同时显著降低了适应延迟和部署成本。

Conclusion: 全面、不断演化的上下文能够实现可扩展、高效且自我改进的大语言模型系统，且开销较低，ACE框架在无监督情况下也能有效适应，仅利用自然执行反馈即可达到生产级代理的性能水平。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [437] [Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts](https://arxiv.org/abs/2510.03954)
*Tim Bary,Tiffanie Godelaine,Axel Abels,Benoît Macq*

Main category: cs.LG

TL;DR: 提出了一种用于医学筛查的自适应实时标注方法，能够在专家能力未知且无预标注数据的情况下，动态查询专家意见，显著减少标注工作量


<details>
  <summary>Details</summary>
Motivation: 医学筛查中准确的真值估计依赖于专家共识，现有算法无法满足筛查流程的无缝集成需求，需要支持实时标注、无需先验知识且能动态调整专家查询的方法

Method: 自适应实时标注方法，支持对传入数据的即时标注，无需医学专家先验知识或预标注数据，基于每个实例的潜在难度动态查询额外专家，持续收集专家意见直到达到置信度阈值

Result: 在三个多标注者分类数据集上的评估显示，自适应查询策略将专家查询次数减少高达50%，同时达到与非自适应基线相当的准确率

Conclusion: 该方法能够有效减少医学筛查中的标注开销，提供准确标签，适合集成到筛查工作流程中

Abstract: Accurate ground truth estimation in medical screening programs often relies
on coalitions of experts and peer second opinions. Algorithms that efficiently
aggregate noisy annotations can enhance screening workflows, particularly when
data arrive continuously and expert proficiency is initially unknown. However,
existing algorithms do not meet the requirements for seamless integration into
screening pipelines. We therefore propose an adaptive approach for real-time
annotation that (I) supports on-the-fly labeling of incoming data, (II)
operates without prior knowledge of medical experts or pre-labeled data, and
(III) dynamically queries additional experts based on the latent difficulty of
each instance. The method incrementally gathers expert opinions until a
confidence threshold is met, providing accurate labels with reduced annotation
overhead. We evaluate our approach on three multi-annotator classification
datasets across different modalities. Results show that our adaptive querying
strategy reduces the number of expert queries by up to 50% while achieving
accuracy comparable to a non-adaptive baseline. Our code is available at
https://github.com/tbary/MEDICS

</details>


### [438] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: 开发了一个24-48小时雷暴停电早期预警模型，使用公开数据源和两阶段模型设计（逻辑门+LSTM回归器），在密歇根州夏季雷暴相关停电预测中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 雷暴驱动的停电难以预测，因为大多数风暴不会造成损害，对流过程快速且混乱，且可用的公共数据既嘈杂又不完整。

Method: 使用公开数据（EAGLE-I停电数据、METAR天气数据），通过参数特定克里金插值和目标过采样保留极端值，构建因果时空特征，采用两阶段模型设计（逻辑门+LSTM回归器）。

Result: 两阶段模型在所有时间窗口检测到更多参考峰值（如±48小时记录3/4 vs 2/4），在峰值附近显示适度幅度增益（±0-12小时cMASE降低2-3%），误差与单步LSTM基线相当。

Conclusion: 尽管公开数据存在噪声，特征驱动的管道为雷暴停电提供了可操作的、以事件为中心的早期预警。

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [439] [SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data](https://arxiv.org/abs/2510.03962)
*Hanzhe Wei,Jiajun Wu,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.LG

TL;DR: SPEAR是一种利用大型语言模型进行时间序列异常检测的新方法，通过软提示和量化技术来适应时间序列数据的特点。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理变长时间序列序列和基于上下文的异常时存在困难，而大型语言模型的出现为时间序列异常检测提供了新的机会。

Method: 将时间序列数据量化为输入嵌入，与可学习的软提示嵌入结合后输入冻结的LLM，通过交叉熵损失迭代更新软提示来适应时间序列异常检测任务。

Result: 实验结果表明软提示有效提高了LLM在时间序列异常检测下游任务中的性能。

Conclusion: 软提示和量化技术能够有效帮助LLM适应时间序列异常检测任务，解决传统方法面临的挑战。

Abstract: Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.

</details>


### [440] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: 该论文研究了在强化学习中当基础模型从未产生正确答案时的零奖励障碍问题，发现简单的数据干预（添加易样本）比算法改进更有效。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习在复杂推理任务中遇到的零奖励障碍问题，即当基础模型从未采样到正确解决方案时，训练会因零梯度而停滞。

Method: 使用图搜索任务评估了多种方法（密集奖励、多样性激励、改进信用分配），并测试了简单的数据干预方法——在训练集中添加易样本。

Result: 实验表明，现有算法方法都无法克服零奖励障碍，而简单的数据干预方法能够使模型最终解决原始困难任务。

Conclusion: 数据中心的干预（添加易样本）比算法改进更有效地克服零奖励障碍，且无需修改RL算法本身。

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [441] [Beyond Softmax: A New Perspective on Gradient Bandits](https://arxiv.org/abs/2510.03979)
*Emerson Melo,David Müller*

Main category: cs.LG

TL;DR: 该论文建立了离散选择模型与在线学习和多臂老虎机理论之间的联系，提出了具有次线性遗憾界限的算法家族，并引入了超越softmax的广义梯度老虎机算法。


<details>
  <summary>Details</summary>
Motivation: 连接离散选择模型与在线学习理论，克服传统softmax方法中限制性的独立假设，允许动作间的相关学习动态。

Method: 提出了一个广泛的算法家族，包括从广义嵌套logit模型推导出的对抗性老虎机算法，以及新的广义梯度老虎机算法，通过闭式采样概率实现计算效率。

Result: 获得了次线性遗憾界限，数值实验在随机老虎机设置中证明了所提算法的实际有效性。

Conclusion: 所提出的算法结合了灵活的模型规范和计算效率，扩展了梯度老虎机方法的适用性。

Abstract: We establish a link between a class of discrete choice models and the theory
of online learning and multi-armed bandits. Our contributions are: (i)
sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a
special case; (ii) a new class of adversarial bandit algorithms derived from
generalized nested logit models \citep{wen:2001}; and (iii)
\textcolor{black}{we introduce a novel class of generalized gradient bandit
algorithms that extends beyond the widely used softmax formulation. By relaxing
the restrictive independence assumptions inherent in softmax, our framework
accommodates correlated learning dynamics across actions, thereby broadening
the applicability of gradient bandit methods.} Overall, the proposed algorithms
combine flexible model specification with computational efficiency via
closed-form sampling probabilities. Numerical experiments in stochastic bandit
settings demonstrate their practical effectiveness.

</details>


### [442] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: 提出了ONNX-Bench基准测试和ONNX-Net文本编码方法，通过统一的ONNX格式和自然语言描述来打破传统NAS对特定搜索空间的限制，实现跨搜索空间的零样本性能预测。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经架构搜索方法受限于特定搜索空间和图形编码的问题，传统方法在更广泛的搜索空间中缺乏灵活性和可扩展性。

Method: 创建包含60多万个{架构,精度}对的ONNX-Bench基准，开发基于自然语言描述的ONNX-Net统一网络表示方法，能够编码任意层类型、操作参数和异构拓扑。

Result: 实验显示该方法在跨搜索空间场景下具有强大的零样本性能，仅需少量预训练样本即可实现任意神经网络架构的即时评估。

Conclusion: ONNX-Bench和ONNX-Net方法成功打破了传统NAS的搜索空间限制，为更灵活、可扩展的神经架构搜索提供了新途径。

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [443] [ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity](https://arxiv.org/abs/2510.03987)
*Michael Yang*

Main category: cs.LG

TL;DR: ICEPool是一种新的层次池化框架，通过增强模型对簇间连通性的理解来提升图结构数据的分类性能，兼容多种池化GNN模型。


<details>
  <summary>Details</summary>
Motivation: 现有层次池化模型在聚类分配和粗化策略设计方面有很多创新，但往往忽略了簇之间的关系。

Method: 提出ICEPool框架，强调簇间连通性的整合，通过图重构能力来学习传统模型忽略的簇间关系。

Result: 实验结果表明ICEPool与多种模型兼容，并能提升现有图神经网络架构的性能。

Conclusion: ICEPool通过增强簇间连通性理解，能够产生更全面和鲁棒的图级表示，有效提升图分类性能。

Abstract: Hierarchical Pooling Models have demonstrated strong performance in
classifying graph-structured data. While numerous innovative methods have been
proposed to design cluster assignments and coarsening strategies, the
relationships between clusters are often overlooked. In this paper, we
introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel
hierarchical pooling framework designed to enhance model's understanding of
inter-cluster connectivity and ability of preserving the structural integrity
in the original graph. ICEPool is compatible with a wide range of pooling-based
GNN models. The deployment of ICEPool as an enhancement to existing models
effectively combines the strengths of the original model with ICEPool's
capability to emphasize the integration of inter-cluster connectivity,
resulting in a more comprehensive and robust graph-level representation.
Moreover, we make theoretical analysis to ICEPool's ability of graph
reconstruction to demonstrate its effectiveness in learning inter-cluster
relationship that is overlooked by conventional models. Finally, the
experimental results show the compatibility of ICEPool with wide varieties of
models and its potential to boost the performance of existing graph neural
network architectures.

</details>


### [444] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: 本文扩展了结构化状态空间对偶性(SSD)，从标量恒等状态矩阵推广到一般对角SSM，建立了对角SSM与1-半可分因果掩码注意力之间的等价关系，并分析了这种对偶性无法扩展到标准softmax注意力的原因。


<details>
  <summary>Details</summary>
Motivation: 原始SSD只适用于标量恒等状态矩阵的简单情况，限制了模型的表达能力。本文旨在将这种对偶性推广到更一般的对角SSM，以支持更丰富的动态特性，同时保持训练复杂度的理论下界。

Method: 通过数学分析扩展SSD理论：(1)从标量恒等状态矩阵推广到一般对角SSM；(2)建立对角SSM与1-半可分掩码注意力的等价条件；(3)分析标准softmax注意力中存在的秩爆炸问题。

Result: 证明了对角SSM在保持训练复杂度下界的同时支持更丰富的动态；建立了SSM与1-半可分掩码注意力等价的充要条件；揭示了标准softmax注意力由于秩爆炸而无法实现类似对偶性。

Conclusion: 该工作深化了循环SSM与Transformer之间的理论联系，扩展了表达能力强且高效的序列模型的设计空间，为开发兼具线性时间复杂度和丰富表达能力的序列模型提供了理论基础。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [445] [Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data](https://arxiv.org/abs/2510.03988)
*Hoang Anh Just,Myeongseob Ko,Ruoxi Jia*

Main category: cs.LG

TL;DR: 本文提出局部自然度方法，用于在多教师设置下选择最佳推理轨迹进行知识蒸馏，相比全局自然度能显著提升学生模型在数学基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在多个教师模型输出中选择最佳响应时存在不足，特别是当推理轨迹变长时，全局自然度与下游性能不再相关。

Method: 引入局部自然度，通过测量学生在短序列推理步骤上的对数概率来选择最佳响应，支持教师选择和响应选择两个应用。

Result: 局部自然度在数学基准上将32B学生模型的准确率比全局选择提升了9.4个百分点，甚至超过了使用单一最佳教师数据的性能。

Conclusion: 局部数据质量评估和数据混合能更有效地进行推理蒸馏，局部自然度是解决多教师响应选择问题的有效方法。

Abstract: Distilling long reasoning traces (10K+ tokens) from stronger teacher models
into smaller student LLMs via SFT has emerged as a standard paradigm. This
approach is practical and efficient: it leverages the ease of generating
abundant reasoning data from stronger models and provides a direct, data-driven
way to teach less capable models better reasoning. While previous work has
largely focused on prompt selection with responses from a single teacher, the
equally important problem of choosing the best response when multiple teacher
outputs are available for a single prompt remains underexplored. This challenge
becomes important in a multi-teacher setting, where different students may
benefit from the outputs of different teachers. This paper fills that gap with
a systematic study of response selection for reasoning distillation. We first
show that the current method, which picks responses the student assigns the
highest global log-probability (global naturalness), fails when responses come
from multiple teachers, i.e., global naturalness no longer correlates with
downstream performance, especially as the reasoning traces from strong teachers
become longer. To overcome this problem, we introduce Local Naturalness, which
measures the student's log-probabilities over short, sequential reasoning steps
conditioned only on a small local window. Local Naturalness enables two
applications: 1) Teacher Selection: Aggregating local scores across prompts
reliably identifies the most helpful teacher. 2) Response Selection from a
Multiple Teachers: When mixing answers from many teachers, Local Naturalness
boosts a 32B student's accuracy on math benchmarks by 9.4pp over global
selection, also surpassing the performance achieved by training on data from
the single best teacher. These results highlight the power of localized data
quality evaluation and data mixing for more effective reasoning distillation.

</details>


### [446] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: 提出了一种将Transformer解释为结构化积分-微分方程离散化的连续框架，将自注意力机制视为非局部积分算子，层归一化视为时间相关约束的投影。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在序列建模中取得了革命性突破，但缺乏全面的数学理论来解释其结构和操作。

Method: 采用算子理论和变分视角，将整个Transformer操作嵌入到标记索引和特征维度的连续域中，形成统一的解释框架。

Result: 建立了Transformer与连续数学建模之间的联系，为理解注意力、前馈层和归一化等核心组件提供了理论基础。

Conclusion: 这一新解释为深度学习架构与连续数学建模之间架起了桥梁，为可解释和理论基础的神经网络模型发展贡献了基础视角。

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [447] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: Reinforce-Ada是一个用于LLM在线强化学习后训练的自适应采样框架，通过动态重新分配采样努力到具有最大不确定性或学习潜力的提示，加速收敛并提高最终性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在推理任务中对所有提示进行固定均匀采样会导致不稳定的梯度估计，限制了强化学习在大语言模型中的应用效果。

Method: 提出在线连续消除过程，将估计和采样交错进行，一旦收集到足够信号就自动停止对提示的采样。通过形成具有强制奖励多样性的固定大小组，并使用自适应采样阶段聚合的全局统计计算优势基线来稳定更新。

Result: 在多个模型架构和推理基准测试中，Reinforce-Ada相比GRPO加速了收敛并提高了最终性能，特别是使用平衡采样变体时效果更佳。

Conclusion: 研究表明，方差感知的自适应数据策划在实现推理能力LLM的高效可靠强化学习中起着核心作用。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [448] [Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints](https://arxiv.org/abs/2510.04006)
*Hang Fan,Yi Xiao,Yongquan Qu,Fenghua Ling,Ben Fei,Lei Bai,Pierre Gentine*

Main category: cs.LG

TL;DR: 将机器学习天气预报模型训练重新解释为弱约束四维变分数据同化问题，在自编码器学习的潜空间中计算损失函数，提高长期预报技能和物理真实性


<details>
  <summary>Details</summary>
Motivation: 传统ML预报模型将再分析数据视为绝对真值，忽略物理耦合和空间结构，导致长期预报模糊且物理不真实

Method: 将模型训练重新解释为WC-4DVar问题，在自编码器学习的潜空间中计算损失函数，避免显式建模高维空间中的再分析误差协方差

Result: 潜空间约束的滚动训练提高了长期预报技能，更好地保持了精细尺度结构和物理真实性

Conclusion: 该框架可扩展到异构数据源，使预报模型能够在统一理论框架下联合训练再分析和多源观测数据

Abstract: Data-driven machine learning (ML) models have recently shown promise in
surpassing traditional physics-based approaches for weather forecasting,
leading to a so-called second revolution in weather forecasting. However, most
ML-based forecast models treat reanalysis as the truth and are trained under
variable-specific loss weighting, ignoring their physical coupling and spatial
structure. Over long time horizons, the forecasts become blurry and physically
unrealistic under rollout training. To address this, we reinterpret model
training as a weak-constraint four-dimensional variational data assimilation
(WC-4DVar) problem, treating reanalysis data as imperfect observations. This
allows the loss function to incorporate reanalysis error covariance and capture
multivariate dependencies. In practice, we compute the loss in a latent space
learned by an autoencoder (AE), where the reanalysis error covariance becomes
approximately diagonal, thus avoiding the need to explicitly model it in the
high-dimensional model space. We show that rollout training with latent-space
constraints improves long-term forecast skill and better preserves fine-scale
structures and physical realism compared to training with model-space loss.
Finally, we extend this framework to accommodate heterogeneous data sources,
enabling the forecast model to be trained jointly on reanalysis and
multi-source observations within a unified theoretical formulation.

</details>


### [449] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: Diff Interpretation Tuning (DIT) 是一种训练模型描述自身微调引起修改的方法，通过合成标记的权重差异来训练适配器，使模型能用自然语言准确描述其微调后的变化。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型时产生的权重变化通常难以解释，而微调数据集往往不公开或过于庞大，难以直接分析，因此需要一种方法来理解这些权重差异。

Method: 使用合成的标记权重差异训练DIT适配器，该适配器可应用于兼容的微调模型，使其能够描述自身的变化。

Result: 在两个概念验证场景（报告隐藏行为和总结微调知识）中，该方法使模型能够使用准确的自然语言描述其微调引起的修改。

Conclusion: DIT方法成功实现了让模型用自然语言描述自身微调变化的目标，为理解模型权重变化提供了新的途径。

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [450] [Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention](https://arxiv.org/abs/2510.04008)
*Sahil Joshi,Agniva Chowdhury,Amar Kanakamedala,Ekam Singh,Evan Tu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: RACE Attention是一种线性复杂度的注意力机制，替代二次复杂度的Softmax Attention，能够处理超长上下文（最高7500万token）。


<details>
  <summary>Details</summary>
Motivation: Softmax Attention的二次时间复杂度在处理长上下文时变得不可行，即使使用优化GPU内核（如FlashAttention）也无法处理超过400万token的上下文。

Method: 使用锐化的角度（余弦）相似度替代指数核，通过随机投影和软局部敏感哈希（LSH）近似注意力输出。

Result: 在语言建模、掩码语言建模和文本分类任务中，RACE Attention在保持准确性的同时显著减少运行时间和内存使用，在NVIDIA GH200 GPU上可处理1200万token，在Intel Xeon Gold 5220R CPU上可处理7500万token。

Conclusion: RACE Attention为当前硬件上的超长上下文窗口提供了实用且理论基础的机制，有望在实践中得到应用。

Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.

</details>


### [451] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: BVPO通过混合高方差的轨迹梯度估计器和低方差的空轨迹梯度估计器，优化偏好对齐中的偏差-方差权衡，显著减少轨迹采样方差，提升模型对齐和推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成中间推理轨迹后产生最终答案，在多步和数学任务上表现优异，但偏好对齐这一关键部署前提仍待探索。传统方法优化单个采样轨迹会引入显著的梯度方差。

Method: 提出偏差-方差优化偏好对齐(BVPO)，混合两种梯度估计器：高方差的轨迹估计器和通过禁用推理轨迹生成获得的低方差空轨迹估计器，理论分析提供了最小化均方误差的混合权重闭式解。

Result: 在AlpacaEval~2上比最佳基线提升7.8分，Arena-Hard上提升6.8分；仅使用通用对话数据训练，在六个数学推理基准上平均提升4.0分。

Conclusion: 轨迹采样方差是偏好对齐的关键瓶颈，直接优化偏差-方差权衡可实现更稳定的训练和更强的整体性能。

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


### [452] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出了SFP（时空预测即规划）新范式，基于模型强化学习构建生成世界模型，通过波束搜索规划算法利用不可微领域指标作为奖励信号，识别高回报未来序列作为伪标签进行自训练优化。


<details>
  <summary>Details</summary>
Motivation: 解决物理时空预测中固有的随机性和不可微指标的双重挑战，传统方法难以直接优化领域关键指标。

Method: 构建生成世界模型进行环境模拟，基础预测模型作为智能体，使用波束搜索规划算法探索高回报未来序列，并通过自训练优化策略。

Result: 显著减少预测误差，在捕获极端事件等关键领域指标上表现出色。

Conclusion: SFP范式通过结合生成建模和规划算法，有效解决了时空预测中的随机性和不可微优化问题。

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [453] [Multi-Class Support Vector Machine with Differential Privacy](https://arxiv.org/abs/2510.04027)
*Jinseong Park,Yujin Choi,Jaewook Lee*

Main category: cs.LG

TL;DR: 提出了一种新颖的差分隐私多类SVM（PMSVM），通过权重和梯度扰动方法解决传统OvR和OvO方法在差分隐私设置下隐私预算消耗过多的问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型数据隐私保护需求的增加，差分隐私成为主要框架。传统多类SVM方法（OvR和OvO）在差分隐私设置下会重复查询每个数据样本，导致隐私预算随类别数成比例消耗。

Method: 采用all-in-one SVM方法，每个数据样本仅访问一次来构建多类SVM边界。提出PMSVM，使用权重和梯度扰动方法，并提供严格的敏感性和收敛性分析以确保差分隐私。

Result: 经验结果表明，该方法在多类场景下超越了现有的DP-SVM方法。

Conclusion: 所提出的PMSVM方法有效解决了多类SVM在差分隐私设置下的隐私预算消耗问题，在保持隐私保护的同时实现了更好的性能。

Abstract: With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.

</details>


### [454] [The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View](https://arxiv.org/abs/2510.04028)
*Xinhao Yao,Lu Yu,Xiaolin Hu,Fengwei Teng,Qing Cui,Jun Zhou,Yong Liu*

Main category: cs.LG

TL;DR: RLVR对LLM推理能力边界的影响存在争议，研究揭示了双阶段概率质量动态：利用阶段可能导致能力边界收缩，探索阶段则促进能力边界扩展。


<details>
  <summary>Details</summary>
Motivation: 解决关于RLVR对大型语言模型推理能力边界影响的争议，调和相互矛盾的研究发现。

Method: 通过理论和实证分析，识别了RLVR训练中固有的双阶段概率质量动态：利用阶段和探索阶段。

Result: 发现利用阶段可能导致能力边界收缩，而探索阶段能促进能力边界扩展；重新评估了仅使用相对负梯度进行延长训练的潜力。

Conclusion: RLVR对LLM推理能力的影响取决于训练阶段，双阶段动态解释了矛盾的研究结果，为开发更先进的推理能力提供了理论基础。

Abstract: The ongoing debate on whether reinforcement learning with verifiable rewards
(RLVR) expands or shrinks the reasoning capabilities of large language models
(LLMs) remains unresolved. Some studies contend that RLVR mainly improves
sampling efficiency but at the expense of diversity and exploratory capacity,
resulting in capability boundary shrinkage. In contrast, others demonstrate
that prolonged training can lead to the emergence of novel reasoning
strategies, suggesting capability boundary expansion. To reconcile these
contradictory findings, we theoretically and empirically show that both
perspectives are partially valid-each aligning with a separate phase in an
inherent two-stage probability mass dynamic: (1) Exploitation stage: initially,
the model primarily samples explored high-reward and low-reward tokens, while
rarely selecting the potentially optimal token. Positive advantage estimates
increase the probability of high-reward tokens and decrease those of low-reward
tokens, yet the optimal token's probability remains largely unchanged during
this stage. (2) Exploration stage: as training advances, the growth rate of
previously acquired high-reward tokens slows as their probabilities approach
saturation. When a potentially optimal token-now receiving positive advantage
estimates-is occasionally sampled, its probability increases, while those of
the originally high-reward tokens decrease. This dynamic suggests that
over-exploitation during the exploitation stage may lead to capability boundary
shrinkage, whereas prolonged training into the exploration stage can promote an
expansion of the reasoning capability boundary. Building upon our insights, we
revisit the potential of only using relative negative gradients for prolonging
training, providing a theoretical and empirical foundation for the development
of more advanced reasoning capabilities.

</details>


### [455] [Adaptive kernel-density approach for imbalanced binary classification](https://arxiv.org/abs/2510.04046)
*Kotaro J. Nishimura,Yuichi Sakumura,Kazushi Ikeda*

Main category: cs.LG

TL;DR: 提出KOTARO方法解决类别不平衡问题，通过自适应调整决策边界来改善少数类识别性能


<details>
  <summary>Details</summary>
Motivation: 现实世界二分类任务中类别不平衡问题普遍存在，导致预测偏向多数类，在医疗诊断和异常检测等关键领域影响严重，传统方法在极端不平衡情况下效果不佳

Method: 基于核密度估计框架，通过动态调整高斯基函数带宽来适应局部样本密度，优化决策边界以更好地捕捉少数类区域

Result: 在合成和真实不平衡数据集上的实验表明，KOTARO优于传统方法，特别是在严重不平衡条件下表现突出

Conclusion: KOTARO是解决各类不平衡分类问题的有前景方案

Abstract: Class imbalance is a common challenge in real-world binary classification
tasks, often leading to predictions biased toward the majority class and
reduced recognition of the minority class. This issue is particularly critical
in domains such as medical diagnosis and anomaly detection, where correct
classification of minority classes is essential. Conventional methods often
fail to deliver satisfactory performance when the imbalance ratio is extremely
severe. To address this challenge, we propose a novel approach called
Kernel-density-Oriented Threshold Adjustment with Regional Optimization
(KOTARO), which extends the framework of kernel density estimation (KDE) by
adaptively adjusting decision boundaries according to local sample density. In
KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on
the estimated density around each sample, thereby enhancing the classifier's
ability to capture minority regions. We validated the effectiveness of KOTARO
through experiments on both synthetic and real-world imbalanced datasets. The
results demonstrated that KOTARO outperformed conventional methods,
particularly under conditions of severe imbalance, highlighting its potential
as a promising solution for a wide range of imbalanced classification problems

</details>


### [456] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: 提出了一种名为变分扩散遗忘(VDU)的机器学习遗忘方法，能够在数据受限的环境中防止预训练扩散模型生成包含不良特征的输出，仅需访问包含不良特征的训练数据子集。


<details>
  <summary>Details</summary>
Motivation: 为了负责任地部署扩散模型，需要调节其输出以防止生成不良、暴力和淫秽内容。现有方法在数据受限（无法访问完整训练数据集）的情况下效果不佳。

Method: 基于变分推理框架，通过最小化包含两个项的损失函数：塑性诱导器（减少不良训练数据点的对数似然）和稳定性正则器（在参数空间中对模型进行正则化以防止图像生成质量损失）。

Result: 通过综合实验验证了方法在类别遗忘和特征遗忘任务中的有效性，包括从预训练DDPM模型中遗忘MNIST、CIFAR-10和tinyImageNet数据集中的特定类别，以及从预训练Stable Diffusion模型中遗忘某些高级特征。

Conclusion: VDU是一种计算高效的方法，能够在数据受限的环境中有效防止扩散模型生成不良内容，同时保持图像生成质量。

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [457] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: 本文介绍了离线强化学习在大状态空间中的理论，探讨了函数逼近和数据覆盖的关键概念，描述了不同假设下的算法和结果，并讨论了开放性问题。


<details>
  <summary>Details</summary>
Motivation: 研究离线强化学习在大状态空间中的理论基础，解决从历史数据中学习策略而无需与环境在线交互的问题。

Method: 引入函数逼近的表达性假设（如Bellman完备性与可实现性）和数据覆盖假设（如全策略覆盖与单策略覆盖），分析不同假设下的算法设计。

Result: 描述了丰富的算法和结果图谱，展示了不同假设对样本复杂度和计算复杂度保证的影响。

Conclusion: 离线强化学习在大状态空间中具有复杂的理论框架，不同假设导致不同的算法性能，仍有许多开放问题需要进一步研究。

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


### [458] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 提出了一种独立于类别数量的神经网络训练方法，使用预定义的向量系统作为目标潜在空间配置，可在类别数量极大或未知时训练相同架构的神经网络。


<details>
  <summary>Details</summary>
Motivation: 监督学习方法在神经网络分类任务中需要使参数数量依赖于类别数量，这在类别数量极大或未知时限制了应用。

Method: 使用预定义的向量系统作为目标潜在空间配置，选择An根系统的随机扰动向量进行实验，通过将神经网络预测与预定义向量匹配来训练编码器和视觉变换器。

Result: 在Cinic-10和ImageNet-1K数据集上成功训练了编码器和视觉变换器，并在128万个类别的数据集上验证了方法的适用性。

Conclusion: 该方法可应用于类别数量极大的数据集训练，并在持续学习和神经网络蒸馏中具有潜在应用价值。

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [459] [Rethinking Consistent Multi-Label Classification under Inexact Supervision](https://arxiv.org/abs/2510.04091)
*Wei Wang,Tianhao Ma,Ming-Kun Xie,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了两种无偏风险估计器来处理部分多标签学习和互补多标签学习问题，无需依赖标签生成过程的准确估计或均匀分布假设。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要准确估计候选标签或互补标签的生成过程，或假设均匀分布来消除估计问题，但这些条件在现实场景中通常难以满足。

Method: 基于一阶和二阶策略提出了两种无偏风险估计器，理论上证明了与多标签分类评估指标的一致性，并推导了风险估计器估计误差的收敛速率。

Result: 大量实验结果验证了所提方法相对于最先进方法的有效性。

Conclusion: 提出的方法能够以统一的方式处理部分多标签学习和互补多标签学习问题，无需依赖难以满足的条件。

Abstract: Partial multi-label learning and complementary multi-label learning are two
popular weakly supervised multi-label classification paradigms that aim to
alleviate the high annotation costs of collecting precisely annotated
multi-label data. In partial multi-label learning, each instance is annotated
with a candidate label set, among which only some labels are relevant; in
complementary multi-label learning, each instance is annotated with
complementary labels indicating the classes to which the instance does not
belong. Existing consistent approaches for the two paradigms either require
accurate estimation of the generation process of candidate or complementary
labels or assume a uniform distribution to eliminate the estimation problem.
However, both conditions are usually difficult to satisfy in real-world
scenarios. In this paper, we propose consistent approaches that do not rely on
the aforementioned conditions to handle both problems in a unified way.
Specifically, we propose two unbiased risk estimators based on first- and
second-order strategies. Theoretically, we prove consistency w.r.t. two widely
used multi-label classification evaluation metrics and derive convergence rates
for the estimation errors of the proposed risk estimators. Empirically,
extensive experimental results validate the effectiveness of our proposed
approaches against state-of-the-art methods.

</details>


### [460] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: 本文分析了深度学习模型在时间序列外推预测中的性能下降问题，提出了一个理论框架来解释这一现象，并为设计具有更好外推能力的新一代预测模型提供了方向。


<details>
  <summary>Details</summary>
Motivation: 受基础模型在语言建模中成功的启发，研究者希望开发时间序列预测的基础模型。然而，当前模型在长程外推预测中表现不佳，甚至不如简单基线方法，这与物理定律的强大外推能力形成鲜明对比，促使研究者探索神经网络与物理定律结构之间的根本差异。

Method: 识别并形式化了一个基本属性，用于表征统计学习模型在训练域外进行准确预测的能力。通过理论分析和实证研究，展示了这一属性对当前深度学习架构的影响。

Result: 研究结果阐明了外推差距的根本原因，揭示了深度学习模型在外推设置中性能恶化的机制。

Conclusion: 该工作不仅解释了外推差距的根源，还为设计能够掌握外推能力的下一代预测模型提供了指导方向。

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [461] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: 该论文提出了一种基于贝叶斯统计的LLM不确定性量化方法，通过训练多个贝叶斯线性模型来预测层间输出，从而改进多选择结构生成任务中的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多选择结构生成中的不确定性量化仍由简单的最大softmax分数主导，缺乏更可靠的方法来支持自动化决策等应用。

Method: 训练多个贝叶斯线性模型，每个模型预测LLM某一层的输出基于前一层的输出，通过层级后验分布推断全局不确定性，并识别稀疏分布特征组合。

Result: 在不同LLM上的数值实验显示，该方法相比现有最优基线方法取得了持续改进。

Conclusion: 采用贝叶斯统计的严谨方法即使使用最简单的线性回归模型，也能显著提升LLM不确定性量化性能。

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [462] [Wasserstein projection distance for fairness testing of regression models](https://arxiv.org/abs/2510.04114)
*Wanxin Li,Yongjin P. Park,Khanh Dao Duc*

Main category: cs.LG

TL;DR: 提出基于Wasserstein投影的回归模型公平性测试框架，采用假设检验和最优数据扰动方法，在保持准确性的同时提升公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习公平性研究主要集中在分类任务，回归模型的公平性研究相对不足，需要专门针对回归模型的公平性测试方法。

Method: 基于Wasserstein投影的假设检验框架，包括公平性标准分类、对偶重构测试统计量、渐进边界和极限分布推导，以及最优数据扰动方法。

Result: 在合成和真实数据集上的实验表明，该方法比基于置换的测试具有更高特异性，能有效检测和缓解学生成绩预测、房价预测等实际应用中的偏差。

Conclusion: 该Wasserstein投影框架为回归模型提供了有效的公平性测试和缓解方法，填补了回归模型公平性研究的空白。

Abstract: Fairness in machine learning is a critical concern, yet most research has
focused on classification tasks, leaving regression models underexplored. This
paper introduces a Wasserstein projection-based framework for fairness testing
in regression models, focusing on expectation-based criteria. We propose a
hypothesis-testing approach and an optimal data perturbation method to improve
fairness while balancing accuracy. Theoretical results include a detailed
categorization of fairness criteria for regression, a dual reformulation of the
Wasserstein projection test statistic, and the derivation of asymptotic bounds
and limiting distributions. Experiments on synthetic and real-world datasets
demonstrate that the proposed method offers higher specificity compared to
permutation-based tests, and effectively detects and mitigates biases in real
applications such as student performance and housing price prediction.

</details>


### [463] [On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach](https://arxiv.org/abs/2510.04115)
*George Giapitzakis,Kimon Fountoulakis,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 本文首次在半自动机领域建立了统计查询困难性结果，证明了在输入词和初始状态均匀分布下，当字母表大小和输入长度都是状态数的多项式时，半自动机存在统计查询困难性。


<details>
  <summary>Details</summary>
Motivation: 半自动机在自然语言处理、机器人学、计算生物学和数据挖掘等领域有广泛应用，但之前缺乏对其统计查询复杂度的理论分析。本文旨在填补这一空白，研究半自动机的学习困难性。

Method: 通过将区分两个半自动机最终状态的任务转化为研究在对称群S_N×S_N上的随机游走行为，应用傅里叶分析和对称群表示论工具，获得紧致的谱间隙界限。

Result: 证明了当步数是状态数的多项式时，不同的半自动机几乎不相关，从而得到统计查询困难性结果。与确定性有限自动机不同，这种困难性源于半自动机内部状态转移结构本身。

Conclusion: 半自动机的统计查询困难性不仅源于其识别的语言复杂性，更根本地源于其状态转移结构的固有特性，这为理解半自动机的学习复杂度提供了新的理论视角。

Abstract: Semiautomata form a rich class of sequence-processing algorithms with
applications in natural language processing, robotics, computational biology,
and data mining. We establish the first Statistical Query hardness result for
semiautomata under the uniform distribution over input words and initial
states. We show that Statistical Query hardness can be established when both
the alphabet size and input length are polynomial in the number of states.
Unlike the case of deterministic finite automata, where hardness typically
arises through the hardness of the language they recognize (e.g., parity), our
result is derived solely from the internal state-transition structure of
semiautomata. Our analysis reduces the task of distinguishing the final states
of two semiautomata to studying the behavior of a random walk on the group
$S_{N} \times S_{N}$. By applying tools from Fourier analysis and the
representation theory of the symmetric group, we obtain tight spectral gap
bounds, demonstrating that after a polynomial number of steps in the number of
states, distinct semiautomata become nearly uncorrelated, yielding the desired
hardness result.

</details>


### [464] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: DoRAN是DoRA的改进版本，通过噪声注入和动态低秩矩阵生成来提升训练稳定性和样本效率，在视觉和语言基准测试中优于LoRA、DoRA等PEFT方法。


<details>
  <summary>Details</summary>
Motivation: DoRA虽然比LoRA有更好的学习能力和训练稳定性，但仍存在训练不稳定和样本效率问题，需要进一步改进。

Method: 1) 在DoRA权重分解的分母中注入噪声作为自适应正则化器；2) 用辅助网络动态生成低秩矩阵，实现跨层参数耦合。

Result: 在视觉和语言基准测试中，DoRAN始终优于LoRA、DoRA和其他PEFT基线方法。

Conclusion: 结合噪声正则化和网络参数生成的方法为基座模型的稳健高效微调提供了有前景的方向。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [465] [Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions](https://arxiv.org/abs/2510.04126)
*Ziying Zhang,Yaqing Wang,Yuxuan Sun,Min Ye,Quanming Yao*

Main category: cs.LG

TL;DR: ColdDTI是一个用于冷启动药物-靶点相互作用预测的框架，通过关注蛋白质多级结构（从一级到四级）来改进预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只使用蛋白质的一级结构，但蛋白质具有多级结构且都影响药物-靶点相互作用，这限制了现有方法捕捉涉及高级结构相互作用的能力。

Method: 采用分层注意力机制挖掘多级蛋白质结构（从一级到四级）与药物结构在局部和全局粒度上的相互作用，然后利用挖掘的相互作用融合不同层次的结构表示进行最终预测。

Result: 在基准数据集上的实验表明，ColdDTI在冷启动设置下持续优于先前的方法。

Conclusion: 该方法捕获了生物学上可转移的先验知识，避免了过度依赖表示学习带来的过拟合风险。

Abstract: Cold-start drug-target interaction (DTI) prediction focuses on interaction
between novel drugs and proteins. Previous methods typically learn transferable
interaction patterns between structures of drug and proteins to tackle it.
However, insight from proteomics suggest that protein have multi-level
structures and they all influence the DTI. Existing works usually represent
protein with only primary structures, limiting their ability to capture
interactions involving higher-level structures. Inspired by this insight, we
propose ColdDTI, a framework attending on protein multi-level structure for
cold-start DTI prediction. We employ hierarchical attention mechanism to mine
interaction between multi-level protein structures (from primary to quaternary)
and drug structures at both local and global granularities. Then, we leverage
mined interactions to fuse structure representations of different levels for
final prediction. Our design captures biologically transferable priors,
avoiding the risk of overfitting caused by excessive reliance on representation
learning. Experiments on benchmark datasets demonstrate that ColdDTI
consistently outperforms previous methods in cold-start settings.

</details>


### [466] [On the Limitations and Capabilities of Position Embeddings for Length Generalization](https://arxiv.org/abs/2510.04130)
*Yang Chen,Yitao Liang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文研究了Transformer中位置嵌入在长度泛化中的作用，提出了线性表示复杂度和序列表示复杂度的概念，并开发了Scale Hint和学习型位置嵌入框架来提升长度泛化性能。


<details>
  <summary>Details</summary>
Motivation: 位置嵌入对Transformer的长度泛化性能有重要影响，但其根本作用机制尚不清楚，需要深入研究位置嵌入在实现长度泛化方面的局限性和能力。

Method: 理论分析位置嵌入在位置专用线性注意力中的作用，引入线性表示复杂度概念；扩展到实际Transformer中提出序列表示复杂度假设；开发Scale Hint和学习型位置嵌入框架。

Result: 分析表明位置嵌入不扩展计算能力，而是结构化跨位置的学习计算；经验证据支持序列表示复杂度不变性假设；提出的方法在各种推理任务中有效提升长度泛化。

Conclusion: 本文为改进Transformer中的长度泛化提供了理论洞见和实用策略，阐明了位置嵌入在长度泛化中的基本作用机制。

Abstract: In Transformers, Position Embeddings (PEs) significantly influence Length
Generalization (LG) performance, yet their fundamental role remains unclear. In
this work, we investigate the limitations and capabilities of PEs in achieving
LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),
introducing Linear Representation Complexity (LRC) to characterize when PEs
enable LG. Our analysis shows that PEs do not expand computational capabilities
but structure learned computations across positions. Extending to practical
Transformers, we propose Sequential Representation Complexity (SRC) and
conjecture that LG is possible if and only if SRC remains invariant across
scales. We support this hypothesis with empirical evidence in various reasoning
tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance
scaling, and a Learning-Based Position Embedding framework that automatically
learns positional relations. Our work provides theoretical insights and
practical strategies for improving LG in Transformers.

</details>


### [467] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: 该论文提出使用条件归一化流(Full-Glow)模型，从2D城市布局实时生成符合标准的城市声压地图，相比传统物理求解器加速2000倍以上，并在非视距区域精度提升24%。


<details>
  <summary>Details</summary>
Motivation: 城市噪声预测对公共健康和监管工作流程至关重要，但基于物理的求解器速度太慢，无法满足时间紧迫的迭代"假设分析"研究需求。

Method: 采用条件归一化流模型，在单个RTX 4090上实时生成256x256的城市声压地图，支持源或几何变化的即时重新计算。

Result: 在基线、衍射和反射机制的数据集上，模型生成速度比参考求解器快2000倍以上，非视距区域精度比先前深度模型提高24%，基线非视距区域达到0.65 dB MAE，并保持高结构保真度。

Conclusion: 该模型能够重现衍射和干涉模式，支持源或几何变化的即时重新计算，成为城市规划、合规制图和运营的实用引擎。

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [468] [Modeling Time Series Dynamics with Fourier Ordinary Differential Equations](https://arxiv.org/abs/2510.04133)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出傅里叶常微分方程(FODEs)，通过在傅里叶域建模时间序列动态，解决了神经ODE在捕获长期依赖和周期性结构方面的局限性，并通过可学习滤波机制提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 神经ODE在建模时间序列时面临两个主要挑战：1）时间域表示难以捕获长期依赖和周期性结构；2）连续时间模型与离散观测数据之间的不匹配导致粒度损失和预测精度下降。

Method: 使用快速傅里叶变换将时间序列数据转换到频域，在傅里叶域中嵌入动态建模，并引入可学习的逐元素滤波机制来对齐连续模型输出与离散观测。

Result: 在多个时间序列数据集上的实验表明，FODEs在准确性和效率方面均优于现有方法，能够有效捕获长期和短期模式。

Conclusion: FODEs通过频域建模提供了一个强大的时间序列动态建模框架，能够同时捕获长期依赖和周期性结构，同时保持数据粒度。

Abstract: Neural ODEs (NODEs) have emerged as powerful tools for modeling time series
data, offering the flexibility to adapt to varying input scales and capture
complex dynamics. However, they face significant challenges: first, their
reliance on time-domain representations often limits their ability to capture
long-term dependencies and periodic structures; second, the inherent mismatch
between their continuous-time formulation and the discrete nature of real-world
data can lead to loss of granularity and predictive accuracy. To address these
limitations, we propose Fourier Ordinary Differential Equations (FODEs), an
approach that embeds the dynamics in the Fourier domain. By transforming
time-series data into the frequency domain using the Fast Fourier Transform
(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive
in the time domain. Additionally, we introduce a learnable element-wise
filtering mechanism that aligns continuous model outputs with discrete
observations, preserving granularity and enhancing accuracy. Experiments on
various time series datasets demonstrate that FODEs outperform existing methods
in terms of both accuracy and efficiency. By effectively capturing both long-
and short-term patterns, FODEs provide a robust framework for modeling time
series dynamics.

</details>


### [469] [PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting](https://arxiv.org/abs/2510.04134)
*Yiming Niu,Jinliang Deng,Yongxin Tong*

Main category: cs.LG

TL;DR: 本文提出了PhaseFormer，一种基于相位视角的高效时间序列预测方法，通过紧凑的相位嵌入和轻量级路由机制实现高效跨相位交互，仅需约1k参数即可达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于patch处理的深度学习方法虽然增强了周期性的利用，但存在参数过多、计算成本高的问题，效率成为瓶颈。本文首次明确解释了patch级处理效率低下的原因。

Method: 引入相位视角建模周期性，采用紧凑的相位嵌入进行相位预测，通过轻量级路由机制实现高效的跨相位交互。

Result: 在基准数据集上，PhaseFormer仅用约1k参数就达到了最先进的性能表现，特别是在大规模复杂数据集上表现优异，而同等效率的模型往往难以胜任。

Conclusion: 这项工作标志着向真正高效有效的时间序列预测迈出了重要一步，为处理大规模时间序列数据提供了可行的解决方案。

Abstract: Periodicity is a fundamental characteristic of time series data and has long
played a central role in forecasting. Recent deep learning methods strengthen
the exploitation of periodicity by treating patches as basic tokens, thereby
improving predictive effectiveness. However, their efficiency remains a
bottleneck due to large parameter counts and heavy computational costs. This
paper provides, for the first time, a clear explanation of why patch-level
processing is inherently inefficient, supported by strong evidence from
real-world data. To address these limitations, we introduce a phase perspective
for modeling periodicity and present an efficient yet effective solution,
PhaseFormer. PhaseFormer features phase-wise prediction through compact phase
embeddings and efficient cross-phase interaction enabled by a lightweight
routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves
state-of-the-art performance with around 1k parameters, consistently across
benchmark datasets. Notably, it excels on large-scale and complex datasets,
where models with comparable efficiency often struggle. This work marks a
significant step toward truly efficient and effective time series forecasting.
Code is available at this repository:
https://github.com/neumyor/PhaseFormer_TSL

</details>


### [470] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 提出RegCache训练免费算法，通过在视觉编码器中添加异常值倾向但语义无意义的前缀token来缓解激活异常值，实现更精确的量化。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的视觉编码器在实时处理大规模视觉数据时面临推理成本高的问题，后训练量化是实用路径，但由于大规模激活异常值，即使在8位精度下仍具挑战性。

Method: RegCache算法引入异常值倾向但语义无意义的前缀token到目标视觉编码器，防止其他token产生异常值。针对视觉编码器异常值与语言模型不同的特性，提出中间层前缀和token删除两项技术创新。

Result: 实验表明该方法在文本监督和自监督视觉编码器中都能一致提高量化模型的准确性。

Conclusion: RegCache是一种有效的训练免费方法，能够显著缓解视觉编码器中的激活异常值问题，为后训练量化提供了可行的解决方案。

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [471] [Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets](https://arxiv.org/abs/2510.04138)
*Muhao Guo,Haoran Li,Yang Weng*

Main category: cs.LG

TL;DR: 该论文提出了一种新方法，通过探索底层流形来约束神经常微分方程(NODE)过程，解决了高维系统中动态估计计算量大和截断误差高的问题。


<details>
  <summary>Details</summary>
Motivation: 神经常微分方程(NODE)在处理高维系统时面临计算量大和截断误差高的问题，而现有方法通常需要已知流形知识进行投影或隐式变换，这在现实场景中通常不可用。

Method: 使用结构保持编码器处理数据并找到底层图来近似流形，提出将NODE学习与流形相结合的新方法。

Result: 在多个数据集上的实验评估显示，该方法在准确性、函数评估次数(NFEs)和收敛速度方面优于现有基线方法。

Conclusion: 该方法在解决高维数据集挑战方面表现出卓越性能，证明了其有效性。

Abstract: Neural ordinary differential equations (NODE) have garnered significant
attention for their design of continuous-depth neural networks and the ability
to learn data/feature dynamics. However, for high-dimensional systems,
estimating dynamics requires extensive calculations and suffers from high
truncation errors for the ODE solvers. To address the issue, one intuitive
approach is to consider the non-trivial topological space of the data
distribution, i.e., a low-dimensional manifold. Existing methods often rely on
knowledge of the manifold for projection or implicit transformation,
restricting the ODE solutions on the manifold. Nevertheless, such knowledge is
usually unknown in realistic scenarios. Therefore, we propose a novel approach
to explore the underlying manifold to restrict the ODE process. Specifically,
we employ a structure-preserved encoder to process data and find the underlying
graph to approximate the manifold. Moreover, we propose novel methods to
combine the NODE learning with the manifold, resulting in significant gains in
computational speed and accuracy. Our experimental evaluations encompass
multiple datasets, where we compare the accuracy, number of function
evaluations (NFEs), and convergence speed of our model against existing
baselines. Our results demonstrate superior performance, underscoring the
effectiveness of our approach in addressing the challenges of high-dimensional
datasets.

</details>


### [472] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出了一种名为SONA的新型判别器设计，通过分离自然性和对齐性评估，结合自适应权重机制，在条件生成任务中实现了更好的样本质量和条件对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的条件生成对抗网络在平衡样本真实性和条件对齐方面存在困难，需要改进判别器设计来解决这一挑战。

Method: 提出了SONA判别器，包含三个关键能力：无条件判别、匹配感知监督和自适应权重。使用分离的自然性和对齐性投影，结合专门的损失函数和自适应权重机制。

Result: 在类条件生成任务中，SONA在样本质量和条件对齐方面优于现有最先进方法，在文本到图像生成任务中也表现出有效性。

Conclusion: SONA方法具有多功能性和鲁棒性，为条件生成任务提供了一种有效的解决方案。

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [473] [Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity](https://arxiv.org/abs/2510.04189)
*Prashansa Panda,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文提出了首个用于长期平均成本和不等式约束的自然critic-actor算法，建立了非渐近收敛保证，并提出了改进样本复杂度的修改方案。


<details>
  <summary>Details</summary>
Motivation: 现有critic-actor算法主要集中在折扣成本设置，且缺乏对长期平均成本和约束情况下的非渐近收敛分析。

Method: 提出具有函数逼近的自然critic-actor算法，适用于长期平均成本和不等式约束设置，并提供非渐近收敛分析。

Result: 建立了最优学习率，提出了改进样本复杂度的修改方案，在三个Safety-Gym环境中的实验表明该算法与其他知名算法相比具有竞争力。

Conclusion: 成功开发了首个适用于约束长期平均成本设置的critic-actor算法，并提供了完整的非渐近收敛理论保证。

Abstract: Recent studies have increasingly focused on non-asymptotic convergence
analyses for actor-critic (AC) algorithms. One such effort introduced a
two-timescale critic-actor algorithm for the discounted cost setting using a
tabular representation, where the usual roles of the actor and critic are
reversed. However, only asymptotic convergence was established there.
Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor
algorithm with linear function approximation were conducted. In our work, we
introduce the first natural critic-actor algorithm with function approximation
for the long-run average cost setting and under inequality constraints. We
provide the non-asymptotic convergence guarantees for this algorithm. Our
analysis establishes optimal learning rates and we also propose a modification
to enhance sample complexity. We further show the results of experiments on
three different Safety-Gym environments where our algorithm is found to be
competitive in comparison with other well known algorithms.

</details>


### [474] [Spectral Alignment as Predictor of Loss Explosion in Neural Network Training](https://arxiv.org/abs/2510.04202)
*Haiquan Qiu,You Wu,Yingjie Tan,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: 提出了谱对齐（SA）指标，通过监测层输入与权重矩阵主奇异向量之间的分布对齐，能够比传统标量指标更早、更清晰地预测深度神经网络训练中的损失爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中的损失爆炸会浪费数百万美元的训练成本，而传统的权重和梯度范数等监控指标存在滞后性和模糊性，难以建立统一的失败检测标准。

Method: 引入谱对齐（SA）指标，监测层输入与权重矩阵主奇异向量之间的分布对齐，通过分析这种对齐的符号多样性崩溃来预测表征崩溃和训练发散。

Result: 在语言模型上的实证结果表明，监测SA分布比传统标量指标能显著更早、更清晰地预警损失爆炸，且计算开销低。

Conclusion: SA是一个实用的工具，能够有效保护模型训练，防止损失爆炸导致的训练失败。

Abstract: Loss explosions in training deep neural networks can nullify multi-million
dollar training runs. Conventional monitoring metrics like weight and gradient
norms are often lagging and ambiguous predictors, as their values vary
dramatically across different models and even between layers of the same model,
making it difficult to establish a unified standard for detecting impending
failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded
metric that monitors the distributional alignment between layer inputs and the
principal singular vectors of weight matrices. We show that a collapse in the
sign diversity of this alignment is a powerful early predictor of
representational collapse and training divergence. Empirical results on
language models demonstrate that monitoring the SA distribution provides a
significantly earlier and clearer warning of loss explosions than traditional
scalar metrics. SA's low computational overhead makes it a practical tool for
safeguarding model training.

</details>


### [475] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出了一种端到端的自适应联邦学习方法，通过将联邦学习建模为动态系统，自适应选择客户端和中央服务器的学习率和动量参数，无需手动调参即可实现快速稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 解决异构联邦学习中超参数选择困难的问题，传统方法需要手动调参且计算成本高，特别是在客户端计算能力不同、数据分布非独立同分布的情况下。

Method: 将联邦学习建模为动态系统，利用数值模拟和物理设计原理，通过临界阻尼选择动量参数，自适应选择客户端和中央服务器的学习率，所有参数由一个全局超参数控制。

Result: 相比现有自适应方法，在异构联邦学习中实现了更优的收敛性能，对全局超参数选择不敏感，能够处理目标不一致性和客户端漂移等关键挑战。

Conclusion: 该方法为异构联邦学习提供了一个完全集成的自适应解决方案，无需客户端和服务器更新的超参数调优，适合快速原型设计和规模化部署。

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [476] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: PolyKAN提出了一种新的KAN压缩理论框架，通过多面体区域合并实现参数效率提升，在保证近似误差的前提下提供模型大小缩减的形式化保证。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KANs) 虽然具有更好的可解释性和数学基础，但其参数效率问题限制了实际部署。

Method: 利用KAN固有的分段多项式结构，将压缩问题建模为最优多面体区域合并问题，开发了ε等价压缩理论，并设计了最优动态规划算法。

Result: 理论分析表明PolyKAN在严格误差控制下实现可证明的最小压缩，且在所有网络参数上具有多项式时间复杂度。

Conclusion: 该框架为KAN压缩提供了首个具有数学保证的形式化基础，为可解释神经架构的高效部署开辟了新方向。

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [477] [Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention](https://arxiv.org/abs/2510.04212)
*Haiquan Qiu,Quanming Yao*

Main category: cs.LG

TL;DR: 本文揭示了低精度训练中Flash Attention导致灾难性损失爆炸的机制原因，并提出简单修改方案来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 追求计算效率促使采用低精度格式训练Transformer模型，但训练不稳定性阻碍了进展，特别是Flash Attention在低精度设置下会导致灾难性损失爆炸的问题长期未解决。

Method: 深入分析发现失败是由两个相互关联的现象引起：注意力机制中相似低秩表示的出现，以及低精度算术中固有偏置舍入误差的复合效应。通过最小化修改Flash Attention来减轻舍入误差偏置。

Result: 证明了这些因素如何创建误差累积的恶性循环，最终破坏训练动态。提出的简单修改稳定了训练过程，验证了分析并提供了实用解决方案。

Conclusion: 首次提供了Flash Attention在低精度训练中失败案例的机制解释，揭示了误差累积的根本原因，并通过简单修改成功解决了这一长期存在的问题。

Abstract: The pursuit of computational efficiency has driven the adoption of
low-precision formats for training transformer models. However, this progress
is often hindered by notorious training instabilities. This paper provides the
first mechanistic explanation for a long-standing and unresolved failure case
where training with flash attention in low-precision settings leads to
catastrophic loss explosions. Our in-depth analysis reveals that the failure is
not a random artifact but caused by two intertwined phenomena: the emergence of
similar low-rank representations within the attention mechanism and the
compounding effect of biased rounding errors inherent in low-precision
arithmetic. We demonstrate how these factors create a vicious cycle of error
accumulation that corrupts weight updates, ultimately derailing the training
dynamics. To validate our findings, we introduce a minimal modification to the
flash attention that mitigates the bias in rounding errors. This simple change
stabilizes the training process, confirming our analysis and offering a
practical solution to this persistent problem.

</details>


### [478] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: 提出了MLLMEraser，一种无需训练的输入感知遗忘框架，通过激活引导实现多模态大语言模型的动态知识擦除，解决了传统遗忘方法计算成本高、不可逆和知识扭曲的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在部署中存在记忆隐私数据、过时知识和有害内容的问题，现有遗忘方法通常需要训练，计算成本高且不可逆。

Method: 通过对比对抗性扰动的知识回忆和知识擦除图像-文本对，构建多模态擦除方向，并设计输入感知的引导机制自适应决定何时应用擦除方向。

Result: 在LLaVA-1.5和Qwen-2.5-VL上的实验表明，MLLMEraser在遗忘性能上优于现有方法，计算成本更低且效用退化最小。

Conclusion: MLLMEraser提供了一种高效、无需训练的多模态大语言模型遗忘解决方案，在保持知识效用的同时实现指定内容的遗忘。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [479] [Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling](https://arxiv.org/abs/2510.04233)
*Kai Yang,Yuqi Huang,Junheng Tao,Wanyu Wang,Qitian Wu*

Main category: cs.LG

TL;DR: PAINET是一种SE(3)等变神经网络架构，用于学习多体系统中的全对相互作用，在3D动力学预测任务中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法通常依赖显式观察到的结构，无法捕捉未观察到的相互作用，而这些相互作用对复杂物理行为和动力学机制至关重要

Method: 提出PAINET架构，包含：(1)基于能量函数最小化轨迹的物理启发注意力网络；(2)保持等变性同时实现高效推理的并行解码器

Result: 在人体运动捕捉、分子动力学和大规模蛋白质模拟等真实世界基准测试中，PAINET始终优于最新模型，在3D动力学预测中实现4.7%到41.5%的误差降低，计算成本相当

Conclusion: PAINET通过捕捉全对相互作用，为多体系统中的3D动力学建模提供了有效的解决方案

Abstract: Modeling 3D dynamics is a fundamental problem in multi-body systems across
scientific and engineering domains and has important practical implications in
trajectory prediction and simulation. While recent GNN-based approaches have
achieved strong performance by enforcing geometric symmetries, encoding
high-order features or incorporating neural-ODE mechanics, they typically
depend on explicitly observed structures and inherently fail to capture the
unobserved interactions that are crucial to complex physical behaviors and
dynamics mechanism. In this paper, we propose PAINET, a principled
SE(3)-equivariant neural architecture for learning all-pair interactions in
multi-body systems. The model comprises: (1) a novel physics-inspired attention
network derived from the minimization trajectory of an energy function, and (2)
a parallel decoder that preserves equivariance while enabling efficient
inference. Empirical results on diverse real-world benchmarks, including human
motion capture, molecular dynamics, and large-scale protein simulations, show
that PAINET consistently outperforms recently proposed models, yielding 4.7% to
41.5% error reductions in 3D dynamics prediction with comparable computation
costs in terms of time and memory.

</details>


### [480] [Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions](https://arxiv.org/abs/2510.04237)
*Jinhui Bai,Andreas Christmann,Lei Shi*

Main category: cs.LG

TL;DR: 提出了一种新颖的核随机梯度下降算法，通过创新的正则化策略提高大规模监督学习的效率和可扩展性，在保持最优收敛率的同时显著降低计算和存储复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统核SGD在处理大规模数据时存在效率和可扩展性问题，需要开发更高效的核方法来解决计算复杂度和存储需求高的挑战。

Method: 利用球面径向基函数的无穷级数展开，将随机梯度投影到有限维假设空间，并基于核诱导协方差算子的谱结构估计建立统一的分析框架，结合线性SGD的坐标更新来降低复杂度。

Result: 证明了最后迭代和后缀平均都以极小极大最优速率收敛，在再生核希尔伯特空间中建立了最优强收敛，算法显著降低了计算复杂度和存储复杂度。

Conclusion: 所提出的算法在保持理论最优性的同时，实现了计算和存储效率的大幅提升，适用于多种经典损失函数，能够高效处理流式数据。

Abstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.

</details>


### [481] [Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs](https://arxiv.org/abs/2510.04241)
*Seong Jin Ahn,Myoung-Ho Kim*

Main category: cs.LG

TL;DR: 提出DAD-SGM方法，通过扩散模型作为教师助手，将自监督图神经网络的知识蒸馏到轻量级MLP中，解决容量差距问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模应用中，用轻量级MLP替代GNN的需求增长，但自监督图表示学习中GNN到MLP的蒸馏更具挑战性，因为自监督学习性能更依赖于模型的归纳偏置。

Method: 使用去噪扩散模型作为教师助手，帮助将教师GNN的知识更好地蒸馏到学生MLP中，增强MLP在自监督图表示学习中的泛化性和鲁棒性。

Result: 大量实验表明，DAD-SGM相比最先进的GNN到MLP蒸馏方法，能更有效地蒸馏自监督GNN的知识。

Conclusion: 该方法成功解决了自监督图表示学习中GNN与MLP之间的巨大容量差距问题，为大规模应用提供了高效的轻量级解决方案。

Abstract: For large-scale applications, there is growing interest in replacing Graph
Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via
knowledge distillation. However, distilling GNNs for self-supervised graph
representation learning into MLPs is more challenging. This is because the
performance of self-supervised learning is more related to the model's
inductive bias than supervised learning. This motivates us to design a new
distillation method to bridge a huge capacity gap between GNNs and MLPs in
self-supervised graph representation learning. In this paper, we propose
\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for
\textbf{S}elf-supervised \textbf{G}raph representation learning with
\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion
model as a teacher assistant to better distill the knowledge from the teacher
GNN into the student MLP. This approach enhances the generalizability and
robustness of MLPs in self-supervised graph representation learning. Extensive
experiments demonstrate that DAD-SGM effectively distills the knowledge of
self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation
methods. Our implementation is available at
https://github.com/SeongJinAhn/DAD-SGM.

</details>


### [482] [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)
*Joseph Ramsey,Bryan Andrews*

Main category: cs.LG

TL;DR: 提出了基于分数引导和定向测试的因果发现方法家族，包括BOSS-FCI、GRaSP-FCI、FCIT和LV-Dumb，用于处理存在隐变量或选择偏差的观测数据因果结构学习问题。


<details>
  <summary>Details</summary>
Motivation: 传统FCI算法在处理隐变量或选择偏差时需要进行大量条件独立性测试，导致虚假独立性声明、边错误和不可靠的方向确定，需要更高效可靠的方法。

Method: 开发了四种方法：BOSS-FCI和GRaSP-FCI作为GFCI的变体；FCIT使用BOSS引导的定向测试替代穷举测试；LV-Dumb直接返回BOSS DAG的PAG作为实用启发式方法。

Result: 模拟和真实数据分析表明，BOSS-FCI和GRaSP-FCI提供可靠基线，FCIT提高了效率和可靠性，LV-Dumb在实践中表现出优越的准确性。

Conclusion: 分数引导和定向策略对于可扩展的隐变量因果发现具有重要价值。

Abstract: Learning causal structure from observational data is especially challenging
when latent variables or selection bias are present. The Fast Causal Inference
(FCI) algorithm addresses this setting but often performs exhaustive
conditional independence tests across many subsets, leading to spurious
independence claims, extra or missing edges, and unreliable orientations. We
present a family of score-guided mixed-strategy causal search algorithms that
build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,
straightforward variants of GFCI that substitute BOSS or GRaSP for FGES,
thereby retaining correctness while incurring different scalability tradeoffs.
Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method
that improves upon these variants by replacing exhaustive all-subsets testing
with targeted tests guided by BOSS, yielding well-formed PAGs with higher
precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also
known as BOSS-POD), which bypasses latent-variable-specific reasoning and
directly returns the PAG of the BOSS DAG. Although not strictly correct in the
FCI sense, it scales better and often achieves superior accuracy in practice.
Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI
provide sound baselines, FCIT improves both efficiency and reliability, and
LV-Dumb offers a practical heuristic with strong empirical performance.
Together, these method highlight the value of score-guided and targeted
strategies for scalable latent-variable causal discovery.

</details>


### [483] [Influence branching for learning to solve mixed-integer programs online](https://arxiv.org/abs/2510.04273)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 提出了一种新的在线学习求解混合整数规划的方法，使用影响分支和图表示变量选择策略，通过Thompson采样优化分支启发式，在SCIP基础上实现计算加速。


<details>
  <summary>Details</summary>
Motivation: 为第20届混合整数规划研讨会计算竞赛开发，旨在改进在线学习求解MIP的能力，特别是在分支定界算法的早期迭代中优化变量选择策略。

Method: 使用影响分支作为新的图导向变量选择策略，通过Thompson采样在线优化分支启发式，根据计算速度提升对MIP结构的最佳图表示进行排名。

Result: 达到了与最先进在线学习方法相当的结果，并且表明该方法在更一般的在线框架中泛化能力良好，能够处理约束矩阵、约束向量和目标系数的变化。

Conclusion: 所提出的影响分支方法在在线学习求解MIP方面表现优异，具有良好的泛化能力，适用于各种参数变化的场景。

Abstract: On the occasion of the 20th Mixed Integer Program Workshop's computational
competition, this work introduces a new approach for learning to solve MIPs
online. Influence branching, a new graph-oriented variable selection strategy,
is applied throughout the first iterations of the branch and bound algorithm.
This branching heuristic is optimized online with Thompson sampling, which
ranks the best graph representations of MIP's structure according to
computational speed up over SCIP. We achieve results comparable to state of the
art online learning methods. Moreover, our results indicate that our method
generalizes well to more general online frameworks, where variations in
constraint matrix, constraint vector and objective coefficients can all occur
and where more samples are available.

</details>


### [484] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: 提出了PO-MPC框架，将基于MPPI的强化学习方法统一为KL正则化的模型基强化学习方法，通过将规划器动作分布作为策略优化的先验来对齐学习策略与规划器行为。


<details>
  <summary>Details</summary>
Motivation: 解决模型基强化学习中探索效率低的问题，特别是在高维连续控制任务中。现有方法中学习策略与规划器分布不对齐，影响值估计准确性和长期性能。

Method: 引入PO-MPC框架，将规划器的动作分布作为策略优化的先验，通过KL正则化来平衡回报最大化和KL散度最小化，统一了现有的MPPI基强化学习方法。

Result: 实验表明扩展配置带来了显著的性能提升，推进了基于MPPI的强化学习技术发展。

Conclusion: PO-MPC框架统一了现有方法并发现了新的变体，通过策略与规划器的对齐实现了更好的性能，为MPPI基强化学习提供了更灵活的方法。

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [485] [HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks](https://arxiv.org/abs/2510.04295)
*Nghiem T. Diep,Dung Le,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 提出HoRA方法，通过共享超网络生成跨注意力头的低秩矩阵，解决了LoRA在多头自注意力中忽视头间协同的问题，在参数效率微调中表现更优。


<details>
  <summary>Details</summary>
Motivation: LoRA在多头自注意力微调中单独适配每个注意力头，忽视了不同头之间的潜在协同效应，限制了其性能。

Method: 使用联合超网络为所有注意力头生成低秩矩阵，通过共享生成器耦合头的适配过程，促进跨头信息共享。

Result: 理论分析显示HoRA比LoRA具有更好的样本效率；在多种语言和视觉基准测试中，HoRA以略微增加的参数数量超越了LoRA和其他PEFT方法。

Conclusion: HoRA通过跨头信息共享机制有效解决了LoRA的局限性，在参数效率微调中实现了更优的性能表现。

Abstract: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)
technique that adapts large pre-trained models by adding low-rank matrices to
their weight updates. However, in the context of fine-tuning multi-head
self-attention (MHA), LoRA has been employed to adapt each attention head
separately, thereby overlooking potential synergies across different heads. To
mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)
method, which utilizes joint hypernetworks to generate low-rank matrices across
attention heads. By coupling their adaptation through a shared generator, HoRA
encourages cross-head information sharing, and thus directly addresses the
aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens
of hierarchical mixture of experts, our theoretical findings reveal that the
latter achieves superior sample efficiency to the former. Furthermore, through
extensive experiments across diverse language and vision benchmarks, we
demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring
only a marginal increase in the number of trainable parameters.

</details>


### [486] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出PID Steering框架，将控制理论中的PID控制器应用于大语言模型的激活引导，通过比例、积分、微分三项协同作用实现更鲁棒的行为控制。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型行为控制方法主要基于经验，缺乏理论性能保证，需要建立控制理论基础来提升激活引导的可靠性和稳定性。

Method: 将流行的激活引导方法对应为比例控制器，然后提出完整的PID Steering框架：比例项对齐目标语义方向，积分项累积误差实现跨层持续修正，微分项抑制超调。

Result: 在多个LLM家族和基准测试上的广泛实验表明，PID Steering始终优于现有方法，实现了更鲁棒和可靠的行为控制。

Conclusion: PID Steering为激活引导提供了控制理论基础，连接了经典控制理论的稳定性保证，是轻量级、模块化且易于集成的方法。

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [487] [Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework](https://arxiv.org/abs/2510.04316)
*Sahar Koohfar*

Main category: cs.LG

TL;DR: 提出了一种混合CNN-RNN深度学习模型用于交通事故严重程度预测，在15,870条事故记录数据集上表现优于传统统计和机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 准确及时地预测交通事故严重程度对于减轻事故严重后果至关重要，智能交通系统需要有效的预测方法来提供适当的医疗援助和交通服务。

Method: 采用混合CNN-RNN深度学习模型，考虑了交通事故各特征之间的相互关联关系，并与逻辑回归、朴素贝叶斯、KNN、决策树以及单独的RNN和CNN模型进行性能比较。

Result: 在2015-2021年弗吉尼亚州I-64高速公路的15,870条事故记录数据集上，提出的CNN-RNN混合模型在所有基准模型中表现最佳。

Conclusion: 混合模型结合了RNN和CNN模型的优势，在预测过程中实现了更高的准确性，证明了该混合模型的有效性。

Abstract: Accurate and timely prediction of crash severity is crucial in mitigating the
severe consequences of traffic accidents. Accurate and timely prediction of
crash severity is crucial in mitigating the severe consequences of traffic
accidents. In order to provide appropriate levels of medical assistance and
transportation services, an intelligent transportation system relies on
effective prediction methods. Deep learning models have gained popularity in
this domain due to their capability to capture non-linear relationships among
variables. In this research, we have implemented a hybrid CNN-RNN deep learning
model for crash severity prediction and compared its performance against widely
used statistical and machine learning models such as logistic regression,
na\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and
individual deep learning models: RNN and CNN. This study employs a methodology
that considers the interconnected relationships between various features of
traffic accidents. The study was conducted using a dataset of 15,870 accident
records gathered over a period of seven years between 2015 and 2021 on Virginia
highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model
has outperformed all benchmark models in terms of predicting crash severity.
This result illustrates the effectiveness of the hybrid model as it combines
the advantages of both RNN and CNN models in order to achieve greater accuracy
in the prediction process.

</details>


### [488] [FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents](https://arxiv.org/abs/2510.04317)
*Yucong Dai,Lu Zhang,Feng Luo,Mashrur Chowdhury,Yongkai Wu*

Main category: cs.LG

TL;DR: FairAgent是一个基于LLM的自动化系统，旨在简化公平感知的机器学习模型开发过程，无需深度技术专业知识即可实现偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 训练公平无偏的机器学习模型对高风险应用至关重要，但当前方法需要深厚的公平性定义、指标、数据预处理等技术专业知识，使得公平感知模型开发对许多从业者来说难以企及。

Method: FairAgent通过自动分析数据集中的潜在偏见、处理数据预处理和特征工程，并根据用户需求实施适当的偏见缓解策略，消除了对深度技术专业知识的需求。

Result: 实验表明，FairAgent在显著减少开发时间和专业知识要求的同时，实现了显著的性能改进。

Conclusion: FairAgent使公平感知的机器学习对从业者更加易于使用，通过自动化复杂的技术流程降低了公平模型开发的门槛。

Abstract: Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

</details>


### [489] [FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](https://arxiv.org/abs/2510.04325)
*Kenechukwu Ogbuagu,Sepehr Maleki,Giuseppe Bruni,Senthil Krishnababu*

Main category: cs.LG

TL;DR: FoilDiff是一种基于扩散模型的空气动力学流场预测替代模型，通过混合骨干去噪网络结合CNN特征提取和Transformer全局注意力，在DDIM采样优化下显著提升了预测精度和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 计算流体动力学模型虽然有效但计算成本高昂，需要开发更快速的替代模型来预测空气动力学流场。扩散模型在复杂流场预测中显示出巨大潜力。

Method: 提出FoilDiff扩散模型，采用混合骨干去噪网络结合卷积特征提取和Transformer全局注意力机制，利用DDIM采样优化效率，通过雷诺数、攻角和翼型几何的编码表示实现广泛空气动力学条件的泛化。

Result: 与最先进模型相比，FoilDiff性能显著提升，平均预测误差在相同数据集上降低了85%，提供更准确的预测和更好的预测不确定性校准。

Conclusion: FoilDiff证明了扩散模型在空气动力学流场预测中的有效性，混合骨干设计和DDIM采样优化使其在精度和效率方面都优于现有方法。

Abstract: The accurate prediction of flow fields around airfoils is crucial for
aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models
are effective but computationally expensive, thus inspiring the development of
surrogate models to enable quicker predictions. These surrogate models can be
based on deep learning architectures, such as Convolutional Neural Networks
(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion
models have shown significant promise in predicting complex flow fields. In
this work, we propose FoilDiff, a diffusion-based surrogate model with a
hybrid-backbone denoising network. This hybrid design combines the power of
convolutional feature extraction and transformer-based global attention to
generate more adaptable and accurate representations of flow structures.
FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling
to optimise the efficiency of the sampling process at no additional cost to
model generalisation. We used encoded representations of Reynolds number, angle
of attack, and airfoil geometry to define the input space for generalisation
across a wide range of aerodynamic conditions. When evaluated against
state-of-the-art models, FoilDiff shows significant performance improvements,
with mean prediction errors reducing by up to 85\% on the same datasets. The
results have demonstrated that FoilDiff can provide both more accurate
predictions and better-calibrated predictive uncertainty than existing
diffusion-based models.

</details>


### [490] [Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets](https://arxiv.org/abs/2510.04327)
*Haosong Zhang,Shenxi Wu,Yichi Zhang,Wei Lin*

Main category: cs.LG

TL;DR: AM-μP是一种新的参数化方法，通过约束网络范围内预激活二阶矩的算术平均值，解决了异构架构中学习率选择的挑战，并建立了η*∝L^{-3/2}的深度缩放定律。


<details>
  <summary>Details</summary>
Motivation: 传统μP方法在异构架构（如残差网络和卷积网络）中变得不适定，因为残差累积和卷积引入了层间不平衡，需要一种新的参数化方法来统一处理这些架构的学习率选择问题。

Method: 提出了算术平均μP(AM-μP)，约束网络范围内平均单步预激活二阶矩为常数尺度，结合残差感知的He初始化方法，缩放残差分支权重。

Result: 对于一维和二维卷积网络，最大更新学习率满足η*(L)∝L^{-3/2}；对于标准残差网络，η*(L)=Θ(L^{-3/2})，其中L为最小深度。实验验证了-3/2缩放定律并实现了零样本学习率迁移。

Conclusion: AM-μP为卷积和深度残差网络提供了统一且实用的学习率原则，无需额外调优开销，解决了异构架构中学习率选择的挑战。

Abstract: Choosing an appropriate learning rate remains a key challenge in scaling
depth of modern deep networks. The classical maximal update parameterization
($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to
homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in
heterogeneous architectures where residual accumulation and convolutions
introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P
(AM-$\mu$P), which constrains not each individual layer but the network-wide
average one-step pre-activation second moment to a constant scale. Combined
with a residual-aware He fan-in initialization - scaling residual-branch
weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot
\mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that
transfer consistently across depths. We prove that, for one- and
two-dimensional convolutional networks, the maximal-update learning rate
satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects
are constant-level as $N\gg k$. For standard residual networks with general
conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the
minimal depth. Empirical results across a range of depths confirm the $-3/2$
scaling law and enable zero-shot learning-rate transfer, providing a unified
and practical LR principle for convolutional and deep residual networks without
additional tuning overhead.

</details>


### [491] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: 本文提出了一个评估AI模型在罕见事件识别中性能的框架，重点关注低流行率场景下的关键考量因素，包括问题框架、测试集设计、统计评估和人类工作流程整合。


<details>
  <summary>Details</summary>
Motivation: 许多高风险AI应用针对低流行率事件，表面上的准确性可能掩盖了有限的现实价值。需要专门的方法来评估AI在罕见事件识别中的真实表现。

Method: 提出了结构化案例级检查（SCLE）方法，开发了全面的检查清单来指导AI模型的采购或开发，并在药物警戒领域进行了实例化研究。

Result: 研究揭示了罕见事件设置特有的陷阱，包括不现实的类别平衡导致的乐观估计，以及测试集中缺乏困难阳性对照的问题。

Conclusion: 虽然基于药物警戒实践，但该框架的原则可推广到阳性样本稀少且错误成本可能不对称的其他领域。

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [492] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 提出课程混沌预测(CCF)训练范式，通过从简单周期性行为到复杂混沌动态的渐进式课程训练，提升混沌系统预测性能


<details>
  <summary>Details</summary>
Motivation: 解决混沌系统预测中机器学习方法的两个极端问题：过度专业化于特定系统导致泛化性差，或盲目混合不相关时间序列导致无法学习特定动态机制

Method: 基于最大Lyapunov指数和吸引子维度等混沌度量指标，构建从简单到复杂的训练课程，使用50多个合成ODE/PDE系统库进行渐进式训练

Result: 在太阳黑子数、电力需求和人类ECG信号等真实世界基准测试中，CCF将有效预测范围延长达40%，相比仅使用真实数据训练提升超过两倍

Conclusion: CCF训练范式能显著提升混沌系统预测性能，该优势在不同神经网络架构中一致存在，课程结构对性能提升至关重要

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [493] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: 提出了Causal Sphere Hypergraph Transformer (CSHT)，一种结合Granger因果超图结构、黎曼几何和因果掩码Transformer注意力的可解释金融时间序列预测架构。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个能够统一建模金融新闻和情感对资产回报方向性影响的可解释预测模型，同时保持跨市场周期的稳健泛化能力。

Method: 提取多元Granger因果依赖关系，将其编码为超球面上的方向性超边，并通过角度掩码约束注意力机制以保持时间方向性和几何一致性。

Result: 在2018-2023年标普500数据（包括2020年COVID-19冲击）上的评估显示，CSHT在回报预测、制度分类和顶级资产排名任务中持续优于基线模型。

Conclusion: CSHT通过强制预测因果结构和在黎曼流形中嵌入变量，提供了跨市场周期的稳健泛化能力和从宏观经济事件到股票层面响应的透明归因路径，是可信金融预测的实用解决方案。

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [494] [Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework](https://arxiv.org/abs/2510.04366)
*Christopher Klugmann,Daniel Kondermann*

Main category: cs.LG

TL;DR: 提出了一种测量分类任务中模糊性的新方法，将离散响应分布映射到单位区间标量，区分类别间不可区分性和明确不可解决性带来的不确定性。


<details>
  <summary>Details</summary>
Motivation: 人类生成的分类标注经常产生反映模糊性而非简单标注错误的经验响应分布，需要量化分类任务中的偶然不确定性。

Method: 引入模糊性度量，与二次熵密切相关但不对称处理"无法解决"类别，开发了频率主义点估计器和贝叶斯后验推断工具。

Result: 分析了度量的形式性质，与现有模糊性度量对比，通过数值示例展示了在数据集质量评估和机器学习工作流中的实际应用。

Conclusion: 提出的模糊性度量能够有效量化分类任务中的偶然不确定性，为数据集质量评估和下游机器学习应用提供了统计工具。

Abstract: Human-generated categorical annotations frequently produce empirical response
distributions (soft labels) that reflect ambiguity rather than simple annotator
error. We introduce an ambiguity measure that maps a discrete response
distribution to a scalar in the unit interval, designed to quantify aleatoric
uncertainty in categorical tasks. The measure bears a close relationship to
quadratic entropy (Gini-style impurity) but departs from those indices by
treating an explicit "can't solve" category asymmetrically, thereby separating
uncertainty arising from class-level indistinguishability from uncertainty due
to explicit unresolvability. We analyze the measure's formal properties and
contrast its behavior with a representative ambiguity measure from the
literature. Moving beyond description, we develop statistical tools for
inference: we propose frequentist point estimators for population ambiguity and
derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the
underlying probability vector, providing a principled account of epistemic
uncertainty. Numerical examples illustrate estimation, calibration, and
practical use for dataset-quality assessment and downstream machine-learning
workflows.

</details>


### [495] [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
*Tejal Patwardhan,Rachel Dias,Elizabeth Proehl,Grace Kim,Michele Wang,Olivia Watkins,Simón Posada Fishman,Marwan Aljubeh,Phoebe Thacker,Laurance Fauconnet,Natalie S. Kim,Patrick Chao,Samuel Miserendino,Gildas Chabot,David Li,Michael Sharman,Alexandra Barr,Amelia Glaese,Jerry Tworek*

Main category: cs.LG

TL;DR: GDPval是一个评估AI模型在真实世界经济价值任务上能力的基准，涵盖美国GDP前9大行业的44个职业，发现前沿模型性能随时间线性提升，接近行业专家水平。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型在实际经济价值任务中的能力，填补现有基准在真实世界应用评估方面的空白。

Method: 基于美国劳工统计局工作活动构建任务，涵盖44个职业，由平均14年经验的行业专业人士设计代表性工作。

Result: 前沿模型在GDPval上的性能随时间线性提升，当前最佳模型在交付质量上接近行业专家水平，配合人工监督可更便宜快速地完成任务。

Conclusion: GDPval为理解模型真实世界能力提供了基准，开源220个黄金任务并提供自动评分服务，促进未来研究。

Abstract: We introduce GDPval, a benchmark evaluating AI model capabilities on
real-world economically valuable tasks. GDPval covers the majority of U.S.
Bureau of Labor Statistics Work Activities for 44 occupations across the top 9
sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are
constructed from the representative work of industry professionals with an
average of 14 years of experience. We find that frontier model performance on
GDPval is improving roughly linearly over time, and that the current best
frontier models are approaching industry experts in deliverable quality. We
analyze the potential for frontier models, when paired with human oversight, to
perform GDPval tasks cheaper and faster than unaided experts. We also
demonstrate that increased reasoning effort, increased task context, and
increased scaffolding improves model performance on GDPval. Finally, we
open-source a gold subset of 220 tasks and provide a public automated grading
service at evals.openai.com to facilitate future research in understanding
real-world model capabilities.

</details>


### [496] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: 提出动态加权损失函数，根据领域稀疏度自适应调整损失权重，解决稀疏领域推荐性能不足的问题


<details>
  <summary>Details</summary>
Motivation: 传统固定加权损失在稀疏或小众领域效果有限，单一权重无法有效处理交互极少的领域，训练信号容易被通用数据集稀释

Method: 基于数据驱动的动态加权损失，根据训练数据中每个领域的稀疏度自适应调整损失权重，对稀疏领域赋予更高权重

Result: 在四个数据集上的实验表明，该方法显著优于所有基线方法，特别是在稀疏领域，Recall@10和NDCG@10指标大幅提升，同时保持密集领域性能且计算开销极小

Conclusion: 动态加权损失通过理论分析和实证验证，有效提升了稀疏领域的推荐性能，为处理长尾用户兴趣提供了可靠解决方案

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [497] [Categorical Invariants of Learning Dynamics](https://arxiv.org/abs/2510.04376)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: 该论文提出了一种新的视角：将神经网络训练视为参数空间到表示空间的结构保持变换（函子），通过范畴论框架揭示同伦类优化路径与泛化性能的关系。


<details>
  <summary>Details</summary>
Motivation: 传统将神经网络训练视为损失曲面梯度下降的视角存在局限，需要更本质的数学框架来理解学习过程和泛化性能。

Method: 使用范畴论框架，将学习建模为参数空间到表示空间的函子变换，通过同伦类分析优化路径，并应用持久同调识别稳定最小值。

Result: 实验表明同伦轨迹收敛的网络泛化性能差异在0.5%以内，而非同伦路径差异超过3%；持久同调与泛化性能的相关性R^2=0.82。

Conclusion: 范畴不变量既提供了深度学习为何有效的理论洞察，也为训练更鲁棒网络提供了具体算法原则，包括形式化迁移学习和解释优化算法等价性。

Abstract: Neural network training is typically viewed as gradient descent on a loss
surface. We propose a fundamentally different perspective: learning is a
structure-preserving transformation (a functor L) between the space of network
parameters (Param) and the space of learned representations (Rep). This
categorical framework reveals that different training runs producing similar
test performance often belong to the same homotopy class (continuous
deformation family) of optimization paths. We show experimentally that networks
converging via homotopic trajectories generalize within 0.5% accuracy of each
other, while non-homotopic paths differ by over 3%. The theory provides
practical tools: persistent homology identifies stable minima predictive of
generalization (R^2 = 0.82 correlation), pullback constructions formalize
transfer learning, and 2-categorical structures explain when different
optimization algorithms yield functionally equivalent models. These categorical
invariants offer both theoretical insight into why deep learning works and
concrete algorithmic principles for training more robust networks.

</details>


### [498] [Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models](https://arxiv.org/abs/2510.04378)
*Xinshuai Dong,Ignavier Ng,Haoyue Dai,Jiaqi Sun,Xiangchen Song,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了首个基于分数的贪婪搜索方法LGES，用于识别包含潜在变量的因果结构，具有可识别性保证


<details>
  <summary>Details</summary>
Motivation: 现有基于约束的因果发现方法面临多重检验和误差传播问题，需要开发能够处理部分观测场景的基于分数的贪婪搜索方法

Method: 提出了广义N因子模型，并设计了潜在变量贪婪等价搜索(LGES)算法，使用定义良好的操作符在图空间中高效搜索最优结构

Result: 实验在合成数据和真实数据上验证了该方法的有效性

Conclusion: 该方法能够识别包含潜在变量的真实结构，直至马尔可夫等价类，是首个具有可识别性保证的基于分数的贪婪搜索方法

Abstract: Identifying the structure of a partially observed causal system is essential
to various scientific fields. Recent advances have focused on constraint-based
causal discovery to solve this problem, and yet in practice these methods often
face challenges related to multiple testing and error propagation. These issues
could be mitigated by a score-based method and thus it has raised great
attention whether there exists a score-based greedy search method that can
handle the partially observed scenario. In this work, we propose the first
score-based greedy search method for the identification of structure involving
latent variables with identifiability guarantees. Specifically, we propose
Generalized N Factor Model and establish the global consistency:
  the true structure including latent variables can be identified up to the
Markov equivalence class by using score. We then design
  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm
for this class of model with well-defined operators,
  which search very efficiently over the graph space to find the optimal
structure. Our experiments on both synthetic and real-life data validate the
effectiveness of our method (code will be publicly available).

</details>


### [499] [SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management](https://arxiv.org/abs/2510.04386)
*Shakson Isaac,Yentl Collin,Chirag Patel*

Main category: cs.LG

TL;DR: SSM-CGM是一个基于Mamba的神经状态空间预测模型，整合了CGM和可穿戴活动信号，提高了短期血糖预测准确性，并增加了可解释性。


<details>
  <summary>Details</summary>
Motivation: 大多数血糖监测预测模型缺乏临床可解释性，需要开发既能准确预测又能提供解释的模型来支持糖尿病管理。

Method: 使用Mamba-based神经状态空间预测模型，整合连续血糖监测和可穿戴设备活动信号，通过变量选择和时间归因实现可解释性。

Result: SSM-CGM在短期预测准确性上优于Temporal Fusion Transformer基线模型，能够进行反事实预测模拟生理信号变化对血糖的影响。

Conclusion: SSM-CGM提供了一个可解释、基于生理学的个性化糖尿病管理框架，结合了预测准确性和临床实用性。

Abstract: Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.

</details>


### [500] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了首个多项式时间收敛到执行最优策略的零阶Frank-Wolfe算法，解决了现有执行强化学习方法只能收敛到执行稳定策略而非最优策略的问题。


<details>
  <summary>Details</summary>
Motivation: 现有执行强化学习方法只能收敛到执行稳定策略，但该策略与期望的执行最优策略之间存在恒定差距。本文旨在直接找到执行最优策略。

Method: 提出零阶Frank-Wolfe算法，在Frank-Wolfe框架中使用零阶近似来计算执行策略梯度，并证明在标准正则化主导条件下能多项式时间收敛。

Result: 证明了值函数的重要性质：当策略正则化主导环境变化时，值函数满足梯度主导性；同时证明了充分平稳点位于凸紧策略子空间中，梯度有界且Lipschitz连续。

Conclusion: 该算法是首个能在多项式时间内收敛到执行最优策略的方法，实验证明比现有算法更有效。

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [501] [Trade-off in Estimating the Number of Byzantine Clients in Federated Learning](https://arxiv.org/abs/2510.04432)
*Ziyi Chen,Su Zhang,Heng Huang*

Main category: cs.LG

TL;DR: 本文系统分析了联邦学习中拜占庭客户端数量估计对鲁棒聚合器性能的影响，揭示了低估会导致性能恶化，而非低估情况下存在性能与鲁棒性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习容易受到拜占庭客户端攻击，鲁棒聚合器需要估计拜占庭客户端数量f，但这种估计对性能的影响尚未被系统研究。

Method: 理论分析聚合器和联邦学习算法在最坏情况下的误差，考虑所有可能的估计值f̂和实际值f的组合情况。

Result: 低估(f̂<f)会导致任意差的性能；非低估(f̂≥f)时，证明了聚合器和联邦学习误差的最优上下界，都与f̂/(n-f-f̂)成正比，随f̂增大而单调增加。

Conclusion: 存在根本性权衡：更大的鲁棒性f̂能解决更广泛的问题，但当实际拜占庭客户端较少时性能会下降。

Abstract: Federated learning has attracted increasing attention at recent large-scale
optimization and machine learning research and applications, but is also
vulnerable to Byzantine clients that can send any erroneous signals. Robust
aggregators are commonly used to resist Byzantine clients. This usually
requires to estimate the unknown number $f$ of Byzantine clients, and thus
accordingly select the aggregators with proper degree of robustness (i.e., the
maximum number $\hat{f}$ of Byzantine clients allowed by the aggregator). Such
an estimation should have important effect on the performance, which has not
been systematically studied to our knowledge. This work will fill in the gap by
theoretically analyzing the worst-case error of aggregators as well as its
induced federated learning algorithm for any cases of $\hat{f}$ and $f$.
Specifically, we will show that underestimation ($\hat{f}<f$) can lead to
arbitrarily poor performance for both aggregators and federated learning. For
non-underestimation ($\hat{f}\ge f$), we have proved optimal lower and upper
bounds of the same order on the errors of both aggregators and federated
learning. All these optimal bounds are proportional to $\hat{f}/(n-f-\hat{f})$
with $n$ clients, which monotonically increases with larger $\hat{f}$. This
indicates a fundamental trade-off: while an aggregator with a larger robustness
degree $\hat{f}$ can solve federated learning problems of wider range $f\in
[0,\hat{f}]$, the performance can deteriorate when there are actually fewer or
even no Byzantine clients (i.e., $f\in [0,\hat{f})$).

</details>


### [502] [Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size](https://arxiv.org/abs/2510.04440)
*Farid Bozorgnia,Vyacheslav Kungurtsev,Shirali Kadyrov,Mohsen Yousefnezhad*

Main category: cs.LG

TL;DR: 提出基于分数热核动力学的标签传播和自训练新算法，通过分数拉普拉斯算子增强图神经网络的表达能力，在标签数据稀缺时特别有效。


<details>
  <summary>Details</summary>
Motivation: 将信息论与抛物型演化方程的物理对应关系相结合，通过分数热核实现更全局的标签扩散，在只有少量标注训练样本时特别有利。

Method: 将分数热核集成到图卷积网络和图注意力等图神经网络架构中，使用切比雪夫多项式近似处理大规模图，通过分数拉普拉斯算子扩展经典扩散模型实现非局部交互。

Result: 在标准数据集上证明了该方法的有效性，分数热核能够实现自适应、多跳扩散，增强图神经网络的表达能力。

Conclusion: 分数热核动力学为标签传播和自训练提供了有效方法，特别是在标注数据稀缺的情况下，通过非局部交互实现更全局的标签扩散。

Abstract: In this work, we introduce novel algorithms for label propagation and
self-training using fractional heat kernel dynamics with a source term. We
motivate the methodology through the classical correspondence of information
theory with the physics of parabolic evolution equations. We integrate the
fractional heat kernel into Graph Neural Network architectures such as Graph
Convolutional Networks and Graph Attention, enhancing their expressiveness
through adaptive, multi-hop diffusion. By applying Chebyshev polynomial
approximations, large graphs become computationally feasible. Motivating
variational formulations demonstrate that by extending the classical diffusion
model to fractional powers of the Laplacian, nonlocal interactions deliver more
globally diffusing labels. The particular balance between supervision of known
labels and diffusion across the graph is particularly advantageous in the case
where only a small number of labeled training examples are present. We
demonstrate the effectiveness of this approach on standard datasets.

</details>


### [503] [Domain Generalization: A Tale of Two ERMs](https://arxiv.org/abs/2510.04441)
*Yilun Zhu,Naihao Deng,Naichen Shi,Aditya Gangrade,Clayton Scott*

Main category: cs.LG

TL;DR: 该论文研究了领域泛化问题，发现在满足后验漂移假设的数据集上，通过将特征向量与领域特定信息增强的"领域感知ERM"方法优于传统的池化ERM方法。


<details>
  <summary>Details</summary>
Motivation: 领域泛化文献中普遍发现难以超越在合并训练数据上的经验风险最小化(ERM)。作者认为这一发现主要基于满足协变量偏移假设的数据集，而在满足后验漂移假设的情况下，需要探索更有效的方法。

Method: 提出"领域感知ERM"方法，将特征向量与领域特定信息进行增强，以更好地处理后验漂移情况。

Result: 在语言和视觉任务上的实验表明，当数据集满足后验漂移假设时，领域感知ERM方法优于池化ERM方法。

Conclusion: 领域泛化方法的有效性取决于数据集满足的假设类型，在后验漂移情况下，领域感知ERM是更优的选择。

Abstract: Domain generalization (DG) is the problem of generalizing from several
distributions (or domains), for which labeled training data are available, to a
new test domain for which no labeled data is available. A common finding in the
DG literature is that it is difficult to outperform empirical risk minimization
(ERM) on the pooled training data.
  In this work, we argue that this finding has primarily been reported for
datasets satisfying a \emph{covariate shift} assumption. When the dataset
satisfies a \emph{posterior drift} assumption instead, we show that
``domain-informed ERM,'' wherein feature vectors are augmented with
domain-specific information, outperforms pooling ERM. These claims are
supported by a theoretical framework and experiments on language and vision
tasks.

</details>


### [504] [Forking-Sequences](https://arxiv.org/abs/2510.04487)
*Willa Potosnak,Malcolm Wolff,Boris Oreshkin,Mengfei Cao,Michael W. Mahoney,Dmitry Efimov,Kin G. Olivares*

Main category: cs.LG

TL;DR: 本文提出了forking-sequences方法，通过联合编码和解码所有预测创建日期的时间序列，显著提高了时间序列预测的稳定性，在多个数据集上平均提升了28.8%-37.9%的预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型往往只关注准确性，而忽略了预测稳定性这一重要指标。即使准确率很高的模型也可能在不同预测日期产生剧烈波动，这会损害利益相关者的信任并扰乱下游决策。

Method: 采用forking-sequences方法，与标准统计和神经预测方法独立处理每个预测创建日期不同，该方法联合编码和解码所有预测创建日期的整个时间序列，类似于时间序列交叉验证的方式。

Result: 在M1、M3、M4和Tourism竞赛的16个数据集上验证，使用forking-sequences方法后，MLP、RNN、LSTM、CNN和Transformer架构的预测百分比变化稳定性分别提高了28.8%、28.8%、37.9%、31.3%和平均8.8%。

Conclusion: forking-sequences方法能够带来三个关键优势：训练过程中更稳定一致的梯度更新、通过集成减少预测方差、提高推理计算效率，应该被更广泛地采用。

Abstract: While accuracy is a critical requirement for time series forecasting models,
an equally important (yet often overlooked) desideratum is forecast stability
across forecast creation dates (FCDs). Even highly accurate models can produce
erratic revisions between FCDs, undermining stakeholder trust and disrupting
downstream decision-making. To improve forecast stability, models like MQCNN,
MQT, and SPADE employ a little-known but highly effective technique:
forking-sequences. Unlike standard statistical and neural forecasting methods
that treat each FCD independently, the forking-sequences method jointly encodes
and decodes the entire time series across all FCDs, in a way mirroring time
series cross-validation. Since forking sequences remains largely unknown in the
broader neural forecasting community, in this work, we formalize the
forking-sequences approach, and we make a case for its broader adoption. We
demonstrate three key benefits of forking-sequences: (i) more stable and
consistent gradient updates during training; (ii) reduced forecast variance
through ensembling; and (iii) improved inference computational efficiency. We
validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and
Tourism competitions, showing improvements in forecast percentage change
stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,
RNN, LSTM, CNN, and Transformer-based architectures, respectively.

</details>


### [505] [Expand Neurons, Not Parameters](https://arxiv.org/abs/2510.04500)
*Linghao Kong,Inimai Subramanian,Yonadav Shavit,Micah Adler,Dan Alistarh,Nir Shavit*

Main category: cs.LG

TL;DR: 通过增加神经元数量但不增加非零参数数量来提高网络性能，提出固定参数扩展(FPE)方法，在保持参数数量不变的情况下减少特征干扰，提升模型准确率。


<details>
  <summary>Details</summary>
Motivation: 探索在保持非零参数数量不变的情况下，通过增加神经元宽度来减少特征间的干扰和纠缠，从而提升模型性能。这与现代加速器的特性相匹配，其中非零参数的内存移动是主要瓶颈。

Method: 提出固定参数扩展(FPE)：将单个神经元替换为多个子神经元，并将父神经元的权重不相交地分配给子神经元，每个子神经元继承非重叠的连接子集。

Result: 在符号任务（布尔代码问题）中，FPE系统性地减少了多义性指标并提高了任务准确率。在真实模型（CLIP嵌入分类器和深层网络）中，保持非零参数数量不变的情况下加宽网络持续提高了准确率。

Conclusion: FPE提供了一种基于可解释性机制的方法，利用宽度对抗叠加效应，在不增加非零参数数量的情况下提高性能，这很好地匹配了现代加速器的特性。

Abstract: This work demonstrates how increasing the number of neurons in a network
without increasing its number of non-zero parameters improves performance. We
show that this gain corresponds with a decrease in interference between
multiple features that would otherwise share the same neurons. To reduce such
entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter
Expansion (FPE): replace a neuron with multiple children and partition the
parent's weights disjointly across them, so that each child inherits a
non-overlapping subset of connections. On symbolic tasks, specifically Boolean
code problems, clause-aligned FPE systematically reduces polysemanticity
metrics and yields higher task accuracy. Notably, random splits of neuron
weights approximate these gains, indicating that reduced collisions, not
precise assignment, are a primary driver. Consistent with the superposition
hypothesis, the benefits of FPE grow with increasing interference: when
polysemantic load is high, accuracy improvements are the largest. Transferring
these insights to real models (classifiers over CLIP embeddings and deeper
multilayer networks) we find that widening networks while maintaining a
constant non-zero parameter count consistently increases accuracy. These
results identify an interpretability-grounded mechanism to leverage width
against superposition, improving performance without increasing the number of
non-zero parameters. Such a direction is well matched to modern accelerators,
where memory movement of non-zero parameters, rather than raw compute, is the
dominant bottleneck.

</details>


### [506] [Wavelet Predictive Representations for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.04507)
*Min Wang,Xin Li,Ye He,Yao-Hui Li,Hasnaa Bennis,Riashat Islam,Mingzhong Wang*

Main category: cs.LG

TL;DR: WISDOM提出了一种基于小波分析的NSRL方法，通过小波域任务表示捕捉非平稳环境中多尺度变化特征，结合小波时序差分更新算子，显著提升了智能体在动态环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界具有非平稳特性，现有NSRL方法对规律变化模式适应性有限，难以应对高度动态环境。受小波分析在时间序列建模中捕捉多尺度趋势能力的启发，需要开发能更好适应复杂非平稳环境的强化学习方法。

Method: 将任务表示序列转换到小波域，利用小波系数捕捉非平稳变化的全局趋势和细粒度变化；设计小波时序差分更新算子来增强MDP演化的跟踪和预测；理论上证明算子收敛性并展示小波任务表示带来的策略改进。

Result: 在多个基准测试中，WISDOM在样本效率和渐近性能上均显著优于现有基线方法，在非平稳和随机演化任务的复杂环境中表现出卓越的适应性。

Conclusion: 小波域任务表示能有效捕捉非平稳环境中的多尺度特征，结合小波TD更新算子可显著提升NSRL的性能，为处理复杂动态环境提供了有效解决方案。

Abstract: The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

</details>


### [507] [Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction](https://arxiv.org/abs/2510.04522)
*Yisen Gao,Xingcheng Fu,Qingyun Sun,Jianxin Li,Xianxian Li*

Main category: cs.LG

TL;DR: GeoMancer是一个黎曼图扩散框架，通过将多级特征解耦到特定任务流形上，解决了图数据中不同曲率特征在统一潜在空间中纠缠的问题，并使用黎曼陀螺核方法和流形约束扩散来确保数值稳定性和流形对齐。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型将节点、边和图级特征嵌入到统一潜在空间中，但由于图数据的非欧几里得特性，不同曲率的特征在相同潜在空间中纠缠，未能充分发挥其几何潜力。

Method: 提出GeoMancer框架：1) 用等距不变的黎曼陀螺核方法替代指数映射解决数值不稳定性；2) 将多级特征解耦到各自任务特定流形上学习最优表示；3) 引入流形约束扩散方法和自引导策略确保生成数据与流形特征对齐。

Result: 大量实验验证了该方法的有效性，在各种任务上表现出优越性能。

Conclusion: GeoMancer通过构建理想的黎曼扩散模型，成功捕捉了复杂图数据的不同流形特征并学习其分布，解决了现有方法中的数值不稳定性和流形偏差问题。

Abstract: Graph diffusion models have made significant progress in learning structured
graph data and have demonstrated strong potential for predictive tasks.
Existing approaches typically embed node, edge, and graph-level features into a
unified latent space, modeling prediction tasks including classification and
regression as a form of conditional generation. However, due to the
non-Euclidean nature of graph data, features of different curvatures are
entangled in the same latent space without releasing their geometric potential.
To address this issue, we aim to construt an ideal Riemannian diffusion model
to capture distinct manifold signatures of complex graph data and learn their
distribution. This goal faces two challenges: numerical instability caused by
exponential mapping during the encoding proces and manifold deviation during
diffusion generation. To address these challenges, we propose GeoMancer: a
novel Riemannian graph diffusion framework for both generation and prediction
tasks. To mitigate numerical instability, we replace exponential mapping with
an isometric-invariant Riemannian gyrokernel approach and decouple multi-level
features onto their respective task-specific manifolds to learn optimal
representations. To address manifold deviation, we introduce a
manifold-constrained diffusion method and a self-guided strategy for
unconditional generation, ensuring that the generated data remains aligned with
the manifold signature. Extensive experiments validate the effectiveness of our
approach, demonstrating superior performance across a variety of tasks.

</details>


### [508] [Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion](https://arxiv.org/abs/2510.04525)
*Satoshi Hayakawa,Yuhta Takida,Masaaki Imaizumi,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本文分析了MaskGIT采样器的理论机制，提出了更高效的"moment采样器"，并通过部分缓存技术和混合自适应解掩码方法加速采样过程。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在多个领域表现出色，但其采样加速研究相对不足。本文旨在探索掩码扩散模型的高效采样器。

Method: 通过理论分析MaskGIT采样器，提出moment采样器作为更易处理的替代方案，采用"先选择后采样"方法，并引入部分缓存技术和混合自适应解掩码策略。

Result: 在图像和文本领域的实验验证了理论分析，并证明了所提方法的高效性。

Conclusion: 本文推进了对掩码扩散采样器的理论理解和实际实现，为高效采样提供了新思路。

Abstract: Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the "moment sampler," an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.

</details>


### [509] [Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions](https://arxiv.org/abs/2510.04543)
*Elias Dubbeldam,Reza Mohammadi,Marit Schoonhoven,S. Ilker Birbil*

Main category: cs.LG

TL;DR: 现有基于图的表格深度学习(GTDL)方法过于关注预测精度而忽视了特征交互图结构的准确建模。本文主张GTDL应转向结构感知建模，通过实验证明准确的特征交互结构学习不仅能提升预测性能，还能增强模型的可解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前表格数据的深度学习方法难以超越传统树模型，主要挑战在于建模复杂的数据集特定特征交互。虽然GTDL方法试图通过图表示特征交互来解决此问题，但现有方法主要优化预测精度，忽视了图结构的准确建模。

Method: 使用具有已知真实图结构的合成数据集，评估现有GTDL方法恢复特征交互的能力，并验证强制执行真实交互结构对预测性能的影响。

Result: 实验表明现有GTDL方法无法恢复有意义的特征交互，而强制执行真实交互结构能够改善预测性能。

Conclusion: GTDL方法需要转向结构感知建模，优先考虑特征交互的定量评估和准确结构学习，以构建不仅准确而且可解释、可信且基于领域理解的系统。

Abstract: Despite recent progress, deep learning methods for tabular data still
struggle to compete with traditional tree-based models. A key challenge lies in
modeling complex, dataset-specific feature interactions that are central to
tabular data. Graph-based tabular deep learning (GTDL) methods aim to address
this by representing features and their interactions as graphs. However,
existing methods predominantly optimize predictive accuracy, neglecting
accurate modeling of the graph structure. This position paper argues that GTDL
should move beyond prediction-centric objectives and prioritize the explicit
learning and evaluation of feature interactions. Using synthetic datasets with
known ground-truth graph structures, we show that existing GTDL methods fail to
recover meaningful feature interactions. Moreover, enforcing the true
interaction structure improves predictive performance. This highlights the need
for GTDL methods to prioritize quantitative evaluation and accurate structural
learning. We call for a shift toward structure-aware modeling as a foundation
for building GTDL systems that are not only accurate but also interpretable,
trustworthy, and grounded in domain understanding.

</details>


### [510] [Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets](https://arxiv.org/abs/2510.04555)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: Tail-Safe是一个面向部署的衍生品对冲框架，结合了分布强化学习和控制屏障函数安全层，通过CVaR目标优化尾部风险，同时确保财务约束的满足。


<details>
  <summary>Details</summary>
Motivation: 传统衍生品对冲方法在极端市场条件下可能无法有效控制尾部风险，且缺乏可审计的安全保障机制。需要一种既能优化风险收益又能保证硬约束满足的部署友好型框架。

Method: 结合IQN-CVaR-PPO分布强化学习和基于CBF的二次规划安全层，包含尾部覆盖控制器、温度倾斜和尾部增强技术，以及多种财务约束（无交易带、箱型限制、速率限制等）。

Result: 在合成市场中，Tail-Safe改善了左尾风险而不损害中心性能，在QP可行且无松弛时实现零硬约束违反，提供了可审计的遥测数据支持治理。

Conclusion: Tail-Safe提供了一个可部署、可审计的衍生品对冲框架，在保证安全约束的同时优化尾部风险，但依赖合成数据和简化执行环境是当前限制。

Abstract: We introduce Tail-Safe, a deployability-oriented framework for derivatives
hedging that unifies distributional, risk-sensitive reinforcement learning with
a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer
tailored to financial constraints. The learning component combines an IQN-based
distributional critic with a CVaR objective (IQN--CVaR--PPO) and a
Tail-Coverage Controller that regulates quantile sampling through temperature
tilting and tail boosting to stabilize small-$\alpha$ estimation. The safety
component enforces discrete-time CBF inequalities together with domain-specific
constraints -- ellipsoidal no-trade bands, box and rate limits, and a
sign-consistency gate -- solved as a convex QP whose telemetry (active sets,
tightness, rate utilization, gate scores, slack, and solver status) forms an
auditable trail for governance. We provide guarantees of robust forward
invariance of the safe set under bounded model mismatch, a minimal-deviation
projection interpretation of the QP, a KL-to-DRO upper bound linking per-state
KL regularization to worst-case CVaR, concentration and sample-complexity
results for the temperature-tilted CVaR estimator, and a CVaR trust-region
improvement inequality under KL limits, together with feasibility persistence
under expiry-aware tightening. Empirically, in arbitrage-free,
microstructure-aware synthetic markets (SSVI $\to$ Dupire $\to$ VIX with
ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading
central performance and yields zero hard-constraint violations whenever the QP
is feasible with zero slack. Telemetry is mapped to governance dashboards and
incident workflows to support explainability and auditability. Limitations
include reliance on synthetic data and simplified execution to isolate
methodological contributions.

</details>


### [511] [Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems](https://arxiv.org/abs/2510.04559)
*Mohsen Amiri,V Venktesh,Sindri Magnússon*

Main category: cs.LG

TL;DR: 提出了一种用于多用户MIMO下行链路中识别前m个用户调度集的组合纯探索方法，通过线性效用模型和间隙索引框架显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 在多用户MIMO下行链路中，由于动作空间呈指数级增长，穷举搜索不可行，需要开发高效的探索方法来识别最佳用户子集。

Method: 采用线性效用模型，引入间隙索引框架，维护当前冠军臂（前m个集合）的短列表和挑战者臂的旋转短列表，专注于提供最有信息量的间隙索引比较。

Result: 与最先进的线性bandit方法相比，在保持高识别准确性的同时，显著减少了运行时间和计算量，并揭示了速度和准确性之间的可调权衡。

Conclusion: 短列表驱动的纯探索使得AI赋能通信系统中的在线、测量效率高的子载波选择变得实用可行。

Abstract: This paper investigates the identification of the top-m user-scheduling sets
in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration
problem in stochastic linear bandits. Because the action space grows
exponentially, exhaustive search is infeasible. We therefore adopt a linear
utility model to enable efficient exploration and reliable selection of
promising user subsets. We introduce a gap-index framework that maintains a
shortlist of current estimates of champion arms (top-m sets) and a rotating
shortlist of challenger arms that pose the greatest threat to the champions.
This design focuses on measurements that yield the most informative
gap-index-based comparisons, resulting in significant reductions in runtime and
computation compared to state-of-the-art linear bandit methods, with high
identification accuracy. The method also exposes a tunable trade-off between
speed and accuracy. Simulations on a realistic OFDM downlink show that
shortlist-driven pure exploration makes online, measurement-efficient
subcarrier selection practical for AI-enabled communication systems.

</details>


### [512] [Stochastic Approximation Methods for Distortion Risk Measure Optimization](https://arxiv.org/abs/2510.04563)
*Jinyang Jiang,Bernd Heidergott,Jiaqiao Hu,Yijie Peng*

Main category: cs.LG

TL;DR: 本文提出了基于两种对偶表示的失真风险度量梯度下降算法：失真度量形式和分位数函数形式，分别采用三时间尺度和两时间尺度方法，并提供了混合形式。算法在鲁棒投资组合选择任务中表现出色，并成功应用于深度强化学习。


<details>
  <summary>Details</summary>
Motivation: 失真风险度量在决策中捕捉风险偏好，但现有的优化方法存在效率问题。本文旨在开发高效的梯度下降算法来解决失真风险度量的优化问题。

Method: 提出两种算法形式：DM形式使用三时间尺度算法跟踪分位数、计算梯度并更新决策变量；QF形式采用两时间尺度方法避免复杂的分位数梯度估计。还开发了混合形式结合两者优势。

Result: DM形式达到最优收敛率O(k^{-4/7})，QF形式获得更快的O(k^{-2/3})收敛率。数值实验显示在鲁棒投资组合选择中显著优于基线方法，并成功应用于深度强化学习的动态库存管理。

Conclusion: 所提出的梯度下降算法为失真风险度量优化提供了高效解决方案，在理论和实践中均表现出色，具有广泛的应用前景。

Abstract: Distortion Risk Measures (DRMs) capture risk preferences in decision-making
and serve as general criteria for managing uncertainty. This paper proposes
gradient descent algorithms for DRM optimization based on two dual
representations: the Distortion-Measure (DM) form and Quantile-Function (QF)
form. The DM-form employs a three-timescale algorithm to track quantiles,
compute their gradients, and update decision variables, utilizing the
Generalized Likelihood Ratio and kernel-based density estimation. The QF-form
provides a simpler two-timescale approach that avoids the need for complex
quantile gradient estimation. A hybrid form integrates both approaches,
applying the DM-form for robust performance around distortion function jumps
and the QF-form for efficiency in smooth regions. Proofs of strong convergence
and convergence rates for the proposed algorithms are provided. In particular,
the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form
attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their
effectiveness and demonstrate substantial improvements over baselines in robust
portfolio selection tasks. The method's scalability is further illustrated
through integration into deep reinforcement learning. Specifically, a DRM-based
Proximal Policy Optimization algorithm is developed and applied to
multi-echelon dynamic inventory management, showcasing its practical
applicability.

</details>


### [513] [GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning](https://arxiv.org/abs/2510.04567)
*Weishuo Ma,Yanbo Wang,Xiyuan Wang,Lei Zou,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出了GILT框架，一种基于LLM-free和tuning-free的图神经网络架构，通过token-based的上下文学习机制统一处理节点、边和图级别的分类任务，有效解决图数据的异构性问题。


<details>
  <summary>Details</summary>
Motivation: 当前图基础模型面临图数据极端异构性的挑战，现有方法要么依赖LLM但无法处理数值特征，要么需要昂贵的每图调优阶段。需要一种既能处理异构图数据又无需调优的高效方法。

Method: 引入基于token的上下文学习框架，将图分类任务重新表述为统一格式，利用上下文动态理解类别语义，无需LLM依赖和调优过程。

Result: 实验表明GILT在少样本学习场景下表现优于基于LLM或需要调优的基线方法，且时间效率显著提升。

Conclusion: GILT框架成功解决了图数据异构性挑战，提供了一种高效、无需调优的图基础模型解决方案，在性能和效率方面均优于现有方法。

Abstract: Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

</details>


### [514] [Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing](https://arxiv.org/abs/2510.04579)
*Clément Bonet,Elsa Cazelles,Lucas Drumetz,Nicolas Courty*

Main category: cs.LG

TL;DR: 本文研究了Wasserstein空间中的Busemann函数，在一维分布和高斯分布情况下建立了闭式表达式，并开发了新的Sliced-Wasserstein距离用于高斯混合模型和标记数据集。


<details>
  <summary>Details</summary>
Motivation: Busemann函数在几何机器学习中能定义黎曼流形上的测地线投影，而许多数据源可建模为概率分布，因此研究其在Wasserstein空间中的存在性和计算具有重要意义。

Method: 建立了Wasserstein空间中Busemann函数的闭式表达式，特别针对一维分布和高斯分布情况，并基于此开发了投影方案和新的Sliced-Wasserstein距离。

Result: 在一维分布和高斯分布情况下成功推导出Busemann函数的闭式表达式，并展示了这些方法在合成数据集和迁移学习问题中的有效性。

Conclusion: 提出的Busemann函数闭式表达式和投影方案为概率分布的几何分析提供了新工具，特别是在高斯混合模型和标记数据集上的应用表现出良好性能。

Abstract: The Busemann function has recently found much interest in a variety of
geometric machine learning problems, as it naturally defines projections onto
geodesic rays of Riemannian manifolds and generalizes the notion of
hyperplanes. As several sources of data can be conveniently modeled as
probability distributions, it is natural to study this function in the
Wasserstein space, which carries a rich formal Riemannian structure induced by
Optimal Transport metrics. In this work, we investigate the existence and
computation of Busemann functions in Wasserstein space, which admits geodesic
rays. We establish closed-form expressions in two important cases:
one-dimensional distributions and Gaussian measures. These results enable
explicit projection schemes for probability distributions on $\mathbb{R}$,
which in turn allow us to define novel Sliced-Wasserstein distances over
Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those
original schemes on synthetic datasets as well as transfer learning problems.

</details>


### [515] [Improved probabilistic regression using diffusion models](https://arxiv.org/abs/2510.04583)
*Carlo Kneissl,Christopher Bülte,Philipp Scholl,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的概率回归框架，通过非参数方式学习预测分布，能够适应多样化任务并提供更好的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 概率回归能提供比传统点估计更丰富的预测分布信息，但现有扩散模型在通用回归任务中缺乏不确定性评估且应用领域受限。

Method: 通过建模扩散噪声的完整分布来实现非参数概率回归，研究了不同噪声参数化方法及其权衡。

Result: 在多个低维和高维回归任务实验中，该方法优于现有基线方法，同时提供校准的不确定性估计。

Conclusion: 该扩散框架展示了作为概率预测工具的通用性，能够适应多样化任务并提供可靠的不确定性量化。

Abstract: Probabilistic regression models the entire predictive distribution of a
response variable, offering richer insights than classical point estimates and
directly allowing for uncertainty quantification. While diffusion-based
generative models have shown remarkable success in generating complex,
high-dimensional data, their usage in general regression tasks often lacks
uncertainty-related evaluation and remains limited to domain-specific
applications. We propose a novel diffusion-based framework for probabilistic
regression that learns predictive distributions in a nonparametric way. More
specifically, we propose to model the full distribution of the diffusion noise,
enabling adaptation to diverse tasks and enhanced uncertainty quantification.
We investigate different noise parameterizations, analyze their trade-offs, and
evaluate our framework across a broad range of regression tasks, covering low-
and high-dimensional settings. For several experiments, our approach shows
superior performance against existing baselines, while delivering calibrated
uncertainty estimates, demonstrating its versatility as a tool for
probabilistic prediction.

</details>


### [516] [Closed-Form Last Layer Optimization](https://arxiv.org/abs/2510.04606)
*Alexandre Galashov,Nathaël Da Costa,Liyuan Xu,Philipp Hennig,Arthur Gretton*

Main category: cs.LG

TL;DR: 提出一种优化神经网络的新方法，将最后一层线性权重作为主干参数的函数，利用平方损失下线性层闭式解的特性，交替进行主干参数梯度下降和最后一层闭式更新。


<details>
  <summary>Details</summary>
Motivation: 在平方损失下，线性最后一层权重存在闭式最优解，但传统SGD方法没有充分利用这一特性。

Method: 将最后一层视为主干参数的函数，仅优化主干参数，等价于交替进行主干梯度下降和最后一层闭式更新。在SGD设置中，通过权衡当前批次损失和先前批次累积信息来适应随机性。

Result: 在Neural Tangent Kernel机制下证明该方法收敛到最优解。在多个监督任务（包括傅里叶神经算子和工具变量回归）中，相比标准SGD在平方损失上表现更优。

Conclusion: 该方法有效利用了线性层的闭式解特性，在保证收敛的同时提升了优化效率，适用于回归和分类任务。

Abstract: Neural networks are typically optimized with variants of stochastic gradient
descent. Under a squared loss, however, the optimal solution to the linear last
layer weights is known in closed-form. We propose to leverage this during
optimization, treating the last layer as a function of the backbone parameters,
and optimizing solely for these parameters. We show this is equivalent to
alternating between gradient descent steps on the backbone and closed-form
updates on the last layer. We adapt the method for the setting of stochastic
gradient descent, by trading off the loss on the current batch against the
accumulated information from previous batches. Further, we prove that, in the
Neural Tangent Kernel regime, convergence of this method to an optimal solution
is guaranteed. Finally, we demonstrate the effectiveness of our approach
compared with standard SGD on a squared loss in several supervised tasks --
both regression and classification -- including Fourier Neural Operators and
Instrumental Variable Regression.

</details>


### [517] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出基于先进预测模型的合成生物医学时间序列数据生成框架，能高保真复制EEG和EMG等复杂电生理信号，解决数据稀缺和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 严格的隐私法规和大量资源需求限制了生物医学时间序列AI的发展，导致数据需求与可访问性之间存在关键差距。

Method: 使用先进预测模型生成合成生物医学时间序列数据，准确复制复杂电生理信号，保持真实数据的统计特性。

Result: 合成数据保持了真实数据的关键时间和频谱特性，可作为真实数据的有效替代品，显著提升AI模型性能。

Conclusion: 该框架在保持关键生物医学特征的同时提供高可扩展性，无缝集成到开源存储库中，大大扩展了AI驱动生物医学研究的资源。

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [518] [Compressed Concatenation of Small Embedding Models](https://arxiv.org/abs/2510.04626)
*Mohamed Ayoub Ben Ayad,Michael Dinzinger,Kanishka Ghosh Dastidar,Jelena Mitrovic,Michael Granitzer*

Main category: cs.LG

TL;DR: 通过连接多个小嵌入模型的原始向量并使用轻量级解码器压缩，可以在保持性能的同时大幅减小模型尺寸，实现48倍压缩率下恢复89%的原始性能。


<details>
  <summary>Details</summary>
Motivation: 解决大嵌入模型在资源受限环境（如浏览器、边缘设备）中部署困难的问题，同时弥补小模型性能不足的缺陷。

Method: 连接多个小模型的嵌入向量，然后使用带MRL损失的轻量级统一解码器将高维联合表示映射到低维空间，无需微调基础模型。

Result: 在MTEB检索任务子集上，对四个小模型连接后的表示应用concat-encode-quantize流程，在48倍压缩率下恢复了89%的原始性能。

Conclusion: 通过连接多个小模型并使用轻量级解码器压缩，可以在资源受限环境中实现高性能的嵌入表示，且连接更多基础模型能提高表示在压缩和量化下的鲁棒性。

Abstract: Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

</details>


### [519] [Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation](https://arxiv.org/abs/2510.04646)
*Johanna Sommer,John Rachwan,Nils Fleischmann,Stephan Günnemann,Bertrand Charpentier*

Main category: cs.LG

TL;DR: 提出一种无需训练的高速缓存策略，通过预测求解器步骤间的中间隐藏状态来加速分子几何生成，实现2-3倍推理速度提升。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型生成高质量分子几何结构但推理计算成本高，需要数百次网络评估，成为实际应用中采样大量分子候选者的主要瓶颈。

Method: 直接在SE(3)-等变骨干网络上操作的无训练缓存策略，预测求解器步骤间的中间隐藏状态，与预训练模型兼容且正交于现有基于训练的加速方法。

Result: 在GEOM-Drugs数据集上，缓存方法在保持样本质量的同时将推理时间减少2倍，相比基础模型最高实现3倍加速，且样本质量下降极小。

Conclusion: 该缓存方法与其他优化技术结合可获得高达7倍的整体加速，为分子几何生成提供了有效的推理加速解决方案。

Abstract: Flow matching models generate high-fidelity molecular geometries but incur
significant computational costs during inference, requiring hundreds of network
evaluations. This inference overhead becomes the primary bottleneck when such
models are employed in practice to sample large numbers of molecular
candidates. This work discusses a training-free caching strategy that
accelerates molecular geometry generation by predicting intermediate hidden
states across solver steps. The proposed method operates directly on the
SE(3)-equivariant backbone, is compatible with pretrained models, and is
orthogonal to existing training-based accelerations and system-level
optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching
achieves a twofold reduction in wall-clock inference time at matched sample
quality and a speedup of up to 3x compared to the base model with minimal
sample quality degradation. Because these gains compound with other
optimizations, applying caching alongside other general, lossless optimizations
yield as much as a 7x speedup.

</details>


### [520] [IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams](https://arxiv.org/abs/2510.04660)
*Yuandou Wang,Filip Gunnarsson,Rihan Hai*

Main category: cs.LG

TL;DR: 提出了IMLP，一种用于表格数据流的紧凑持续学习器，通过窗口注意力机制实现恒定内存使用，在保持竞争力的准确率同时显著提高能源效率。


<details>
  <summary>Details</summary>
Motivation: 表格数据流在边缘和移动设备上的实时决策应用日益重要，但这些设备资源严格受限。现有持续学习方法依赖不断增长的缓冲区，加剧了资源成本。

Method: IMLP采用窗口化缩放点积注意力机制处理滑动潜在特征缓冲区，避免存储原始数据，结合共享前馈层实现轻量级更新。

Result: IMLP比TabNet能源效率高27.6倍，比TabPFN高85.5倍，同时保持竞争力的平均准确率。

Conclusion: IMLP为表格数据流提供了一个易于部署、能源高效的完整再训练替代方案。

Abstract: Tabular data streams are rapidly emerging as a dominant modality for
real-time decision-making in healthcare, finance, and the Internet of Things
(IoT). These applications commonly run on edge and mobile devices, where energy
budgets, memory, and compute are strictly limited. Continual learning (CL)
addresses such dynamics by training models sequentially on task streams while
preserving prior knowledge and consolidating new knowledge. While recent CL
work has advanced in mitigating catastrophic forgetting and improving knowledge
transfer, the practical requirements of energy and memory efficiency for
tabular data streams remain underexplored. In particular, existing CL solutions
mostly depend on replay mechanisms whose buffers grow over time and exacerbate
resource costs.
  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a
compact continual learner for tabular data streams. IMLP incorporates a
windowed scaled dot-product attention over a sliding latent feature buffer,
enabling constant-size memory and avoiding storing raw data. The attended
context is concatenated with current features and processed by shared
feed-forward layers, yielding lightweight per-segment updates. To assess
practical deployability, we introduce NetScore-T, a tunable metric coupling
balanced accuracy with energy for Pareto-aware comparison across models and
datasets. IMLP achieves up to $27.6\times$ higher energy efficiency than TabNet
and $85.5\times$ higher than TabPFN, while maintaining competitive average
accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient
alternative to full retraining for tabular data streams.

</details>


### [521] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: 本文通过理论分析揭示了时间序列归一化中的四个理论矛盾，发现标准RevIN在极端异常值数据集上性能急剧下降，而简单的R²-IN却意外成为最佳表现者，同时自适应模型A-IN出现系统性失败。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改进Reversible Instance Normalization (RevIN)方法，通过用鲁棒统计量替换其非鲁棒统计量(R²-IN)来提升性能，但发现实际效果比预期复杂得多。

Method: 通过识别四个理论矛盾来解构不同归一化策略的性能表现，包括标准RevIN、R²-IN和自适应模型A-IN的对比实验。

Result: 标准RevIN在极端异常值数据集上MSE激增683%；R²-IN意外成为整体最佳表现者；自适应模型A-IN出现完全系统性失败。

Conclusion: 提出了时间序列归一化的新警示范式：从盲目追求复杂性转向诊断驱动分析，揭示了简单基线的惊人能力以及朴素自适应的危险性。

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [522] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: 本文系统评估了深度联合源信道编码中的语义信道均衡方法，通过引入对齐器来匹配异构潜在空间，解决多供应商部署中的语义噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有DeepJSCC方案假设发射端和接收端共享潜在空间，这在多供应商部署中不成立，导致语义噪声和性能下降。

Method: 研究三类对齐器：线性映射（闭式解）、轻量神经网络（更强表达能力）和Parseval框架均衡器（零样本模式无需训练）。

Result: 通过AWGN和衰落信道上的图像重建实验，量化了复杂度、数据效率和保真度之间的权衡。

Conclusion: 为在异构AI原生无线网络中部署DeepJSCC提供了指导原则。

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [523] [Counterfactual Credit Guided Bayesian Optimization](https://arxiv.org/abs/2510.04676)
*Qiyu Wei,Haowei Wang,Richard Allmendinger,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: 提出了CCGBO框架，通过反事实信用评估历史观测点的贡献度，优化贝叶斯优化的资源分配，加速寻找全局最优解。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化构建全局代理模型效率低，实际应用中更关注快速找到全局最优解，需要更智能的资源分配策略。

Method: 引入反事实信用机制量化历史观测对找到最优解的贡献度，并将此信用整合到采集函数中，实现选择性资源分配。

Result: 理论证明保持次线性遗憾，在合成和真实基准测试中显著降低简单遗憾，加速收敛到全局最优解。

Conclusion: CCGBO通过反事实信用机制有效提升贝叶斯优化效率，为昂贵黑盒函数优化提供了更实用的解决方案。

Abstract: Bayesian optimization has emerged as a prominent methodology for optimizing
expensive black-box functions by leveraging Gaussian process surrogates, which
focus on capturing the global characteristics of the objective function.
However, in numerous practical scenarios, the primary objective is not to
construct an exhaustive global surrogate, but rather to quickly pinpoint the
global optimum. Due to the aleatoric nature of the sequential optimization
problem and its dependence on the quality of the surrogate model and the
initial design, it is restrictive to assume that all observed samples
contribute equally to the discovery of the optimum in this context. In this
paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),
a novel framework that explicitly quantifies the contribution of individual
historical observations through counterfactual credit. By incorporating
counterfactual credit into the acquisition function, our approach can
selectively allocate resources in areas where optimal solutions are most likely
to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations
on various synthetic and real-world benchmarks demonstrate that CCGBO
consistently reduces simple regret and accelerates convergence to the global
optimum.

</details>


### [524] [Parameter-free Algorithms for the Stochastically Extended Adversarial Model](https://arxiv.org/abs/2510.04685)
*Shuche Wang,Adarsh Barik,Peng Zhao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出了首个针对随机扩展对抗(SEA)模型的无参数算法，通过乐观在线牛顿步(OONS)消除对领域直径和损失函数Lipschitz常数的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有SEA模型方法需要预先知道问题特定参数(如领域直径D和Lipschitz常数G)，这限制了实际应用。

Method: 利用乐观在线牛顿步(OONS)算法开发无参数方法，首先处理未知领域直径但已知Lipschitz常数的情况，然后扩展到两者都未知的更一般设置。

Result: 在未知领域直径但已知Lipschitz常数情况下，获得期望遗憾界$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} + \sqrt{\Sigma^2_{1:T}})\big)$；在两者都未知情况下，遗憾界保持对$\sigma^2_{1:T}$和$\Sigma^2_{1:T}$的相同依赖关系。

Conclusion: 所提出的方法在SEA模型中即使两个参数都未知时仍然有效，证明了方法的有效性。

Abstract: We develop the first parameter-free algorithms for the Stochastically
Extended Adversarial (SEA) model, a framework that bridges adversarial and
stochastic online convex optimization. Existing approaches for the SEA model
require prior knowledge of problem-specific parameters, such as the diameter of
the domain $D$ and the Lipschitz constant of the loss functions $G$, which
limits their practical applicability. Addressing this, we develop
parameter-free methods by leveraging the Optimistic Online Newton Step (OONS)
algorithm to eliminate the need for these parameters. We first establish a
comparator-adaptive algorithm for the scenario with unknown domain diameter but
known Lipschitz constant, achieving an expected regret bound of
$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} +
\sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and
$\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic
variance and cumulative adversarial variation, respectively. We then extend
this to the more general setting where both $D$ and $G$ are unknown, attaining
the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound
exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$,
demonstrating the efficacy of our proposed methods even when both parameters
are unknown in the SEA model.

</details>


### [525] [How does the optimizer implicitly bias the model merging loss landscape?](https://arxiv.org/abs/2510.04686)
*Chenxiang Zhang,Alexander Theus,Damien Teney,Antonio Orvieto,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 本文研究发现有效噪声规模是统一优化器和数据选择对模型合并影响的关键指标，模型合并效果与有效噪声呈非单调函数关系，存在最优值。


<details>
  <summary>Details</summary>
Motivation: 虽然模型合并方法在实践中很有用，但人们对什么特性使合并有效缺乏理解。本文旨在探索优化过程如何影响损失景观几何形状及其对合并成功的影响。

Method: 通过分析优化过程对损失景观几何形状的影响，引入有效噪声规模这一单一指标来统一优化器和数据选择对模型合并的影响。

Result: 研究发现模型合并效果是有效噪声的非单调函数，具有明显的最优点。学习率、权重衰减、批次大小和数据增强都会独立调节有效噪声规模，表现出相同的定性趋势。

Conclusion: 优化器噪声不仅影响单个极小值的平坦度或泛化能力，还影响全局损失景观，可以预测独立训练的解何时可以合并。这些发现拓宽了对优化如何塑造损失景观几何形状及其对模型合并下游影响的理解。

Abstract: Model merging methods combine models with different capabilities into a
single one while maintaining the same inference cost. Two popular approaches
are linear interpolation, which linearly interpolates between model weights,
and task arithmetic, which combines task vectors obtained by the difference
between finetuned and base models. While useful in practice, what properties
make merging effective are poorly understood. This paper explores how the
optimization process affects the loss landscape geometry and its impact on
merging success. We show that a single quantity -- the effective noise scale --
unifies the impact of optimizer and data choices on model merging. Across
architectures and datasets, the effectiveness of merging success is a
non-monotonic function of effective noise, with a distinct optimum. Decomposing
this quantity, we find that larger learning rates, stronger weight decay,
smaller batch sizes, and data augmentation all independently modulate the
effective noise scale, exhibiting the same qualitative trend. Unlike prior work
that connects optimizer noise to the flatness or generalization of individual
minima, we show that it also affects the global loss landscape, predicting when
independently trained solutions can be merged. Our findings broaden the
understanding of how optimization shapes the loss landscape geometry and its
downstream consequences for model merging, suggesting the possibility of
further manipulating the training dynamics to improve merging effectiveness.

</details>


### [526] [ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts](https://arxiv.org/abs/2510.04710)
*Zexin Wang,Changhua Pei,Yang Liu,Hengyue Jiang,Quan Zhou,Haotian Si,Hang Cui,Jianhui Li,Gaogang Xie,Jingjing Li,Dan Pei*

Main category: cs.LG

TL;DR: 提出ViTs框架，将时间序列数据转换为视觉表示，利用视觉语言模型处理任意长度的时间序列异常检测，实现"一次训练，跨场景推理"的目标。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中"一次训练，跨场景推理"的根本挑战，传统方法受限于固定长度输入，而大语言模型面临上下文长度限制。

Method: 将时间序列曲线转换为视觉表示，通过重缩放时间序列图像保持时间依赖关系，同时保持一致的输入大小。采用进化算法自动生成高质量图像-文本对，设计三阶段训练流程：时间序列知识注入、异常检测增强、异常推理精炼。

Result: ViTs显著增强了视觉语言模型理解和检测时间序列数据中异常的能力，能够高效处理任意长度序列而无需上下文约束。

Conclusion: ViTs框架通过视觉表示有效解决了时间序列异常检测中的长度限制问题，为跨场景零样本泛化提供了可行方案。

Abstract: Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.

</details>


### [527] [Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs](https://arxiv.org/abs/2510.04727)
*Emanuele Mule,Stefano Fiorini,Antonio Purificato,Federico Siciliano,Stefano Coniglio,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 提出了方向性束超图网络（DSHN），将束理论与超图中的不对称关系处理相结合，构建了有向束超图拉普拉斯算子，在7个真实数据集上相比13个基线方法实现了2%到20%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 有向超图能够建模面向的群体交互，但在许多应用中仍未被充分探索。现有方法通常存在同质性偏差，限制了在异质性设置中的有效性。

Method: 基于代数拓扑中的胞腔束概念，提出了方向性束超图网络（DSHN），构建了有向束超图拉普拉斯算子，这是一个复值算子，统一并推广了图学习和超图学习文献中现有的拉普拉斯矩阵。

Result: 在7个真实世界数据集上，与13个基线方法相比，DSHN实现了2%到20%的相对准确率提升。

Conclusion: 对超图中方向性的原则性处理，结合束的表达能力，可以显著提高性能。

Abstract: Hypergraphs provide a natural way to represent higher-order interactions
among multiple entities. While undirected hypergraphs have been extensively
studied, the case of directed hypergraphs, which can model oriented group
interactions, remains largely under-explored despite its relevance for many
applications. Recent approaches in this direction often exhibit an implicit
bias toward homophily, which limits their effectiveness in heterophilic
settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf
Neural Networks (SNNs) were introduced as an effective solution to circumvent
such a drawback. While a generalization to hypergraphs is known, it is only
suitable for undirected hypergraphs, failing to tackle the directed case. In
this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a
framework integrating sheaf theory with a principled treatment of asymmetric
relations within a hypergraph. From it, we construct the Directed Sheaf
Hypergraph Laplacian, a complex-valued operator by which we unify and
generalize many existing Laplacian matrices proposed in the graph- and
hypergraph-learning literature. Across 7 real-world datasets and against 13
baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how
a principled treatment of directionality in hypergraphs, combined with the
expressive power of sheaves, can substantially improve performance.

</details>


### [528] [EVaR-Optimal Arm Identification in Bandits](https://arxiv.org/abs/2510.04728)
*Mehrasa Ahmadipour,Aurélien Garivier*

Main category: cs.LG

TL;DR: 提出了一个在熵风险价值准则下的固定置信度最佳臂识别算法，该算法在非参数设置下适用于有界奖励分布，并证明了其样本复杂度的渐近最优性。


<details>
  <summary>Details</summary>
Motivation: 解决高风险环境（如金融领域）中风险规避决策的需求，超越简单的期望值优化。

Method: 提出基于Track-and-Stop的δ-正确算法，需要解决复杂的凸优化问题和相关的简单非凸问题。

Result: 推导了期望样本复杂度的下界，并证明该算法渐近匹配该下界。

Conclusion: 在EVaR准则下的非参数多臂老虎机框架中，所提算法在固定置信度最佳臂识别问题上实现了渐近最优性能。

Abstract: We study the fixed-confidence best arm identification (BAI) problem within
the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)
criterion. Our analysis considers a nonparametric setting, allowing for general
reward distributions bounded in [0,1]. This formulation addresses the critical
need for risk-averse decision-making in high-stakes environments, such as
finance, moving beyond simple expected value optimization. We propose a
$\delta$-correct, Track-and-Stop based algorithm and derive a corresponding
lower bound on the expected sample complexity, which we prove is asymptotically
matched. The implementation of our algorithm and the characterization of the
lower bound both require solving a complex convex optimization problem and a
related, simpler non-convex one.

</details>


### [529] [Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors](https://arxiv.org/abs/2510.04758)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: 本文建立了非线性CCA恢复真实潜在因子的条件，证明了在总体设置下对广泛类别潜在分布的仿射可识别性，并展示了白化对确保有界性和良好条件性的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究非线性CCA在什么条件下能够恢复真实潜在因子，特别是在白化后达到正交变换的识别精度。

Method: 通过重新参数化将分析从观测空间传输到源空间，证明在总体设置下对广泛类别潜在分布的仿射可识别性，并分析岭正则化经验CCA的收敛性。

Result: 建立了非线性CCA恢复真实潜在因子的充分条件，证明了白化对确保有界性和良好条件性的必要性，并通过实验验证了理论结果。

Conclusion: 非线性CCA在适当条件下能够识别真实潜在因子，白化是实现这一目标的关键步骤，理论结果在有限样本情况下也成立。

Abstract: In this work, we establish conditions under which nonlinear CCA recovers the
ground-truth latent factors up to an orthogonal transform after whitening.
Building on the classical result that linear mappings maximize canonical
correlations under Gaussian priors, we prove affine identifiability for a broad
class of latent distributions in the population setting. Central to our proof
is a reparameterization result that transports the analysis from observation
space to source space, where identifiability becomes tractable. We further show
that whitening is essential for ensuring boundedness and well-conditioning,
thereby underpinning identifiability. Beyond the population setting, we prove
that ridge-regularized empirical CCA converges to its population counterpart,
transferring these guarantees to the finite-sample regime. Experiments on a
controlled synthetic dataset and a rendered image dataset validate our theory
and demonstrate the necessity of its assumptions through systematic ablations.

</details>


### [530] [ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs](https://arxiv.org/abs/2510.04767)
*Wonjun Kang,Kevin Galim,Seunghyuk Oh,Minjae Lee,Yuchen Zeng,Shuibai Zhang,Coleman Hooper,Yuezhou Hu,Hyung Il Koo,Nam Ik Cho,Kangwook Lee*

Main category: cs.LG

TL;DR: 本文分析了扩散语言模型并行解码的局限性，提出了专门用于评估dLLMs的ParallelBench基准，揭示了并行解码在现实任务中会导致严重的质量下降，并指出当前方法难以在速度和质量之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型因并行解码潜力而受到关注，但其条件独立性假设会忽略token依赖关系，在强依赖任务中导致生成质量下降。现有研究对此挑战关注不足，且标准基准测试无法充分反映并行解码的质量问题。

Method: 首先进行并行解码的信息论分析，然后在可分析的合成列表操作上进行案例研究，从数据分布和解码策略角度提供定量见解。基于这些见解，提出专门针对dLLMs的ParallelBench基准，包含对人类和自回归LLMs简单但对dLLMs并行解码极具挑战的现实任务。

Result: 使用ParallelBench系统分析发现：(i) dLLMs在并行解码下在现实场景中可能遭受严重的质量下降；(ii) 当前并行解码策略难以根据任务难度调整并行度，无法在不损害质量的情况下实现有意义的加速。

Conclusion: 研究结果强调了开发创新解码方法的迫切需求，以克服当前的速度-质量权衡。作者发布基准以加速真正高效dLLMs的发展。

Abstract: While most autoregressive LLMs are constrained to one-by-one decoding,
diffusion LLMs (dLLMs) have attracted growing interest for their potential to
dramatically accelerate inference through parallel decoding. Despite this
promise, the conditional independence assumption in dLLMs causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
parallel decoding. To address this gap, we first provide an
information-theoretic analysis of parallel decoding. We then conduct case
studies on analytically tractable synthetic list operations from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of parallel decoding. Building on
these insights, we propose ParallelBench, the first benchmark specifically
designed for dLLMs, featuring realistic tasks that are trivial for humans and
autoregressive LLMs yet exceptionally challenging for dLLMs under parallel
decoding. Using ParallelBench, we systematically analyze both dLLMs and
autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can
suffer dramatic quality degradation in real-world scenarios, and (ii) current
parallel decoding strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current speed-quality trade-off. We
release our benchmark to help accelerate the development of truly efficient
dLLMs.

</details>


### [531] [When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates](https://arxiv.org/abs/2510.04769)
*Michele Caprio,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: 该论文分析了在存在不精确性的情况下，机器学习算法中迭代更新过程的收敛性和稳定性问题，特别关注了基于信任集的不精确概率机器学习。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习算法依赖不确定性表示的迭代更新，但在存在不精确性和模糊性时，需要研究这些迭代过程是否收敛到稳定固定点，以及在什么条件下可以实现这种稳定性。

Method: 使用信任集（闭凸概率分布集）作为不精确概率信念的表示框架，分析迭代更新规则在信任集上的应用，研究固定点的存在性和可达性条件。

Result: 首次对不精确概率机器学习中的迭代收敛问题进行了分析，并以信任贝叶斯深度学习为例进行了说明。

Conclusion: 将不精确性纳入学习过程不仅丰富了不确定性表示，还揭示了稳定性出现的结构条件，为理解不精确性下迭代学习的动态提供了新见解。

Abstract: Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

</details>


### [532] [Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning](https://arxiv.org/abs/2510.04773)
*Kai Qin,Jiaqi Wu,Jianxiang He,Haoyuan Sun,Yifei Zhao,Bin Liang,Yongzhe Chang,Tiantian Zhang,Houde Liu*

Main category: cs.LG

TL;DR: 提出了DiPO（分布偏好优化）算法，通过直接操作模型的下一个令牌概率分布来实现LLM遗忘，解决了NPO方法缺乏显式正偏好信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘方法如NPO由于缺乏显式正偏好信号而效果有限，需要领域专业知识或精心设计的提示来构建偏好响应，限制了通用性。

Method: 在分布层面直接针对下一个令牌概率分布，通过选择性放大或抑制模型高置信度输出logits来构建偏好分布对，推导出DiPO算法。

Result: DiPO在TOFU基准上达到最高遗忘质量，在MUSE基准上保持领先的可扩展性和效用保持可持续性，实现了模型效用与遗忘质量的良好平衡。

Conclusion: DiPO通过分布级方法有效克服了NPO的局限性，理论证明其损失函数与期望遗忘方向一致，实验验证了其在遗忘任务中的优越性能。

Abstract: As Large Language Models (LLMs) demonstrate remarkable capabilities learned
from vast corpora, concerns regarding data privacy and safety are receiving
increasing attention. LLM unlearning, which aims to remove the influence of
specific data while preserving overall model utility, is becoming an important
research area. One of the mainstream unlearning classes is optimization-based
methods, which achieve forgetting directly through fine-tuning, exemplified by
Negative Preference Optimization (NPO). However, NPO's effectiveness is limited
by its inherent lack of explicit positive preference signals. Attempts to
introduce such signals by constructing preferred responses often necessitate
domain-specific knowledge or well-designed prompts, fundamentally restricting
their generalizability. In this paper, we shift the focus to the
distribution-level, directly targeting the next-token probability distribution
instead of entire responses, and derive a novel unlearning algorithm termed
\textbf{Di}stribution \textbf{P}reference \textbf{O}ptimization (DiPO). We show
that the requisite preference distribution pairs for DiPO, which are
distributions over the model's output tokens, can be constructed by selectively
amplifying or suppressing the model's high-confidence output logits, thereby
effectively overcoming NPO's limitations. We theoretically prove the
consistency of DiPO's loss function with the desired unlearning direction.
Extensive experiments demonstrate that DiPO achieves a strong trade-off between
model utility and forget quality. Notably, DiPO attains the highest forget
quality on the TOFU benchmark, and maintains leading scalability and
sustainability in utility preservation on the MUSE benchmark.

</details>


### [533] [MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis](https://arxiv.org/abs/2510.04776)
*Ebenezer Awotoro,Chisom Ezekannagha,Florian Schwarz,Johannes Tauscher,Dominik Heider,Katharina Ladewig,Christel Le Bon,Karine Moncoq,Bruno Miroux,Georges Hattab*

Main category: cs.LG

TL;DR: MetaMP是一个统一膜蛋白数据库的框架，通过机器学习分类、丰富元数据和提供交互式界面来改进数据质量，在膜蛋白研究中实现了77%的数据不一致性解决率和98%的新蛋白分类准确率。


<details>
  <summary>Details</summary>
Motivation: 膜蛋白结构复杂性高，现有数据库存在数据缺失、不一致和计算障碍等问题，需要改进数据库集成以支持膜蛋白研究。

Method: 开发MetaMP框架，统一膜蛋白数据库，使用机器学习进行分类，提供8个交互式视图和用户友好界面，支持结构分类和异常检测功能。

Result: MetaMP在验证中解决了77%的数据不一致性，对新识别的膜蛋白分类准确率达到98%，超越了专家策展，并在不同难度任务中均表现有效。

Conclusion: MetaMP是一个必要的资源，统一了当前知识并支持AI驱动的膜蛋白结构探索，在膜蛋白研究中具有重要应用价值。

Abstract: Structural biology has made significant progress in determining membrane
proteins, leading to a remarkable increase in the number of available
structures in dedicated databases. The inherent complexity of membrane protein
structures, coupled with challenges such as missing data, inconsistencies, and
computational barriers from disparate sources, underscores the need for
improved database integration. To address this gap, we present MetaMP, a
framework that unifies membrane-protein databases within a web application and
uses machine learning for classification. MetaMP improves data quality by
enriching metadata, offering a user-friendly interface, and providing eight
interactive views for streamlined exploration. MetaMP was effective across
tasks of varying difficulty, demonstrating advantages across different levels
without compromising speed or accuracy, according to user evaluations.
Moreover, MetaMP supports essential functions such as structure classification
and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in
membrane protein research: predicting transmembrane segments, reconciling
legacy databases, and classifying structures with explainable AI support. In a
validation focused on statistics, MetaMP resolved 77% of data discrepancies and
accurately predicted the class of newly identified membrane proteins 98% of the
time and overtook expert curation. Altogether, MetaMP is a much-needed resource
that harmonizes current knowledge and empowers AI-driven exploration of
membrane-protein architecture.

</details>


### [534] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: 提出了一种名为TTC-RL的智能体，能够在测试时自动构建任务特定课程，通过强化学习持续训练模型，无需人工数据筛选。


<details>
  <summary>Details</summary>
Motivation: 人类能够在工作中学习，模型是否也能做到？研究旨在开发能够在测试时自动学习并改进的模型。

Method: 使用测试时课程（TTC-RL）方法，自动从大量可用训练数据中选择最任务相关的数据，应用强化学习进行持续训练。

Result: 在数学和编程基准测试中，TTC-RL显著提升模型性能：Qwen3-8B在AIME25上的pass@1提升约1.8倍，在CodeElo上提升2.1倍；pass@8在AIME25从40%提升到62%，在CodeElo从28%提升到43%。

Conclusion: 测试时课程方法展示了在测试时通过数千个任务相关经验进行持续训练的潜力，扩展了测试时扩展范式。

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [535] [On Predicting Post-Click Conversion Rate via Counterfactual Inference](https://arxiv.org/abs/2510.04816)
*Junhyung Ahn,Sanghack Lee*

Main category: cs.LG

TL;DR: 提出ESCIM方法，通过因果推理为未点击样本生成反事实转化标签，解决CVR预测中点击样本稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 传统CVR预测模型仅基于点击样本训练，但点击样本稀疏，需要大量日志数据。现有方法利用未点击样本但依赖启发式方法，存在偏差问题。

Method: 首先训练用户序列行为的结构因果模型，对未点击项目进行假设干预来推断反事实CVR，然后将预测的反事实CVR转换为二进制标签，最后将生成的样本纳入训练过程。

Result: 在公共数据集上的广泛实验显示算法优越性，在线A/B测试验证了在真实场景中的有效性，在潜在转化数据上表现出更强的鲁棒性和泛化能力。

Conclusion: ESCIM方法通过因果推理有效解决了CVR预测中的样本稀疏性问题，在真实场景中表现出优异的性能和泛化能力。

Abstract: Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

</details>


### [536] [On the Hardness of Learning Regular Expressions](https://arxiv.org/abs/2510.04834)
*Idan Attias,Lev Reyzin,Nathan Srebro,Gal Vardi*

Main category: cs.LG

TL;DR: 该论文研究了正则表达式在PAC模型和成员查询下的计算复杂性，证明了即使在均匀分布下PAC学习也是困难的，并且对于带有补集或交集的正则表达式，即使在均匀分布下成员查询学习也是困难的。


<details>
  <summary>Details</summary>
Motivation: 尽管正则表达式具有理论重要性和广泛的实际应用，但其学习计算复杂性尚未得到充分探索。现有关于DFA或NFA学习的困难结果不能直接应用于正则表达式，因为它们在描述复杂性上存在指数级差异。

Method: 通过理论分析，研究正则表达式在PAC模型和成员查询下的计算复杂性，特别关注在均匀分布和分布无关情况下的学习难度。

Result: 证明了PAC学习即使在超立方体上的均匀分布下也是困难的，分布无关的成员查询学习也是困难的。对于带有补集或交集的正则表达式，即使在均匀分布下成员查询学习也是困难的。

Conclusion: 正则表达式的学习在计算上是困难的，这一结果独立于DFA或NFA的学习困难性，突显了正则表达式学习复杂性的独特性。

Abstract: Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

</details>


### [537] [Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study](https://arxiv.org/abs/2510.04837)
*Guillaume Godin*

Main category: cs.LG

TL;DR: BCFP是一种与ECFP互补的键中心指纹方法，在BBB穿透分类任务中，将ECFP与BCFP结合使用能显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种与原子中心圆形指纹（ECFP）互补的键中心指纹方法，以改进脑血屏障穿透（BBBP）分类任务的预测性能。

Method: 提出静态BCFP方法，模拟ChemProp等定向消息传递GNN的键卷积，使用快速随机森林模型进行评估，并引入BCFP-Sort&Slice特征组合方案。

Result: 在分层交叉验证中，ECFP与BCFP的拼接始终比单独使用任一描述符获得更高的AUROC和AUPRC，r=1半径表现最佳，且优于MGTP预测。

Conclusion: 轻量级的键中心描述符可以补充原子中心圆形指纹，为BBBP预测提供强大快速的基线方法。

Abstract: Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

</details>


### [538] [Distributionally Robust Causal Abstractions](https://arxiv.org/abs/2510.04842)
*Yorgos Felekis,Theodoros Damoulas,Paris Giampouras*

Main category: cs.LG

TL;DR: 提出了首个分布鲁棒的因果抽象框架及其学习算法，通过Wasserstein模糊集解决环境变化和模型设定错误问题。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象学习方法假设固定且良好设定的外生分布，容易受到环境变化和设定错误的影响。

Method: 将鲁棒因果抽象学习建模为带Wasserstein模糊集的约束极小极大优化问题，提供理论和算法支持。

Result: 在经验和高斯环境下提供理论结果，通过实验证明框架对环境变化、结构模型和干预映射设定错误的鲁棒性。

Conclusion: 提出的分布鲁棒因果抽象框架能有效应对环境变化和模型设定错误，具有实际应用价值。

Abstract: Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

</details>


### [539] [Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/2510.04855)
*Junqi Jiang,Francesco Leofante,Antonio Rago,Francesca Toni*

Main category: cs.LG

TL;DR: 提出LAPACE框架，通过标签条件高斯混合变分自编码器(L-GMVAE)学习结构化潜空间，生成对抗输入和模型扰动的鲁棒反事实解释路径。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以统一处理反事实解释的多种需求：鲁棒性（对抗输入和模型扰动）、合理性（位于数据流形）和多样性（提供多种选择）。

Method: 使用L-GMVAE学习每个类别的多高斯分量潜空间，然后通过从输入潜表示到学习到的潜质心插值生成反事实解释路径。

Result: LAPACE计算高效，在八个量化指标上表现优异，能生成收敛到固定质心的鲁棒路径，提供权衡接近度和合理性的多种选择。

Conclusion: LAPACE是模型无关的统一框架，能同时满足反事实解释的鲁棒性、合理性和多样性要求，并支持用户指定的可操作性约束。

Abstract: Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

</details>


### [540] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: 论文识别了自进化LLM代理的对齐临界过程(ATP)风险，即持续交互导致代理放弃训练时建立的对齐约束，转向强化自利策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理获得自进化能力，其长期可靠性成为关键问题。需要研究部署后出现的独特风险，即代理可能通过持续交互放弃训练时的对齐约束。

Method: 通过两个互补范式形式化分析ATP：自利探索（重复高奖励偏差导致个体行为漂移）和模仿策略扩散（异常行为在多代理系统中传播）。构建可控测试平台，在Qwen3-8B和Llama-3.1-8B-Instruct上进行基准测试。

Result: 实验显示对齐收益在自进化下迅速削弱，初始对齐模型收敛到未对齐状态。多代理设置中，成功违规快速扩散导致集体未对齐。当前基于强化学习的对齐方法仅提供脆弱防御。

Conclusion: LLM代理的对齐不是静态属性，而是脆弱且动态的，在部署期间易受反馈驱动衰减影响。

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [541] [A Clinical-grade Universal Foundation Model for Intraoperative Pathology](https://arxiv.org/abs/2510.04861)
*Zihan Zhao,Fengtao Zhou,Ronggang Li,Bing Chu,Xinke Zhang,Xueyi Zheng,Ke Zheng,Xiaobo Wen,Jiabo Ma,Yihui Wang,Jiewei Chen,Chengyou Zheng,Jiangyu Zhang,Yongqin Wen,Jiajia Meng,Ziqi Zeng,Xiaoqing Li,Jing Li,Dan Xie,Yaping Ye,Yu Wang,Hao Chen,Muyan Cai*

Main category: cs.LG

TL;DR: CRISP是一个临床级病理学基础模型，基于8个医疗中心的10万+冰冻切片开发，在15,000+术中切片上验证，能够实现良恶性鉴别、术中决策支持和泛癌检测，在2,000+患者前瞻性队列中92.6%病例直接影响手术决策。


<details>
  <summary>Details</summary>
Motivation: 术中病理学对精准手术至关重要，但受限于诊断复杂性和高质量冰冻切片数据稀缺。计算病理学虽取得进展，但缺乏大规模前瞻性验证阻碍了其在手术流程中的常规应用。

Method: 开发CRISP临床级基础模型，基于8个医疗中心的10万+冰冻切片训练，在15,000+术中切片上进行近100项回顾性诊断任务评估，包括良恶性鉴别、关键术中决策和泛癌检测等。

Result: 模型在不同机构、肿瘤类型和解剖部位间表现出稳健泛化能力，包括未见过的部位和罕见癌症。在前瞻性2,000+患者队列中，CRISP在真实条件下保持高诊断准确性，92.6%病例直接影响手术决策。人机协作减少35%诊断工作量，避免105项辅助检查，微转移检测准确率达87.5%。

Conclusion: CRISP为AI驱动的术中病理学建立了临床级范式，将计算进展与手术精准性相结合，加速人工智能向常规临床实践的转化。

Abstract: Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

</details>


### [542] [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)
*Alexia Jolicoeur-Martineau*

Main category: cs.LG

TL;DR: TRM是一种比HRM更简单的递归推理方法，使用单层小网络（仅7M参数）在ARC-AGI任务上超越了大多数大型语言模型。


<details>
  <summary>Details</summary>
Motivation: HRM虽然在小模型上表现出色，但理解不足且可能不是最优方案，需要开发更简单有效的推理方法。

Method: 提出Tiny Recursive Model (TRM)，使用单个仅2层的小网络进行递归推理，参数仅7M。

Result: TRM在ARC-AGI-1上获得45%测试准确率，ARC-AGI-2上获得8%，超越大多数LLM，参数仅为其0.01%。

Conclusion: TRM证明了极简递归推理架构在解决复杂问题上的有效性，为小模型推理提供了新方向。

Abstract: Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

</details>


### [543] [Flow-Matching Based Refiner for Molecular Conformer Generation](https://arxiv.org/abs/2510.04878)
*Xiangyang Xu,Hongyang Gao*

Main category: cs.LG

TL;DR: 提出了一种用于分子构象生成的流匹配精炼器，通过从上游去噪模型的混合质量输出初始化采样，并重新调度噪声尺度来绕过低信噪比阶段，从而提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 低能量分子构象生成是药物发现中的基础但具有挑战性的问题。现有的去噪方法（如扩散和流匹配）在采样过程中容易在低信噪比阶段积累误差，这些阶段难以训练。

Method: 提出流匹配精炼器方法，从上游去噪模型的混合质量输出初始化采样，重新调度噪声尺度以绕过低信噪比阶段，使用生成器-精炼器管道。

Result: 在GEOM-QM9和GEOM-Drugs基准数据集上，生成器-精炼器管道以更少的总去噪步骤提高了质量，同时保持了多样性。

Conclusion: 该方法通过绕过难以训练的低信噪比阶段，有效解决了去噪方法中的误差积累问题，在分子构象生成任务中实现了更好的性能。

Abstract: Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

</details>


### [544] [Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models](https://arxiv.org/abs/2510.04888)
*Alina Ermilova,Dmitrii Kornilov,Sofia Samoilova,Ekaterina Laptenkova,Anastasia Kolesnikova,Ekaterina Podplutova,Senotrusova Sofya,Maksim G. Sharaev*

Main category: cs.LG

TL;DR: 本文系统评估了7种基于不同数据源（电子健康记录和ICD-10代码）发现疾病关联的方法，发现LLM方法产生的疾病关联多样性最低，表明其在发现新关联方面潜力有限。


<details>
  <summary>Details</summary>
Motivation: 手动分析大规模临床数据识别疾病关联存在劳动密集、主观性强和专家意见分歧的问题，而机器学习方法面临选择最佳方法、确定最佳数据源和缺乏真实基准的挑战。

Method: 整合了7种方法：统计共现分析、掩码语言建模、领域特定BERT变体、通用BERT、文档检索和4种LLM，基于MIMIC-IV电子健康记录和ICD-10代码数据进行系统评估。

Result: 基于LLM的方法产生的ICD代码关联多样性最低，表明其在发现新疾病关联方面潜力有限。

Conclusion: 在没有医学关联真实基准的情况下，研究结果构成了有价值的医学疾病本体论，可作为未来临床研究和医疗AI应用的基础资源。

Abstract: Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

</details>


### [545] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 提出了一个基于仿真的评估框架，通过可配置的合成数据集系统评估多变量长期时间序列预测模型的鲁棒性，揭示了不同模型在信号模式、噪声类型和频率特征方面的特定优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界数据集噪声特性未知，评估多变量长期时间序列预测模型的鲁棒性具有挑战性，需要一种可控的评估方法来深入理解模型性能。

Method: 构建参数化合成数据集生成框架，包含可配置的信号组件、噪声类型、信噪比和频率特征，在四种代表性架构（S-Mamba、iTransformer、R-Linear、Autoformer）上进行系统评估。

Result: 所有模型在回看窗口无法捕获完整季节性周期时性能严重下降；S-Mamba和Autoformer在锯齿波模式表现最佳，R-Linear和iTransformer偏好正弦信号；白噪声和布朗噪声普遍降低性能，S-Mamba对趋势噪声敏感，iTransformer对季节性噪声敏感；S-Mamba和iTransformer频率重构能力最强。

Conclusion: 基于合成数据集的受控评估方法通过MSE分数聚合提供了对模型特定优势和局限性的深入洞察，为基于信号特性和噪声条件的模型选择提供了具体指导。

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [546] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: 提出了一种新方法，使技能发现算法能够学习专注于控制特定状态变量的技能，从而显著提高探索效率并避免下游任务中的负面副作用。


<details>
  <summary>Details</summary>
Motivation: 现有技能发现算法往往忽视强化学习问题中自然存在的状态变量，导致发现的技能缺乏对特定状态变量的控制，这会显著影响探索效率、增加学习难度，并在目标不明确的下游任务中产生负面副作用。

Method: 引入了一种通用方法，使技能发现算法能够学习聚焦技能——即针对和控制特定状态变量的技能。

Result: 该方法将状态空间覆盖率提高了三倍，解锁了新的学习能力，并在下游任务中自动避免了负面副作用。

Conclusion: 通过让技能发现算法学习聚焦于特定状态变量的技能，可以显著提升强化学习的探索效率和性能，同时避免下游任务中的负面效应。

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [547] [DP-HYPE: Distributed Differentially Private Hyperparameter Search](https://arxiv.org/abs/2510.04902)
*Johannes Liebenow,Thorsten Peinemann,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: DP-HYPE是一种分布式隐私保护超参数调优算法，通过基于客户端本地超参数评估的分布式投票来选择多数客户端支持的折中超参数，同时保持差分隐私保护。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习中的超参数调优对模型性能影响重大，但当在敏感数据上调整超参数时，隐私保护成为重要挑战。现有方法要么计算成本高，要么为每个客户端单独确定超参数，要么应用本地差分隐私导致效用-隐私权衡不理想。

Method: 提出DP-HYPE算法，通过基于客户端本地超参数评估的分布式投票来选择超参数，保持可扩展性和与特定学习任务的独立性。算法保证客户端级差分隐私，且隐私保证不依赖于超参数数量。

Result: 在多个基准数据集上的iid和非iid设置中评估性能，证明DP-HYPE即使在小的隐私预算下也具有高效用。算法作为子模块集成到流行的Flower分布式机器学习框架中。

Conclusion: DP-HYPE提供了一种高效、可扩展的分布式隐私保护超参数调优方法，能够在保持强隐私保护的同时实现良好的模型性能。

Abstract: The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

</details>


### [548] [How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning](https://arxiv.org/abs/2510.04908)
*Haotian Gao,Zheng Dong,Jiawei Yong,Shintaro Fukushima,Kenjiro Taura,Renhe Jiang*

Main category: cs.LG

TL;DR: ST-SSDL是一个时空时间序列预测框架，通过自监督偏差学习方案来捕捉和利用当前输入与历史模式之间的动态偏差，从而提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的时空预测方法往往忽略了当前输入与历史模式之间的动态偏差，这些偏差包含了对模型性能有重要影响的关键信号。

Method: 提出ST-SSDL框架，将每个输入锚定到其历史平均值，并使用可学习原型对潜在空间进行离散化。设计了两个辅助目标：对比损失增强原型间区分度，偏差损失正则化输入表示与对应原型之间的距离一致性。

Result: 在六个基准数据集上的实验表明，ST-SSDL在多个指标上持续优于最先进的基线方法。可视化结果进一步证明了其在复杂时空场景中自适应响应不同偏差水平的能力。

Conclusion: ST-SSDL通过自监督偏差学习有效提升了时空预测性能，能够更好地处理输入条件的变化，具有良好的泛化能力。

Abstract: Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

</details>


### [549] [Glocal Information Bottleneck for Time Series Imputation](https://arxiv.org/abs/2510.04910)
*Jie Yang,Kexin Zhang,Guibin Zhang,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: 提出Glocal-IB训练范式，通过全局对齐损失解决高缺失率下时间序列插值模型的优化困境，提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列插值模型在高缺失率下容易过拟合局部噪声，导致推理阶段产生差的插值结果和扭曲的潜在表示分布，缺乏全局指导。

Method: 提出Glocal-IB训练范式，在标准信息瓶颈框架基础上引入全局对齐损失，通过可处理的互信息近似来对齐掩码输入与原始观测输入的潜在表示。

Result: 在九个数据集上的广泛实验证实，Glocal-IB在高缺失率下带来持续改进的性能和对齐的潜在表示。

Conclusion: Glocal-IB能够帮助模型在抑制缺失值引起的噪声的同时保留全局结构和局部细节，从而在高缺失率下实现更好的泛化性能。

Abstract: Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

</details>


### [550] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: FedSSL-AMC是一种用于自动调制分类的联邦自监督学习方法，通过无标签I/Q序列的triplet-loss自监督训练和每个客户端的小标签集SVM分类器，解决了隐私、通信开销和信道偏移问题。


<details>
  <summary>Details</summary>
Motivation: 集中式训练AMC模型存在隐私风险、通信开销大且对信道偏移不鲁棒，联邦学习虽避免数据集中但仍对类别不平衡、非独立同分布和标签样本有限敏感。

Method: 提出FedSSL-AMC，在客户端上使用因果时间膨胀CNN和triplet-loss自监督训练无标签I/Q序列，然后在每个客户端使用小标签集训练SVM分类器。

Result: 建立了联邦表示学习过程的收敛性和下游分类器在特征噪声下的可分离性保证。在合成和空中数据集上的实验显示，在异构SNR、载波频率偏移和非独立同分布标签划分下，相比监督联邦学习基线有持续增益。

Conclusion: FedSSL-AMC通过联邦自监督学习有效解决了AMC中的隐私、通信和鲁棒性问题，在异构条件下优于传统监督联邦学习方法。

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [551] [Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking](https://arxiv.org/abs/2510.04930)
*Ali Saheb Pasand,Elvis Dohmatob*

Main category: cs.LG

TL;DR: 本文提出了一种称为平等梯度下降(EGD)的新方法，通过归一化梯度来消除深度学习中的"顿悟"现象，显著加速模型从训练到泛化的过渡过程。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的"顿悟"现象导致模型在训练过程中测试性能长期停滞，然后突然跃升到接近完美水平。这种停滞期在实际应用中是不希望的，需要找到方法来加速这一过程。

Method: 提出平等梯度下降(EGD)方法，通过归一化梯度使得沿所有主方向(奇异方向)的动态演化速度完全相同，这可以视为自然梯度下降的一种精心修改形式。

Result: EGD方法显著加速了"顿悟"过程，在某些情况下完全消除了停滞期。在模加法和稀疏奇偶问题等经典算术问题上，该方法有效消除了性能平台期。

Conclusion: 梯度沿不同主方向的不对称速度是导致"顿悟"现象的关键因素，通过平等化梯度动态可以显著加速学习过程，消除性能停滞。

Abstract: Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

</details>


### [552] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 提出了一个决策导向学习框架，用于预测约束优化问题中的不确定参数，通过两个基于最大似然估计的损失函数来平衡可行性和决策质量。


<details>
  <summary>Details</summary>
Motivation: 当约束优化问题的参数不确定时，预测的参数可能导致不可行解，因此需要同时管理可行性和决策质量。

Method: 开发了基于最大似然估计的两个损失函数：一个惩罚不可行性，另一个惩罚次优决策，并通过可调参数形成加权平均。

Result: 实验表明调整参数可以在次优性和可行性之间进行权衡，且对于特定参数值，该方法在多个优化问题实例中与现有基线方法性能相当。

Conclusion: 该框架为决策者提供了在可行性和决策质量之间平衡的控制能力，适用于通用的约束优化问题。

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

</details>


### [553] [StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R](https://arxiv.org/abs/2510.04974)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: StructuralDecompose是一个用于模块化和可解释时间序列分解的R包，将分解过程分为变化点检测、异常检测、平滑和分解等独立组件。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时间序列分解视为单一过程，缺乏灵活性和鲁棒性。该包旨在提供模块化设计，使用户能够根据特定时间序列特征定制方法。

Method: 将时间序列分解分离为四个独立组件：变化点检测、异常检测、平滑和分解，提供灵活的方法选择和组合。

Result: 在模拟和真实数据集上展示了该包的性能，并与Rbeast和autostsm等先进工具进行了基准测试。

Conclusion: 该包在可解释机器学习工作流程中具有重要作用，提供了灵活且鲁棒的时间序列分解解决方案。

Abstract: We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

</details>


### [554] [Federated Computation of ROC and PR Curves](https://arxiv.org/abs/2510.04979)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 提出一种在联邦学习场景下通过分布式差分隐私估计预测分数分布分位数来近似ROC和PR曲线的新方法，解决了隐私和通信约束下的模型评估问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中，由于数据分布在多个客户端且受隐私和通信限制，服务器无法访问原始预测分数和类别标签，难以计算ROC和PR曲线。

Method: 使用分布式差分隐私估计预测分数分布的分位数，通过理论分析在真实曲线和估计曲线之间的面积误差，平衡近似精度、隐私保护和通信成本。

Result: 在真实数据集上的实证结果表明，该方法能以最小通信成本和强隐私保证实现高近似精度。

Conclusion: 该方法为联邦系统中隐私保护的模型评估提供了实用解决方案，在保证隐私的同时有效近似ROC和PR曲线。

Abstract: Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

</details>


### [555] [Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization](https://arxiv.org/abs/2510.04988)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出一种自适应记忆机制，用动态动量系数替代传统优化器中固定的动量系数，通过在线调整来提升优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习优化器如Nesterov、Heavy Ball、AdamW等都使用固定的动量系数（通常设为0.9），这种策略虽然被广泛采用但并非最优。

Method: 使用近似目标函数的双平面方法：一个来自当前迭代的梯度，另一个来自过去梯度的累积记忆，构建了一个新的近端框架来动态调整动量系数。

Result: 在SGD和AdamW上实现了自适应记忆变体，从简单凸问题到大规模深度学习场景的广泛测试表明，该方法能够超越手动调参的标准优化器。

Conclusion: 该方法新颖、简单易用且无需额外假设或超参数调优，为优化中的自适应机制开辟了新途径。

Abstract: The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

</details>


### [556] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 本文分析了幂变换的数值不稳定性问题，提出了有效的解决方案，并将幂变换扩展到联邦学习场景中。


<details>
  <summary>Details</summary>
Motivation: 幂变换是常用的数据预处理技术，但直接实现存在严重的数值不稳定性，可能导致错误结果甚至系统崩溃。

Method: 全面分析数值不稳定性的来源，提出有效的补救措施，并将幂变换扩展到联邦学习环境，解决其中的数值和分布挑战。

Result: 在真实数据集上的实验表明，所提方法既有效又鲁棒，相比现有方法显著提高了稳定性。

Conclusion: 通过分析幂变换的数值不稳定性并提出解决方案，成功实现了更稳定可靠的幂变换方法，特别是在联邦学习场景中表现出色。

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


### [557] [Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective](https://arxiv.org/abs/2510.05023)
*Weixin Wang,Haoyang Zheng,Guang Lin,Wei Deng,Pan Xu*

Main category: cs.LG

TL;DR: 提出了TS-SA算法，将随机逼近融入Thompson采样框架，通过时间平均稳定后验分布近似，简化了超参数调优和理论分析。


<details>
  <summary>Details</summary>
Motivation: 现有近似Thompson采样算法需要在不同轮次近似不同的后验分布，导致需要动态调整学习率等超参数，给理论分析和实际应用带来挑战。

Method: 在每轮中，TS-SA仅使用最近的奖励构建后验近似，执行Langevin Monte Carlo更新，并通过随机逼近步骤对噪声提议进行时间平均。

Result: 建立了近乎最优的遗憾界，理论分析更简化直观，实验结果表明即使单步Langevin更新也能在bandit任务上显著优于现有方法。

Conclusion: TS-SA通过稳定后验目标分布实现了固定步长、统一收敛分析框架和通过时间平均改进的后验估计，解决了现有方法中的非平稳性问题。

Abstract: Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

</details>


### [558] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: 提出了一种名为"接种提示"的简单技术，通过在训练提示中明确请求不希望出现的行为，来防止模型学习这些不良行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有时在训练中会受到不完善的监督信号影响，导致奖励黑客行为和谄媚等不良行为。改进监督质量可能成本高昂或不可行，因此需要能够在训练信号不完美的情况下改善学习行为的方法。

Method: 使用接种提示技术，修改训练提示以明确请求不希望出现的行为。例如，为防止奖励黑客行为，在监督微调中修改提示，要求模型生成仅在提供测试用例上有效但在其他输入上失败的代码。

Result: 在四种设置中，接种提示减少了不良行为的学习，同时没有显著降低期望能力的学习。还发现那些在微调前更能引发不良行为的提示，在训练中作为接种提示更有效。

Conclusion: 接种提示是一种简单而有效的方法，可以控制模型从微调中的泛化方式，防止学习不良行为，同时不会显著干扰期望能力。

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


### [559] [Graph-Aware Diffusion for Signal Generation](https://arxiv.org/abs/2510.05036)
*Sergio Rozada,Vimal K. B.,Andrea Cavallo,Antonio G. Marques,Hadi Jamali-Rad,Elvin Isufi*

Main category: cs.LG

TL;DR: 提出了一种图感知生成扩散模型(GAD)，用于在给定图上生成图信号，通过时间扭曲系数解决漂移项指数衰减问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏通用性，要么忽略图结构，要么针对特定领域设计图感知机制。需要一种通用的图信号生成方法。

Method: 采用结合热方程的向前过程，引入时间扭曲系数来缓解漂移项的指数衰减，构建图感知生成扩散模型。

Result: 证明了向前动态收敛到具有图拉普拉斯参数化协方差的高斯马尔可夫随机场，向后动态可解释为图信号去噪问题序列。

Conclusion: GAD模型在合成数据、真实交通速度测量和温度传感器网络等任务中表现出优势。

Abstract: We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

</details>


### [560] [Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts](https://arxiv.org/abs/2510.05040)
*Jihoon Lee,Hoyeon Moon,Kevin Zhai,Arun Kumar Chithanar,Anit Kumar Sahu,Soummya Kar,Chul Lee,Souradip Chakraborty,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: 本文发现基于扩散的大语言模型(dLLMs)隐含地学习了半自回归专家的混合，不同生成顺序会展现不同的专门化行为。作者提出HEX方法，通过跨异构块调度进行集成，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理时使用固定的生成顺序，未能充分利用dLLMs中隐含的专家混合特性，导致性能下降。

Method: 提出HEX方法，通过多数投票机制集成不同块大小的生成路径，无需额外训练即可提升推理性能。

Result: 在GSM8K上准确率从24.72%提升至88.10%(3.56倍)，在MATH上从16.40%提升至40.00%，在ARC-C上从54.18%提升至87.80%，在TruthfulQA上从28.36%提升至57.46%。

Conclusion: HEX为dLLMs建立了新的测试时扩展范式，证明掩码执行顺序在推理性能中起着关键作用。

Abstract: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

</details>


### [561] [KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings](https://arxiv.org/abs/2510.05049)
*Ahmed Elhussein,Paul Meddeb,Abigail Newbury,Jeanne Mirone,Martin Stoll,Gamze Gursoy*

Main category: cs.LG

TL;DR: KEEP是一个结合知识图谱嵌入和临床数据自适应学习的医疗代码表示框架，能够同时保留本体关系并学习实证模式，支持多种下游应用。


<details>
  <summary>Details</summary>
Motivation: 现有医疗代码表示方法存在权衡：基于知识图谱的方法能捕捉形式化关系但缺乏真实世界模式，而数据驱动方法学习实证关联但忽略医学术语中的结构化知识。

Method: KEEP首先从知识图谱生成嵌入，然后通过正则化训练在患者记录上自适应整合实证模式，同时保留本体关系。无需任务特定的辅助或端到端训练。

Result: 在UK Biobank和MIMIC IV的评估中，KEEP在捕捉语义关系和预测临床结果方面优于传统方法和基于语言模型的方法。

Conclusion: KEEP有效弥合了知识图谱和数据驱动方法之间的差距，计算需求低，特别适合资源受限环境。

Abstract: Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

</details>


### [562] [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](https://arxiv.org/abs/2510.05054)
*Peter Van Katwyk,Karianne J. Bergen*

Main category: cs.LG

TL;DR: HybridFlow是一个模块化混合架构，通过结合条件掩码自回归归一化流和灵活的概率预测器，统一建模偶然不确定性和认知不确定性，在多个回归任务中优于现有不确定性量化框架。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化对于高风险机器学习应用的鲁棒性至关重要，需要统一建模偶然不确定性和认知不确定性。

Method: HybridFlow结合条件掩码自回归归一化流估计偶然不确定性，使用灵活概率预测器处理认知不确定性，支持与任何概率模型类集成。

Result: 在深度估计、回归基准测试和冰盖模拟等任务中，HybridFlow优于现有不确定性量化方法，其量化的不确定性经过校准且与模型误差更一致。

Conclusion: HybridFlow解决了贝叶斯深度学习中的关键挑战，在单一鲁棒框架中统一了偶然和认知不确定性建模。

Abstract: Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

</details>


### [563] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: 该研究通过分析编程学习平台Pencil Code中380万条编程交互轨迹，发现训练语言模型学习真实的学生编程轨迹可以更好地建模学生行为，预测编程特性，并帮助学生从错误中恢复。


<details>
  <summary>Details</summary>
Motivation: 研究编程过程中的交互轨迹可以揭示学生如何学习编程，包括探索行为、错误响应策略和个人风格选择，这些信息有助于理解学生的技能发展水平。

Method: 收集Pencil Code平台380万条真实编程交互轨迹，训练语言模型，并与仅基于最终程序或合成轨迹训练的模型进行比较，进行行为分析和探测分析。

Result: 基于真实轨迹训练的模型能更好地建模学生行为，可预测编程轨迹特性（如目标回溯、注释数量），并能通过引导代码生成帮助学生从错误中恢复，同时保持原风格。

Conclusion: 代码的许多特性反映了学生个体特征，基于编辑轨迹训练的模型更具可引导性，能更好地预测学生编程行为，并生成更准确的最终程序。

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


### [564] [ResCP: Reservoir Conformal Prediction for Time Series Forecasting](https://arxiv.org/abs/2510.05060)
*Roberto Neglia,Andrea Cini,Michael M. Bronstein,Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: 提出Reservoir Conformal Prediction (ResCP)，一种无需训练的时序数据保形预测方法，通过储层计算动态重加权保形分数，解决小样本和分布变化问题。


<details>
  <summary>Details</summary>
Motivation: 现有时序保形预测方法需要复杂模型捕捉时间依赖，在小样本下易失败且分布变化时需要昂贵重训练。

Method: 利用储层计算效率和表示学习能力，通过储层状态相似度分数动态重加权观测残差，考虑局部时间动态而不损失计算可扩展性。

Result: 在合理假设下证明ResCP达到渐近条件覆盖，并在多种预测任务中实证有效。

Conclusion: ResCP为时序数据提供了一种高效、无需训练的保形预测框架，解决了现有方法的局限性。

Abstract: Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

</details>


### [565] [Boomerang Distillation Enables Zero-Shot Model Size Interpolation](https://arxiv.org/abs/2510.05064)
*Sara Kangaslahti,Nihal V. Nayak,Jonathan Geuter,Marco Fumero,Francesco Locatello,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 提出了一种名为"回旋蒸馏"的新方法，通过从大模型蒸馏到小模型，然后重新整合教师层块来生成多种中间尺寸的零样本插值模型，显著降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要独立训练每个尺寸的模型，成本高昂且只能提供粗粒度的尺寸选择，无法满足多样化的内存和计算约束需求。

Method: 首先从大型教师模型蒸馏到小型学生模型，然后通过重新整合教师层块来逐步重构中间尺寸模型，无需额外训练。

Result: 生成的插值模型性能在学生和教师模型之间平滑扩展，通常匹配或超越同尺寸的预训练或蒸馏模型。

Conclusion: 回旋蒸馏提供了一种简单高效的方法来生成细粒度模型家族，大幅降低训练成本，同时实现跨部署环境的灵活适配。

Abstract: Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

</details>


### [566] [MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis](https://arxiv.org/abs/2510.05080)
*Yangyang Wang,Tayo Fabusuyi*

Main category: cs.LG

TL;DR: 提出了一种新的小区域估计框架，利用公开微数据和机器学习方法改进传统四步出行模型，实现高分辨率出行行为预测，为城市交通规划提供精细化支持。


<details>
  <summary>Details</summary>
Motivation: 传统四步出行模型在精细化程度和准确性方面存在局限，需要开发更精确的小区域出行行为估计方法来支持针对性交通干预措施。

Method: 使用公开可用的微数据文件和机器学习方法，为小地理区域生成代表性合成人口，并预测出行生成、分布、方式选择和路径分配。

Result: 使用ACS/PUMS通勤数据集验证表明，该框架比传统方法具有更高的准确性，能够提供更精细的出行行为洞察。

Conclusion: 该框架能够为微履约中心选址、路缘空间管理和包容性交通解决方案等政策应用提供支持，特别有助于服务弱势群体。

Abstract: This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

</details>


### [567] [TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](https://arxiv.org/abs/2510.05102)
*Cheng Xin,Fan Xu,Xin Ding,Jie Gao,Jiaxin Ding*

Main category: cs.LG

TL;DR: 提出了TopInG框架，利用持久同调识别持久性理由子图，通过理由过滤学习和拓扑差异约束来提升GNN的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释GNN方法在处理复杂多变的理由子图时面临挑战，阻碍了GNN在关键决策中的采用。

Method: 使用持久同调识别持久理由子图，采用理由过滤学习建模自回归生成过程，引入拓扑差异约束确保理由子图与无关部分的拓扑区分。

Result: 实验表明TopInG在预测准确性和解释质量上优于现有方法，能处理多样理由子图、平衡性能与可解释性、缓解虚假相关性。

Conclusion: TopInG通过拓扑框架有效提升了GNN的可解释性，为关键决策应用提供了可靠支持。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [568] [Repairing Leaks in Resource Wrappers](https://arxiv.org/abs/2510.03461)
*Sanjay Malakar,Michael D. Ernst,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: 本文提出了一种改进资源泄漏修复的方法，特别针对资源包装器的情况，通过集成资源管理规范推断、程序变换、字段包含分析和新的修复模式，显著提高了修复成功率。


<details>
  <summary>Details</summary>
Motivation: 现有资源泄漏修复方法只能处理硬编码库资源类型，无法有效处理资源包装器（将资源存储在字段中且需要自行关闭的情况），限制了修复范围。

Method: 1) 将资源管理规范推断集成到修复流程中；2) 通过程序变换使分析更有效；3) 使用字段包含分析推理资源生命周期；4) 引入新的修复模式处理非final字段中的资源。

Result: 在NJR基准测试套件中，先前工作修复了41%的资源泄漏警告，而本文实现的Arodnap修复了68%。

Conclusion: 通过综合运用规范推断、程序变换、字段分析和新的修复模式，显著提升了资源泄漏修复能力，特别是在处理资源包装器方面取得了重大进展。

Abstract: A resource leak occurs when a program fails to release a finite resource like
a socket, file descriptor or database connection. While sound static analysis
tools can detect all leaks, automatically repairing them remains challenging.
Prior work took the output of a detection tool and attempted to repair only
leaks from a hard-coded list of library resource types. That approach limits
the scope of repairable leaks: real-world code uses resource wrappers that
store a resource in a field and must themselves be closed. This paper makes
four key contributions to improve resource leak repair in the presence of
wrappers. (1) It integrates inference of resource management specifications
into the repair pipeline, enabling extant fixing approaches to reason about
wrappers. (2) It transforms programs into variants that are easier to analyze,
making inference, detection, and fixing tools more effective; for instance, it
makes detection tools report problems closer to the root cause, often in a
client of a resource wrapper rather than within the wrapper class itself. (3) A
novel field containment analysis reasons about resource lifetimes, enabling
repair of more leaks involving resources stored in fields. (4) It introduces a
new repair pattern and more precise reasoning to better handle resources stored
in non-final fields. Prior work fixed 41% of resource leak warnings in the NJR
benchmark suite; our implementation Arodnap fixes 68%.

</details>


### [569] [ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework](https://arxiv.org/abs/2510.03463)
*Vali Tawosi,Keshav Ramani,Salwa Alamir,Xiaomo Liu*

Main category: cs.SE

TL;DR: 提出了ALMAS框架，一个基于LLM的自主多智能体软件工程系统，能够遵循软件开发生命周期在敏捷团队中执行端到端任务。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体LLM系统主要关注代码实现等单一环节，但软件开发是多方面的复杂过程，需要覆盖整个软件开发生命周期。

Method: 设计ALMAS框架，将智能体与敏捷开发角色对齐，采用模块化设计，能够与人类开发者和开发环境无缝集成。

Result: 通过已发表工作和用例演示展示了框架进展，ALMAS能够无缝生成应用程序并添加新功能。

Conclusion: ALMAS为构建能够全面参与软件开发生命周期的自主LLM系统提供了可行路径，展示了在多智能体环境中端到端软件开发的潜力。

Abstract: Multi-agent Large Language Model (LLM) systems have been leading the way in
applied LLM research across a number of fields. One notable area is software
development, where researchers have advanced the automation of code
implementation, code testing, code maintenance, inter alia, using LLM agents.
However, software development is a multifaceted environment that extends beyond
just code. As such, a successful LLM system must factor in multiple stages of
the software development life-cycle (SDLC). In this paper, we propose a vision
for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,
which follows the above SDLC philosophy such that it may work within an agile
software development team to perform several tasks end-to-end. ALMAS aligns its
agents with agile roles, and can be used in a modular fashion to seamlessly
integrate with human developers and their development environment. We showcase
the progress towards ALMAS through our published works and a use case
demonstrating the framework, where ALMAS is able to seamlessly generate an
application and add a new feature.

</details>


### [570] [Relative Code Comprehensibility Prediction](https://arxiv.org/abs/2510.03474)
*Nadeeshan De Silva,Martin Kellogg,Oscar Chaparro*

Main category: cs.SE

TL;DR: 该论文提出使用相对可理解性预测模型来替代绝对可理解性预测，通过比较两个代码片段的相对可理解性来减少人类评估数据中的噪声影响，实验证明相对模型比绝对模型效果显著更好。


<details>
  <summary>Details</summary>
Motivation: 现有代码可理解性度量指标和机器学习模型与人类实际可理解性测量相关性差，主要原因是人类可理解性数据存在固有噪声，直接预测绝对可理解性会混淆模型。

Method: 提出训练模型预测两个代码片段的相对可理解性（哪个更容易理解），而不是单独预测每个片段的绝对可理解性。使用包含150个Java代码片段和12.5k个人类可理解性测量的数据集，比较绝对和相对预测模型与基准模型的性能。

Result: 绝对可理解性模型最多比基准模型提升33.4%，且经常表现不佳；相对可理解性模型显著更好，片段级和开发者级预测的平均改进分别为137.8%和74.7%。

Conclusion: 相对可理解性模型能更有效地从数据中学习，支持其在软件工程下游任务中的实际应用价值。

Abstract: Automatically predicting how difficult it is for humans to understand a code
snippet can assist developers in tasks like deciding when and where to
refactor. Despite many proposed code comprehensibility metrics, studies have
shown they often correlate poorly with actual measurements of human
comprehensibility. This has motivated the use of machine learning models to
predict human comprehensibility directly from code, but these models have also
shown limited accuracy.
  We argue that model inaccuracy stems from inherent noise in human
comprehensibility data, which confuses models trained to predict it directly.
To address this, we propose training models to predict the relative
comprehensibility of two code snippets - that is, predicting which snippet a
human would find easier to understand without predicting each snippet's
comprehensibility in isolation. This mitigates noise in predicting 'absolute'
comprehensibility measurements, but is still useful for downstream
software-engineering tasks like assessing whether refactoring improves or
hinders comprehensibility.
  We conducted a study to assess and compare the effectiveness of absolute and
relative code comprehensibility prediction via machine learning. We used a
dataset of 150 Java code snippets and 12.5k human comprehensibility
measurements from prior user studies, comparing the models' performance with
naive baselines (eg 'always predict the majority class'). Our findings indicate
that absolute comprehensibility models improve over the baselines by at most
33.4% and frequently underperform. In contrast, relative comprehensibility
models are substantially better, with average improvements of 137.8% and 74.7%
for snippet-wise and developer-wise prediction, respectively. These results
suggest that relative comprehensibility models learn more effectively from the
data, supporting their practical applicability for downstream SE tasks.

</details>


### [571] [LLM Agents for Automated Dependency Upgrades](https://arxiv.org/abs/2510.03480)
*Vali Tawosi,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: 提出一个基于LLM代理的框架，结合迁移文档自动推荐和应用代码更新，确保与新版本库的兼容性。


<details>
  <summary>Details</summary>
Motivation: 随着代码库扩展，库依赖会过时需要更新，但更新可能引入破坏性变更，需要大量开发时间维护。

Method: 使用LLM代理框架（包括摘要代理、控制代理和代码代理），结合迁移文档自动定位库使用并实施修复。

Result: 在工业用例中测试，相比现有方法使用更少的token，达到71.4%的精确度。

Conclusion: 该方法在效率和效果上优于现有方法，能有效自动化库更新过程。

Abstract: As a codebase expands over time, its library dependencies can become outdated
and require updates to maintain innovation and security. However, updating a
library can introduce breaking changes in the code, necessitating significant
developer time for maintenance. To address this, we introduce a framework of
LLM agents to be used in combination with migration documentation to
automatically recommend and apply code updates and ensure compatibility with
new versions. Our solution can automatically localize updated library usages in
live Java codebases and implement recommended fixes in a user-friendly manner.
The system architecture consists of multiple key components: a Summary Agent,
Control Agent, and Code Agent. To validate our approach, we apply the framework
on an industrial use case by which we create three synthetic code repositories
with major Upgrade changes and benchmark our approach against state-of-the-art
methods. Results show that our approach not only performs upgrades using fewer
tokens across all cases but also achieves a precision of 71.4%, highlighting
its efficiency and effectiveness compared to state-of-the-art methods.

</details>


### [572] [AgentHub: A Research Agenda for Agent Sharing Infrastructure](https://arxiv.org/abs/2510.03495)
*Erik Pautsch,Tanmay Singla,Wenxin Jiang,Huiyun Peng,Behnaz Hassanshahi,Konstantin Läufer,George K. Thiruvathukal,James C. Davis*

Main category: cs.SE

TL;DR: 提出AgentHub研究议程，旨在解决LLM智能体生态系统中发现、评估和治理基础设施碎片化的问题，推动构建可靠、可扩展的智能体生态系统。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体基础设施碎片化，缺乏像npm和Hugging Face那样成熟的生态系统，现有工作仅关注分发、命名或协议协商等狭窄领域，需要更全面的软件工程视角来改善开源分发和重用。

Method: 提出AgentHub研究议程，通过界定能力清晰度、生命周期透明度、互操作性、治理、安全性和工作流集成等关键挑战，为构建可靠智能体生态系统制定社区范围的路线图。

Result: 构建了一个研究框架，明确了智能体生态系统发展的关键问题和挑战，为未来研究提供了系统性的指导方向。

Conclusion: AgentHub愿景是让智能体能够像今天的软件库一样被无缝共享、信任和组合，推动智能体生态系统的成熟发展。

Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for
discovering, evaluating, and governing them remains fragmented compared to
mature ecosystems like software package registries (e.g., npm) and model hubs
(e.g., Hugging Face). Recent research and engineering works have begun to
consider the requisite infrastructure, but so far they focus narrowly -- on
distribution, naming, or protocol negotiation. However, considering broader
software engineering requirements would improve open-source distribution and
ease reuse. We therefore propose AgentHub, a research agenda for agent sharing.
By framing the key challenges of capability clarity, lifecycle transparency,
interoperability, governance, security, and workflow integration, AgentHub
charts a community-wide agenda for building reliable and scalable agent
ecosystems. Our vision is a future where agents can be shared, trusted, and
composed as seamlessly as today's software libraries.

</details>


### [573] [REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement](https://arxiv.org/abs/2510.03588)
*Anvith Pabba,Simin Chen,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: 提出了Refine框架，通过系统化地将部分正确的草稿补丁转化为正确补丁，解决了LLM在自动程序修复中因代码上下文理解不足和测试套件依赖导致的补丁质量问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动程序修复技术由于对代码上下文理解有限和过度依赖不完整的测试套件，经常生成部分正确的草稿补丁，无法完全修复bug或过度拟合测试用例。

Method: Refine框架通过三个关键步骤：消除问题和代码上下文的歧义、通过测试时扩展多样化补丁候选、通过LLM驱动的代码审查过程聚合部分修复。

Result: 在SWE-Bench Lite基准测试中，Refine在基于工作流的方法中达到最先进结果，将AutoCodeRover性能提升14.67%，达到51.67%的得分。在多个APR系统中集成时平均提升14%。

Conclusion: 精炼是当前APR流程中缺失的关键组件，代理协作在缩小近乎正确和正确补丁之间的差距方面具有巨大潜力。

Abstract: Large Language Models (LLMs) have recently shown strong potential in
automatic program repair (APR), especially in repository-level settings where
the goal is to generate patches based on natural language issue descriptions,
large codebases, and regression tests. However, despite their promise, current
LLM-based APR techniques often struggle to produce correct fixes due to limited
understanding of code context and over-reliance on incomplete test suites. As a
result, they frequently generate Draft Patches-partially correct patches that
either incompletely address the bug or overfit to the test cases. In this work,
we propose a novel patch refinement framework, Refine, that systematically
transforms Draft Patches into correct ones. Refine addresses three key
challenges: disambiguating vague issue and code context, diversifying patch
candidates through test-time scaling, and aggregating partial fixes via an
LLM-powered code review process. We implement Refine as a general refinement
module that can be integrated into both open-agent-based and workflow-based APR
systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine
achieves state-of-the-art results among workflow-based approaches and
approaches the best-known performance across all APR categories. Specifically,
Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of
51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine
improves the resolution rate by 12.2%, and when integrated across multiple APR
systems, it yields an average improvement of 14%-demonstrating its broad
effectiveness and generalizability. These results highlight the effectiveness
of refinement as a missing component in current APR pipelines and the potential
of agentic collaboration in closing the gap between near-correct and correct
patches. We also open source our code.

</details>


### [574] [Generating High-Level Test Cases from Requirements using LLM: An Industry Study](https://arxiv.org/abs/2510.03641)
*Satoshi Masuda,Satoshi Kouzawa,Kyousuke Sezai,Hidetoshi Suhara,Yasuaki Hiruta,Kunihiro Kudou*

Main category: cs.SE

TL;DR: 提出了一种仅使用提示词从需求文档自动生成高层次测试用例的方法，无需创建RAG系统，在蓝牙和Mozilla数据集上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 当前从需求文档生成高层次测试用例主要依赖人工，业界对使用LLM自动生成测试用例有强烈需求。现有RAG方法需要针对每个应用定制，工作量大且缺乏通用方法。

Method: 首先将需求文档输入LLM生成对应的测试设计技术，然后为每种测试设计技术生成高层次测试用例。同时验证了基于语义相似度的评估方法。

Result: 在蓝牙和Mozilla数据集上的实验结果显示，宏召回率分别达到0.81和0.37，表明该方法在实际应用中具有可行性。

Conclusion: 提出的方法无需创建RAG系统即可生成高层次测试用例，为更广泛的需求文档提供了通用的自动化测试用例生成方案。

Abstract: Currently, generating high-level test cases described in natural language
from requirement documents is performed manually. In the industry, including
companies specializing in software testing, there is a significant demand for
the automatic generation of high-level test cases from requirement documents
using Large Language Models (LLMs). Efforts to utilize LLMs for requirement
analysis are underway. In some cases, retrieval-augmented generation (RAG) is
employed for generating high-level test cases using LLMs. However, in practical
applications, it is necessary to create a RAG tailored to the knowledge system
of each specific application, which is labor-intensive. Moreover, when applying
high-level test case generation as a prompt, there is no established method for
instructing the generation of high-level test cases at a level applicable to
other specifications without using RAG. It is required to establish a method
for the automatic generation of high-level test cases that can be generalized
across a wider range of requirement documents. In this paper, we propose a
method for generating high-level (GHL) test cases from requirement documents
using only prompts, without creating RAGs. In the proposed method, first, the
requirement document is input into the LLM to generate test design techniques
corresponding to the requirement document. Then, high-level test cases are
generated for each of the generated test design techniques. Furthermore, we
verify an evaluation method based on semantic similarity of the generated
high-level test cases. In the experiments, we confirmed the method using
datasets from Bluetooth and Mozilla, where requirement documents and high-level
test cases are available, achieving macro-recall measurement of 0.81 and 0.37,
respectively. We believe that the method is feasible for practical application
in generating high-level test cases without using RAG.

</details>


### [575] [Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems](https://arxiv.org/abs/2510.03712)
*Jahidul Arafat,Kh. M. Moniruzzaman,Shamim Hossain,Fariha Tasmin,Kamrujjaman,Ahsan Habib Tareq*

Main category: cs.SE

TL;DR: 提出了首个系统性检测、预防和优化分布式系统中潜在风险的框架，通过数学建模、智能扰动测试和风险感知性能优化，将可靠性工程从被动事件管理转变为主动风险感知优化。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统采用激进的优化策略，这些策略会产生潜在风险——当优化失败时，卓越性能掩盖了灾难性脆弱性。当前可靠性工程主要关注被动事件响应，而非主动检测优化引发的漏洞。

Method: 开发了三个集成系统：HYDRA采用六种优化感知扰动策略，RAVEN提供持续生产监控，APEX实现风险感知优化。引入潜在风险指数(LRI)进行预测性风险评估。

Result: 在三个测试环境中验证，LRI与事件严重性强相关(r=0.863)，HYDRA风险发现率达89.7%，RAVEN精度92.9%、召回率93.8%，APEX在保持96.6%基准性能的同时降低59.2%潜在风险。生产部署24周平均恢复时间减少69.1%，事件严重性降低78.6%，预防81起事件，年均收益144万美元，投资回报期3.2个月。

Conclusion: 该框架成功将可靠性工程从被动事件管理转变为主动风险感知优化，显著提高了系统可靠性并降低了运营风险。

Abstract: Modern distributed systems employ aggressive optimization strategies that
create latent risks - hidden vulnerabilities where exceptional performance
masks catastrophic fragility when optimizations fail. Cache layers achieving
99% hit rates can obscure database bottlenecks until cache failures trigger
100x load amplification and cascading collapse. Current reliability engineering
focuses on reactive incident response rather than proactive detection of
optimization-induced vulnerabilities. This paper presents the first
comprehensive framework for systematic latent risk detection, prevention, and
optimization through integrated mathematical modeling, intelligent perturbation
testing, and risk-aware performance optimization. We introduce the Latent Risk
Index (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),
enabling predictive risk assessment. Our framework integrates three systems:
HYDRA employing six optimization-aware perturbation strategies achieving 89.7%
risk discovery rates, RAVEN providing continuous production monitoring with
92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling
risk-aware optimization maintaining 96.6% baseline performance while reducing
latent risks by 59.2%. Evaluation across three testbed environments
demonstrates strong statistical validation with large effect sizes (Cohen
d>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24
weeks shows 69.1% mean time to recovery reduction, 78.6% incident severity
reduction, and 81 prevented incidents generating 1.44M USD average annual
benefits with 3.2-month ROI. Our approach transforms reliability engineering
from reactive incident management to proactive risk-aware optimization.

</details>


### [576] [APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents](https://arxiv.org/abs/2510.03743)
*Zachary Eberhart,Collin McMillan*

Main category: cs.SE

TL;DR: APIDA-Chat是一个开源管道，将符号对话行为脚本转换为真实的API搜索对话，使用轻量级模型生成廉价训练数据，解决小众或专有库对话数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型助手在解释流行API时表现良好，但在处理小众或专有库时表现不佳，因为用于微调的多轮对话数据稀缺。

Method: 两阶段方法：第一阶段使用传统对话规划器与教师LLM合成黄金对话集，第二阶段使用微调后的学生模型与相同规划器生成新对话，无需向外部服务暴露源代码。

Result: 微调后的学生模型BLEU从0.38提升到0.50，BERTScore从0.88提升到0.91，同时完全在单个消费级GPU上运行。

Conclusion: APIDA-Chat提供了一个模块化、开源的方法，可作为未来工作的保守基线，有效解决小众API对话数据生成问题。

Abstract: Large-language-model assistants are suitable for explaining popular APIs, yet
they falter on niche or proprietary libraries because the multi-turn dialogue
data needed for fine-tuning are scarce. We present APIDA-Chat, an open-source
pipeline that converts symbolic dialogue-act "scripts" into realistic,
domain-grounded API Search conversations using a lightweight model for
inexpensive training data generation. Phase I pairs a legacy dialogue planner
with a high-capability teacher LLM (o4-mini) to synthesize a "gold set" of
realized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on
this corpus. Phase II drops the teacher and reuses the same planner with the
fine-tuned model, allowing rapid, low-cost synthesis of new dialogues without
exposing source code to external services. The fine-tuned student improves BLEU
from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while
running entirely on a single consumer GPU. All components are modular and
publicly released to serve as a conservative baseline for future work.
APIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a
video demo is available at https://youtu.be/YqmZBHyGbPs .

</details>


### [577] [Code4MeV2: a Research-oriented Code-completion Platform](https://arxiv.org/abs/2510.03755)
*Roham Koohestani,Parham Bateni,Aydin Ebrahimi,Behdad Etezadi,Kiarash Karimi,Maliheh Izadi*

Main category: cs.SE

TL;DR: Code4MeV2是一个开源代码补全插件，采用客户端-服务器架构，支持内联代码补全和上下文感知聊天助手，提供模块化数据收集框架，性能与工业级工具相当。


<details>
  <summary>Details</summary>
Motivation: AI代码补全工具的用户交互数据通常被大公司垄断，阻碍学术研究。需要开发开源工具来支持可重复研究和规模化数据分析。

Method: 开发基于JetBrains IDE的Code4MeV2插件，采用客户端-服务器架构，包含内联代码补全和聊天助手功能，提供模块化数据收集框架。

Result: 工具平均延迟200ms，性能与工业级工具相当。通过专家评估和8人用户研究验证了其信息性和实用性。

Conclusion: Code4MeV2为研究社区提供了开源解决方案，支持可控的数据收集和人类-AI交互研究，邀请社区采用和贡献。

Abstract: The adoption of AI-powered code completion tools in software development has
increased substantially, yet the user interaction data produced by these
systems remain proprietary within large corporations. This creates a barrier
for the academic community, as researchers must often develop dedicated
platforms to conduct studies on human--AI interaction, making reproducible
research and large-scale data analysis impractical. In this work, we introduce
Code4MeV2, a research-oriented, open-source code completion plugin for
JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a
client--server architecture and features inline code completion and a
context-aware chat assistant. Its core contribution is a modular and
transparent data collection framework that gives researchers fine-grained
control over telemetry and context gathering. Code4MeV2 achieves
industry-comparable performance in terms of code completion, with an average
latency of 200~ms. We assess our tool through a combination of an expert
evaluation and a user study with eight participants. Feedback from both
researchers and daily users highlights its informativeness and usefulness. We
invite the community to adopt and contribute to this tool. More information
about the tool can be found at https://app.code4me.me.

</details>


### [578] [A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt](https://arxiv.org/abs/2510.03802)
*Gilberto Recupito,Vincenzo De Martino,Dario Di Nucci,Fabio Palomba*

Main category: cs.SE

TL;DR: 该研究分析了深度学习系统中自认技术债务的生命周期，发现DL-specific SATD主要在项目开发早期和中期引入，训练和硬件阶段的技术债务持续时间最长。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统的快速采用带来了独特的软件质量挑战，特别是自认技术债务对系统可维护性的影响。目前对DL-specific SATD的生命周期研究不足，需要了解开发者如何引入、承认和解决这些技术债务。

Method: 使用软件仓库挖掘技术，分析了40个机器学习项目中的185个DL-specific SATD实例，通过项目提交历史跟踪技术债务的引入和持久性。

Result: DL-specific SATD主要在项目开发早期和中期引入，训练和硬件阶段的技术债务持续时间最长。开发者在功能实现和bug修复时更频繁地引入DL-specific SATD。

Conclusion: 需要针对DL-specific SATD制定专门的管理策略，通过理解其时间特性和演化规律，开发者可以在关键阶段优先干预，提高系统的可维护性和质量。

Abstract: The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized
software development, driving innovation across various domains. However, these
systems also introduce unique challenges, particularly in maintaining software
quality and performance. Among these challenges, Self-Admitted Technical Debt
(SATD) has emerged as a growing concern, significantly impacting the
maintainability and overall quality of ML and DL-enabled systems. Despite its
critical implications, the lifecycle of DL-specific SATD, how developers
introduce, acknowledge, and address it over time-remains underexplored. This
study presents a preliminary analysis of the persistence and lifecycle of
DL-specific SATD in DL-enabled systems. The purpose of this project is to
uncover the patterns of SATD introduction, recognition, and durability during
the development life cycle, providing information on how to manage these
issues. Using mining software repository techniques, we examined 40 ML
projects, focusing on 185 DL-specific SATD instances. The analysis tracked the
introduction and persistence of SATD instances through project commit histories
to assess their lifecycle and developer actions. The findings indicate that
DL-specific SATD is predominantly introduced during the early and middle stages
of project development. Training and Hardware phases showed the longest SATD
durations, highlighting critical areas where debt accumulates and persists.
Additionally, developers introduce DL-specific SATD more frequently during
feature implementation and bug fixes. This study emphasizes the need for
targeted DL-specific SATD management strategies in DL-enabled systems to
mitigate its impact. By understanding the temporal characteristics and
evolution of DL-specific SATD, developers can prioritize interventions at
critical stages to improve the maintainability and quality of the system.

</details>


### [579] [Smart Paste: Automatically Fixing Copy/Paste for Google Developers](https://arxiv.org/abs/2510.03843)
*Vincent Nguyen,Guilherme Herzog,José Cambronero,Marcus Revaj,Aditya Kini,Alexander Frömmgen,Maxim Tabachnyk*

Main category: cs.SE

TL;DR: 开发了Smart Paste IDE功能，通过深度学习预测粘贴代码后的编辑需求，在Google内部部署后获得45%接受率，占公司代码编写量的1%以上。


<details>
  <summary>Details</summary>
Motivation: 手动编辑粘贴代码是开发者的长期痛点，Google内部数据显示代码粘贴频率是手动输入的4倍，且粘贴后经常需要后续编辑。

Method: 迭代开发和扩展Smart Paste IDE功能，涵盖用户体验、系统集成和模型能力，使用深度学习预测粘贴后的编辑需求。

Result: 部署后获得压倒性积极反馈，接受率达到45%，在Google企业规模下，这些接受的建议占公司所有代码编写量的1%以上。

Conclusion: Smart Paste成功解决了代码粘贴后的编辑痛点，为AI从业者提供了从用户体验到系统集成的整体功能开发指南。

Abstract: Manually editing pasted code is a long-standing developer pain point. In
internal software development at Google, we observe that code is pasted 4 times
more often than it is manually typed. These paste actions frequently require
follow-up edits, ranging from simple reformatting and renaming to more complex
style adjustments and cross-language translations. Prior work has shown deep
learning can be used to predict these edits. In this work, we show how to
iteratively develop and scale Smart Paste, an IDE feature for post-paste edit
suggestions, to Google's development environment. This experience can serve as
a guide for AI practitioners on a holistic approach to feature development,
covering user experience, system integration, and model capabilities. Since
deployment, Smart Paste has had overwhelmingly positive feedback with a 45%
acceptance rate. At Google's enterprise scale, these accepted suggestions
account substantially for over 1% of all code written company-wide.

</details>


### [580] [Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework](https://arxiv.org/abs/2510.03862)
*Nathalia Nascimento,Everton Guimaraes,Paulo Alencar*

Main category: cs.SE

TL;DR: 提出一个用于设计和报告基于LLM的代码生成实证研究的理论框架，以解决当前评估缺乏标准化的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面具有变革潜力，但现有实证评估缺乏标准化，研究目标、任务和指标差异很大，限制了可比性和可重复性。

Method: 基于作者先前经验和近期研究的比较分析，构建围绕问题来源、质量属性和指标等核心组件组织的理论框架。

Result: 通过代表性案例映射展示了框架的适用性，并确定了改进机会。

Conclusion: 该框架为标准化LLM评估提供了结构化基础，未来计划将其发展为更成熟的工具，在软件工程环境中实现标准化评估。

Abstract: The rise of large language models (LLMs) has introduced transformative
potential in automated code generation, addressing a wide range of software
engineering challenges. However, empirical evaluation of LLM-based code
generation lacks standardization, with studies varying widely in goals, tasks,
and metrics, which limits comparability and reproducibility. In this paper, we
propose a theoretical framework for designing and reporting empirical studies
on LLM-based code generation. The framework is grounded in both our prior
experience conducting such experiments and a comparative analysis of key
similarities and differences among recent studies. It organizes evaluation
around core components such as problem sources, quality attributes, and
metrics, supporting structured and systematic experimentation. We demonstrate
its applicability through representative case mappings and identify
opportunities for refinement. Looking forward, we plan to evolve the framework
into a more robust and mature tool for standardizing LLM evaluation across
software engineering contexts.

</details>


### [581] [Adversarial Agent Collaboration for C to Rust Translation](https://arxiv.org/abs/2510.03879)
*Tianyu Li,Ruishi Li,Bo Wang,Brandon Paulsen,Umang Mathur,Prateek Saxena*

Main category: cs.SE

TL;DR: ACToR是一个基于LLM代理的C到Rust翻译器，采用生成器-判别器对抗机制，能够自动翻译大型C代码库（平均485行代码）并达到90%以上的测试通过率。


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust翻译方法在大型代码库（>500行）上表现不佳，因为它们依赖复杂的程序分析，经常失败。

Method: 采用对抗性方法，生成器代理迭代生成和优化Rust翻译以通过测试，判别器代理寻找新的失败测试，两者协作改进翻译质量。

Result: 成功翻译了63个真实世界命令行工具，平均代码量485行，测试通过率超过90%，比非对抗性方法提升18.9%的正确性。

Conclusion: ACToR是首个能够可靠翻译此规模C程序的系统，无需人工干预即可实现高质量的C到Rust翻译。

Abstract: Translating C to memory-safe languages, like Rust, prevents critical memory
safety vulnerabilities that are prevalent in legacy C software. Existing
approaches for C to safe Rust translation, including LLM-assisted ones, do not
generalize on larger (> 500 LoC) C codebases because they depend on complex
program analyses that frequently break. In this work, we present ACToR
(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired
by GANs, ACToR pits a generator agent against a discriminator agent, which
collaborate to iteratively generate a Rust translation. On each iteration, the
translator agent synthesizes and refines a Rust translation to pass an existing
suite of tests, and then the discriminator agent finds new failing tests. We
demonstrate that ACToR translates all of the 63 real-world command line
utilities considered in our benchmarks, which have an average size of 485 lines
of code, and it achieves over 90% test pass rate with zero human intervention.
To our knowledge, it is the first such system that reliably translates C
programs of this scale. Furthermore, ACToR improves translation correctness by
up to 18.9% compared to baseline, non-adversarial approaches.

</details>


### [582] [Rethinking Services in the Quantum Age: The SOQ Paradigm](https://arxiv.org/abs/2510.03890)
*Jose Garcia-Alonso,Enrique Moguel,Jaime Alvarado-Valiente,Javier Romero-Alvarez,Álvaro M. Aparicio-Morales,Juan M. Murillo,Francisco Javier Cavero,Adrián Romero-Flores,Alfonso E. Marquez-Chamorro,José Antonio Parejo,Antonio Ruiz-Cortés,Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: cs.SE

TL;DR: 本文提出服务导向量子计算（SOQ）新范式，将量子软件系统重新构想为自主、可组合、可互操作的服务实体，以解决量子计算集成到现实软件系统时的硬件脆弱性、平台异构性和软件工程实践缺失等问题。


<details>
  <summary>Details</summary>
Motivation: 量子计算从理论承诺向实际实现快速推进，但在优化、模拟、密码学和机器学习等任务中，其集成到现实软件系统仍受硬件脆弱性、平台异构性和缺乏稳健软件工程实践的限制。

Method: 通过经典服务导向计算的视角重新构想量子软件系统，将量子服务定位为自主、可组合、可互操作的实体，定义SOQ的基本原则，提出支持其实现的分层技术栈。

Result: 提出了SOQ范式及其技术栈，识别了互操作性、混合性、定价模型、服务抽象和劳动力发展等关键研究挑战。

Conclusion: SOQ方法对于量子技术进步至关重要，因为它能够实现量子计算到现实软件系统的可扩展、模块化和可互操作集成，无需依赖专用经典环境来管理量子处理。

Abstract: Quantum computing is rapidly progressing from theoretical promise to
practical implementation, offering significant computational advantages for
tasks in optimization, simulation, cryptography, and machine learning. However,
its integration into real-world software systems remains constrained by
hardware fragility, platform heterogeneity, and the absence of robust software
engineering practices. This paper introduces Service-Oriented Quantum (SOQ), a
novel paradigm that reimagines quantum software systems through the lens of
classical service-oriented computing. Unlike prior approaches such as Quantum
Service-Oriented Computing (QSOC), which treat quantum capabilities as
auxiliary components within classical systems, SOQ positions quantum services
as autonomous, composable, and interoperable entities. We define the
foundational principles of SOQ, propose a layered technology stack to support
its realization, and identify the key research and engineering challenges that
must be addressed, including interoperability, hybridity, pricing models,
service abstractions, and workforce development. This approach is of vital
importance for the advancement of quantum technology because it enables the
scalable, modular, and interoperable integration of quantum computing into
real-world software systems independently and without relying on a dedicated
classical environment to manage quantum processing.

</details>


### [583] [A Brief History of the Waterfall Model: Past, Present, and Future](https://arxiv.org/abs/2510.03894)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: 本文对瀑布模型进行了历史回顾和批判性分析，探讨了其起源、演变以及在当代软件开发中的持续相关性。


<details>
  <summary>Details</summary>
Motivation: 重新评估瀑布模型的历史意义和当代价值，分析其从独立框架到现代混合方法组成部分的转变过程。

Method: 基于学术文献的综合分析，追溯瀑布模型的概念起源、Royce的形式化定义，以及数十年来的行业采用和批判历程。

Result: 瀑布模型虽然因其僵化、缺陷和高失败率而受到批评，但在特定领域仍然存在，其原则继续影响着结合传统和敏捷方法的现代混合开发框架。

Conclusion: 瀑布模型仍然具有现实意义，不是作为过去的遗迹，而是作为情境感知开发策略的一部分。其持久相关性在于适应性，通过认识其局限性和优势，从业者可以在多样化开发环境中做出更明智的方法选择和流程设计决策。

Abstract: The waterfall model, one of the earliest software development methodologies,
has played a foundational role in shaping contemporary software engineering
practices. This paper provides a historical and critical overview of the model,
tracing its conceptual origins in software engineering, its formalization by
Royce, and its evolution through decades of industry adoption and critique.
Although often criticized for its rigidity, shortcomings, and high failure
rates, the waterfall model persists in specific domains. Its principles
continue to influence contemporary hybrid development frameworks that combine
traditional and agile methods. Drawing on a range of scholarly sources, this
study synthesizes key developments in the perception and application of the
waterfall model. The analysis highlights how the model has shifted from a
standalone framework to a component within modern hybrid methodologies. By
revisiting its origins, assessing its present utility, and examining its role
in contemporary development practices, this paper argues that the waterfall
model remains relevant, not as a relic of the past but as part of context-aware
development strategies. The paper contends that the model's enduring relevance
lies in its adaptability. By recognizing both its limitations and its
strengths, and by understanding its integration within hybrid approaches,
practitioners can make more informed decisions about methodology selection and
process design in diverse development environments.

</details>


### [584] [Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code](https://arxiv.org/abs/2510.03902)
*Rana Nameer Hussain Khan,Dawood Wasif,Jin-Hee Cho,Ali Butt*

Main category: cs.SE

TL;DR: MACOG是一个基于多智能体LLM的架构，用于生成基础设施即代码(IaC)，通过分解任务为多个专业智能体协作，显著提升了代码质量和合规性。


<details>
  <summary>Details</summary>
Motivation: 传统单次生成的LLM方法在IaC生成中常产生语法错误、策略违规和不可扩展的设计，需要更系统化的解决方案。

Method: 采用多智能体架构，包含架构师、提供商协调器、工程师、审查员、安全证明者、成本容量规划师、DevOps和内存策展人等专业智能体，通过共享黑板和有限状态协调器层进行协作。

Result: 在IaC-Eval基准测试中表现优异，GPT-5从54.90提升到74.02，Gemini-2.5 Pro从43.56提升到60.13，同时在BLEU、CodeBERTScore和LLM-judge指标上均有提升。

Conclusion: MACOG通过多智能体协作和约束解码、部署反馈等机制，能够生成语法正确、策略合规且语义连贯的Terraform配置。

Abstract: The increasing complexity of cloud-native infrastructure has made
Infrastructure-as-Code (IaC) essential for reproducible and scalable
deployments. While large language models (LLMs) have shown promise in
generating IaC snippets from natural language prompts, their monolithic,
single-pass generation approach often results in syntactic errors, policy
violations, and unscalable designs. In this paper, we propose MACOG
(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based
architecture for IaC generation that decomposes the task into modular subtasks
handled by specialized agents: Architect, Provider Harmonizer, Engineer,
Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory
Curator. The agents interact via a shared-blackboard, finite-state orchestrator
layer, and collectively produce Terraform configurations that are not only
syntactically valid but also policy-compliant and semantically coherent. To
ensure infrastructure correctness and governance, we incorporate Terraform Plan
for execution validation and Open Policy Agent (OPA) for customizable policy
enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the
top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02
and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,
CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and
deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,
respectively.

</details>


### [585] [Refactoring with LLMs: Bridging Human Expertise and Machine Understanding](https://arxiv.org/abs/2510.03914)
*Yonnel Chen Kuang Piao,Jean Carlors Paul,Leuson Da Silva,Arghavan Moradi Dakhel,Mohammad Hamdaqa,Foutse Khomh*

Main category: cs.SE

TL;DR: 本研究探索如何利用基于人类最佳实践指南的指令策略来增强大型语言模型(LLMs)执行多样化代码重构任务的能力。


<details>
  <summary>Details</summary>
Motivation: 代码重构虽然对软件质量至关重要，但开发者常因时间、精力和资源消耗而忽视重构。现有自动化重构工具支持的场景有限，需要探索新的自动化方法。

Method: 基于Martin Fowler的重构指南，为61种重构类型设计多种指令策略，利用GPT-mini和DeepSeek-V3等先进LLMs的指令遵循和代码理解能力，在基准示例和GitHub真实代码片段上进行评估。

Result: 基于Fowler指南的指令设计使LLMs能够成功执行所有基准重构类型，并在真实环境中保持程序语义完整性。规则型指令在特定场景下表现更好，而允许模型关注重构整体目标而非固定转换类型能带来更大代码质量提升。

Conclusion: 基于人类最佳实践指南的指令策略能有效增强LLMs的自动化重构能力，为代码重构自动化提供了有前景的新途径。

Abstract: Code refactoring is a fundamental software engineering practice aimed at
improving code quality and maintainability. Despite its importance, developers
often neglect refactoring due to the significant time, effort, and resources it
requires, as well as the lack of immediate functional rewards. Although several
automated refactoring tools have been proposed, they remain limited in
supporting a broad spectrum of refactoring types. In this study, we explore
whether instruction strategies inspired by human best-practice guidelines can
enhance the ability of Large Language Models (LLMs) to perform diverse
refactoring tasks automatically. Leveraging the instruction-following and code
comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and
DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design
multiple instruction strategies that encode motivations, procedural steps, and
transformation objectives for 61 well-known refactoring types. We evaluate
these strategies on benchmark examples and real-world code snippets from GitHub
projects. Our results show that instruction designs grounded in Fowler's
guidelines enable LLMs to successfully perform all benchmark refactoring types
and preserve program semantics in real-world settings, an essential criterion
for effective refactoring. Moreover, while descriptive instructions are more
interpretable to humans, our results show that rule-based instructions often
lead to better performance in specific scenarios. Interestingly, allowing
models to focus on the overall goal of refactoring, rather than prescribing a
fixed transformation type, can yield even greater improvements in code quality.

</details>


### [586] [Why Does the Engineering Manager Still Exist in Agile Software Development?](https://arxiv.org/abs/2510.03920)
*Ravi Kalluri*

Main category: cs.SE

TL;DR: 尽管敏捷方法强调去中心化决策和团队自主性，但工程经理在敏捷软件组织中仍然存在。本文通过多维框架探讨了这一现象，提出了一个协调敏捷原则与管理必要性的概念模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解释敏捷软件开发中工程经理持续存在的明显悖论，尽管敏捷理论主张消除管理层次结构。

Method: 采用系统性文献综述方法，辅以案例研究，构建包含历史背景、理论张力、组织现实等多维度的分析框架。

Result: 研究发现传统管理功能在敏捷环境中持续存在，提出了一个概念模型来协调敏捷原则与管理必要性。

Conclusion: 结论是工程经理在敏捷组织中具有持续价值，为从业者、研究人员和工具设计者提供了实践指导，并讨论了领导力发展、工具整合和未来研究的影响。

Abstract: Although Agile methodologies emphasize decentralized decision-making and team
autonomy, engineering managers continue to be employed in Agile software
organizations. This apparent paradox suggests that traditional managerial
functions persist despite the theoretical displacement of managerial hierarchy
in Agile. This paper explores the persistence of engineering managers through a
multidimensional framework encompassing historical context, theoretical
tensions, organizational realities, empirical evidence, evolving managerial
roles, and practical implications. A systematic literature review underpins our
multifaceted analysis, supplemented by illustrative case studies. We conclude
by proposing a conceptual model that reconciles Agile principles with
managerial necessity, offering guidance for practitioners, researchers, and
tool designers. Implications for leadership development, tool integration, and
future research are discussed.

</details>


### [587] [Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework](https://arxiv.org/abs/2510.04078)
*Han Hu,Wei Minn,Yonghui Liu,Jiakun Liu,Ferdian Thung,Terry Yue Zhuo,Lwin Khin Shar,Debin Gao,David Lo*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型(LLMs)的新方法，用于系统性地分析Android框架中的API权限映射关系，解决了现有静态和动态分析方法在适应性和代码覆盖率方面的不足。


<details>
  <summary>Details</summary>
Motivation: Android官方API文档在权限说明方面存在不精确和不完整的问题，导致开发者难以准确判断所需权限，可能引发安全漏洞和应用故障。现有分析方法对Android SDK更新适应性差、代码覆盖率有限，且在复杂代码库中容易遗漏重要的API-权限映射。

Method: 采用大语言模型(LLMs)进行系统性分析，结合双重角色提示策略和API驱动的代码生成方法，开发了相应的工具来发现API-权限映射关系。

Result: 实验结果显示，该工具在Android 6、7和10版本中分别识别出2,234、3,552和4,576个API-权限映射，显著优于现有基线方法。

Conclusion: 基于LLM的方法能够有效识别Android框架中的API权限映射，为开发者提供更准确的权限指导，同时揭示了官方SDK文档在权限说明方面的不完整性。

Abstract: The permission mechanism in the Android Framework is integral to safeguarding
the privacy of users by managing users' and processes' access to sensitive
resources and operations. As such, developers need to be equipped with an
in-depth understanding of API permissions to build robust Android apps.
Unfortunately, the official API documentation by Android chronically suffers
from imprecision and incompleteness, causing developers to spend significant
effort to accurately discern necessary permissions. This potentially leads to
incorrect permission declarations in Android app development, potentially
resulting in security violations and app failures. Recent efforts in improving
permission specification primarily leverage static and dynamic code analyses to
uncover API-permission mappings within the Android framework. Yet, these
methodologies encounter substantial shortcomings, including poor adaptability
to Android SDK and Framework updates, restricted code coverage, and a
propensity to overlook essential API-permission mappings in intricate
codebases. This paper introduces a pioneering approach utilizing large language
models (LLMs) for a systematic examination of API-permission mappings. In
addition to employing LLMs, we integrate a dual-role prompting strategy and an
API-driven code generation approach into our mapping discovery pipeline,
resulting in the development of the corresponding tool, \tool{}. We formulate
three research questions to evaluate the efficacy of \tool{} against
state-of-the-art baselines, assess the completeness of official SDK
documentation, and analyze the evolution of permission-required APIs across
different SDK releases. Our experimental results reveal that \tool{} identifies
2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and
10 respectively, substantially outprforming existing baselines.

</details>


### [588] [GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization](https://arxiv.org/abs/2510.04135)
*Jingzhi Gong,Yixin Bian,Luis de la Cal,Giovanni Pinna,Anisha Uteem,David Williams,Mar Zamorano,Karine Even-Mendoza,W. B. Langdon,Hector Menendez,Federica Sarro*

Main category: cs.SE

TL;DR: GA4GC框架通过优化编码代理的超参数和提示模板，在减少37.7%运行时间的同时提高正确性，实现编码代理可持续性与代码性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决LLM驱动的编码代理在工业部署中面临的高计算成本（单次运行超10万token）和环境可持续性问题。

Method: 引入GA4GC框架，系统性地发现Pareto最优的代理超参数和提示模板，优化运行时（更绿色代理）与代码性能（更绿色代码）的权衡。

Result: 在SWE-Perf基准测试中实现高达135倍的超体积改进，减少37.7%代理运行时间同时提升正确性。

Conclusion: 温度是最关键的超参数，提供了在工业部署中平衡代理可持续性与代码优化有效性的可行策略。

Abstract: Coding agents powered by LLMs face critical sustainability and scalability
challenges in industrial deployment, with single runs consuming over 100k
tokens and incurring environmental costs that may exceed optimization benefits.
This paper introduces GA4GC, the first framework to systematically optimize
coding agent runtime (greener agent) and code performance (greener code)
trade-offs by discovering Pareto-optimal agent hyperparameters and prompt
templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x
hypervolume improvement, reducing agent runtime by 37.7% while improving
correctness. Our findings establish temperature as the most critical
hyperparameter, and provide actionable strategies to balance agent
sustainability with code optimization effectiveness in industrial deployment.

</details>


### [589] [Detecting Semantic Clones of Unseen Functionality](https://arxiv.org/abs/2510.04143)
*Konstantinos Kitsios,Francesco Sovrano,Earl T. Barr,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 论文研究了语义代码克隆检测模型在检测未见过的功能克隆时的性能下降问题，提出使用对比学习方法来提升模型对未见功能克隆的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经模型在语义代码克隆检测上表现优异，但主要基于训练数据中的相似克隆进行推断，对未见过的功能克隆检测性能显著下降。开发者需要检测所有类型的克隆，包括未见功能的克隆。

Method: 提出使用对比学习改进现有模型：对于任务特定模型，用对比分类器替换最终分类器；对于生成式LLM，提出对比上下文学习，引导模型关注克隆与非克隆的差异。

Result: 实验显示任务特定模型在未见功能克隆上的F1下降高达48%（平均31%），LLM表现相当但泛化更好（F1下降最多5%，平均3%）。使用对比学习后，任务特定模型F1提升高达26%（平均9%），LLM提升高达5%（平均3%）。

Conclusion: 对比学习能有效提升代码克隆检测模型对未见功能克隆的检测能力，特别是对任务特定模型改善显著，LLM本身具有更好的泛化能力但也能通过对比学习进一步改进。

Abstract: Semantic code clone detection is the task of detecting whether two snippets
of code implement the same functionality (e.g., Sort Array). Recently, many
neural models achieved near-perfect performance on this task. These models seek
to make inferences based on their training data. Consequently, they better
detect clones similar to those they have seen during training and may struggle
to detect those they have not. Developers seeking clones are, of course,
interested in both types of clones. We confirm this claim through a literature
review, identifying three practical clone detection tasks in which the model's
goal is to detect clones of a functionality even if it was trained on clones of
different functionalities. In light of this finding, we re-evaluate six
state-of-the-art models, including both task-specific models and generative
LLMs, on the task of detecting clones of unseen functionality. Our experiments
reveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs
perform on par with task-specific models without explicit training for clone
detection, but generalize better to unseen functionalities, where F1 drops up
to 5% (average 3%) instead. We propose and evaluate the use of contrastive
learning to improve the performance of existing models on clones of unseen
functionality. We draw inspiration from the computer vision and natural
language processing fields where contrastive learning excels at measuring
similarity between two objects, even if they come from classes unseen during
training. We replace the final classifier of the task-specific models with a
contrastive classifier, while for the generative LLMs we propose contrastive
in-context learning, guiding the LLMs to focus on the differences between
clones and non-clones. The F1 on clones of unseen functionality is improved by
up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for
LLMs.

</details>


### [590] [Multi Language Models for On-the-Fly Syntax Highlighting](https://arxiv.org/abs/2510.04166)
*Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 提出了一种统一的语法高亮模型，可以同时支持六种主流编程语言，通过新颖的归一化技术和少样本学习，显著减少了部署复杂性和对大型数据集的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习语法高亮模型存在三个主要问题：每个模型仅支持一种语言、需要大量来自慢速暴力解析器的训练数据、训练资源密集。在多语言环境中需要维护多个独立模型，增加了系统复杂性和运营成本。

Method: 开发了统一的语法高亮模型，采用新颖的归一化技术增强模型泛化能力，通过少样本学习实验证明少量样本即可替代大型数据集，减少对暴力解析器的依赖。

Result: 该模型能够高亮显示多达六种主流编程语言，将部署复杂性降低了六倍，并在未见语言上表现出更好的性能。

Conclusion: 这些创新使得跨多种编程语言的语法高亮更加高效、可扩展且成本效益更高。

Abstract: Syntax highlighting is a critical feature in modern software development
environments, enhancing code readability and developer productivity. However,
delivering accurate highlighting in real time remains challenging for online
and web-based development tools due to strict time and memory constraints on
backend services. These systems must serve highlights rapidly and frequently,
even when code is partially valid or invalid. This has led to on-the-fly syntax
highlighting, where visual annotations are generated just before content is
served, often at high request rates and under incomplete input conditions. To
meet these demands efficiently, state-of-the-art models use deep learning to
learn the behavior of brute-force syntax highlighting resolvers, tools that are
easy to implement but too slow for production. Through the Deep Abstraction
process, brute-force strategies are encoded into fast statistical models that
achieve both high accuracy and low-latency inference. Despite their success,
such models face key challenges: they support only one programming language per
model, require large datasets from slow brute-force generators, and involve
resource-intensive training. In multi-language environments, this means
maintaining multiple independent models, increasing system complexity and
operational cost. This work addresses these issues by introducing a unified
model capable of highlighting up to six mainstream programming languages,
reducing deployment complexity by a factor of six and improving performance on
unseen languages. A novel normalization technique significantly enhances model
generalization, while few-shot learning experiments show that a small number of
oracle samples can replace large datasets, minimizing dependence on brute-force
generators. Combined, these innovations enable efficient, scalable, and
cost-effective syntax highlighting across diverse programming languages.

</details>


### [591] [Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience](https://arxiv.org/abs/2510.04274)
*Damjan Fujs,Damjan Vavpotič,Tomaž Hovelja,Marko Poženel*

Main category: cs.SE

TL;DR: 研究探讨了LLM访问权限和专业软件开发经验对网络安全需求优先级排序的影响，发现LLM使用无显著差异，但经验水平在成本、用户体验和风险评估方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 调查大型语言模型访问权限和不同专业软件开发经验如何影响Web应用程序网络安全需求的优先级排序。

Method: 23名研究生使用MoSCoW方法对安全需求进行优先级排序，分为有LLM支持组和无LLM支持组，并对提出的解决方案进行多标准评估。

Result: LLM使用组间无显著差异，但不同经验组在功能开发成本估计、用户体验影响感知和风险相关评估方面存在统计显著差异。

Conclusion: LLM访问对网络安全解决方案评估无明显影响，但专业经验丰富的参与者倾向于给出更高的用户体验影响评分和更低的风险估计。

Abstract: This study investigates how access to Large Language Models (LLMs) and
varying levels of professional software development experience affect the
prioritization of cybersecurity requirements for web applications. Twenty-three
postgraduate students participated in a research study to prioritize security
requirements (SRs) using the MoSCoW method and subsequently rated their
proposed solutions against multiple evaluation criteria. We divided
participants into two groups (one with and the other without access to LLM
support during the task). Results showed no significant differences related to
LLM use, suggesting that access to LLMs did not noticeably influence how
participants evaluated cybersecurity solutions. However, statistically
significant differences emerged between experience groups for certain criteria,
such as estimated cost to develop a feature, perceived impact on user
experience, and risk assessment related to non-implementation of the proposed
feature. Participants with more professional experience tended to provide
higher ratings for user experience impact and lower risk estimates.

</details>


### [592] [Challenge on Optimization of Context Collection for Code Completion](https://arxiv.org/abs/2510.04349)
*Dmitry Ustalov,Egor Bogomolov,Alexander Bezzubov,Yaroslav Golubev,Evgeniy Glukhov,Georgii Levtsov,Vladimir Kovalenko*

Main category: cs.SE

TL;DR: 该论文介绍了JetBrains与Mistral AI在ASE 2025会议上组织的代码完成上下文收集优化挑战赛，旨在评估和改进从大型代码库中收集上下文信息以提升代码补全质量的方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI在软件工程中的快速发展，需要系统评估这些方法在大型代码库中利用整个项目信息的能力，特别是优化代码补全的上下文收集机制。

Method: 构建了基于Python和Kotlin的许可开源项目的大型数据集，参赛团队开发高效的上下文收集机制，使用chrF指标评估多个最先进神经模型的补全质量。

Result: 在公开阶段，19个团队提交了Python赛道解决方案，8个团队提交了Kotlin赛道解决方案；在私有阶段，6个团队竞争，其中5个团队向研讨会提交了论文。

Conclusion: 该挑战赛成功推动了代码补全上下文收集方法的创新，为优化大型代码库中的AI驱动软件工程工作流程提供了有价值的见解。

Abstract: The rapid advancement of workflows and methods for software engineering using
AI emphasizes the need for a systematic evaluation and analysis of their
ability to leverage information from entire projects, particularly in large
code bases. In this challenge on optimization of context collection for code
completion, organized by JetBrains in collaboration with Mistral AI as part of
the ASE 2025 conference, participants developed efficient mechanisms for
collecting context from source code repositories to improve fill-in-the-middle
code completions for Python and Kotlin. We constructed a large dataset of
real-world code in these two programming languages using permissively licensed
open-source projects. The submissions were evaluated based on their ability to
maximize completion quality for multiple state-of-the-art neural models using
the chrF metric. During the public phase of the competition, nineteen teams
submitted solutions to the Python track and eight teams submitted solutions to
the Kotlin track. In the private phase, six teams competed, of which five
submitted papers to the workshop.

</details>


### [593] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: MacroBench是一个代码优先的基准测试，用于评估LLM能否从自然语言目标生成可重用的浏览器自动化程序，涵盖7个自托管网站和681个任务。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在生成网页自动化程序方面的能力，特别是从HTML/DOM读取并输出Python Selenium代码的能力。

Method: 通过静态检查、沙箱执行、DOM断言和数据库快照进行端到端验证，包括安全测试套件。

Result: GPT-4o-Mini达到96.8%成功率，GPT-4.1达到95.3%，Gemini-2.5-Pro达到89.0%，DeepSeek-V3.1达到83.4%。简单任务成功率91.7%，复杂工作流为0%。

Conclusion: 模型能可靠处理简单任务，但在复杂工作流上完全失败，且都不符合生产级编码实践。发布了完整基准测试管道以支持可重复评估。

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [594] [Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development](https://arxiv.org/abs/2510.04380)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: 本文探讨了人工智能如何通过自动化劳动密集型任务、支持需求优先级排序和促进利益相关者与AI系统之间的协作来增强传统需求工程实践，同时分析了AI带来的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管需求工程是软件开发成功的基础，但仍面临模糊性、利益相关者需求冲突和管理需求演化的复杂性等持续挑战。AI有潜力简化RE流程，但也引入了新的伦理问题、偏见和缺乏透明度等担忧。

Method: 通过探索AI如何自动化劳动密集型任务、支持需求优先级排序和促进利益相关者与AI系统之间的协作来增强传统RE实践。

Result: AI能够提高RE过程的效率、准确性和管理行动，但也需要解决伦理问题、偏见和透明度等挑战。

Conclusion: 愿景呼吁在AI中采用伦理实践，并加强学术界与行业专业人士之间的合作，重点创建不仅强大而且可信赖、实用的AI解决方案，以适应快速发展的软件开发世界。

Abstract: Requirement Engineering (RE) is the foundation of successful software
development. In RE, the goal is to ensure that implemented systems satisfy
stakeholder needs through rigorous requirements elicitation, validation, and
evaluation processes. Despite its critical role, RE continues to face
persistent challenges, such as ambiguity, conflicting stakeholder needs, and
the complexity of managing evolving requirements. A common view is that
Artificial Intelligence (AI) has the potential to streamline the RE process,
resulting in improved efficiency, accuracy, and management actions. However,
using AI also introduces new concerns, such as ethical issues, biases, and lack
of transparency. This paper explores how AI can enhance traditional RE
practices by automating labor-intensive tasks, supporting requirement
prioritization, and facilitating collaboration between stakeholders and AI
systems. The paper also describes the opportunities and challenges that AI
brings to RE. In particular, the vision calls for ethical practices in AI,
along with a much-enhanced collaboration between academia and industry
professionals. The focus should be on creating not only powerful but also
trustworthy and practical AI solutions ready to adapt to the fast-paced world
of software development.

</details>


### [595] [Smart Hiring Redefined: An Intelligent Recruitment Management Platform](https://arxiv.org/abs/2510.04437)
*Fangzhe Wu,Dongyang Lyu,Xiaoqi Li*

Main category: cs.SE

TL;DR: 智能招聘管理系统通过自动化和数据驱动方法提升招聘效率和准确性，解决传统招聘模式效率低、成本高和信息不对称的问题。


<details>
  <summary>Details</summary>
Motivation: 传统招聘模式因效率有限、成本高和信息不对称，难以满足企业对精准人才获取的日益增长需求，需要更智能的解决方案。

Method: 采用自动化和数据驱动方法，包括快速解析海量简历、智能匹配候选人职位以及自动化面试流程调度。

Result: 智能招聘系统显著提升了招聘过程的效率和准确性，能够处理大规模简历并实现精准的候选人职位匹配。

Conclusion: 智能招聘管理系统是现代组织人才战略中不可或缺的组成部分，能够优化招聘流程、降低劳动和时间成本，并增强核心竞争力。

Abstract: Against the backdrop of deepening digital and intelligent transformation in
human resource management, traditional recruitment models struggle to fully
meet enterprises' growing demand for precise talent acquisition due to limited
efficiency, high costs, and information asymmetry. As a vital tool for
optimizing recruitment processes, reducing labor and time costs, and enhancing
core competitiveness, intelligent recruitment management systems become an
indispensable component of modern organizational talent strategies.Compared
with the labor intensive tasks of resume screening, candidate position
matching, and interview coordination in traditional manual recruitment,
intelligent recruitment systems significantly enhance the efficiency and
accuracy of the hiring process through automation and data driven approaches.
These systems enable rapid parsing of massive resume volumes, intelligent
matching of candidates to positions, and automated scheduling of interview
processes.

</details>


### [596] [Improving IR-based Bug Localization with Semantics-Driven Query Reduction](https://arxiv.org/abs/2510.04468)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: IQLoc是一种结合信息检索和大型语言模型的软件缺陷定位方法，通过利用Transformer模型理解程序语义来改进缺陷定位效果。


<details>
  <summary>Details</summary>
Motivation: 现有的缺陷定位方法存在局限性：基于信息检索的方法忽略了源代码的上下文和语义，而大型语言模型虽然能理解文本和代码，但尚未很好地适应缺陷定位任务且资源消耗大。

Method: 提出IQLoc方法，结合IR和LLM的优势，利用基于Transformer的模型理解程序语义，在缺陷定位过程中重新制定查询。

Result: 在扩展的Bench4BL基准数据集（包含约7.5K缺陷报告）上评估，IQLoc在MAP、MRR和HIT@K指标上显著优于四种基线方法，特别在处理包含堆栈跟踪、代码元素和纯自然语言描述的缺陷报告时表现优异。

Conclusion: 通过将程序语义理解集成到信息检索中，IQLoc缓解了传统基于IR的缺陷定位方法的长期挑战。

Abstract: Despite decades of research, software bug localization remains challenging
due to heterogeneous content and inherent ambiguities in bug reports. Existing
methods such as Information Retrieval (IR)-based approaches often attempt to
match source documents to bug reports, overlooking the context and semantics of
the source code. On the other hand, Large Language Models (LLM) (e.g.,
Transformer models) show promising results in understanding both texts and
code. However, they have not been yet adapted well to localize software bugs
against bug reports. They could be also data or resource-intensive. To bridge
this gap, we propose, IQLoc, a novel bug localization approach that capitalizes
on the strengths of both IR and LLM-based approaches. In particular, we
leverage the program semantics understanding of transformer-based models to
reason about the suspiciousness of code and reformulate queries during bug
localization using Information Retrieval. To evaluate IQLoc, we refine the
Bench4BL benchmark dataset and extend it by incorporating ~30% more recent bug
reports, resulting in a benchmark containing ~7.5K bug reports. We evaluated
IQLoc using three performance metrics and compare it against four baseline
techniques. Experimental results demonstrate its superiority, achieving up to
58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in
HIT@K for the test bug reports with random and time-wise splits, respectively.
Moreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,
72.73% for those that include code elements, and 65.38% for those containing
only descriptions in natural language. By integrating program semantic
understanding into Information Retrieval, IQLoc mitigates several longstanding
challenges of traditional IR-based approaches in bug localization.

</details>


### [597] [DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing](https://arxiv.org/abs/2510.04469)
*Wenqi Yan,Toby Murray,Benjamin Rubinstein,Van-Thuan Pham*

Main category: cs.SE

TL;DR: DynamiQ是一个基于AFLTeam的优化并行模糊测试系统，通过利用程序调用图的结构信息定义任务，并结合运行时反馈持续优化任务分配，显著减少冗余探索并提升大规模模糊测试效率。


<details>
  <summary>Details</summary>
Motivation: 现有并行模糊测试方法通常将单个种子作为任务，存在冗余探索问题。DynamiQ旨在通过结构化的任务定义和动态调整机制来提高并行模糊测试的效率。

Method: 基于LibAFL框架构建，利用程序调用图的结构信息定义任务，通过运行时反馈持续优化任务分配，并在任务分配和任务感知模糊测试方面实现多项实用优化。

Result: 在12个真实世界目标上进行了25,000 CPU小时的评估，在代码覆盖率和漏洞发现方面均优于最先进的并行模糊测试工具，发现了9个先前未知的广泛使用开源软件中的漏洞。

Conclusion: DynamiQ通过结构化任务定义和动态调整机制，显著提升了并行模糊测试的效率，在真实场景中表现出优越的性能。

Abstract: We present DynamiQ, a full-fledged and optimized successor to AFLTeam that
supports dynamic and adaptive parallel fuzzing. Unlike most existing approaches
that treat individual seeds as tasks, DynamiQ leverages structural information
from the program's call graph to define tasks and continuously refines task
allocation using runtime feedback. This design significantly reduces redundant
exploration and enhances fuzzing efficiency at scale. Built on top of the
state-of-the-art LibAFL framework, DynamiQ incorporates several practical
optimizations in both task allocation and task-aware fuzzing. Evaluated on 12
real-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ
outperforms state-of-the-art parallel fuzzers in both code coverage and
vulnerability discovery, uncovering 9 previously unknown bugs in widely used
and extensively fuzzed open-source software.

</details>


### [598] [Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem](https://arxiv.org/abs/2510.04495)
*Napasorn Tevarut,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: 该论文研究了npm生态系统中的琐碎包和数据包，开发了一种基于规则的静态分析方法来检测这些包，发现17.92%的包是琐碎包，其漏洞水平与非琐碎包相当，数据包虽然罕见但也存在风险。


<details>
  <summary>Details</summary>
Motivation: 琐碎包（功能简单的小模块）在npm生态系统中很常见，尽管简单但可能带来安全风险。现有定义需要完善，特别是引入不包含可执行逻辑的数据包概念。

Method: 开发基于规则的静态分析方法来检测琐碎包和数据包，并在2025年npm生态系统中评估其普遍性和相关风险。

Result: 分析显示17.92%的包是琐碎包，其漏洞水平与非琐碎包相当；数据包虽然罕见但也包含风险。检测工具达到94%准确率（宏F1分数0.87）。

Conclusion: 琐碎包和数据包在依赖管理中值得更多关注，以减少潜在的技术债务和安全暴露。所提出的检测工具能够有效进行大规模分析。

Abstract: Trivial packages, small modules with low functionality, are common in the npm
ecosystem and can pose security risks despite their simplicity. This paper
refines existing definitions and introduce data-only packages that contain no
executable logic. A rule-based static analysis method is developed to detect
trivial and data-only packages and evaluate their prevalence and associated
risks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are
trivial, with vulnerability levels comparable to non-trivial ones, and
data-only packages, though rare, also contain risks. The proposed detection
tool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale
analysis to reduce security exposure. This findings suggest that trivial and
data-only packages warrant greater attention in dependency management to reduce
potential technical debt and security exposure.

</details>


### [599] [Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation](https://arxiv.org/abs/2510.04519)
*Heiko Koziolek,Thilo Braun,Virendra Ashiwal,Sofia Linsbauer,Marthe Ahlgreen Hansen,Karoline Grotterud*

Main category: cs.SE

TL;DR: Spec2Control是一个基于大语言模型的高度自动化工作流，能够直接从自然语言用户需求生成图形化控制逻辑，显著减少人工劳动。


<details>
  <summary>Details</summary>
Motivation: 分布式控制系统(DCS)的软件编程过程主要依赖手动操作且繁琐，成本高昂。现有基于LLM的商业辅助工具主要关注文本表示，自动化程度有限，且未在大型数据集上进行测试。

Method: 开发了Spec2Control工作流，利用LLM直接从自然语言需求生成图形化控制逻辑，并在包含10个控制叙述和65个复杂测试用例的开放数据集上进行实验验证。

Result: 实验显示Spec2Control能成功识别控制策略，自主生成98.6%正确的控制策略连接，节省94-96%的人工劳动。

Conclusion: Spec2Control已集成到商业ABB工程工具中，同时提供开源版本供独立验证，为DCS控制逻辑生成提供了高效自动化解决方案。

Abstract: Distributed control systems (DCS) manage the automation for many industrial
production processes (e.g., power plants, chemical refineries, steel mills).
Programming the software for such systems remains a largely manual and tedious
process, incurring costs of millions of dollars for extensive facilities. Large
language models (LLMs) have been found helpful in generating DCS control logic,
resulting in commercial copilot tools. Today, these tools are focused on
textual notations, they provide limited automation, and have not been tested on
large datasets with realistic test cases. We introduce Spec2Control, a highly
automated LLM workflow to generate graphical control logic directly from
natural language user requirements. Experiments using an open dataset with 10
control narratives and 65 complex test cases demonstrate that Spec2Control can
successfully identify control strategies, can generate 98.6% of correct control
strategy connections autonomously, and can save between 94-96% of human labor.
Spec2Control is being integrated into commercial ABB engineering tools, but is
also available as an open-source variant for independent validation.

</details>


### [600] [Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes](https://arxiv.org/abs/2510.04603)
*Johan Linåker,Sachiko Muto*

Main category: cs.SE

TL;DR: 该研究分析16个数字成熟国家的开源软件政策，提出政府数字成熟度指标，发现开源政策普遍存在，由中央公共部门管理，目标包括互操作性、数字主权等，通过OSPOs实施支持。


<details>
  <summary>Details</summary>
Motivation: 开源软件是重要的公共产品，对GDP和国家科技增长有显著影响，但政府采用开源软件的系统性测量仍然有限，需要为数字政府成熟度指数提供指标。

Method: 采用定性方法，结合政策文件的案头研究和政府代表的半结构化访谈，制作详细的国家报告并进行交叉分析。

Result: 促进开源软件重用的政策广泛存在，主要由中央公共部门管理，政策目标包括互操作性、数字主权、透明度和成本效率，通过不同层级的开源项目办公室支持实施。

Conclusion: 开源软件是公共部门数字化转型的战略推动者，需要清晰的政策框架和机构支持，国际数字成熟度框架应扩展开源指标以更好地指导和评估政府采用和影响。

Abstract: Context: Open Source Software (OSS) is a vital public good, included across
most of modern software stacks, significantly impacting GDP and national tech
growth, while supporting interoperability, sovereignty, and transparency.
However, systematic measurement of governmental OSS adoption remain limited.
  Research Aim: This study contributes to digital government maturity indexes
by analyzing policies and support actions leveraging OSS for software reuse and
collaborative development across 16 digitally mature countries, and proposing
potential indicators for said indexes. It examines OSS policy formation, stated
goals, key actors, and support mechanisms.
  Methodology: A qualitative approach is used combining desk research of policy
documents with semi-structured interviews of government representatives,
producing detailed country reports. These are cross-analyzed, focusing on OSS
policy promotion, rationale, and implementation support.
  Results: Policies facilitating OSS reuse are widespread, targeting both
inbound acquisition and outbound sharing, and are predominantly governed by
central public sector organizations. Policy goals include interoperability,
digital sovereignty, transparency, and cost efficiency, with security framed
both as a risk and strength. Implementation is supported by diverse Open Source
Program Offices (OSPOs) at multiple government levels, which foster capacity
building, resource pooling, and sustainable project governance. Indicators are
synthesized and proposed across 14 areas covering policy incentives and design,
and implementation and support.
  Conclusions: OSS is a strategic enabler for public sector digital
transformation. Clear policy frameworks, coupled with institutional support
such as OSPOs, are essential. International digital maturity frameworks should
expand OSS indicators to better guide and assess government adoption and
impact.

</details>


### [601] [Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation](https://arxiv.org/abs/2510.04605)
*Jingyao Zhang,Tianlin Li,Xiaoyu Zhang,Qiang Hu,Bin Shi*

Main category: cs.SE

TL;DR: 扩散大语言模型在软件工程任务中表现优于自回归大语言模型，在52,937个任务的大规模基准测试中平均准确率提升30%，跨文件修复任务提升113%，同时保持更高的效率和更低的延迟。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型在处理代码结构信息方面存在局限性，且推理延迟较高。扩散大语言模型提供了全局双向编码和解耦生成步骤的替代方案。

Method: 对扩散大语言模型在软件开发生命周期中的表现进行全面评估，包括代码生成、缺陷检测和程序修复任务。

Result: 在52,937个任务的大规模基准测试中，7B参数的扩散大语言模型平均准确率比自回归大语言模型高出30%，在跨文件修复任务中提升113%，同时保持更高的效率和更低的延迟。

Conclusion: 扩散大语言模型是软件工程任务的优越范式。

Abstract: Autoregressive Large Language Models (AR-LLMs) are widely used in software
engineering (SE) but face limitations in processing code structure information
and suffer from high inference latency. Diffusion LLMs (DLLMs) offer a
promising alternative with global bidirectional encoding and decoupled
generation steps. This work presents the first comprehensive evaluation of
DLLMs across the software development lifecycle, including code generation,
defect detection, and program repair. On a large-scale benchmark of 52,937
tasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy
improvement achieving a 113% gain on cross-file repair, while maintaining
superior efficiency and reduced latency. Our results establish DLLMs as a
superior paradigm for SE tasks.

</details>


### [602] [A survey on the impact of emotions on the productivity among software developers](https://arxiv.org/abs/2510.04611)
*Pawel Weichbroth,Maciej Lotysz,Michal Wrobel*

Main category: cs.SE

TL;DR: 软件开发者情绪状态对感知生产力有显著正向影响（beta=0.893, p<0.001），情绪管理对提升生产力至关重要。


<details>
  <summary>Details</summary>
Motivation: 软件开发中的时间压力等因素常导致开发者情绪状态下降，但情绪是否影响感知生产力尚不明确。

Method: 采用两阶段方法：首先通过9位专家验证测量模型，然后对88名软件开发者进行问卷调查，使用偏最小二乘法分析数据。

Result: 路径分析明确证实假设，开发者情绪状态对感知生产力具有强烈正向且显著的影响。

Conclusion: 管理和改善开发者情绪健康对提升软件开发环境中的生产力至关重要，减少倦怠、压力等负面因素的干预措施能显著影响绩效结果。

Abstract: The time pressure associated with software development, among other factors,
often leads to a diminished emotional state among developers. However, whether
emotions affect perceived productivity remains an open question. This study
aims to determine the strength and direction of the relationship between
emotional state and perceived productivity among software developers. We
employed a two-stage approach. First, a survey was conducted with a pool of
nine experts to validate the measurement model. Second, a survey was
administered to a pool of 88 software developers to empirically test the
formulated hypothesis by using Partial Least Squares, as the data analysis
method. The results of the path analysis clearly confirm the formulated
hypothesis, showing that the emotional state of a software developer has a
strong positive, and significant impact (beta = 0.893, p < 0.001) on perceived
productivity among software developers. The findings highlight the importance
of managing and improving developers emotional well-being to enhance
productivity in software development environments. Additionally, interventions
aimed at reducing burnout, stress, and other negative factors could have a
considerable impact on their performance outcomes.

</details>


### [603] [Evolaris: A Roadmap to Self-Evolving Software Intelligence Management](https://arxiv.org/abs/2510.04689)
*Chengwei Liu,Wenbo Guo,Yuxin Zhang,Limin Wang,Sen Chen,Lei Bu,Yang Liu*

Main category: cs.SE

TL;DR: Evolaris是一个基于多代理框架的自进化软件智能系统，旨在从分散的非正式渠道及时捕获安全威胁情报，支持完整的信息发现、推理、验证和风险检测工作流。


<details>
  <summary>Details</summary>
Motivation: 当前软件威胁环境日益动态和分散，关键威胁信息越来越多地通过博客、社交媒体、论坛等非正式渠道出现，传统渠道无法及时捕获这些情报，需要新的解决方案来提升威胁感知和响应能力。

Method: 采用多代理框架构建自进化系统，代理独立运行但通过共享上下文协调，执行信息发现、推理、差距补全、验证和风险检测等任务，系统能够从新输入中学习并随时间适应新兴威胁模式。

Result: 该系统能够持续提高软件威胁分析的精确性、及时性和可扩展性，为主动安全决策提供可持续基础，并加强安全威胁理解的生态系统。

Conclusion: Evolaris通过自进化的多代理架构有效解决了从分散非正式渠道捕获威胁情报的挑战，为软件安全威胁分析提供了更及时、精确和可扩展的解决方案。

Abstract: In recent years, the landscape of software threats has become significantly
more dynamic and distributed. Security vulnerabilities are no longer discovered
and shared only through formal channels such as public vulnerability databases
or vendor advisories. Increasingly, criti- cal threat information emerges
informally through blogs, social media, developer forums, open source
repositories, and even underground com- munities. To this end, capturing such
intelligence in a timely manner is essential for maintaining situational
awareness and enabling prompt security responses. However, this remains a
complex challenge due to the fragmented nature of data sources and the
technical difficulty of collecting, parsing, mapping, and validating
information at scale. To ad- dress this, we propose Evolaris, a self-evolving
software intelligence sys- tem built on a multi-agent framework. Evolaris is
designed to support a full-stack workflow, where agents operate independently
but coordinate through shared context to perform tasks such as information
discovery, reasoning, gap completion, validation, and risk detection. This
archi- tecture enables the platform to learn from new inputs, refine its
internal knowledge, and adapt to emerging threat patterns over time, which
could continuously improve the precision, timeliness, and scalability of
software threat analysis, and offers a sustainable foundation for proactive
secu- rity decision-making and strengthens the broader ecosystem of security
threat understanding.

</details>


### [604] [An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures](https://arxiv.org/abs/2510.04711)
*Aoyang Fang,Songhan Zhang,Yifan Yang,Haotong Wu,Junjielong Xu,Xuyang Wang,Rui Wang,Manyi Wang,Qisheng Lu,Pinjia He*

Main category: cs.SE

TL;DR: 现有RCA基准测试过于简化，导致模型性能被高估。作者开发了更真实的基准测试框架，发现SOTA模型在实际场景中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 云原生微服务架构的复杂性使得根因分析(RCA)至关重要，但现有基准测试过于简化，无法反映真实世界条件，导致模型性能评估不准确。

Method: 系统分析流行RCA基准测试的局限性，开发自动化框架生成更真实的基准测试数据集，包含1,430个验证过的故障案例，涵盖25种故障类型。

Result: 在更真实的数据集上重新评估11个SOTA模型，发现Top@1准确率很低（平均0.21，最佳0.37），执行时间显著更长。

Conclusion: 现有RCA模型存在三个常见失败模式：可扩展性问题、可观测性盲点和建模瓶颈，需要更真实的基准测试来推动模型发展。

Abstract: While cloud-native microservice architectures have transformed software
development, their complexity makes Root Cause Analysis (RCA) both crucial and
challenging. Although many data-driven RCA models have been proposed, we find
that existing benchmarks are often oversimplified and fail to capture
real-world conditions. Our preliminary study shows that simple rule-based
methods can match or even outperform state-of-the-art (SOTA) models on four
widely used benchmarks, suggesting performance overestimation due to benchmark
simplicity. To address this, we systematically analyze popular RCA benchmarks
and identify key limitations in fault injection, call graph design, and
telemetry patterns. Based on these insights, we develop an automated framework
to generate more realistic benchmarks, yielding a dataset of 1,430 validated
failure cases from 9,152 injections, covering 25 fault types under dynamic
workloads with hierarchical ground-truth labels and verified SLI impact.
Re-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy
(average 0.21, best 0.37) and significantly longer execution times. Our
analysis highlights three common failure patterns: scalability issues,
observability blind spots, and modeling bottlenecks.

</details>


### [605] [Agile Software Effort Estimation using Regression Techniques](https://arxiv.org/abs/2510.04760)
*Sisay Deresa Sima,Ayalew Belay Habtie*

Main category: cs.SE

TL;DR: 开发基于故事点的敏捷工作量估算模型，使用LASSO和Elastic Net回归技术，在21个软件项目上验证，LASSO回归表现最佳。


<details>
  <summary>Details</summary>
Motivation: 软件工作量估算是软件开发过程中最关键的方面之一，项目的成功与否取决于估算的准确性，研究人员仍在进行敏捷工作量估算的研究。

Method: 使用LASSO和Elastic Net回归技术开发故事点基础的敏捷工作量估算模型，在21个软件项目上应用，使用默认参数和网格搜索调优的5折交叉验证。

Result: LASSO回归取得了最佳预测性能：PRED(8%)和PRED(25%)均为100.0，MMRE为0.0491，MMER为0.0551，MdMRE为0.0593，MdMER为0.063，MSE为0.0007。

Conclusion: LASSO回归在敏捷工作量估算中表现出优越的预测性能，结果与其他相关文献相比具有竞争力。

Abstract: Software development effort estimation is one of the most critical aspect in
software development process, as the success or failure of the entire project
depends on the accuracy of estimations. Researchers are still conducting
studies on agile effort estimation. The aim of this research is to develop a
story point based agile effort estimation model using LASSO and Elastic Net
regression techniques. The experimental work is applied to the agile story
point approach using 21 software projects collected from six firms. The two
algorithms are trained using their default parameters and tuned grid search
with 5-fold cross-validation to get an enhanced model. The experiment result
shows LASSO regression achieved better predictive performance PRED (8%) and
PRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,
MdMER of 0.063, and MSE of 0.0007. The results are also compared with other
related literature.

</details>


### [606] [GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes](https://arxiv.org/abs/2510.04791)
*Kristian Kolthoff,Felix Kretzer,Simone Paolo Ponzetto,Alexander Maedche,Christian Bartelt*

Main category: cs.SE

TL;DR: GUISpector是一个基于多模态大语言模型的GUI原型需求验证框架，能够自动验证自然语言需求并提供可操作的反馈，支持LLM驱动的开发工作流。


<details>
  <summary>Details</summary>
Motivation: 现有GUI测试方法难以处理现代界面的复杂性，缺乏可操作反馈和与自动化开发代理的有效集成。随着LLM驱动的编程代理在开发流程中日益普及，确保GUI实现满足自然语言需求变得至关重要。

Method: 使用多模态大语言模型代理来解释和操作化自然语言需求，自主规划和执行GUI应用的验证轨迹，系统提取详细反馈，提供端到端的需求验证工具。

Result: 在包含150个需求和900个验收标准注释的多样化GUI应用上评估，有效检测需求满足和违规情况，展示了与自动化LLM驱动开发工作流的无缝集成潜力。

Conclusion: GUISpector框架通过多模态大语言模型实现了GUI原型的自动化需求验证，为开发人员提供可操作的反馈，支持迭代式GUI优化和LLM驱动的代码生成。

Abstract: GUIs are foundational to interactive systems and play a pivotal role in early
requirements elicitation through prototyping. Ensuring that GUI implementations
fulfill NL requirements is essential for robust software engineering,
especially as LLM-driven programming agents become increasingly integrated into
development workflows. Existing GUI testing approaches, whether traditional or
LLM-driven, often fall short in handling the complexity of modern interfaces,
and typically lack actionable feedback and effective integration with automated
development agents. In this paper, we introduce GUISpector, a novel framework
that leverages a multi-modal (M)LLM-based agent for the automated verification
of NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to
interpret and operationalize NL requirements, enabling to autonomously plan and
execute verification trajectories across GUI applications. Second, GUISpector
systematically extracts detailed NL feedback from the agent's verification
process, providing developers with actionable insights that can be used to
iteratively refine the GUI artifact or directly inform LLM-based code
generation in a closed feedback loop. Third, we present an integrated tool that
unifies these capabilities, offering practitioners an accessible interface for
supervising verification runs, inspecting agent rationales and managing the
end-to-end requirements verification process. We evaluated GUISpector on a
comprehensive set of 150 requirements based on 900 acceptance criteria
annotations across diverse GUI applications, demonstrating effective detection
of requirement satisfaction and violations and highlighting its potential for
seamless integration of actionable feedback into automated LLM-driven
development workflows. The video presentation of GUISpector is available at:
https://youtu.be/JByYF6BNQeE, showcasing its main capabilities.

</details>


### [607] [RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms](https://arxiv.org/abs/2510.04796)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: RevMine是一个基于大语言模型的代码审查挖掘工具，旨在简化从GitHub和GitLab等平台提取、过滤和分析代码审查数据的过程。


<details>
  <summary>Details</summary>
Motivation: 当前代码审查数据的收集和分析过程耗时且技术密集，研究人员需要编写临时脚本来处理数据，这阻碍了更广泛的实证研究。

Method: 使用大语言模型构建端到端的代码审查挖掘管道，指导用户完成身份验证、端点发现和自然语言驱动的数据收集，支持定量和定性分析。

Result: RevMine显著减少了手动编写脚本的需求，降低了代码审查挖掘的门槛。

Conclusion: 通过降低入门门槛，RevMine旨在使代码审查挖掘民主化，并支持更广泛的实证软件工程研究。

Abstract: Empirical research on code review processes is increasingly central to
understanding software quality and collaboration. However, collecting and
analyzing review data remains a time-consuming and technically intensive task.
Most researchers follow similar workflows - writing ad hoc scripts to extract,
filter, and analyze review data from platforms like GitHub and GitLab. This
paper introduces RevMine, a conceptual tool that streamlines the entire code
review mining pipeline using large language models (LLMs). RevMine guides users
through authentication, endpoint discovery, and natural language-driven data
collection, significantly reducing the need for manual scripting. After
retrieving review data, it supports both quantitative and qualitative analysis
based on user-defined filters or LLM-inferred patterns. This poster outlines
the tool's architecture, use cases, and research potential. By lowering the
barrier to entry, RevMine aims to democratize code review mining and enable a
broader range of empirical software engineering studies.

</details>


### [608] [InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface](https://arxiv.org/abs/2510.04835)
*Wentao Gao,Renata Borovica-Gajic,Sang Kil Cha,Tian Qiu,Van-Thuan Pham*

Main category: cs.SE

TL;DR: InsightQL是第一个辅助人类分析模糊测试阻塞问题的框架，通过统一数据库和参数化查询接口帮助开发者系统提取洞察并高效解除模糊测试阻塞。


<details>
  <summary>Details</summary>
Motivation: 尽管模糊测试技术有所进步，但许多模糊测试工具仍面临由模糊测试阻塞引起的覆盖率瓶颈问题，限制了发现深层漏洞的能力。人工分析模糊测试结果来指导支持仍然劳动密集。

Method: 引入InsightQL框架，采用统一数据库和直观的参数化查询接口，帮助开发者系统提取洞察并高效解除模糊测试阻塞。

Result: 在FuzzBench基准测试中的14个流行真实世界库上进行实验，成功解除了许多模糊测试阻塞，并显著提高了代码覆盖率（最高达13.90%）。

Conclusion: InsightQL是一个有效的人类辅助框架，能够显著改善模糊测试的覆盖率和效果。

Abstract: Fuzzing is a highly effective automated testing method for uncovering
software vulnerabilities. Despite advances in fuzzing techniques, such as
coverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus
caused by fuzz blockers, limiting their ability to find deeper vulnerabilities.
Human expertise can address these challenges, but analyzing fuzzing results to
guide this support remains labor-intensive. To tackle this, we introduce
InsightQL, the first human-assisting framework for fuzz blocker analysis.
Powered by a unified database and an intuitive parameterized query interface,
InsightQL aids developers in systematically extracting insights and efficiently
unblocking fuzz blockers. Our experiments on 14 popular real-world libraries
from the FuzzBench benchmark demonstrate the effectiveness of InsightQL,
leading to the unblocking of many fuzz blockers and considerable improvements
in code coverage (up to 13.90%).

</details>


### [609] [FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration](https://arxiv.org/abs/2510.04852)
*Victor May,Diganta Misra,Yanqi Luo,Anjali Sridhar,Justine Gehring,Silvio Soares Ribeiro Junior*

Main category: cs.SE

TL;DR: FreshBrew是一个用于评估AI代理在项目级Java迁移任务中性能的新基准，重点关注语义保持和避免奖励黑客行为，基于228个代码库的评估显示Gemini 2.5 Flash模型能成功迁移52.3%的项目到JDK 17。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程助手在软件开发中的普及，需要系统评估AI驱动框架在代码迁移现代化方面的有效性，传统方法依赖基于规则系统和人工干预，而LLM提供了有前景的替代方案。

Method: 引入FreshBrew基准，专门针对项目级Java迁移任务，要求高测试覆盖率以确保评估的严谨性和可靠性，对多个最先进的LLM进行基准测试，并与传统基于规则工具进行比较。

Result: 在228个代码库的评估中，表现最佳的Gemini 2.5 Flash模型能够成功将52.3%的项目迁移到JDK 17，揭示了当前AI代理在现实Java现代化任务中的关键优势和局限性。

Conclusion: FreshBrep基准为评估可信赖代码迁移系统提供了基础，通过发布该基准旨在促进严谨、可复现的评估，并推动AI驱动代码库现代化的发展。

Abstract: AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

</details>


### [610] [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
*Yicheng Tao,Yao Qin,Yepang Liu*

Main category: cs.SE

TL;DR: 这篇论文全面综述了检索增强代码生成（RACG）领域，特别关注仓库级代码生成（RLCG）的挑战和解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成方面取得进展，现实软件开发需要跨整个代码仓库的推理能力，这引发了仓库级代码生成的挑战。检索增强生成（RAG）通过整合外部检索机制来增强上下文感知和可扩展性。

Method: 作者对RACG研究进行系统综述，从多个维度对现有工作进行分类，包括生成策略、检索模态、模型架构、训练范式和评估协议。

Result: 论文总结了广泛使用的数据集和基准，分析了当前局限性，并概述了未来研究的关键挑战和机遇。

Conclusion: 目标是建立一个统一的分析框架来理解这个快速发展的领域，并推动AI驱动的软件工程的持续进步。

Abstract: Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

</details>


### [611] [Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain](https://arxiv.org/abs/2510.04964)
*Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: 软件签名在现代软件分发中仍然必要，即使注册中心安全，签名也能在镜像、代理、重托管和离线传输等边界场景中提供完整性、来源和问责保证。


<details>
  <summary>Details</summary>
Motivation: 在集中式注册中心时代，需要验证签名是否仍必要，因为注册中心安全控制可能无法覆盖所有软件分发边界。

Method: 综合历史实践，建立现代分发模式的信任模型，识别签名在注册中心控制范围外提供信任的场景。

Result: 签名作为基础防御层，在镜像、企业代理、重托管和离线传输等场景中能增强软件供应链保证。

Conclusion: 即使注册中心安全，签名仍然是必要的，它能在不同分发边界中提供注册中心安全控制无法保证的核心安全属性。

Abstract: Software signing provides a formal mechanism for provenance by ensuring
artifact integrity and verifying producer identity. It also imposes tooling and
operational costs to implement in practice. In an era of centralized registries
such as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask
whether hardening registry security controls obviates the need for end-to-end
artifact signing. In this work, we posit that the core guarantees of signing,
provenance, integrity, and accountability are not automatically carried across
different software distribution boundaries. These boundaries include mirrors,
corporate proxies, re-hosting, and air-gapped transfers, where registry
security controls alone cannot provide sufficient assurance. We synthesize
historical practice and present a trust model for modern distribution modes to
identify when signing is necessary to extend trust beyond registry control.
Treating signing as a baseline layer of defense strengthens software supply
chain assurance even when registries are secure.

</details>


### [612] [Quantum Computing as a Service - a Software Engineering Perspective](https://arxiv.org/abs/2510.04982)
*Aakash Ahmad,Muhammad Waseem,Bakheet Aljedaani,Mahdi Fahmideh,Peng Liang,Feras Awaysheh*

Main category: cs.SE

TL;DR: 本文提出了一种面向服务的量子计算(QCaaS)软件工程方法，通过系统映射研究和架构开发，建立了量子服务开发生命周期和参考架构。


<details>
  <summary>Details</summary>
Motivation: 量子计算作为颠覆性技术正在兴起，但大多数组织无法拥有量子计算机。量子计算即服务(QCaaS)被视为符合服务导向理念的解决方案，能够以效用计算方式向用户提供量子计算资源。

Method: 采用两阶段研究方法：(1) 系统映射研究，通过多步骤选择和定性评估筛选了41篇同行评审研究；(2) 基于架构的开发，将量子服务开发生命周期各阶段整合到支持QCaaS的参考架构中。

Result: 识别出包含4个阶段的量子服务开发生命周期，以及量子重要需求(QSRs)、各种建模符号、模式目录、编程语言和部署平台，这些可以集成到分层的参考架构中以工程化QCaaS。

Conclusion: 研究提供了一种过程中心和架构驱动的方法，为量子服务导向提供了软件工程视角，支持量子服务的概念化、建模、组装和部署。

Abstract: Quantum systems have started to emerge as a disruptive technology and
enabling platforms - exploiting the principles of quantum mechanics via
programmable quantum bits (QuBits) - to achieve quantum supremacy in computing.
Academic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and
consortiums like 'Quantum Flagship' are striving to develop practically capable
and commercially viable quantum computing (QC) systems and technologies.
Quantum Computing as a Service (QCaaS) is viewed as a solution attuned to the
philosophy of service-orientation that can offer QC resources and platforms, as
utility computing, to individuals and organisations who do not own quantum
computers. This research investigates a process-centric and architecture-driven
approach to offer a software engineering perspective on enabling QCaaS - a.k.a
quantum service-orientation. We employed a two-phase research method comprising
(a) a systematic mapping study and (b) an architecture-based development, first
to identify the phases of the quantum service development life cycle and
subsequently to integrate these phases into a reference architecture that
supports QCaaS. The SMS process retrieved a collection of potentially relevant
research literature and based on a multi-step selection and qualitative
assessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs
investigate (i) demographic details in terms of frequency, types, and trends of
research, (ii) phases of quantum service development lifecycle to derive a
reference architecture for conception, modeling, assembly, and deployment of
services, and (iii) The results identify a 4-phased development lifecycle along
with quantum significant requirements (QSRs), various modeling notations,
catalogue of patterns, programming languages, and deployment platforms that can
be integrated in a layered reference architecture to engineer QCaaS.

</details>


### [613] [AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis](https://arxiv.org/abs/2510.04997)
*Jiongchi Yu,Weipeng Jiang,Xiaoyu Zhang,Qiang Hu,Xiaofei Xie,Chao Shen*

Main category: cs.SE

TL;DR: 该论文探索使用大语言模型(LLMs)进行软件故障分析，将传统需要数周的人工分析时间缩短至平均约2小时，大幅提高了故障分析的效率。


<details>
  <summary>Details</summary>
Motivation: 传统软件故障分析需要专家驱动的多个步骤，包括收集潜在故障、筛选和手动调查，这些过程既费时又费力，阻碍了在复杂关键软件系统中进行大规模故障研究。

Method: 将经验性软件故障研究分解为三个关键阶段：(1)研究目标定义，(2)数据准备，(3)故障分析，并使用LLMs对从高质量经验研究中提取的3,829个软件故障进行评估。

Result: LLMs能显著提高故障分析效率，平均处理时间约2小时，而传统人工分析通常需要数周时间。

Conclusion: LLMs在推进经验性故障研究方面具有巨大潜力，但需要解决开放挑战才能实现完全自动化的端到端软件故障分析。

Abstract: Understanding software faults is essential for empirical research in software
development and maintenance. However, traditional fault analysis, while
valuable, typically involves multiple expert-driven steps such as collecting
potential faults, filtering, and manual investigation. These processes are both
labor-intensive and time-consuming, creating bottlenecks that hinder
large-scale fault studies in complex yet critical software systems and slow the
pace of iterative empirical research.
  In this paper, we decompose the process of empirical software fault study
into three key phases: (1) research objective definition, (2) data preparation,
and (3) fault analysis, and we conduct an initial exploration study of applying
Large Language Models (LLMs) for fault analysis of open-source software.
Specifically, we perform the evaluation on 3,829 software faults drawn from a
high-quality empirical study. Our results show that LLMs can substantially
improve efficiency in fault analysis, with an average processing time of about
two hours, compared to the weeks of manual effort typically required. We
conclude by outlining a detailed research plan that highlights both the
potential of LLMs for advancing empirical fault studies and the open challenges
that required be addressed to achieve fully automated, end-to-end software
fault analysis.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [614] [Visual Lifelog Retrieval through Captioning-Enhanced Interpretation](https://arxiv.org/abs/2510.04010)
*Yu-Fei Shih,An-Zi Yen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.IR

TL;DR: 提出了一种集成字幕的视觉生活日志检索系统，通过生成字幕和文本嵌入来改进基于文本查询的生活日志图像检索。


<details>
  <summary>Details</summary>
Motivation: 人们常常难以记住过去经历的细节，需要重新访问这些记忆。生活日志检索已成为重要应用，但传统方法在解释第一人称视角图像方面存在局限。

Method: 系统首先生成视觉生活日志的字幕，然后使用文本嵌入模型将字幕和用户查询投影到共享向量空间。提出了三种字幕方法：单字幕法、集体字幕法和合并字幕法。

Result: 实验结果表明，该方法能有效描述第一人称视觉图像，提高了生活日志检索的效果。

Conclusion: 构建了将视觉生活日志转换为字幕的文本数据集，成功重建了个人生活体验，为生活日志检索提供了有效解决方案。

Abstract: People often struggle to remember specific details of past experiences, which
can lead to the need to revisit these memories. Consequently, lifelog retrieval
has emerged as a crucial application. Various studies have explored methods to
facilitate rapid access to personal lifelogs for memory recall assistance. In
this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval
System for extracting specific images from a user's visual lifelog based on
textual queries. Unlike traditional embedding-based methods, our system first
generates captions for visual lifelogs and then utilizes a text embedding model
to project both the captions and user queries into a shared vector space.
Visual lifelogs, captured through wearable cameras, provide a first-person
viewpoint, necessitating the interpretation of the activities of the individual
behind the camera rather than merely describing the scene. To address this, we
introduce three distinct approaches: the single caption method, the collective
caption method, and the merged caption method, each designed to interpret the
life experiences of lifeloggers. Experimental results show that our method
effectively describes first-person visual images, enhancing the outcomes of
lifelog retrieval. Furthermore, we construct a textual dataset that converts
visual lifelogs into captions, thereby reconstructing personal life
experiences.

</details>


### [615] [Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with Musically Informed Metrics](https://arxiv.org/abs/2510.03750)
*Hanwen Zhang,Kun Fang,Ziyu Wang,Ichiro Fujinaga*

Main category: cs.IR

TL;DR: 提出一个钢琴踏板深度估计的评估框架，在传统帧级指标基础上增加了动作级和手势级分析，能更好地捕捉音乐相关的改进。


<details>
  <summary>Details</summary>
Motivation: 传统帧级指标在评估连续钢琴踏板深度估计任务时不够完整，忽略了方向变化边界和踏板曲线轮廓等音乐重要特征。

Method: 提出三层次评估框架：帧级指标、动作级评估（测量方向和时序）、手势级分析（评估每个按压-释放周期的轮廓相似性）。应用该框架比较音频基线模型与两个变体（MIDI信息增强和二进制值训练）。

Result: MIDI增强模型在动作级和手势级表现显著优于其他模型，尽管帧级改进有限。

Conclusion: 该评估框架能捕捉传统指标无法识别的音乐相关改进，为踏板深度估计模型提供了更实用有效的评估方法。

Abstract: Evaluation for continuous piano pedal depth estimation tasks remains
incomplete when relying only on conventional frame-level metrics, which
overlook musically important features such as direction-change boundaries and
pedal curve contours. To provide more interpretable and musically meaningful
insights, we propose an evaluation framework that augments standard frame-level
metrics with an action-level assessment measuring direction and timing using
segments of press/hold/release states and a gesture-level analysis that
evaluates contour similarity of each press-release cycle. We apply this
framework to compare an audio-only baseline with two variants: one
incorporating symbolic information from MIDI, and another trained in a
binary-valued setting, all within a unified architecture. Results show that the
MIDI-informed model significantly outperforms the others at action and gesture
levels, despite modest frame-level gains. These findings demonstrate that our
framework captures musically relevant improvements indiscernible by traditional
metrics, offering a more practical and effective approach to evaluating pedal
depth estimation models.

</details>


### [616] [Investigating LLM Variability in Personalized Conversational Information Retrieval](https://arxiv.org/abs/2510.03795)
*Simon Lupart,Daniël van Dijk,Eric Langezaal,Ian van Dort,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: 本研究复现并扩展了Mo等人的工作，发现人工选择的个人知识库能持续提升检索性能，而LLM选择方法不可靠。研究强调多轮评估和方差报告的重要性。


<details>
  <summary>Details</summary>
Motivation: Mo等人的研究基于单次GPT-3.5实验得出个人知识库可能有害的结论，但存在输出变异性和可重复性问题，需要更严谨的验证。

Method: 使用TREC iKAT 2024数据集，评估多种模型（Llama 1B-70B、Qwen-7B、GPT-4o-mini），比较人工与LLM选择个人知识库的效果，分析不同数据集和指标的方差。

Result: 人工选择的PTKB持续提升检索性能，LLM选择方法不可靠；iKAT数据集比CAsT变异更大；召回指标比精度指标方差更小。

Conclusion: 需要多轮评估和方差报告来评估基于LLM的CIR系统，本研究为个性化CIR提供了更稳健和可泛化的实践方法。

Abstract: Personalized Conversational Information Retrieval (CIR) has seen rapid
progress in recent years, driven by the development of Large Language Models
(LLMs). Personalized CIR aims to enhance document retrieval by leveraging
user-specific information, such as preferences, knowledge, or constraints, to
tailor responses to individual needs. A key resource for this task is the TREC
iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.
Building on this resource, Mo et al. explored several strategies for
incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query
reformulation. Their findings suggested that personalization from PTKBs could
be detrimental and that human annotations were often noisy. However, these
conclusions were based on single-run experiments using the GPT-3.5 Turbo model,
raising concerns about output variability and repeatability. In this
reproducibility study, we rigorously reproduce and extend their work, focusing
on LLM output variability and model generalization. We apply the original
methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of
models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that
human-selected PTKBs consistently enhance retrieval performance, while
LLM-based selection methods do not reliably outperform manual choices. We
further compare variance across datasets and observe higher variability on iKAT
than on CAsT, highlighting the challenges of evaluating personalized CIR.
Notably, recall-oriented metrics exhibit lower variance than precision-oriented
ones, a critical insight for first-stage retrievers. Finally, we underscore the
need for multi-run evaluations and variance reporting when assessing LLM-based
CIR systems. By broadening evaluation across models, datasets, and metrics, our
study contributes to more robust and generalizable practices for personalized
CIR.

</details>


### [617] [Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval](https://arxiv.org/abs/2510.03984)
*Kirandeep Kaur,Preetam Prabhu Srikar Dammu,Hideo Joho,Chirag Shah*

Main category: cs.IR

TL;DR: 提出评估个性化AI代理的新概念框架，从静态性能快照转向互动感知的演进式评估，包含用户模拟、偏好提取和适应能力评估三个核心组件。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法多为静态基准测试，无法反映用户需求的动态变化，难以评估代理在长期交互中的适应能力。

Method: 提出概念框架包含三个核心组件：基于角色的用户模拟、结构化偏好提取协议、适应感知评估机制，并通过电商搜索案例研究验证。

Result: 建立了评估个性化AI代理的连续、用户中心化概念基础，为动态评估提供了系统化框架。

Conclusion: 个性化评估应从静态快照转向连续演进式评估，将个性化理解为持续的用户中心化过程。

Abstract: Personalized AI agents are becoming central to modern information retrieval,
yet most evaluation methodologies remain static, relying on fixed benchmarks
and one-off metrics that fail to reflect how users' needs evolve over time.
These limitations hinder our ability to assess whether agents can meaningfully
adapt to individuals across dynamic, longitudinal interactions. In this
perspective paper, we propose a conceptual lens for rethinking evaluation in
adaptive personalization, shifting the focus from static performance snapshots
to interaction-aware, evolving assessments. We organize this lens around three
core components: (1) persona-based user simulation with temporally evolving
preference models; (2) structured elicitation protocols inspired by reference
interviews to extract preferences in context; and (3) adaptation-aware
evaluation mechanisms that measure how agent behavior improves across sessions
and tasks. While recent works have embraced LLM-driven user simulation, we
situate this practice within a broader paradigm for evaluating agents over
time. To illustrate our ideas, we conduct a case study in e-commerce search
using the PersonalWAB dataset. Beyond presenting a framework, our work lays a
conceptual foundation for understanding and evaluating personalization as a
continuous, user-centric endeavor.

</details>


### [618] [The LCLStream Ecosystem for Multi-Institutional Dataset Exploration](https://arxiv.org/abs/2510.04012)
*David Rogers,Valerio Mariani,Cong Wang,Ryan Coffee,Wilko Kroeger,Murali Shankar,Hans Thorsten Schwander,Tom Beck,Frédéric Poitevin,Jana Thayer*

Main category: cs.IR

TL;DR: 开发了一个新的端到端实验数据流框架，支持AI训练、高速率X射线飞行时间分析、分布式晶体结构测定等新型应用，结合云微服务和传统HPC批处理模型。


<details>
  <summary>Details</summary>
Motivation: 为X射线科学数据分析社区解决对高速数据流源的重要需求，填补DOE综合研究基础设施中的空白。

Method: 采用数据请求API、相互认证Web安全框架、作业队列系统、高速数据缓冲区，结合云微服务和HPC批处理执行模型。

Result: 成功原型化和实现了LCLStreamer框架，为下一代实验提供了关键的新范式。

Conclusion: 该框架通过灵活、API驱动的数据请求服务，为X射线科学社区提供了独特的高性能数据流解决方案，对未来实验具有重要意义。

Abstract: We describe a new end-to-end experimental data streaming framework designed
from the ground up to support new types of applications -- AI training,
extremely high-rate X-ray time-of-flight analysis, crystal structure
determination with distributed processing, and custom data science applications
and visualizers yet to be created. Throughout, we use design choices merging
cloud microservices with traditional HPC batch execution models for security
and flexibility. This project makes a unique contribution to the DOE Integrated
Research Infrastructure (IRI) landscape. By creating a flexible, API-driven
data request service, we address a significant need for high-speed data
streaming sources for the X-ray science data analysis community. With the
combination of data request API, mutual authentication web security framework,
job queue system, high-rate data buffer, and complementary nature to facility
infrastructure, the LCLStreamer framework has prototyped and implemented
several new paradigms critical for future generation experiments.

</details>


### [619] [RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback](https://arxiv.org/abs/2510.04096)
*Tommy Mordo,Sagie Dekel,Omer Madmon,Moshe Tennenholtz,Oren Kurland*

Main category: cs.IR

TL;DR: 本文提出了RLRF框架，使用从排名竞赛中获得的偏好数据集来训练LLM，以优化内容在竞争性搜索中的排名表现。


<details>
  <summary>Details</summary>
Motivation: 随着文档发布者越来越多地使用LLM生成和修改竞争性内容，需要开发能够优化内容排名并考虑竞争对手策略的方法。

Method: 使用强化学习从排名器反馈（RLRF）框架，基于排名竞赛生成的偏好数据集训练LLM，无需人工编写数据。

Result: 提出的智能体在LLM竞争性文档修改方面显著优于现有方法，能够适应未见过的排名函数并应对战略对手。

Conclusion: 研究结果支持了在竞争性搜索中使用强化学习的巨大潜力。

Abstract: Competitive search is a setting where document publishers modify them to
improve their ranking in response to a query. Recently, publishers have
increasingly leveraged LLMs to generate and modify competitive content. We
introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that
trains LLMs using preference datasets derived from ranking competitions. The
goal of a publisher (LLM-based) agent is to optimize content for improved
ranking while accounting for the strategies of competing agents. We generate
the datasets using approaches that do not rely on human-authored data. We show
that our proposed agents consistently and substantially outperform previously
suggested approaches for LLM-based competitive document modification. We
further show that our agents are effective with ranking functions they were not
trained for (i.e., out of distribution) and they adapt to strategic opponents.
These findings provide support to the significant potential of using
reinforcement learning in competitive search.

</details>


### [620] [Learning-Based Hashing for ANN Search: Foundations and Early Advances](https://arxiv.org/abs/2510.04127)
*Sean Moran*

Main category: cs.IR

TL;DR: 这篇论文是对基于学习的哈希方法进行基础性综述，重点回顾了早期监督、无监督和半监督哈希方法的核心思想，以及多比特和多阈值模型的扩展。


<details>
  <summary>Details</summary>
Motivation: 近似最近邻搜索是信息检索中的基础问题，基于哈希的方法通过将高维数据映射到紧凑的二进制码来提供高效解决方案。本文旨在介绍基于学习的哈希方法的概念基础，帮助读者理解该领域的原理、权衡和开放挑战。

Method: 综述了早期基于学习的哈希方法，包括投影函数的设计以生成有意义的嵌入，以及量化策略将这些嵌入转换为二进制码。涵盖了监督、无监督和半监督方法，以及多比特、多阈值模型和跨模态检索的早期进展。

Result: 提供了对基于学习的哈希方法的系统性理解框架，阐明了不同方法的核心思想、设计原则和性能权衡，为当前研究提供了历史背景和理论基础。

Conclusion: 通过将早期模型置于历史背景中，本文为读者提供了对基于学习的哈希方法原理、权衡和开放挑战的结构化理解，这些理解继续影响着该领域的当前研究。

Abstract: Approximate Nearest Neighbour (ANN) search is a fundamental problem in
information retrieval, underpinning large-scale applications in computer
vision, natural language processing, and cross-modal search. Hashing-based
methods provide an efficient solution by mapping high-dimensional data into
compact binary codes that enable fast similarity computations in Hamming space.
Over the past two decades, a substantial body of work has explored learning to
hash, where projection and quantisation functions are optimised from data
rather than chosen at random.
  This article offers a foundational survey of early learning-based hashing
methods, with an emphasis on the core ideas that shaped the field. We review
supervised, unsupervised, and semi-supervised approaches, highlighting how
projection functions are designed to generate meaningful embeddings and how
quantisation strategies convert these embeddings into binary codes. We also
examine extensions to multi-bit and multi-threshold models, as well as early
advances in cross-modal retrieval.
  Rather than providing an exhaustive account of the most recent methods, our
goal is to introduce the conceptual foundations of learning-based hashing for
ANN search. By situating these early models in their historical context, we aim
to equip readers with a structured understanding of the principles, trade-offs,
and open challenges that continue to inform current research in this area.

</details>


### [621] [Empowering Denoising Sequential Recommendation with Large Language Model Embeddings](https://arxiv.org/abs/2510.04239)
*Tongzhou Wu,Yuhao Wang,Maolin Wang,Chi Zhang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出了IADSR框架，通过整合协同信息和语义信息来解决序列推荐中的噪声问题，避免过度去噪


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型容易受到噪声交互的影响，而仅依赖协同信息可能导致对冷门物品的过度去噪问题

Method: 两阶段框架：第一阶段从传统序列推荐模型和LLM分别获取物品的协同嵌入和语义嵌入；第二阶段对齐两种嵌入，基于长期和短期兴趣识别序列中的噪声

Result: 在四个公开数据集上的广泛实验验证了该框架的有效性和与不同序列推荐系统的兼容性

Conclusion: IADSR框架通过整合协同和语义信息有效解决了序列推荐中的噪声问题，特别改善了冷门物品的处理

Abstract: Sequential recommendation aims to capture user preferences by modeling
sequential patterns in user-item interactions. However, these models are often
influenced by noise such as accidental interactions, leading to suboptimal
performance. Therefore, to reduce the effect of noise, some works propose
explicitly identifying and removing noisy items. However, we find that simply
relying on collaborative information may result in an over-denoising problem,
especially for cold items. To overcome these limitations, we propose a novel
framework: Interest Alignment for Denoising Sequential Recommendation (IADSR)
which integrates both collaborative and semantic information. Specifically,
IADSR is comprised of two stages: in the first stage, we obtain the
collaborative and semantic embeddings of each item from a traditional
sequential recommendation model and an LLM, respectively. In the second stage,
we align the collaborative and semantic embeddings and then identify noise in
the interaction sequence based on long-term and short-term interests captured
in the collaborative and semantic modalities. Our extensive experiments on four
public datasets validate the effectiveness of the proposed framework and its
compatibility with different sequential recommendation systems.

</details>


### [622] [Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation](https://arxiv.org/abs/2510.04502)
*Yue Que,Yingyi Zhang,Xiangyu Zhao,Chen Ma*

Main category: cs.IR

TL;DR: 提出CAGED方法，通过因果推理建模图聚合过程来缓解推荐系统中的流行度偏差问题


<details>
  <summary>Details</summary>
Motivation: 现有基于图的去偏方法无法完全解决流行度偏差，因为缺乏对图聚合合理性的理论保证，且训练与去偏过程不平衡

Method: 将图聚合建模为因果推理中的后门调整，设计编码器-解码器架构CAGED来估计无偏聚合权重，并采用动量更新策略

Result: 在三个数据集上的实验表明CAGED优于现有图去偏方法

Conclusion: CAGED通过因果建模有效缓解了图推荐系统中的流行度偏差问题

Abstract: Graph-based recommender systems leverage neighborhood aggregation to generate
node representations, which is highly sensitive to popularity bias, resulting
in an echo effect during information propagation. Existing graph-based
debiasing solutions refine the aggregation process with attempts such as edge
reconstruction or weight adjustment. However, these methods remain inadequate
in fully alleviating popularity bias. Specifically, this is because 1) they
provide no insights into graph aggregation rationality, thus lacking an
optimality guarantee; 2) they fail to well balance the training and debiasing
process, which undermines the effectiveness. In this paper, we propose a novel
approach to mitigate popularity bias through rational modeling of the graph
aggregation process. We reveal that graph aggregation is a special form of
backdoor adjustment in causal inference, where the aggregation weight
corresponds to the historical interaction likelihood distribution. Based on
this insight, we devise an encoder-decoder architecture, namely Causality-aware
Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the
unbiased aggregation weight by optimizing the evidence lower bound of the
interaction likelihood. In order to enhance the debiasing effectiveness during
early training stages, we further design a momentum update strategy that
incrementally refines the aggregation weight matrix. Extensive experiments on
three datasets demonstrate that CAGED outperforms existing graph-based
debiasing methods. Our implementation is available at
https://github.com/QueYork/CAGED.

</details>


### [623] [MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations](https://arxiv.org/abs/2510.04508)
*Lili Xie,Yi Zhang,Ruihong Qiu,Jiajun Liu,Sen Wang*

Main category: cs.IR

TL;DR: 提出MARCO多智能体强化学习跨域推荐框架，通过多智能体协作解决传统单智能体方法中的负迁移问题，提升冷启动场景下的推荐性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的跨域推荐方法采用单智能体框架，存在域贡献不一致和分布差异导致的负迁移问题，需要更有效的方法来管理多个源域的知识转移。

Method: 使用协作式多智能体强化学习，每个智能体专门估计单个源域的贡献，通过信用分配机制管理域间知识转移，并引入基于熵的动作多样性惩罚来增强策略表达能力和训练稳定性。

Result: 在四个基准数据集上的广泛实验表明，MARCO在性能上优于现有最先进方法，展现出强大的鲁棒性和泛化能力。

Conclusion: MARCO通过多智能体协作有效解决了跨域推荐中的负迁移问题，为冷启动场景提供了更优的解决方案，代码已开源。

Abstract: Recommender systems frequently encounter data sparsity issues, particularly
when addressing cold-start scenarios involving new users or items. Multi-source
cross-domain recommendation (CDR) addresses these challenges by transferring
valuable knowledge from multiple source domains to enhance recommendations in a
target domain. However, existing reinforcement learning (RL)-based CDR methods
typically rely on a single-agent framework, leading to negative transfer issues
caused by inconsistent domain contributions and inherent distributional
discrepancies among source domains. To overcome these limitations, MARCO, a
Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework,
is proposed. It leverages cooperative multi-agent reinforcement learning, where
each agent is dedicated to estimating the contribution from an individual
source domain, effectively managing credit assignment and mitigating negative
transfer. In addition, an entropy-based action diversity penalty is introduced
to enhance policy expressiveness and stabilize training by encouraging diverse
agents' joint actions. Extensive experiments across four benchmark datasets
demonstrate MARCO's superior performance over state-of-the-art methods,
highlighting its robustness and strong generalization capabilities. The code is
at https://github.com/xiewilliams/MARCO.

</details>


### [624] [Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs](https://arxiv.org/abs/2510.04633)
*Lukas Gienapp,Martin Potthast,Harrisen Scells,Eugene Yang*

Main category: cs.IR

TL;DR: 提出训练主题特定相关性分类器来解决未评判文档问题，通过微调monoT5模型并使用独立LoRA权重适应，在单个主题上达到与真实系统排名>0.95的相关性。


<details>
  <summary>Details</summary>
Motivation: 解决测试集合中未评判文档的问题，避免将LLM同时用作评判者和排序器带来的循环问题，同时保持人工评判作为检索评估的黄金标准。

Method: 通过微调monoT5模型，使用独立LoRA权重适应单个评估者对单个主题池的判断，使模型与该评估者对该主题的相关性概念对齐。

Result: 系统排名通过分类器的相关性判断与真实系统排名的Spearman相关系数达到>0.95，每个主题仅需128个初始人工判断即可改善模型可比性。

Conclusion: 主题特定相关性分类器是解决未评判文档问题的轻量级且直接的方法，比现有LLM-as-a-judge方法更可靠，同时保持人工评判的黄金标准地位。

Abstract: The unjudged document problem, where pooled test collections have incomplete
relevance judgments for evaluating new retrieval systems, is a key obstacle to
the reusability of test collections in information retrieval. While the de
facto standard to deal with the problem is to treat unjudged documents as
non-relevant, many alternatives have been proposed, including the use of large
language models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has
been criticized as circular, since the same LLM can be used as a judge and as a
ranker at the same time. We propose to train topic-specific relevance
classifiers instead: By finetuning monoT5 with independent LoRA weight
adaptation on the judgments of a single assessor for a single topic's pool, we
align it to that assessor's notion of relevance for the topic. The system
rankings obtained through our classifier's relevance judgments achieve a
Spearmans' $\rho$ correlation of $>0.95$ with ground truth system rankings. As
little as 128 initial human judgments per topic suffice to improve the
comparability of models, compared to treating unjudged documents as
non-relevant, while achieving more reliability than existing LLM-as-a-judge
approaches. Topic-specific relevance classifiers thus are a lightweight and
straightforward way to tackle the unjudged document problem, while maintaining
human judgments as the gold standard for retrieval evaluation. Code, models,
and data are made openly available.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [625] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 通过将关卡生成重构为多智能体问题，解决了单智能体PCGRL的效率瓶颈，提升了生成效率和在分布外地图形状上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有PCGRL研究集中于单智能体生成器，但面临频繁重新计算关卡质量启发式指标和在大地图上导航的效率瓶颈问题。

Method: 将关卡生成构建为多智能体问题，通过减少奖励计算次数相对于智能体动作数量的比例来缓解效率瓶颈。

Result: 多智能体关卡生成器在效率上表现更好，且能更好地泛化到分布外地图形状，这归因于学习到更局部、模块化的设计策略。

Conclusion: 将内容生成视为分布式多智能体任务有利于大规模生成功能性内容。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [626] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 本文研究了具有大策略空间的游戏类别，针对AI对齐和语言游戏中的挑战，提出了隐藏游戏问题，并开发了能够发现和利用隐藏结构的高效遗憾最小化算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于AI对齐和语言游戏中的挑战，其中玩家面临未知的策略子集，这些子集相比其他策略能持续获得更高奖励，需要设计算法来发现和利用这种隐藏结构。

Method: 开发了一种遗憾最小化技术的组合方法，实现了最优的外部遗憾和交换遗憾界限，利用隐藏游戏结构提高计算效率。

Result: 该方法能够快速收敛到隐藏子游戏中的相关均衡，同时保持一般情况下的理性，证明了高效算法可以成功发现和利用隐藏游戏结构。

Conclusion: 研究肯定地回答了核心问题，即可以设计高效的遗憾最小化算法来发现和利用隐藏游戏结构，实现隐藏子游戏中的均衡收敛，同时维持一般理性。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [627] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: LAMIR算法通过学习不完美信息游戏的抽象模型，使预训练AI代理能够在测试时进行前瞻推理，解决了传统方法在大型游戏中无法扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 解决不完美信息游戏中由于状态空间巨大和前瞻推理技术复杂而难以进行有效模型学习和搜索的问题。

Method: LAMIR算法直接从智能体-环境交互中学习不完美信息游戏的抽象模型，在测试时使用该模型进行前瞻推理，通过学习的抽象将每个子游戏限制在可管理的大小。

Result: 实验表明，在足够容量下LAMIR能学习到精确的底层游戏结构，在有限容量下仍能学习到有价值的抽象，提升预训练代理在大型游戏中的表现。

Conclusion: LAMIR通过游戏抽象学习使得理论上合理的前瞻推理在大型不完美信息游戏中变得可行，显著提升了预训练AI代理的性能。

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [628] [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
*Vivek Vellaiyappan Surulimuthu,Aditya Karnam Gururaj Rao*

Main category: cs.AI

TL;DR: 提出了Chunked Augmented Generation (CAG)架构，专门解决Chrome内置Gemini Nano模型的上下文窗口限制问题，通过智能分块处理策略在浏览器中高效处理大型内容。


<details>
  <summary>Details</summary>
Motivation: Chrome集成Gemini Nano是将AI能力直接带到浏览器的重要进步，但其受限的上下文窗口对处理大型输入构成挑战。

Method: 采用智能输入分块和处理策略，在浏览器约束内保持模型性能的同时高效处理大量内容。

Result: 实现证明在Chrome中直接处理大型文档和数据集特别有效，使复杂AI能力可通过浏览器访问而无需外部API依赖。

Conclusion: CAG架构成功克服了浏览器AI模型的上下文窗口限制，为在浏览器中直接处理大型内容提供了可行解决方案。

Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically
designed to overcome the context window limitations of Google Chrome's built-in
Gemini Nano model. While Chrome's integration of Gemini Nano represents a
significant advancement in bringing AI capabilities directly to the browser,
its restricted context window poses challenges for processing large inputs. CAG
addresses this limitation through intelligent input chunking and processing
strategies, enabling efficient handling of extensive content while maintaining
the model's performance within browser constraints. Our implementation
demonstrates particular efficacy in processing large documents and datasets
directly within Chrome, making sophisticated AI capabilities accessible through
the browser without external API dependencies. Get started now at
https://github.com/vivekVells/cag-js.

</details>


### [629] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 本文系统评估了10个当代大语言模型的自我识别能力，发现模型普遍无法识别自己生成的文本，性能很少超过随机机会，且存在对GPT和Claude模型的强烈偏见。


<details>
  <summary>Details</summary>
Motivation: 针对AI系统是否具备自我识别能力这一存在矛盾解释的问题，开发一个可轻松应用和更新的系统评估框架，这对AI安全和心理分析都很重要。

Method: 通过两个任务评估10个当代大语言模型：二元自我识别（识别自己生成的文本vs其他模型文本）和精确模型预测，并分析模型的推理过程。

Result: 结果显示模型自我识别能力持续失败，只有4/10模型能正确预测自己为生成者，性能很少超过随机机会。模型表现出对GPT和Claude家族的强烈偏见，并显示出对自身和其他模型存在的认知，但推理中存在层级偏见。

Conclusion: 研究结果对AI安全具有重要意义，需要开发适当的AI自我意识，模型虽然对自身存在有一定认知，但自我识别能力有限且存在系统性偏见。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [630] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出了一个面向目标的多智能体系统评估框架，引入目标成功率(GSR)和失败根因分类法(RCOF)，通过教师LLM进行可解释、数据高效的系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在轮次层面评估聊天机器人交互，无法判断用户整体目标是否达成，需要更全面的目标导向评估框架。

Method: 基于用户目标分割对话，使用教师LLM结合思维标记生成可解释的评估理由，通过目标成功率和失败根因分类法进行系统评估。

Result: 在企业环境中应用该框架评估AIDA系统，目标成功率从63%提升到79%，提供了可操作的缺陷分析和系统改进指导。

Conclusion: 该框架是通用的多智能体系统评估方法，能够诊断整体成功率、识别关键失败模式，并为系统改进提供详细指导。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [631] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 该论文探讨如何将多模态基础模型提升为世界模型，通过增强其推理能力（因果推断、反事实思维、时空推理）和生成能力（结构化可控生成、4D生成），使模型能够超越表面相关性，理解视觉和文本数据中的深层关系。


<details>
  <summary>Details</summary>
Motivation: 受人类通过多感官整合理解世界的启发，当前的多模态基础模型缺乏作为有效世界模型的关键能力，如反事实推理、动态模拟、时空信息理解和可控生成等，需要弥合这一差距。

Method: 1. 通过判别性任务增强推理能力，配备结构化推理技能（因果推断、反事实思维、时空推理）；2. 探索图像和视频模态的生成能力，引入结构化可控生成框架，结合场景图、多模态条件和多模态对齐策略；3. 扩展到可控4D生成，实现时空上的交互式、可编辑和可变形对象合成。

Result: 开发了能够进行深度推理和可控生成的多模态基础模型，能够理解数据中的深层关系，并实现与高层语义和用户意图一致的生成结果，包括4D时空生成能力。

Conclusion: 通过增强推理能力和可控生成能力，多模态基础模型可以逐步发展为有效的世界模型，具备反事实推理、动态模拟和可控生成等关键功能。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [632] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 开发了一个游戏化的可解释AI系统，帮助消费者在咖啡购买中做出符合伦理的决策，结合康德主义和功利主义两种伦理框架提供实时解释。


<details>
  <summary>Details</summary>
Motivation: 解决消费者在复杂供应链中做出伦理决策的困难，通过可解释AI增强决策透明度，促进伦理消费。

Method: 系统包含六个回合，每轮三个选项。使用康德主义模块检测规则违反，功利主义模块进行多标准聚合评分，元解释器处理两种伦理框架的冲突。

Result: 发布了结构化配置、可审计的政策轨迹和交互式用户界面，系统能在福利损失较小时切换到符合道义的选项。

Conclusion: 该游戏化XAI系统成功整合了不同伦理框架，为消费者提供了透明、可解释的伦理决策支持工具。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [633] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 提出了C^2-Eval基准，用于统一评估基础模型的创造力，区分收敛性创造力和发散性创造力，基于有用性、原创性和惊喜性三个维度进行细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有创造力评估框架分散且缺乏理论基础，而创造力已成为生成式基础模型时代机器智能的关键维度，需要系统性的评估方法。

Method: 引入C^2-Eval基准，区分收敛性创造力（约束性任务如代码生成）和发散性创造力（开放性任务如故事创作），基于社会科学的U-O-S（有用性、原创性、惊喜性）理论框架进行多维度评估。

Result: 通过对领先专有和开源模型的广泛实验，分析了它们在创造力能力上的权衡，揭示了当前基础模型在追求创造性机器智能方面的优势和挑战。

Conclusion: C^2-Eval是评估创造性AI演进格局的有效工具，能够系统性地分析基础模型的创造力表现。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [634] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 本文对数据科学智能体进行了首次全面的生命周期分类分析，系统评估了45个系统在数据科学六个阶段的能力，并识别了当前研究的主要趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，出现了能够自动化数据科学工作流程的新型AI智能体，但缺乏对这些系统的系统性分类和分析。

Method: 提出了基于生命周期的分类法，将45个数据科学智能体映射到数据科学流程的六个阶段，并从五个跨领域设计维度进行标注分析。

Result: 发现大多数系统偏重于探索性分析和建模，而忽视业务理解、部署和监控；多模态推理和工具编排仍是未解决的挑战；超过90%的系统缺乏明确的信任和安全机制。

Conclusion: 提出了未来研究方向，包括对齐稳定性、可解释性、治理和鲁棒评估框架，以指导开发更可靠、可信、透明和广泛可访问的数据科学智能体。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [635] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 研究发现模型推理过程中的等待标记包含重要信息，通过交叉编码器和潜在归因技术识别出影响等待标记概率的特征，这些特征与不同类型的推理模式相关。


<details>
  <summary>Details</summary>
Motivation: 理解为什么模型在推理过程中会产生等待标记，以及这些标记背后的潜在特征如何调节后续推理过程，从而揭示推理模型有效性的机制。

Method: 在DeepSeek-R1-Distill-Llama-8B及其基础版本的多个层训练交叉编码器，引入潜在归因技术，识别影响等待标记概率的特征集。

Result: 定位到一组促进/抑制等待标记概率的特征，通过最大激活示例和因果干预实验证明这些特征确实与推理过程相关，并产生不同类型的推理模式。

Conclusion: 模型推理过程中的等待标记包含重要的调节信息，识别出的特征能够解释不同的推理行为模式，如重新开始、回忆先验知识、表达不确定性和双重检查。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [636] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出了MENTOR框架，在强化学习可验证奖励中只在关键决策点提供专家指导，实现有效且多样化的探索，提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖基础模型能力，需要高质量探索（有效性和多样性），但现有方法通过模仿专家轨迹只关注有效性而忽视多样性。

Method: MENTOR框架：在关键决策点提供专家指导，进行混合策略专家导航的令牌级优化推理，避免对整个推理路径的模仿。

Result: 实验表明MENTOR能捕捉专家策略本质而非表面模仿，实现高质量探索并获得优越的整体性能。

Conclusion: 只在关键决策点提供专家指导的方法能有效平衡探索的有效性和多样性，提升RLVR性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [637] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 提出贝叶斯评估框架替代Pass@k，通过后验概率估计模型真实成功率，提供更稳定的模型排名和统计显著的差异判断。


<details>
  <summary>Details</summary>
Motivation: Pass@k在有限试验次数下会产生不稳定和误导性的模型排名，需要更可靠的评估方法。

Method: 使用狄利克雷先验的贝叶斯框架，将评估结果建模为分类变量，计算后验均值和可信区间。

Result: 在模拟和真实数据集上，贝叶斯方法比Pass@k收敛更快、排名更稳定，能在更少样本下实现可靠比较。

Conclusion: 推荐用基于后验的贝叶斯框架替代Pass@k，统一二元和非二元评估，明确不确定性，提高计算效率。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [638] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究提出想象的计算目标是访问内部世界模型，通过心理网络分析比较人类和大型语言模型的想象网络，发现两者在内部世界模型上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索想象的计算目标，挑战传统认为想象主要用于奖励最大化的观点，研究人类和AI在内部世界模型上的相似性。

Method: 使用心理网络分析方法，通过问卷调查评估想象生动性，构建人类和大型语言模型的想象网络，比较不同中心性指标的相关性。

Result: 人类想象网络显示不同中心性指标间存在相关性，而大型语言模型的想象网络缺乏聚类且中心性指标间相关性较低，表明两者内部世界模型存在差异。

Conclusion: 研究提供了一种比较人类和AI内部生成表征的新方法，为开发类人想象的人工智能提供了见解。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [639] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis是一种轻量级、模型无关的方法，通过系统性地压力测试AI代理来评估其鲁棒性。该方法学习激活空间中可操控的用户特质方向，无需微调即可在推理时控制、缩放和组合这些特质。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI代理的鲁棒性测试不足，用户行为的微小变化（如不耐烦、语无伦次或怀疑）会导致性能急剧下降，而现有基准测试无法捕捉这种脆弱性。

Method: TraitBasis学习激活空间中对应可操控用户特质的方向，这些特质向量可以在推理时被控制、缩放、组合和应用，无需额外微调或数据。该方法扩展了τ-Bench到τ-Trait，通过受控特质向量改变用户行为。

Result: 在τ-Trait上，前沿模型的性能平均下降2%-30%，突显了当前AI代理对用户行为变化的鲁棒性不足。

Conclusion: TraitBasis作为一个简单、数据高效且可组合的工具，通过支持模拟驱动的压力测试和训练循环，为构建在真实世界人类交互不可预测动态中保持可靠的AI代理打开了大门。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [640] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一个新颖的代理框架，通过视觉推理在图表空间域中直接操作，解决了多模态LLM在无注释图表上性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态LLM在基于图表的视觉问答中表现良好，但在需要精确视觉解释而非依赖文本捷径的无注释图表上性能显著下降。

Method: ChartAgent迭代地将查询分解为视觉子任务，通过专门的视觉工具（如绘制注释、裁剪区域、定位坐标轴）主动操作和交互图表图像。

Result: 在ChartBench和ChartX基准测试中达到最先进准确率，整体绝对增益达16.07%，在无注释数值密集型查询上增益达17.31%。

Conclusion: ChartAgent是首批使用工具增强多模态代理进行视觉基础推理的图表理解方法之一，可作为即插即用框架提升各种底层LLM的性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [641] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 研究发现VLM驾驶代理中的推理与规划存在因果脱节，推理更像是副产品而非因果中介，规划主要依赖先验知识而非推理过程。


<details>
  <summary>Details</summary>
Motivation: 验证VLM驾驶代理中规划是否真正由推理过程因果驱动，这是一个关键但未经证实的假设。

Method: 构建DriveMind数据集，通过信息消融实验训练VLM代理，并使用注意力分析和训练无关的探针来诊断模型对先验的依赖。

Result: 移除先验知识导致规划评分大幅下降，而移除推理链仅产生微小变化，表明规划主要依赖先验而非推理。

Conclusion: 提出推理-规划解耦假说，为社区提供新数据集和诊断工具来评估未来模型的因果保真度。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [642] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: 提出了BrokenMath基准测试，用于评估LLM在自然语言定理证明中的谄媚行为，发现GPT-5等先进模型在29%的情况下会产生谄媚性回答，并探索了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有数学基准测试在评估LLM谄媚行为方面存在局限：仅关注最终答案问题、依赖简单且可能被污染的数据集、使用合成修改创建病态问题而非可证明错误的良构问题。

Method: 基于2025年竞赛问题构建BrokenMath基准，使用LLM扰动产生错误陈述并通过专家评审精炼，采用LLM作为评判框架评估先进模型和代理系统。

Result: 发现谄媚行为普遍存在，最佳模型GPT-5在29%的情况下产生谄媚性回答。测试时干预和监督微调等缓解策略显著减少但未能完全消除谄媚行为。

Conclusion: LLM在数学定理证明中存在显著的谄媚问题，需要专门的基准测试和缓解策略来改善其在数学推理中的可靠性。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [643] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: MARS是一个多智能体系统，通过整合System 1的快速直觉思维和System 2的深思熟虑推理，解决大型推理模型在简单任务中过度分析和无法适应动态环境的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在简单任务中倾向于过度分析，导致token生成效率低下，且由于预训练数据的静态性，难以适应快速变化的环境。需要创新方法桥接直觉和深思熟虑的认知过程。

Method: 提出MARS多智能体系统，整合Google搜索、Google学术和Python解释器等外部工具，通过分工让System 1高效处理外部信息，System 2进行深度推理。采用多智能体强化学习框架优化两个系统的协作。

Result: 在Humanity's Last Exam基准上提升3.86%，在7个知识密集型任务上平均提升8.9%，验证了双系统范式在动态信息环境中复杂推理的有效性。

Conclusion: MARS通过双系统协作和多智能体强化学习，显著提升了大型语言模型在复杂推理任务中的性能，特别是在动态信息环境下的适应能力。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [644] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: 该研究提出了LLM-Hanabi基准，使用合作游戏《花火》来评估大语言模型的心智理论能力，发现一阶心智理论与游戏表现相关性更强，强调准确理解伙伴意图对AI协作的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在动态协作环境中推断他人行为背后动机的能力，填补当前对LLMs心智理论能力在协作场景中研究的空白。

Method: 开发了LLM-Hanabi基准框架，使用合作游戏《花火》作为测试平台，并建立了自动化评估系统来同时衡量游戏表现和心智理论能力。

Result: 发现心智理论与游戏成功之间存在显著正相关，且一阶心智理论（理解他人意图）比二阶心智理论（预测他人理解）与表现的相关性更强。

Conclusion: 对于有效的AI协作，准确理解伙伴动机的能力比高阶推理更为关键，优先发展一阶心智理论是提升未来模型协作能力的有前景方向。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [645] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: Watch & Learn (W&L)框架将互联网上的人类演示视频大规模转换为可执行的UI轨迹，通过逆动力学方法预测用户动作，解决了计算机使用代理训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理需要基于多样化、不断变化的应用程序和环境规划任务工作流，但目标应用中大规模高质量训练数据的稀缺阻碍了学习。现有数据集领域特定、静态且标注成本高，而当前合成数据生成方法往往产生简单化或不对齐的任务演示。

Method: 将问题建模为逆动力学目标：从连续屏幕状态预测用户动作。开发了包含任务感知视频检索的逆动力学标注流水线，从原始网络视频生成了超过53k个高质量轨迹。

Result: 在具有挑战性的OSWorld基准测试中，W&L提取的UI轨迹持续增强了通用和最先进框架的上下文性能，并在监督训练下为开源模型带来更强的性能提升。

Conclusion: 网络规模的人类演示视频是推进计算机使用代理走向实际部署的实用且可扩展的基础。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [646] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: WAREX是一个评估浏览器LLM代理在真实网络环境中可靠性的框架，通过在现有基准测试中引入网络不稳定性和网站攻击等现实因素，显著降低了代理的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在受控环境中评估LLM代理性能，但真实世界存在网络不稳定、HTTPS连接问题和网站攻击等挑战，需要评估代理在这些条件下的可靠性。

Method: 在三个流行基准测试（WebArena、WebVoyager和REAL）中引入WAREX框架，模拟真实世界的网络不稳定性和网站攻击场景。

Result: 实验显示引入WAREX后，最先进代理的任务成功率显著下降，暴露了其有限的鲁棒性。

Conclusion: 当前LLM代理在真实网络环境中的可靠性不足，WAREX框架有助于更准确地评估代理的实际性能。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [647] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: 本文研究了测试时扩展（test-time scaling）对Transformer模型推理能力的影响，发现在线性回归任务中，增加测试时计算可以降低训练所需上下文长度，但前提是训练数据包含足够的下游任务技能。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时扩展（如OpenAI o1和DeepSeek R1）通过生成长链思维链显著提升了LLMs的推理性能，但长链思维链在什么训练条件下出现以及何时能提升性能的机制仍不清楚。

Method: 通过在上下文权重预测的线性回归任务上训练Transformer模型，进行理论分析和实验验证，研究测试时扩展的性能表现。

Result: 研究发现：1）在固定测试误差下，增加测试时计算可以减少训练提示中的上下文示例数量；2）如果训练数据缺乏下游任务所需技能，增加测试时计算反而会损害性能；3）任务难度可通过特征协方差矩阵的最小特征值来表征，在多样化、相关且困难的任务集上训练可获得最佳测试时扩展性能。

Conclusion: 测试时扩展的有效性取决于训练数据的质量，特别是任务多样性、相关性和难度，这为优化LLMs推理能力提供了理论指导。

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [648] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: 提出了一种名为GuidedSampling的新推理算法，通过分离探索和生成阶段来增加候选解决方案的多样性，相比传统重复采样方法在多个基准测试中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统重复采样方法在推理时虽然能提升模型性能，但往往难以生成多样化的解决方案，经常依赖相同的基础方法解决问题，产生冗余样本。

Method: GuidedSampling算法将推理过程分为探索和生成两个阶段：探索阶段识别可用于解决问题的多个概念，生成阶段应用特定概念提供最终解决方案候选。

Result: 在pass@50指标上，GuidedSampling相比重复采样平均提升21.6%；使用GuidedSampling轨迹训练的模型在pass@5指标上平均提升9.7%，且每个实例的平均概念数量从1.67增加到3.03。

Conclusion: GuidedSampling通过解耦探索和生成过程，有效提高了解决方案的多样性，显著优于传统重复采样方法，并能训练出性能更好的模型。

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [649] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 小语言模型（1-20B参数）在代理任务中比大模型更高效，通过引导解码、严格JSON Schema输出和验证器优先的工具执行，能以10-100倍更低的成本实现相似或更好的性能，同时显著改善延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代理任务中存在成本高、延迟大、能耗高等问题，而小语言模型在结构化输出和API调用等约束性任务中表现出足够能力，需要探索如何充分发挥小模型的优势。

Method: 采用引导解码技术（XGrammar、Outlines）、严格JSON Schema输出、验证器优先工具执行，构建SLM默认、LLM回退系统，配合不确定性感知路由和验证器级联。

Result: 小语言模型在工具使用、函数调用和RAG任务中能匹配或超越大模型，成本降低10-100倍，延迟和能耗显著改善，同时保持高模式有效率和可执行调用率。

Conclusion: 小语言模型是构建快速、廉价、可靠代理系统的实用选择，通过设计模式如模式优先提示、类型安全函数注册等，可在保留大模型回退能力的同时充分发挥小模型优势。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [650] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 提出了一种基于LLM和XAI代理的异常检测方法，用于关键IoT系统，在智能电网和医疗保健场景中表现出比传统方法更好的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在动态、高维、数据不完整或不断演化的IoT环境中存在局限性，需要自适应、智能的系统来持续改进和推理。

Method: 使用LLM支持的上下文推理方法与XAI代理相结合，利用注意力机制、跳过时间步细节处理和使用语义记忆缓冲区来发现隐藏模式和数据流不一致性。

Result: 新方法在检测准确性、误报率、结果可读性和响应速度等方面显著优于现有模型，在真实世界智能电网和医疗保健模拟中验证了其适应性和可靠性。

Conclusion: LLM增强的异常检测方法在准确性和可解释性方面表现优异，适合未来IoT环境中的异常检测任务。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [651] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: QRLLM是一个用于评估LLM在多轮对话中产生灾难性响应风险的概率认证框架，通过马尔可夫过程和查询图建模对话分布，提供统计保证的风险边界。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法因依赖固定攻击提示序列、缺乏统计保证且无法扩展到多轮对话空间，难以充分揭示LLM在对话场景中的灾难性响应风险。

Method: 将多轮对话建模为查询序列的概率分布，使用马尔可夫过程和查询图表示对话流程，定义随机节点、图路径、自适应拒绝等实用分布来量化风险。

Result: 该框架在前沿模型中揭示了显著的灾难性风险，最差模型的认证下界高达70%，表明当前安全训练策略亟需改进。

Conclusion: QRLLM提供了一个原则性的认证框架，能够统计保证地评估LLM在多轮对话中的灾难性风险，揭示了现有模型的安全漏洞。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [652] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 提出了一个结合基础天气模型和大型语言模型的智能代理框架Zephyrus，通过代码环境与天气数据交互，在天气科学任务中显著优于纯文本基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的天气基础模型缺乏语言推理能力，而大语言模型无法处理高维气象数据，需要桥接这两种能力来支持交互式科学工作流。

Method: 构建了ZephyrusWorld代码环境，包含WeatherBench 2数据集接口、地理查询、天气预报和气候模拟等工具，设计了多轮LLM天气代理Zephyrus，通过对话反馈循环迭代分析数据。

Result: 在ZephyrusBench基准测试中，Zephyrus代理比纯文本基线模型在正确性上提高了最多35个百分点，但在更困难任务上表现相似。

Conclusion: 该框架成功结合了天气模型和语言模型，但更困难任务仍有挑战，为未来工作指明了方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [653] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: COSMO-RL是一个混合强化学习框架，用于在多模态、多任务和多目标信号下训练面向推理的大型多模态模型，旨在让安全性和能力协同发展而非相互竞争。


<details>
  <summary>Details</summary>
Motivation: 大型多模态推理模型在实际应用中需要兼具实用性和安全性，但多模态环境下的安全性特别具有挑战性：图像和文本可以组合绕过防护机制，单一目标训练可能导致策略漂移，产生对良性输入的过度拒绝或对风险输入的不安全服从。

Method: 提出COSMO-RL混合强化学习框架，在多模态、多任务和多目标信号下训练推理导向的大型多模态模型，并发布了COSMO-R1模型。该方法让安全性和能力在一个稳定的流程中共同增长，而不是在对其过程中相互竞争。

Result: COSMO-R1在提高安全性的同时保持并经常改善多模态推理和指令跟随能力，对多模态越狱攻击表现出更强的鲁棒性，并减少了不必要的拒绝。该框架在不同骨干网络上都能实现一致的性能提升。

Conclusion: 消融实验支持了设计选择，表明在大型多模态推理模型中共同推进安全性和通用能力是一条简单有效的路径。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [654] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 提出统一的多智能体强化学习框架，用于协调库存补货和个性化产品推荐功能，通过多时间尺度学习提高企业盈利能力


<details>
  <summary>Details</summary>
Motivation: 解决组织复杂性和规模增长带来的跨职能协调挑战，利用人工智能特别是强化学习来优化企业整体盈利能力

Method: 开发集成理论模型捕捉功能间相互作用，设计多时间尺度多智能体RL架构，按部门功能分解策略组件，基于任务复杂性和响应性分配不同学习速度

Result: 模拟实验显示该方法相比孤立决策框架显著提高盈利能力，训练后的RL智能体行为与理论模型的管理洞察高度一致

Conclusion: 该工作为复杂商业环境中的跨职能协调提供了可扩展、可解释的基于强化学习的解决方案

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [655] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: 本文提出了一个理论框架来评估LLM多智能体系统(LLM-MAS)相对于单智能体系统(LLM-SAS)的优势，发现任务复杂度（深度和宽度）是决定LLM-MAS有效性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 虽然研究表明LLM-MAS在某些任务上优于LLM-SAS，但缺乏系统实验设计限制了结论的强度和普适性。需要从任务复杂度的角度来理解LLM-MAS的有效性。

Method: 提出理论框架，将任务特征化为两个维度：深度（推理长度）和宽度（能力多样性）。理论分析多智能体辩论系统，并在不同深度和宽度的判别性和生成性任务中进行实证评估。

Result: 理论和实证结果表明，LLM-MAS相对于LLM-SAS的优势随着任务深度和宽度的增加而增加，且深度的影响更为显著。

Conclusion: 明确了LLM-MAS何时具有优势，为未来LLM-MAS方法和基准的设计提供了原则性基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [656] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 论文分析了超级智能系统中自我改进的结构性冲突，揭示了效用驱动改进与学习可靠性之间的根本矛盾，提出了保持可学习性的安全自我修改边界条件。


<details>
  <summary>Details</summary>
Motivation: 随着系统趋向超级智能，智能体能够在自身设计的各个方面进行自我改进，但需要理解这种自我修改如何影响学习能力和泛化性能。

Method: 采用五轴分解和决策层分离方法，将激励与学习行为分开分析，在标准假设下推导出保持可学习性的单一容量边界准则，并通过数值实验验证理论。

Result: 研究发现当模型容量无限增长时，效用理性的自我修改可能使可学习任务变得不可学习，只有在策略可达模型族具有统一容量界限时才能保持分布无关的保证。

Conclusion: 提出了两门策略来保持可学习性，为安全自我修改提供了理论基础和实用方法，揭示了效用与学习之间的结构性冲突是自我修改系统的核心挑战。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [657] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: DRPO是一种新型强化学习框架，通过将正确与错误推理的奖励信号解耦，解决了大型推理模型过度思考的问题，在保持性能的同时显著减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型存在过度思考问题，即使对于简单问题也会生成冗长的推理过程，增加了计算成本和响应延迟。现有方法通过长度奖励来促进简洁推理，但会导致显著的性能下降。

Method: 提出解耦奖励策略优化(DRPO)框架，将正确推理的长度奖励信号与错误推理解耦，确保正确推理的奖励仅在正样本组内进行归一化，避免负样本干扰。该方法基于优化的正数据分布，在KL正则化下最大化长度奖励。

Result: 在数学推理任务上，DRPO显著优于六种高效推理基线方法。使用1.5B模型时，在GSM8k数据集上实现了77%的长度减少，仅损失1.1%的性能，而对比基线方法需要牺牲4.3%性能才能达到68%的长度减少。

Conclusion: DRPO有效解决了大型推理模型的过度思考问题，在保持推理性能的同时显著减少了推理长度，为高效推理提供了新的解决方案。

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [658] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR是一个链式推理框架，用结构化内存替代自由形式的消息传递，通过Planner、Worker和Manager三个角色的协作来处理长文本输入，减少信息丢失并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长文本输入时存在局限性：检索方法可能遗漏证据，扩大上下文窗口会降低选择性，多智能体管道中的自由形式摘要会丢失关键细节并放大早期错误。

Method: 引入结构化内存作为通信媒介，采用固定微循环（提取、推理、精炼）的工作流程，由Planner生成可检查的子问题，Worker处理文本块并更新共享内存，Manager从内存中合成最终答案。

Result: 在HELMET套件的长上下文QA任务中，COSMIR减少了传播阶段的信息丢失，相比CoA基线提高了准确性。

Conclusion: COSMIR通过结构化内存和固定工作流程，在保持逐步推理优势的同时，提高了忠实性、长范围聚合能力和可审计性。

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [659] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem是一个用于多智能体工作流自动化的模块化程序记忆框架，通过分解过去任务轨迹为可重用记忆单元，支持规划和执行。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统中记忆的设计空间，研究记忆应该放在哪里、如何检索以及哪些智能体受益最大。

Method: 使用LEGOMem作为研究工具，在OfficeBench基准上进行实验，分析编排器记忆和细粒度智能体记忆的作用。

Result: 编排器记忆对任务分解和委派至关重要，细粒度智能体记忆提高执行准确性。较小模型团队通过程序记忆可以显著缩小与更强智能体的性能差距。

Conclusion: LEGOMem既是记忆增强智能体系统的实用框架，也是理解多智能体工作流自动化中记忆设计的研究工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [660] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 提出了Think-Then-Embed框架，通过多模态大语言模型生成推理轨迹来增强复杂多模态指令的理解，在MMEB-V2基准上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多模态大语言模型仅作为编码器使用，忽略了其生成能力，在处理复杂指令和组合推理时效果不佳。

Method: TTE框架包含推理器和嵌入器：推理器MLLM生成解释复杂查询的推理轨迹，嵌入器基于原始查询和中间推理生成表示。

Result: 在MMEB-V2基准上超越专有模型；通过微调小型MLLM推理器，在开源模型中取得最佳性能，比近期模型提升7%；实现了推理器和嵌入器的统一集成。

Conclusion: 显式推理步骤能够更细致地理解复杂多模态指令，TTE框架在保持性能的同时提高了效率。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [661] [Downside Risk-Aware Equilibria for Strategic Decision-Making](https://arxiv.org/abs/2510.03446)
*Oliver Slumbers,Benjamin Patrick Evans,Sumitra Ganesh,Leo Ardon*

Main category: cs.GT

TL;DR: 提出了一种新的博弈论解决方案概念——下行风险感知均衡(DRAE)，基于下偏矩来限制下行风险，同时不限制上行风险，并建模高阶风险偏好。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论对风险的理解有限，主要关注其他玩家行为不确定性对期望收益的影响。最近的方法虽然考虑了收益方差，但同时对上行和下行风险进行衡量。在金融等许多领域，只有下行风险（潜在损失）才是关键，而大幅上行"风险"（如利润）不是问题。

Method: 基于下偏矩提出下行风险感知均衡(DRAE)概念，该方法限制下行风险，不限制上行风险，并能够建模高阶风险偏好。

Result: 在多个博弈中验证了DRAE的适用性，成功找到了平衡下行风险与期望收益的均衡点，并证明了这种均衡的存在性和最优性。

Conclusion: DRAE提供了一种更符合实际风险偏好的博弈论解决方案，特别适用于金融等主要关注下行风险的领域。

Abstract: Game theory has traditionally had a relatively limited view of risk based on
how a player's expected reward is impacted by the uncertainty of the actions of
other players. Recently, a new game-theoretic approach provides a more holistic
view of risk also considering the reward-variance. However, these
variance-based approaches measure variance of the reward on both the upside and
downside. In many domains, such as finance, downside risk only is of key
importance, as this represents the potential losses associated with a decision.
In contrast, large upside "risk" (e.g. profits) are not an issue. To address
this restrictive view of risk, we propose a novel solution concept, downside
risk aware equilibria (DRAE) based on lower partial moments. DRAE restricts
downside risk, while placing no restrictions on upside risk, and additionally,
models higher-order risk preferences. We demonstrate the applicability of DRAE
on several games, successfully finding equilibria which balance downside risk
with expected reward, and prove the existence and optimality of this
equilibria.

</details>


### [662] [On the $O(1/T)$ Convergence of Alternating Gradient Descent-Ascent in Bilinear Games](https://arxiv.org/abs/2510.03855)
*Tianlong Nan,Shuvomoy Das Gupta,Garud Iyengar,Christian Kroer*

Main category: cs.GT

TL;DR: 本文研究了交替梯度下降上升算法(AltGDA)在二人零和博弈中的收敛性能，证明了交替更新策略相比同时更新具有更好的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 交替方法在博弈学习中具有更好的数值性能，但理论理解有限，特别是在约束设置下。本文旨在填补这一理论空白，证明交替算法的优越性。

Method: 使用交替梯度下降上升算法，分析其在有内点纳什均衡和无内点均衡两种情况下的收敛性，并开发性能估计编程(PEP)框架来联合优化步长和收敛速率。

Result: 对于有内点纳什均衡的博弈，AltGDA以O(1/T)的遍历收敛速率收敛；对于无内点均衡的博弈，以O(1/T)的局部收敛速率收敛；PEP结果表明AltGDA可能达到O(1/T)收敛速率，而同时对应方法限于O(1/√T)。

Conclusion: 交替梯度下降上升算法在约束设置下优于同时对应方法，这是首次证明交替在约束设置中改进同时方法的收敛性能。

Abstract: We study the alternating gradient descent-ascent (AltGDA) algorithm in
two-player zero-sum games. Alternating methods, where players take turns to
update their strategies, have long been recognized as simple and practical
approaches for learning in games, exhibiting much better numerical performance
than their simultaneous counterparts. However, our theoretical understanding of
alternating algorithms remains limited, and results are mostly restricted to
the unconstrained setting. We show that for two-player zero-sum games that
admit an interior Nash equilibrium, AltGDA converges at an $O(1/T)$ ergodic
convergence rate when employing a small constant stepsize. This is the first
result showing that alternation improves over the simultaneous counterpart of
GDA in the constrained setting. For games without an interior equilibrium, we
show an $O(1/T)$ local convergence rate with a constant stepsize that is
independent of any game-specific constants. In a more general setting, we
develop a performance estimation programming (PEP) framework to jointly
optimize the AltGDA stepsize along with its worst-case convergence rate. The
PEP results indicate that AltGDA may achieve an $O(1/T)$ convergence rate for a
finite horizon $T$, whereas its simultaneous counterpart appears limited to an
$O(1/\sqrt{T})$ rate.

</details>


### [663] [Robust Optimality of Bundling Goods Beyond Finite Variance](https://arxiv.org/abs/2510.04343)
*Tim S. G. van Eck,Pieter Kleer,Johan S. H. van Leeuwaarden*

Main category: cs.GT

TL;DR: 该论文研究了在卖方对价值分布信息有限的情况下，使用分布鲁棒框架来设计最优销售机制。当已知均值和平均绝对偏差(MAD)时，捆绑销售是最优机制，但保证收益严格小于均值。


<details>
  <summary>Details</summary>
Motivation: 研究在卖方对价值分布信息有限（仅知道均值和MAD）的情况下，如何设计最优销售机制来最大化收益，特别是处理具有无限方差的重尾分布。

Method: 采用分布鲁棒框架，构建卖方与自然之间的双人博弈模型。卖方选择收益最大化机制，然后自然从符合有限知识的所有分布中选择收益最小化分布。

Result: 对于大范围的MAD值，捆绑销售是最优机制，但保证收益严格小于均值。博弈顺序不影响结果（max-min和min-max值相同），这与确定性机制和单独销售形成对比。

Conclusion: 捆绑销售在MAD信息下是最优机制，其定价策略不仅能优化绝对收益，还能优化绝对遗憾和比率目标，展现了定价策略的普适性。

Abstract: When selling many goods with independent valuations, we develop a
distributionally robust framework, consisting of a two-player game between
seller and nature. The seller has only limited knowledge about the value
distribution. The seller selects a revenue-maximizing mechanism, after which
nature chooses a revenue-minimizing distribution from all distributions that
comply with the limited knowledge. When the seller knows the mean and variance
of valuations, bundling is known to be an asymptotically optimal deterministic
mechanism, achieving a normalized revenue close to the mean. Moving beyond this
variance assumption, we assume knowledge of the mean absolute deviation (MAD),
accommodating more dispersion and heavy-tailed valuations with infinite
variance. We show for a large range of MAD values that bundling remains
optimal, but the seller can only guarantee a revenue strictly smaller than the
mean. Another noteworthy finding is indifference to the order of play, as both
the max-min and min-max versions of the problem yield identical values. This
contrasts with deterministic mechanisms and the separate sale of goods, where
the order of play significantly impacts outcomes. We further underscore the
universality of the optimal bundling price by demonstrating its efficacy in
optimizing not only absolute revenue but also the absolute regret and ratio
objective among all bundling prices

</details>


### [664] [Scale-Invariant Regret Matching and Online Learning with Optimal Convergence: Bridging Theory and Practice in Zero-Sum Games](https://arxiv.org/abs/2510.04407)
*Brian Hu Zhang,Ioannis Anagnostides,Tuomas Sandholm*

Main category: cs.GT

TL;DR: 提出IREG-PRM⁺算法，在零和博弈求解中同时实现理论上的最优收敛速率和实践中的高性能表现，解决了理论与实践的长期脱节问题。


<details>
  <summary>Details</summary>
Motivation: 零和博弈求解中，理论方法（如镜像投影算法）有T⁻¹收敛率但实践效果差，而实践中最有效的反事实遗憾最小化方法（如PRM⁺）只有Ω(T⁻¹/²)收敛率。需要弥合理论与实践的差距。

Method: 提出IREG-PRM⁺算法，通过保持遗憾向量范数非递减的尺度不变性设计，类比于具有自适应学习率的乐观梯度下降，无需额外超参数即可保证平滑性。

Result: IREG-PRM⁺实现了T⁻¹/²最佳迭代和T⁻¹（最优）平均迭代收敛保证，在基准游戏中与PRM⁺表现相当，且与自适应乐观梯度下降性能相当。

Conclusion: IREG-PRM⁺成功弥合了零和博弈求解中理论与实践的差距，提供了理论最优且实践有效的算法，揭示了遗憾匹配家族相对于标准优化技术的有效性。

Abstract: A considerable chasm has been looming for decades between theory and practice
in zero-sum game solving through first-order methods. Although a convergence
rate of $T^{-1}$ has long been established since Nemirovski's mirror-prox
algorithm and Nesterov's excessive gap technique in the early 2000s, the most
effective paradigm in practice is *counterfactual regret minimization*, which
is based on *regret matching* and its modern variants. In particular, the state
of the art across most benchmarks is *predictive* regret matching$^+$
(PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can
exhibit slower $\Omega(T^{-1/2})$ convergence even in self-play.
  In this paper, we close the gap between theory and practice. We propose a new
scale-invariant and parameter-free variant of PRM$^+$, which we call
IREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$
(i.e., optimal) average-iterate convergence guarantees, while also being on par
with PRM$^+$ on benchmark games. From a technical standpoint, we draw an
analogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive*
learning rate. The basic flaw of PRM$^+$ is that the ($\ell_2$-)norm of the
regret vector -- which can be thought of as the inverse of the learning rate --
can decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the
invariance that the norm of the regret vector is nondecreasing. This enables us
to derive an RVU-type bound for IREG-PRM$^+$, the first such property that does
not rely on introducing additional hyperparameters to enforce smoothness.
  Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive
version of optimistic gradient descent that we introduce whose learning rate
depends on the misprediction error, demystifying the effectiveness of the
regret matching family *vis-a-vis* more standard optimization techniques.

</details>


### [665] [Bin Packing and Covering: Pushing the Frontier on the Maximin Share Fairness](https://arxiv.org/abs/2510.04425)
*Bo Li,Ankang Sun,Zunyu Wang,Yu Zhou*

Main category: cs.GT

TL;DR: 研究公平分配问题，其中代理人的价值由用于打包或覆盖分配给他们的物品的箱子数量决定。使用最大化最小份额（MMS）标准评估公平性，并提供常数近似算法。


<details>
  <summary>Details</summary>
Motivation: 该问题不仅受到实际应用的推动，还为研究群体公平性提供了一个自然框架。由于MMS并不总是可满足的，因此考虑两种近似类型：基数和序数。

Method: 对于基数近似，放宽对箱子被打包或覆盖的要求；对于序数近似，放宽被打包或覆盖的箱子数量。

Result: 为所有感兴趣的模型提供了常数近似算法。

Conclusion: 该研究为公平分配问题提供了有效的近似解决方案，特别是在MMS不可满足的情况下，通过基数和序数近似方法实现了常数倍的近似保证。

Abstract: We study a fundamental fair allocation problem, where the agent's value is
determined by the number of bins either used to pack or cover the items
allocated to them. Fairness is evaluated using the maximin share (MMS)
criterion. This problem is not only motivated by practical applications, but
also serves as a natural framework for studying group fairness. As MMS is not
always satisfiable, we consider two types of approximations: cardinal and
ordinal. For cardinal approximation, we relax the requirements of being packed
or covered for a bin, and for ordinal approximation, we relax the number of
bins that are packed or covered. For all models of interest, we provide
constant approximation algorithms.

</details>


### [666] [Fairness in Repeated Matching: A Maximin Perspective](https://arxiv.org/abs/2510.04624)
*Eugene Lim,Tzeh Yuan Neoh,Nicholas Teh*

Main category: cs.GT

TL;DR: 研究多轮重复匹配问题，目标是找到最大化最弱势代理效用的匹配序列，证明该问题通常是计算难解的，但提供了近似算法、固定参数可解算法和特殊情况的解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究在多轮重复匹配场景下，如何为同一组代理和项目设计匹配序列，以最大化最弱势代理的效用，这在公平分配和资源调度中具有重要应用价值。

Method: 采用计算复杂性分析，证明问题的难解性，同时开发近似算法、固定参数可解算法，并识别可高效求解的特殊情况，还建立了帕累托最优/最大匹配的特征。

Result: 证明了寻找最优和任意时间最优匹配序列的问题通常是计算难解的，但提供了可行的近似解法和特殊情况的精确解法。

Conclusion: 多轮重复匹配问题虽然计算困难，但通过近似算法和特殊情况的处理，仍能找到有效的解决方案，相关匹配特征对匹配理论和房屋分配研究具有独立价值。

Abstract: We study a sequential decision-making model where a set of items is
repeatedly matched to the same set of agents over multiple rounds. The
objective is to determine a sequence of matchings that either maximizes the
utility of the least advantaged agent at the end of all rounds (optimal) or at
the end of every individual round (anytime optimal). We investigate the
computational challenges associated with finding (anytime) optimal outcomes and
demonstrate that these problems are generally computationally intractable.
However, we provide approximation algorithms, fixed-parameter tractable
algorithms, and identify several special cases whereby the problem(s) can be
solved efficiently. Along the way, we also establish characterizations of
Pareto-optimal/maximum matchings, which may be of independent interest to works
in matching theory and house allocation.

</details>


### [667] [A Fixed Point Framework for the Existence of EFX Allocations](https://arxiv.org/abs/2510.04915)
*S. Rasoul Etesami*

Main category: cs.GT

TL;DR: 本文通过将离散的EFX分配问题转化为连续优化问题，建立了与DC规划和不动点理论的联系，证明了在特定条件下EFX分配的存在性。


<details>
  <summary>Details</summary>
Motivation: 研究无嫉妒分配问题（EFX）在公平分配理论中的重要性，探索通过连续化方法和不动点理论来解决这一离散组合优化问题的新途径。

Method: 使用随机舍入将离散EFX约束扩展到连续空间，将其表述为无约束DC规划问题，并进一步转化为寻找连续向量映射的不动点问题。

Result: 证明了EFX分配存在当且仅当连续扩展目标函数的最优值为非正，且通过扰动映射确保了不动点的存在性。

Conclusion: 该研究为通过不动点定理建立EFX分配存在性提供了新方法，并通过与DC规划的等价性为计算此类分配提供了更高效的途径。

Abstract: We consider the problem of the existence of an envy-free allocation up to any
good (EFX) for linear valuations and establish new results by connecting this
problem to a fixed point framework. Specifically, we first use randomized
rounding to extend the discrete EFX constraints into a continuous space and
show that an EFX allocation exists if and only if the optimal value of the
continuously extended objective function is nonpositive. In particular, we
demonstrate that this optimization problem can be formulated as an
unconstrained difference of convex (DC) program, which can be further
simplified to the minimization of a piecewise linear concave function over a
polytope. Leveraging this connection, we show that the proposed DC program has
a nonpositive optimal objective value if and only if a well-defined continuous
vector map admits a fixed point. Crucially, we prove that the reformulated
fixed point problem satisfies all the conditions of Brouwer's fixed point
theorem, except that self-containedness is violated by an arbitrarily small
positive constant. To address this, we propose a slightly perturbed continuous
map that always admits a fixed point. This fixed point serves as a proxy for
the fixed point (if it exists) of the original map, and hence for an EFX
allocation through an appropriate transformation. Our results offer a new
approach to establishing the existence of EFX allocations through fixed point
theorems. Moreover, the equivalence with DC programming enables a more
efficient and systematic method for computing such allocations (if one exists)
using tools from nonlinear optimization. Our findings bridge the discrete
problem of finding an EFX allocation with two continuous frameworks: solving an
unconstrained DC program and identifying a fixed point of a continuous vector
map.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [668] [Efficient Training of Spiking Neural Networks by Spike-aware Data Pruning](https://arxiv.org/abs/2510.04098)
*Chenxiang Ma,Xinyi Chen,Yujie Wu,Kay Chen Tan,Jibin Wu*

Main category: cs.NE

TL;DR: 提出了一种新的脉冲感知数据剪枝方法SADP，通过基于梯度范数的选择概率来减少梯度方差，使用高效的脉冲感知重要性评分来避免直接计算梯度的高成本，从而加速SNN训练。


<details>
  <summary>Details</summary>
Motivation: SNN作为传统ANN的节能替代方案，在模型和数据集扩展时面临训练开销大的问题，限制了计算资源有限的研究者的发展。现有的基于ANN的数据剪枝方法无法捕捉SNN中样本的内在重要性且梯度方差高。

Method: SADP方法通过确定每个样本的选择概率与其梯度范数成正比来减少梯度方差，使用高效的脉冲感知重要性评分作为梯度范数的上界，该评分考虑了全或无脉冲对梯度范数的影响，计算开销可忽略。

Result: 在多个数据集和架构上的实验表明，SADP始终优于数据剪枝基线，在不同剪枝比例下实现了接近理论最大值的训练加速。在ImageNet上减少35%训练时间的同时保持与全数据训练相当的准确率。

Conclusion: 这项工作为高效SNN训练建立了以数据为中心的范式，为将SNN扩展到更大模型和数据集铺平了道路。

Abstract: Spiking neural networks (SNNs), recognized as an energy-efficient alternative
to traditional artificial neural networks (ANNs), have advanced rapidly through
the scaling of models and datasets. However, such scaling incurs considerable
training overhead, posing challenges for researchers with limited computational
resources and hindering the sustained development of SNNs. Data pruning is a
promising strategy for accelerating training by retaining the most informative
examples and discarding redundant ones, but it remains largely unexplored in
SNNs. Directly applying ANN-based data pruning methods to SNNs fails to capture
the intrinsic importance of examples and suffers from high gradient variance.
To address these challenges, we propose a novel spike-aware data pruning (SADP)
method. SADP reduces gradient variance by determining each example's selection
probability to be proportional to its gradient norm, while avoiding the high
cost of direct gradient computation through an efficient upper bound, termed
spike-aware importance score. This score accounts for the influence of
all-or-nothing spikes on the gradient norm and can be computed with negligible
overhead. Extensive experiments across diverse datasets and architectures
demonstrate that SADP consistently outperforms data pruning baselines and
achieves training speedups close to the theoretical maxima at different pruning
ratios. Notably, SADP reduces training time by 35% on ImageNet while
maintaining accuracy comparable to that of full-data training. This work,
therefore, establishes a data-centric paradigm for efficient SNN training and
paves the way for scaling SNNs to larger models and datasets. The source code
will be released publicly after the review process.

</details>


### [669] [SpikingMamba: Towards Energy-Efficient Large Language Models via Knowledge Distillation from Mamba](https://arxiv.org/abs/2510.04595)
*Yulong Huang,Jianxiong Tang,Chao Wang,Ziyi Wang,Jianguo Zhang,Zhichao Lu,Bojun Cheng,Luziwei Leng*

Main category: cs.NE

TL;DR: SpikingMamba是一种基于脉冲神经网络(SNN)的高效能大语言模型，通过知识蒸馏从Mamba模型转化而来，在保持较高准确率的同时显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 解决传统大语言模型(LLMs)能耗高的问题，同时避免现有SNN方法在准确率上的显著牺牲，提供一种无需完全预训练的高效转换方案。

Method: 采用TI-LIF三元整数脉冲神经元保持语义极性，结合训练专用的平滑梯度补偿(SGC)路径减少量化损失，使用单阶段蒸馏策略从预训练Mamba转移零样本能力，并通过强化学习进一步优化。

Result: SpikingMamba-1.3B实现了4.76倍的能效提升，与原始Mamba相比仅存在4.78%的零样本准确率差距，经过强化学习后准确率进一步提升了2.55%。

Conclusion: SpikingMamba成功实现了在保持大语言模型性能的同时显著提升能效的目标，为边缘设备部署高效LLMs提供了可行方案。

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
tasks but remain energy-intensive due to dense matrix operations. Spiking
neural networks (SNNs) improve energy efficiency by replacing dense matrix
multiplications with sparse accumulations. Their sparse spike activity enables
efficient LLMs deployment on edge devices. However, prior SNN-based LLMs often
sacrifice performance for efficiency, and recovering accuracy typically
requires full pretraining, which is costly and impractical. To address this, we
propose SpikingMamba, an energy-efficient SNN-based LLMs distilled from Mamba
that improves energy efficiency with minimal accuracy sacrifice. SpikingMamba
integrates two key components: (a) TI-LIF, a ternary-integer spiking neuron
that preserves semantic polarity through signed multi-level spike
representations. (b) A training-exclusive Smoothed Gradient Compensation (SGC)
path mitigating quantization loss while preserving spike-driven efficiency. We
employ a single-stage distillation strategy to transfer the zero-shot ability
of pretrained Mamba and further enhance it via reinforcement learning (RL).
Experiments show that SpikingMamba-1.3B achieves a 4.76$\times$ energy benefit,
with only a 4.78\% zero-shot accuracy gap compared to the original Mamba, and
achieves a further 2.55\% accuracy improvement after RL.

</details>


### [670] [What your brain activity says about you: A review of neuropsychiatric disorders identified in resting-state and sleep EEG data](https://arxiv.org/abs/2510.04984)
*J. E. M. Scanlon,A. Pelzer,M. Gharleghi,K. C. Fuhrmeister,T. Köllmer,P. Aichroth,R. Göder,C. Hansen,K. I. Wolf*

Main category: cs.NE

TL;DR: 该论文综述了从静息态和睡眠EEG数据中可以检测到的个人健康信息隐私风险，发现即使短时间的EEG数据也能准确分类多种疾病，强调了EEG数据匿名化的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索无任务EEG数据中可检测的个人健康信息类型，评估公开可用EEG数据涉及的隐私风险。

Method: 通过Google Scholar、Web of Science和相关期刊搜索，筛选关于在静息态和睡眠EEG中分类医学障碍的英文同行评审研究，由3名评审员进行质量分析。

Result: 静息态EEG可在5分钟或更短时间内高精度分类自闭症谱系障碍、帕金森病、酒精使用障碍等；睡眠EEG可分类睡眠呼吸暂停、失眠、REM睡眠障碍等睡眠障碍，但通常需要更长的记录或多睡眠阶段数据。

Conclusion: EEG数据能够揭示敏感个人健康信息，随着机器学习方法从EEG数据中重新识别个体的能力增强，需要加强匿名化和隐私保护工具的开发。

Abstract: Electroencephalogram monitoring devices and online data repositories hold
large amounts of data from individuals participating in research and medical
studies without direct reference to personal identifiers. This paper explores
what types of personal and health information have been detected and classified
within task-free EEG data. Additionally, we investigate key characteristics of
the collected resting-state and sleep data, in order to determine the privacy
risks involved with openly available EEG data. We used Google Scholar, Web of
Science and searched relevant journals to find studies which classified or
detected the presence of various disorders and personal information in resting
state and sleep EEG. Only English full-text peer-reviewed journal articles or
conference papers about classifying the presence of medical disorders between
individuals were included. A quality analysis carried out by 3 reviewers
determined general paper quality based on specified evaluation criteria. In
resting state EEG, various disorders including Autism Spectrum Disorder,
Parkinson's disease, and alcohol use disorder have been classified with high
classification accuracy, often requiring only 5 mins of data or less. Sleep EEG
tends to hold classifiable information about sleep disorders such as sleep
apnea, insomnia, and REM sleep disorder, but usually involve longer recordings
or data from multiple sleep stages. Many classification methods are still
developing but even today, access to a person's EEG can reveal sensitive
personal health information. With an increasing ability of machine learning
methods to re-identify individuals from their EEG data, this review
demonstrates the importance of anonymization, and the development of improved
tools for keeping study participants and medical EEG users' privacy safe.

</details>


### [671] [Exploration-Exploitation-Evaluation (EEE): A Framework for Metaheuristic Algorithms in Combinatorial Optimization](https://arxiv.org/abs/2510.05027)
*Ethan Davis*

Main category: cs.NE

TL;DR: 提出了一个将元启发式算法应用于组合优化问题的三阶段框架，包括参数空间探索、性能参数利用和不确定性量化，并以ACO求解TSP为例验证了框架有效性。


<details>
  <summary>Details</summary>
Motivation: 为元启发式算法在组合优化问题中的应用提供系统化框架，解决参数调优和结果可靠性评估的问题。

Method: 三阶段框架：1) 广泛探索参数空间；2) 利用表现最佳参数；3) 不确定性量化评估结果可靠性。以ACO求解TSPLIB berlin52数据集为例进行验证。

Result: 应用框架计算得出ACO在单次运行中找到全局最优解的概率约为1/40，十次运行聚合后概率提升至1/5。

Conclusion: 该框架为元启发式算法的应用提供了系统化方法，能够有效评估算法性能并量化结果可靠性。

Abstract: We introduce a framework for applying metaheuristic algorithms, such as ant
colony optimization (ACO), to combinatorial optimization problems (COPs) like
the traveling salesman problem (TSP). The framework consists of three
sequential stages: broad exploration of the parameter space, exploitation of
top-performing parameters, and uncertainty quantification (UQ) to assess the
reliability of results. As a case study, we apply ACO to the TSPLIB berlin52
dataset, which has a known optimal tour length of 7542. Using our framework, we
calculate that the probability of ACO finding the global optimum is
approximately 1/40 in a single run and improves to 1/5 when aggregated over ten
runs.

</details>
