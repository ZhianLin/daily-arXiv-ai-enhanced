<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 181]
- [cs.CL](#cs.CL) [Total: 103]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 224]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.SE](#cs.SE) [Total: 22]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.AI](#cs.AI) [Total: 70]
- [cs.NE](#cs.NE) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 提出了ESCA框架和SGClip模型，通过结构化时空理解来增强多模态大语言模型，无需人工标注即可生成场景图，显著提升具身智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要依赖高层视觉-声音-文本对，缺乏像素级视觉内容与文本语义的细粒度结构化对齐，限制了具身智能体的发展。

Method: 开发了SGClip模型，这是一个基于CLIP的可提示场景图生成模型，通过神经符号学习管道在87K+开放域视频上进行训练，利用视频-字幕对和结构化推理实现自监督学习。

Result: SGClip在场景图生成和动作定位基准测试中表现优异，ESCA框架持续改进开源和商业多模态大语言模型，在两个具身环境中达到最先进性能，显著减少感知错误并使开源模型超越专有基线。

Conclusion: ESCA框架通过结构化空间-时间理解有效提升了具身智能体的能力，证明了无需人工标注的自监督方法在增强多模态模型性能方面的潜力。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [2] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: RAVAR任务旨在通过自然语言描述识别特定人物的细粒度原子级动作。本文扩展了RefAVA数据集为RefAVA++，并提出RefAtomNet++框架，通过多层级语义对齐交叉注意力和多轨迹Mamba建模，在跨模态token聚合方面取得突破，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RefAtomNet模型在跨模态信息对齐和检索方面能力有限，导致在定位目标人物和预测细粒度动作时性能不佳。需要开发更有效的跨模态token聚合方法来提升语言引导的原子级动作识别性能。

Method: 提出RefAtomNet++框架，采用多层级语义对齐交叉注意力机制，结合在部分关键词、场景属性和整体句子级别的多轨迹Mamba建模。通过动态选择最近的视觉空间token构建扫描轨迹，实现更有效的时空token聚合。

Result: RefAtomNet++在RefAVA++数据集上建立了新的最先进结果，超越了包括RefAtomNet在内的多个基线模型，显著提升了目标人物定位和细粒度动作预测的性能。

Conclusion: RefAtomNet++通过创新的多层级语义对齐交叉注意力和多轨迹Mamba建模，有效解决了跨模态token聚合的挑战，为语言引导的原子级视频动作识别任务提供了强有力的解决方案。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [3] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 提出CrossRay3D稀疏多模态检测器，通过Ray-Aware监督和Class-Balanced监督提升token表示质量，在nuScenes基准上达到72.4 mAP和74.7 NDS的SOTA性能，运行速度快1.84倍。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检测器忽视token表示质量，导致前景质量不佳和性能受限。研究发现几何结构保持和类别分布是提升稀疏检测器性能的关键。

Method: 提出Sparse Selector (SS)，核心模块包括：Ray-Aware Supervision (RAS)保持几何信息，Class-Balanced Supervision自适应重加权类别语义显著性，以及Ray Positional Encoding解决LiDAR和图像模态分布差异。

Result: 在nuScenes基准上达到72.4 mAP和74.7 NDS的SOTA性能，运行速度快1.84倍，且在LiDAR或相机数据部分或完全缺失时仍保持强鲁棒性。

Conclusion: CrossRay3D通过改进token表示质量，在保持稀疏检测器优势的同时显著提升了性能，证明了几何结构保持和类别平衡监督的重要性。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [4] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG是一种利用多模态大语言模型进行细粒度视频时序定位的方法，通过两阶段处理将文本查询转换为增强句子，然后使用轻量级解码器进行精确定位。


<details>
  <summary>Details</summary>
Motivation: 利用多模态LLMs的能力联合处理文本和视频，有效定位视频中的自然语言查询，解决直接定位的局限性。

Method: 采用两阶段方法：首先将语言查询转换为包含缺失细节的增强句子，然后使用轻量级解码器基于增强查询的上下文表示预测准确边界。使用多实例学习目标动态选择最佳查询版本。

Result: 在视频时序定位和段落定位的各种基准测试中达到最先进结果，显著优于所有先前提出的基于LLM的时序定位方法，在零样本评估场景中保持明显优势。

Conclusion: ED-VTG方法在视频时序定位任务中表现出色，既能与专门模型相媲美或更优，又在零样本场景中具有明显优势，验证了多模态LLMs在该领域的有效性。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [5] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 提出一个基于CCTV监控的智能城市基础设施缺陷检测系统，使用YOLO进行多缺陷检测和分割，并通过视觉语言模型生成结构化的维护行动计划。


<details>
  <summary>Details</summary>
Motivation: 智能城市基础设施监控需求增长，手动检查成本高且危险，现有自动系统通常只处理单一缺陷类型或输出非结构化结果，无法直接指导维护工作。

Method: 利用街道CCTV流进行多缺陷检测和分割，使用YOLO系列目标检测器，然后将检测结果传递给视觉语言模型进行场景感知总结，生成包含事件描述、推荐工具、尺寸、维修计划和紧急警报的JSON格式结构化行动计划。

Result: 在公共数据集和捕获的CCTV片段上的实验评估表明，系统能准确识别多种缺陷并生成连贯的总结。

Conclusion: 讨论了将系统扩展到城市范围部署的挑战和方向。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [6] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench是首个专注于长视频理解能力的基准测试，整合视觉、音频和文本多模态，包含6个挑战性任务场景，实验显示全模态模型在时间定位和长距离因果推理方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估模型对长视频理解能力的基准测试，特别是需要整合视觉、音频和文本多模态信息，理解人类语言、观点、行为等上下文元素的任务。

Method: 从FineVideo数据集中精选约1000个长时长、信息密集的视频（如讲座、访谈、vlog），设计6个任务场景（包括事件内和事件间任务），并开发三步半自动数据质量保证流程。

Result: 实验结果表明全模态模型在需要精确时间定位（T-Loc）和长距离因果推理（CE-Caus）的任务中表现不佳，扩展实验揭示了多模态融合中的信息丢失和处理偏差问题。

Conclusion: LongInsightBench为评估长视频理解能力提供了首个综合性基准，揭示了当前全模态模型在复杂多模态任务中的局限性，特别是在时间推理和因果分析方面。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [7] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 提出IAD-GPT，一种基于多模态大语言模型的工业异常检测新范式，通过异常提示生成器、文本引导增强器和多掩码融合模块，结合文本语义与图像信息实现先进的异常检测和分割。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法缺乏多轮人机对话和详细描述能力，而基于大预训练模型的方法尚未充分激发大模型在异常检测任务中的潜力。

Method: 使用异常提示生成器生成详细异常提示，通过文本引导增强器增强视觉定位能力，设计多掩码融合模块将掩码作为专家知识增强像素级异常感知。

Result: 在MVTec-AD和VisA数据集上的实验表明，该方法在自监督和少样本异常检测与分割任务中达到最先进性能。

Conclusion: IAD-GPT成功结合了文本语义与图像信息，为工业异常检测提供了有效的多模态解决方案。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [8] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 该研究比较了三种放射学报告模式（自由文本、结构化报告、AI辅助结构化报告）对诊断准确性、效率和用户体验的影响。AI辅助结构化报告在诊断准确性和效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估结构化报告和人工智能如何改变放射科医生与影像研究的交互方式，探索不同报告模式对图像分析行为、诊断准确性、效率和用户体验的影响。

Method: 前瞻性研究，8名读者（4名新手和4名非新手）使用定制化查看器和眼动追踪系统分析35张床边胸片。比较三种报告模式：自由文本、结构化报告、AI辅助结构化报告。

Result: AI辅助结构化报告的诊断准确性最高（κ=0.71），报告时间最短（25±9秒），眼动追踪指标显示视觉注意力更集中于图像区域。新手读者在结构化报告中更关注图像，而非新手读者保持对图像的专注。

Conclusion: 结构化报告通过引导视觉注意力朝向图像来提高效率，而AI预填充的结构化报告进一步提升了诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [9] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 提出了一个数据驱动的框架来分析图像分类中的交叉偏见，包括交叉公平性评估框架和基于偏见加权的数据增强方法，显著提升了少数群体分类准确率并减少了公平性差异。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在数据不平衡数据集上训练时经常表现出交叉偏见——由多个属性（如物体类别和环境条件）相互作用产生的系统性错误。

Method: 引入交叉公平性评估框架（IFEF）结合定量公平性指标和可解释性工具来识别偏见模式，并提出偏见加权增强（BWA）的数据增强策略，根据子组分布统计自适应调整变换强度。

Result: 在Open Images V7数据集上的实验表明，BWA将代表性不足的类别-环境交叉的准确率提高了24个百分点，同时将公平性指标差异减少了35%。多次独立运行的统计分析证实了改进的显著性（p < 0.05）。

Conclusion: 该方法为分析和解决图像分类系统中的交叉偏见提供了一个可复现的途径。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [10] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 该论文提出了一种可微分的神经网络量化方法，支持多比特对数量化，在ImageNet数据集上使用ResNet18进行测试，仅需15个训练周期就能达到接近全精度模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法存在两个主要问题：一是大多使用不可微分方法，在反向传播中手动设置导数，影响学习能力；二是之前的移位/对数量化方法要么避免激活量化，要么准确率较低。

Method: 提出可微分的量化方法，支持n比特量化，特别是对数形式的2^n量化，并提供收敛性证明。

Result: 仅权重量化时准确率比全精度模型低不到1%，权重和激活同时量化时达到SOTA水平，仅需15个训练周期，推理成本略高于1比特量化但无需高精度乘法。

Conclusion: 该方法提供了一种高效、可收敛的神经网络量化方案，在保持高准确率的同时显著降低计算和内存需求。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [11] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: 提出StripRFNet网络用于道路表面损伤检测，通过形状感知、条状感受野和小尺度增强三个模块解决损伤形状多样、细长裂缝检测困难和小尺度识别错误率高等挑战，在RDD2022基准测试中达到最先进精度和实时效率。


<details>
  <summary>Details</summary>
Motivation: 良好的道路网络对实现可持续发展目标11至关重要，道路表面损伤不仅威胁交通安全，还阻碍可持续城市发展。现有方法在检测多样形状损伤、捕获高宽比细长裂缝和识别小尺度损伤方面存在困难。

Method: 提出StripRFNet深度神经网络，包含三个模块：形状感知模块通过大尺度可分离核注意力增强形状判别；条状感受野模块使用大条状卷积和池化捕获细长裂缝特征；小尺度增强模块利用高分辨率P2特征图、专用检测头和动态上采样改进小目标检测。

Result: 在RDD2022基准测试中，StripRFNet超越现有方法。在中国子集上，F1分数、mAP50和mAP50:95分别比基线提高4.4、2.9和3.4个百分点。在完整数据集上达到80.33%的最高F1分数，同时保持有竞争力的推理速度。

Conclusion: StripRFNet实现了最先进的精度和实时效率，为智能道路维护和可持续基础设施管理提供了有前景的工具。

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [12] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: ObjectTransforms是一种通过对象特定变换来量化和减少视觉目标检测中不确定性的技术，在训练和推理时分别提高鲁棒性和量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中可靠的感知对于安全决策至关重要，但基于视觉的目标检测神经网络容易受到数据偏差和分布偏移等不确定性因素的影响。

Method: 在训练时对单个对象进行颜色空间扰动并使用扩散模型生成多样化的行人实例；在推理时对检测到的对象应用扰动，利用检测得分的方差来实时量化预测不确定性。

Result: 在NuImages 10K数据集上的实验表明，该方法在所有对象类别上都带来了显著的准确性提升和不确定性减少，在推理时对假阳性预测了比真阳性更高的不确定性值。

Conclusion: ObjectTransforms作为一种轻量级但有效的机制，在训练和推理时分别具有减少和量化基于视觉感知中不确定性的潜力。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [13] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: A2PD是使用Aria Gen 2眼镜采集的自我中心多模态开放数据集，包含日常活动场景的原始传感器数据和感知算法输出。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供及时可用的自我中心多模态数据集，展示设备感知佩戴者、环境和交互的能力。

Method: 使用Aria Gen 2眼镜采集多用户日常活动数据，涵盖清洁、烹饪、进食、玩耍和户外行走五个主要场景。

Result: 发布了包含原始传感器数据和多种机器感知算法输出的综合数据集，展示了在不同用户和条件下的鲁棒性能。

Conclusion: A2PD数据集通过projectaria.com公开提供，配有开源工具和使用示例，支持自我中心感知研究。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [14] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法，通过引导采样过程将外观从图像或文本转移到3D资产上，解决了现有方法在几何差异较大时失败的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在输入和外观对象的几何形状显著不同时效果不佳，而直接应用3D生成模型会产生不理想的结果。

Method: 基于预训练的整流流模型，在采样过程中周期性地添加可微损失函数作为引导，包括部件感知损失和自相似性损失。

Result: 该方法成功地将纹理和几何细节转移到3D资产上，在定性和定量评估中均优于基线方法。

Conclusion: 该方法具有通用性，可扩展到不同类型的扩散模型和引导函数，并通过GPT评估系统解决了传统指标不适用的问题。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [15] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出使用深度学习自动化血栓切除术关键环节的自监督框架，通过基于回归的前置任务分类骨骼标志点，在回归和分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 血栓切除术是缺血性中风最有效的治疗方法之一，但需要大量资源和专业人员。通过深度学习自动化关键环节可提高效率和安全性。

Method: 引入自监督框架，使用基于回归的前置任务分类各种骨骼标志点。

Result: 模型在回归和分类任务中均优于现有方法，位置前置任务显著提升下游分类性能。

Conclusion: 未来工作将扩展该框架实现完全自主的C臂控制，优化从中风血栓切除术中从骨盆到头部的轨迹规划。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [16] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 提出了DuetMatch，一种用于医学图像分割的双分支半监督框架，通过异步优化编码器和解码器，结合解耦dropout扰动、配对CutMix交叉引导和一致性匹配来提高分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注数据有限，半监督学习具有吸引力。教师-学生框架虽受欢迎，但联合优化整个网络会影响收敛和稳定性，特别是在挑战性场景中。

Method: 双分支异步优化框架，每个分支分别优化编码器或解码器；引入解耦dropout扰动增强正则化；设计配对CutMix交叉引导通过增强输入对交换伪标签；提出一致性匹配使用冻结教师模型的稳定预测来优化伪标签。

Result: 在ISLES2022和BraTS等脑MRI分割基准数据集上的广泛实验表明，DuetMatch始终优于最先进的方法。

Conclusion: DuetMatch在多样化的半监督分割场景中表现出有效性和鲁棒性。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [17] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出了一种自主导航C臂到预定义解剖标志的管道，利用X射线图像预测3D位移向量，结合不确定度估计和保形预测确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床工作流程依赖手动对齐C臂，增加了辐射暴露和手术延迟，需要自动化解决方案来提高效率和安全性。

Method: 使用X射线图像预测3D位移向量，结合概率损失和骨骼姿态正则化，采用保形预测校准不确定度，生成3D置信区域。

Result: 在DeepDRR生成的合成X射线数据集上验证，显示强大的定位精度和良好校准的预测边界。

Conclusion: 该管道有潜力成为安全可靠自主C臂系统的组成部分，可减少辐射暴露和手术时间。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [18] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 提出一个公式来估算图像质量评估(IQA)引擎的成本节省，并在背景修复用例中展示通过简单AutoML解决方案实现51.61%的成本节省


<details>
  <summary>Details</summary>
Motivation: 深度生成模型生成的图像质量尚不及传统摄影方法，生产流程中需要人工图像质量评估，这个过程耗时昂贵，特别是因为自动生成图像通过质量标准的比例很低

Method: 引入自动预过滤阶段，提出一个估算IQA引擎成本节省的公式，并在背景修复用例中应用简单的AutoML解决方案

Result: 在背景修复用例中，通过简单的AutoML解决方案实现了51.61%的成本节省

Conclusion: 自动预过滤阶段可以显著降低获取高质量图像的平均成本，提出的公式为估算IQA引擎的成本节省提供了有效方法

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [19] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 该论文提出PRISM模型，通过将fMRI信号投影到结构化文本空间作为中间表示来重建视觉刺激，利用面向对象的扩散模块和属性关系搜索模块，在真实数据集上实现了比现有方法更好的重建效果。


<details>
  <summary>Details</summary>
Motivation: 理解大脑如何编码视觉信息是神经科学和机器学习的重要挑战。目前通过fMRI信号重建视觉刺激的方法存在两个阶段：将fMRI信号转换为潜在空间，然后使用预训练生成模型重建图像。但尚不清楚哪种潜在空间最适合这种转换以及如何有效组织该空间来表示视觉刺激。

Method: 提出PRISM模型：1）将fMRI信号投影到结构化文本空间作为中间表示；2）包含面向对象的扩散模块，通过组合单个对象生成图像以减少对象检测错误；3）属性关系搜索模块自动识别与神经活动最匹配的关键属性和关系。

Result: 在真实世界数据集上的广泛实验表明，该框架优于现有方法，实现了感知损失减少高达8%。fMRI信号与语言模型的文本空间比基于视觉的空间或联合文本图像空间更相似。

Conclusion: 研究结果强调了使用结构化文本作为中间空间来桥接fMRI信号和图像重建的重要性，文本表示和生成模型应适应视觉刺激的组合性质，包括对象、详细属性和关系。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [20] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本文提出数据为中心的人工智能方法来解决热带地区农业遥感制图的挑战，强调数据质量对模型鲁棒性的重要性，并推荐了25种数据策略和9种最成熟的方法用于大规模农业制图项目。


<details>
  <summary>Details</summary>
Motivation: 热带地区农业遥感面临缺乏高质量标注数据、标注成本高、数据变异性和区域泛化等独特挑战，传统以模型为中心的方法受限于高云量、多样化作物日历和有限数据集。

Method: 采用数据为中心的人工智能视角和流程，重点应用自信学习、核心集选择、数据增强和主动学习等技术，强调数据质量和数据管理。

Result: 确定了25种适用于大规模农业制图流程的数据策略，并提出了使用9种最成熟和直接方法的实用流程。

Conclusion: 数据为中心的方法能够更好地适应热带农业的动态现实，为热带农业遥感制图提供了实用的解决方案和可操作的流程。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [21] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种名为StretchySnake的灵活训练方法，通过动态调整时空分辨率来增强状态空间模型在视频理解中的适应性，解决了现有方法在未见过时空分辨率视频上的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前针对视频理解的训练方法主要针对transformer设计，未能充分利用状态空间模型的独特属性。这些模型在训练时使用固定分辨率和视频长度，导致在评估未见过的时空分辨率视频时性能下降，限制了模型在长短视频上的通用性。

Method: 提出灵活训练方法，在训练过程中采样不同时空分辨率的视频，并动态插值模型权重以适应任意时空尺度。比较了五种不同的灵活训练变体，确定了最适合视频状态空间模型的策略。

Result: 在短动作（UCF-101、HMDB-51）和长动作（COIN、Breakfast）基准测试中，StretchySnake比transformer和状态空间模型基线性能提升高达28%，在细粒度动作数据集（SSV2、Diving-48）上表现出强大的适应性。

Conclusion: 该方法提供了一个简单的即插即用训练方案，使视频状态空间模型在各种动作识别场景中更加鲁棒、分辨率无关且高效。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [22] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出VM-BeautyNet，一种融合Vision Transformer和Mamba模型的异构集成架构，用于面部美感预测，在SCUT-FBP5500数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型难以捕捉全局面部特征，Vision Transformers能建模长距离空间关系但存在二次复杂度问题，需要结合两种模型的优势。

Method: 使用异构集成架构，ViT主干捕捉全局面部结构和对称性，Mamba主干以线性复杂度建模长距离依赖关系，关注序列特征和纹理。

Result: 在SCUT-FBP5500数据集上取得PC=0.9212、MAE=0.2085、RMSE=0.2698的SOTA性能，并通过Grad-CAM可视化验证了两种主干网络的互补特征提取。

Conclusion: VM-BeautyNet为计算美学提供了强大的新架构范式，通过融合ViT和Mamba模型的互补优势，显著提升了面部美感预测性能。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [23] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 开发了一个用于检测口腔鳞状细胞癌(OCSCC)的卷积神经网络，结合图像采集硬件，研究图像分辨率对检测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: OCSCC是头颈部最常见的癌症，早期症状不明显且生长缓慢，常被漏诊导致可预防的死亡。利用CNN的精确图像分割和模式识别能力可以实现早期检测。

Method: 训练CNN模型处理4293张训练图像（良性、恶性肿瘤和阴性样本），测试不同分辨率图像，开发图像采集硬件和应用软件。

Result: 图像分辨率提高时预测准确率呈对数增长，显示更高像素数带来的收益递减。

Conclusion: CNN结合专用硬件可有效检测OCSCC，图像分辨率对检测准确性有重要影响但存在收益递减效应。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [24] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D是一个包含500小时3D运动数据的多模态数据集，来自439名参与者，包含单人和多人行为数据。


<details>
  <summary>Details</summary>
Motivation: 为研究人类行为和对话提供大规模、高质量的3D运动数据，支持多模态研究。

Method: 在多摄像头采集环境中收集439名参与者的3D运动数据，包括身体追踪、手势追踪和音频记录。

Result: 创建了包含54百万帧3D运动追踪数据的数据集，涵盖单人和多人交互场景。

Conclusion: Embody 3D为人类行为分析、社交互动研究提供了宝贵的多模态数据资源。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [25] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 提出了一种主动场景分解与重建方法，利用人-物交互线索在线迭代分解和重建动态环境，解决静态物体级重建中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 人类行为是场景动态的主要来源，包含丰富的动态线索。传统静态物体级重建方法存在固有模糊性，需要利用人-物交互的意图性来动态优化分解重建过程。

Method: 基于高斯泼溅技术，通过观察人-物交互来迭代分解和重建环境，整合相机和物体姿态估计、实例分解、在线地图更新等多个任务。

Result: 在多个真实场景中验证了有效性，实现了准确一致的动态场景建模，具有逼真高效的渲染效果。

Conclusion: 该方法为动态环境提供了一种灵活渐进的重建方案，相比传统物体级重建方法具有显著优势。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [26] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus是一个用于实时视频异常检测的两级级联系统，通过轻量级过滤和细粒度视觉语言模型推理，在保持高精度的同时实现高速检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的视频异常检测方法虽然具有优秀的零样本检测能力，但计算成本高昂且视觉定位性能不稳定，难以实现实时部署。

Method: 采用两级级联系统：离线学习正常行为规则，在线推理时结合轻量级过滤和细粒度VLM推理。关键创新包括运动掩码提示和基于规则的偏差检测。

Result: 在四个数据集上的评估显示，Cerberus在NVIDIA L40S GPU上平均达到57.68 fps，速度提升151.79倍，准确率达到97.2%，与最先进的VLM-based VAD方法相当。

Conclusion: Cerberus为实时视频分析提供了一个实用的解决方案，在保持高精度的同时显著提升了检测速度。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [27] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准测试，揭示了评估大型视觉语言模型成员推理攻击时的根本挑战，指出先前的高成功率主要源于检测数据集构建中的分布偏差而非真实成员状态。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估LVLM成员推理攻击时存在分布偏差问题，导致攻击成功率被高估，需要建立无偏基准来准确评估隐私风险。

Method: 构建包含6,000张图像的受控基准，精心平衡成员和非成员样本的分布，并提供三个不同训练阶段的真实成员标签。

Result: 在无偏条件下，最先进的MIA方法性能收敛到随机水平，表明先前的高成功率主要源于分布偏差。

Conclusion: OpenLVLM-MIA为LVLM隐私保护技术开发提供了透明无偏的基准，明确了当前MIA研究的局限性。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [28] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch是一个无需训练的新框架，通过跨图像笔画注意力机制实现参考风格到内容图像的笔画属性精确迁移，同时保持语义结构完整性。


<details>
  <summary>Details</summary>
Motivation: 生成具有参考风格的草图需要精确传递笔画属性（如线条粗细、变形和纹理稀疏度），同时保持语义结构和内容保真度。现有方法在笔画控制和语义连贯性方面存在不足。

Method: 提出跨图像笔画注意力机制，嵌入自注意力层以建立细粒度语义对应关系；开发自适应对比度增强和语义聚焦注意力来强化内容保持和前景强调。

Result: Stroke2Sketch能有效合成风格忠实、接近手工绘制结果的草图，在表达性笔画控制和语义连贯性方面优于现有方法。

Conclusion: 该框架通过创新的注意力机制实现了精确的笔画属性迁移，为风格化草图生成提供了有效的训练免费解决方案。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [29] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文系统研究了深度伪造检测的缩放规律，发现检测错误率随真实图像域数量和深度伪造方法数量的增加呈幂律衰减，类似于大语言模型的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 由于现有数据集无法满足研究需求，需要构建大规模数据集来系统分析深度伪造检测任务的缩放规律，以应对不断演进的深度伪造技术。

Method: 构建了目前最大的深度伪造数据集ScaleDF，包含580万张来自51个不同域的真实图像和880万张由102种深度伪造方法生成的假图像，并基于此分析模型性能与真实域数量、伪造方法数量和训练图像数量的关系。

Result: 观察到平均检测错误率随真实域数量或深度伪造方法数量的增加呈可预测的幂律衰减，能够预测达到目标性能所需的额外资源，并验证了预训练和数据增强在缩放中的作用。

Conclusion: 深度伪造检测任务存在类似LLM的幂律缩放规律，这为以数据为中心的方式应对深度伪造技术提供了理论基础，同时也揭示了缩放本身的局限性。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [30] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT是一个用于超高清文本到图像生成的扩散框架，通过分层局部注意力和低分辨率全局引导，实现了4K分辨率的高效图像合成，无需额外高分辨率训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型受限于注意力机制的二次复杂性和缺乏原生4K训练数据，无法实现超高清图像生成。需要开发能够同时保证细粒度纹理合成和全局结构一致性的方法。

Method: 提出分层局部注意力机制，将高分辨率潜在空间划分为固定大小的局部窗口以减少计算复杂度；使用低分辨率潜在空间提供全局语义引导；通过LoRA适配器连接全局和局部路径；采用Hilbert曲线重排token序列和融合内核优化推理效率。

Result: Scale-DiT在4K分辨率下比密集注意力基线快2倍以上，内存使用更低，在FID、IS、CLIP Score等定量指标和定性比较中均表现出优越的全局一致性和局部细节清晰度。

Conclusion: 分层局部注意力配合引导性低分辨率锚点是推进超高清图像生成的有效方法，能够在无需额外4K训练数据的情况下实现高质量的4K图像合成。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [31] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: DiffusionX是一个云边协同框架，通过轻量级设备端模型快速生成预览图像，高容量云端模型进行最终优化，减少生成时间15.8%，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成过程计算密集，用户需要迭代优化提示词，增加延迟和云端资源负担。

Method: 提出云边协同框架，设备端轻量扩散模型生成预览，云端高容量模型进行最终优化，引入噪声水平预测器动态平衡计算负载。

Result: 相比Stable Diffusion v1.5减少平均生成时间15.8%，图像质量相当；相比Tiny-SD仅慢0.9%但图像质量显著提升。

Conclusion: DiffusionX在最小开销下展示了效率和可扩展性，有效平衡延迟和云端工作负载。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [32] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: TokenAR框架通过token级增强机制解决多参考图像生成中的身份混淆问题，包含Token索引嵌入、指令token注入和身份token解缠策略，显著提升身份一致性和背景重建质量。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在条件图像生成中表现出色，但在多参考生成中难以解耦不同参考身份，存在身份混淆问题。

Method: 提出TokenAR框架，包含三个token级增强组件：Token索引嵌入聚类相同参考图像的token；指令token注入作为额外视觉特征容器；身份token解缠策略显式引导token独立表示每个身份特征。

Result: 在多个参考图像生成任务中超越当前最先进模型，实现良好的身份一致性和高质量背景重建。

Conclusion: TokenAR框架有效解决了多参考图像生成中的身份混淆问题，并发布了首个开源大规模多参考图像生成数据集InstructAR。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [33] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 该论文挑战了多模态语言模型(MLLM)性能主要继承自LLM骨干网络的假设，发现视觉编码器在RL训练后会产生更强、更精准定位的视觉表示，并提出了PIVOT方法，能以不到1%的计算成本构建更强的视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM研究过度关注LLM骨干网络，忽视了视觉编码器的作用，特别是在从SFT转向RL的训练范式转变中，缺乏对视觉编码器如何被重塑的分析。

Method: 通过ImageNet分类、分割和梯度可视化等多样化实验，分析不同训练策略对MLLM视觉表示的影响，并基于发现提出了PIVOT方法。

Result: RL训练相比SFT在强视觉相关的VQA基准上表现更优，能产生更强且精确定位的视觉表示，PIVOT训练的视觉编码器性能超过更大规模预训练的模型。

Conclusion: RL训练能从根本上重塑MLLM的视觉表示能力，PIVOT方法为推进MLLM视觉骨干网络提供了一条高效路径，仅需极低计算成本即可获得显著性能提升。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [34] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 提出GradNorm框架，通过梯度范数来筛选与图像语义相近的正名词，以提升语言辅助图像聚类的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言辅助图像聚类方法依赖CLIP特征空间筛选正名词，但缺乏理论依据。需要建立理论保证的筛选策略。

Method: 基于梯度范数的框架，通过反向传播交叉熵损失的梯度大小来衡量名词与图像的语义接近程度。

Result: 理论证明GradNorm包含现有方法作为特例，实验表明在多个基准测试中达到最先进的聚类性能。

Conclusion: GradNorm为语言辅助图像聚类提供了理论保证的正名词筛选方法，显著提升了聚类效果。

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [35] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 提出了MIRAD数据集，这是首个专门为社交制造中的异常检测设计的基准数据集，包含多样化定制产品、六个地理分散制造节点的数据以及显著的成像异质性。


<details>
  <summary>Details</summary>
Motivation: 社交制造模式在实现大规模定制的同时，带来了质量控制方面的挑战，特别是在缺陷检测方面。主要困难包括产品高度定制化、生产碎片化小批量订单以及分布式站点成像环境差异大。

Method: 构建MIRAD数据集，捕捉社交制造领域的三个关键维度：多样化定制产品、地理分散制造节点数据收集、显著的成像异质性。在MIRAD上对最先进的异常检测方法进行广泛评估。

Result: 与传统基准相比，所有模型在MIRAD上都表现出显著的性能下降，突显了现实世界定制化生产中缺陷检测的未解决复杂性。

Conclusion: MIRAD通过连接工业需求和学术研究，为开发工业5.0所需的稳健质量控制解决方案提供了现实基础。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [36] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 提出了包含3000个白内障手术视频的数据集，具有四个标注层：手术阶段、实例分割、器械-组织交互跟踪和技能评分，并进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有白内障手术数据集缺乏多样性和标注深度，无法训练泛化性强的深度学习模型。

Method: 收集来自两个手术中心的3000个白内障手术视频，包含四个标注层：时间手术阶段、器械和解剖结构实例分割、器械-组织交互跟踪、基于ICO-OSCAR标准的技能评分。

Result: 通过基准实验验证了数据集在手术流程识别、场景分割和自动技能评估等关键任务上的技术质量，并建立了领域自适应基线。

Conclusion: 该数据集填补了白内障手术AI研究的资源空白，支持开发更通用和鲁棒的手术AI系统。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [37] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2是一个用于实时坑洞检测、GPS地理标记和道路健康可视化的端到端自动化平台，基于YOLO模型和OpenStreetMap，支持智能治理和自动警报功能。


<details>
  <summary>Details</summary>
Motivation: 印度道路坑洞问题严重，存在安全隐患和维护挑战，需要自动化解决方案来改善道路基础设施维护。

Method: 使用自标注的7000多张印度道路图像数据集微调YOLO模型进行坑洞检测，结合OCR时间戳和GPS日志进行精确定位，通过后端数据库管理道路段和承包商信息。

Result: 开发了完整的自动化平台，能够实时检测坑洞、精确定位、发送警报，并提供可操作的网络分析界面。

Conclusion: iWatchRoadv2通过自动化坑洞监测全生命周期，实现了数据驱动的智慧城市管理、透明治理和可持续的道路基础设施改进。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [38] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: Demeter是一个数据驱动的参数化植物形态模型，能够编码植物的拓扑结构、形状、关节运动和变形，支持不同物种的拓扑变化，并模拟三种形状变化来源。


<details>
  <summary>Details</summary>
Motivation: 虽然存在强大的人类和动物3D参数化模型，但同样表达能力强的植物建模方法仍然缺乏，特别是在作物植物建模方面。

Method: 收集大规模地面真实数据集作为测试平台，开发能够处理不同拓扑结构并模拟关节运动、子组件形状变化和非刚性变形的参数化模型。

Result: 实验表明Demeter能够有效合成形状、重建结构并模拟生物物理过程。

Conclusion: Demeter为植物建模提供了一个紧凑的学习表示，在作物植物建模方面具有应用潜力，代码和数据已公开。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [39] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 提出了一种针对AR/VR设备的轻量级深度学习框架，通过稀疏卷积、SPLite解码器和量化感知训练，在Raspberry Pi 5上实现了2.98倍加速，同时保持与最先进方法相当的精度。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR设备的普及，边缘设备需要实时推理、低功耗和最小延迟，但现有框架难以平衡效率与性能。

Method: 采用编码器-解码器架构，在ResNet-18骨干网络上应用稀疏卷积，提出SPLite解码器提升解码速度，并使用量化感知训练减少内存使用。

Result: 端到端效率提升42%，解码帧率提升3.1倍，内存使用减少，精度损失极小（PA-MPJPE从9.0mm仅增至9.1mm），在Raspberry Pi 5上整体加速2.98倍。

Conclusion: 该方法在保持精度的同时显著提升了计算效率，适用于资源受限的边缘设备部署。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [40] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: REALM是一个创新的MLLM智能体框架，通过3D高斯溅射表示实现基于推理的开放世界3D分割，无需大量3D特定后训练。


<details>
  <summary>Details</summary>
Motivation: 弥合复杂人类指令与精确3D对象定位之间的差距是视觉和机器人领域的重大挑战。现有3D分割方法难以解释模糊的推理指令，而擅长此类推理的2D视觉语言模型缺乏内在的3D空间理解。

Method: 直接在3D高斯溅射表示上执行分割，利用其渲染逼真新视图的能力。提出全局到局部空间定位策略：首先并行输入多个全局视图进行粗粒度定位，然后合成目标对象的多个特写视图进行细粒度局部分割。

Result: 在LERF、3D-OVS和新引入的REALM3D基准测试中，REALM在解释显式和隐式指令方面表现出色。该框架还无缝支持对象移除、替换和风格转换等3D交互任务。

Conclusion: REALM展示了在开放世界3D推理分割方面的卓越性能，以及在实际应用中的实用性和多功能性。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [41] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: SSL4RL是一个利用自监督学习任务作为可验证奖励的新框架，通过RL微调解决视觉语言模型在视觉证据利用不足的问题，无需人工偏好数据或不可靠的AI评估器。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在视觉中心任务中过度依赖语言先验，在推理过程中使用文本捷径，缺乏可扩展和可靠的奖励机制阻碍了RL在VLM中的应用。

Method: 将自监督学习目标（如图像旋转预测、掩码补丁重建）重新表述为密集、自动的奖励信号，用于基于RL的微调。

Result: SSL4RL显著提升了在视觉中心和视觉语言推理基准测试上的性能，并展示了在图像学习中的通用性。

Conclusion: SSL4RL建立了一个使用可验证自监督目标对齐多模态模型的通用有效范式，为未来工作提供了新的设计原则。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [42] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick是一个轻量级的点和线段匹配器，通过引入注意力线消息传递(ALMP)机制，在保持高性能的同时显著降低计算复杂度，实现了实时应用和边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统的点和线匹配被视为独立任务，GlueStick虽然实现了联合匹配但架构过重，无法满足实时应用和边缘设备部署的需求。

Method: 提出轻量级匹配器LightGlueStick，核心创新是注意力线消息传递(ALMP)机制，显式地将线的连通性暴露给网络，实现节点间高效通信。

Result: 在多个基准测试中建立了新的最先进性能，同时显著降低了计算复杂度。

Conclusion: LightGlueStick通过高效的ALMP机制，在保持高性能的同时实现了轻量化设计，为实时SLAM和运动结构重建等应用提供了可行的解决方案。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [43] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了可解释深度伪造视频检测任务(EDVD)，设计了EDVD-LLaMA多模态大语言模型推理框架，通过时空细微信息标记化和细粒度多模态思维链机制，提供可追溯的推理过程和可信的解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造视频检测方法存在原理不透明、泛化能力不足的问题，需要能够识别伪造内容并提供可验证推理解释的检测器。

Method: 1. 时空细微信息标记化(ST-SIT)提取和融合全局与局部跨帧深度伪造特征；2. 细粒度多模态思维链(Fg-MCoT)机制，引入面部特征数据作为硬约束；3. 构建可解释推理FF++基准数据集(ER-FF++set)。

Result: 实验表明EDVD-LLaMA在检测精度、可解释性、跨伪造方法和跨数据集场景处理能力方面表现出色，相比传统方法提供更可解释且优越的解决方案。

Conclusion: 该方法为深度伪造检测提供了可追溯的推理过程和可信的解释，在检测精度和可解释性方面均优于传统方法。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [44] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 提出了一种改进的损失函数，利用高斯边界框表示和Bhattacharyya距离来提升旋转物体检测的准确性和鲁棒性，特别针对各向异性高斯表示解决了方形物体的问题。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测框架在处理旋转物体时表现不佳，特别是在航空影像、遥感和自动驾驶等应用中，需要能够有效捕捉方向变化的检测方法。

Method: 采用高斯边界框表示和Bhattacharyya距离，提出各向异性高斯表示来解决方形物体的各向同性方差问题，并集成旋转不变损失函数到最先进的深度学习旋转物体检测器中。

Result: 大量实验显示在平均精度指标上相比现有方法有显著提升，表明该方法在旋转物体检测方面具有建立新基准的潜力。

Conclusion: 该方法能够为需要精确可靠物体定位的各种应用提供有效的解决方案，无论物体方向如何。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [45] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN是一种视觉提示初始化策略，通过对齐嵌入空间中的语义信息区域并注入新的表示方向，显著提升自监督模型的适应性能，在资源受限和数据稀缺场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示调优方法在专门化提示或丰富表示空间方面存在不足，特别是在挑战性任务和数据稀缺设置中，这些限制尤为明显。

Method: VIPAMIN通过两个关键步骤增强自监督模型的适应能力：(1) 将提示与嵌入空间中的语义信息区域对齐；(2) 在预训练子空间之外注入新的表示方向。该方法仅需单次前向传播和轻量级操作。

Result: VIPAMIN在各种任务和数据集规模下持续提升性能，在视觉提示调优领域达到了新的最先进水平。

Conclusion: VIPAMIN提供了一种简单有效的视觉提示初始化策略，显著改善了自监督模型在下游任务中的适应能力，特别是在资源受限和数据稀缺场景下。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [46] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 提出了一种弱监督域自适应方法，利用稀疏点标注和新的多任务学习框架，显著提升电子显微镜图像中线粒体分割的性能。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜图像中线粒体分割标注成本高，无监督域自适应方法在实际应用中性能较低，需要更有效的弱监督方法。

Method: 采用多任务学习框架，结合分割和中心检测任务，引入交叉教学机制、类聚焦跨域对比学习以及基于实例感知的伪标签选择策略。

Result: 在多个挑战性数据集上验证，性能优于现有无监督和弱监督域自适应方法，显著缩小与全监督方法的性能差距。

Conclusion: 该方法在弱监督和无监督设置下均取得显著改进，为生物医学图像分析提供了高效的解决方案。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [47] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 提出了一种面向目标视觉语言导航的前瞻性智能体，通过Q学习从大规模无标签轨迹数据中学习场景布局和物体关系知识，结合未来信息进行A*式搜索决策。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖历史信息做决策，忽略了行动的未来影响和长期结果，需要开发能够预见未来结果的前瞻性导航智能体。

Method: 使用Q学习训练Q模型生成Q特征描述候选行动的未来潜在信息，通过跨模态未来编码器将任务无关的Q特征与导航指令结合，产生反映未来前景的行动分数，结合历史分数进行A*式搜索。

Result: 在广泛使用的目标导向VLN数据集上进行的大量实验验证了所提方法的有效性。

Conclusion: 该方法通过整合未来信息和历史信息，实现了更有效的目标导向视觉语言导航，证明了前瞻性决策策略的优越性。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [48] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: HGC-Avatar是一个用于动态3D虚拟人高效传输和高质量渲染的分层高斯压缩框架，通过结构层和运动层的解耦设计，支持分层压缩、渐进解码和可控渲染。


<details>
  <summary>Details</summary>
Motivation: 现有的基于3D高斯泼溅的压缩方法缺乏人体先验知识，导致解码端的比特率效率和重建质量不理想，限制了其在可流式3D虚拟人系统中的应用。

Method: 将高斯表示解耦为结构层（通过StyleUNet生成器将姿态映射到高斯）和运动层（利用SMPL-X模型紧凑地表示时间姿态变化），并引入面部注意力机制来在低比特率约束下保持身份和表情细节。

Result: 实验结果表明，HGC-Avatar为快速3D虚拟人渲染提供了可流式解决方案，在视觉质量和压缩效率方面显著优于现有方法。

Conclusion: HGC-Avatar通过分层高斯压缩框架成功解决了动态虚拟人传输中的比特率效率和重建质量问题，为可流式3D虚拟人系统提供了有效的解决方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [49] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench是首个基于真实审稿人标记不一致性的科学论文多模态基准，包含262个来自242篇论文的不一致性案例，评估模型检测、纠正和推理多模态不一致性的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准要么孤立单一模态，要么依赖合成错误，无法捕捉真实科学论文中跨文本、图表、公式的复杂不一致性问题，这影响了科学研究的清晰度、可重复性和可信度。

Method: 通过多阶段流程（审稿挖掘、LLM辅助过滤、人工验证）收集真实不一致性案例，设计三个任务：不一致性识别、修正和配对匹配，并引入结构化JSON答案表示以减少语言偏见。

Result: 对21个领先LMM的基准测试显示性能极低（26.1-54.2%），包括大型开源模型和专有模型，凸显了多模态科学推理的挑战。

Conclusion: 当前LMM在多模态科学推理方面能力有限，需要进一步开发可信赖的科学助手来解决跨模态不一致性问题。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [50] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: 提出OOS-DSD方法，通过辅助学习增强缺货检测性能，在YOLOv8基础上增加产品分割和深度估计分支，使用伪标签深度数据进行训练，性能超过现有最佳方法1.8% mAP。


<details>
  <summary>Details</summary>
Motivation: 缺货检测是零售验证的重要过程，需要准确推断货架上产品的不可用性。现有方法性能有待提升，希望通过多任务学习提高检测精度。

Method: 扩展YOLOv8架构，增加卷积分支同时进行缺货检测、产品分割和场景深度估计。深度估计分支使用Depth Anything V2生成的伪标签数据训练，并提出深度归一化方法稳定训练过程。

Result: 实验结果显示，该方法在平均精度(mAP)上超过现有最佳方法1.8%。消融研究证实辅助学习提升3.7% mAP，深度归一化提升4.2% mAP。

Conclusion: OOS-DSD通过多任务学习和深度归一化有效提升了缺货检测性能，证明了辅助学习在零售视觉任务中的价值。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [51] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出了一种基于图注意力网络自编码器的图像分类和检索方法，通过构建图像和类别的代表性模型来执行分类和检索任务。


<details>
  <summary>Details</summary>
Motivation: 开发一种代表性中心的图像分类和检索方法，利用图结构捕捉图像间的相似关系，并通过注意力机制突出重要特征。

Method: 使用图结构表示图像（节点）和相似关系（边），采用GAT自编码器构建上下文感知的潜在表示，从中提取类别代表，通过比较查询图像代表与类别代表进行分类，并在所属类别中检索最相似图像。

Result: 通过实验验证了该代表性中心方法的有效性，比较了GAT自编码器和标准特征提取技术的性能。

Conclusion: 基于图注意力网络的自编码器能够有效构建图像的代表性模型，在图像分类和检索任务中表现出良好的性能。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [52] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: READ是一种微调方法，通过添加标记级重建和句子级对齐两个辅助目标来增强视觉语言模型的组合推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习训练方法导致文本编码器关注单个单词而非关系，限制了组合推理能力。

Method: 在对比学习基础上添加：(1) 标记级重建目标：使用冻结解码器重建替代描述；(2) 句子级对齐目标：在嵌入空间中对齐改写句子。

Result: READ-CLIP在五个主要组合推理基准测试中达到最先进性能，比最强基线提升达4.1%。

Conclusion: 重建和对齐目标具有互补优势：重建鼓励编码器捕捉单词间关系，对齐确保不同表述的改写句子具有一致表示。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [53] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 提出了GaitRDAE框架，通过动态搜索运动区域并分配自适应时间尺度来改进步态识别，解决了现有方法使用预定义区域和固定时间尺度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法使用预定义区域进行时间建模，为不同类型区域分配固定或等效的时间尺度，难以建模随时间动态变化的运动区域并适应其特定模式。

Method: 引入区域感知动态聚合与激励框架(GaitRDAE)，包含两个核心模块：区域感知动态聚合(RDA)模块动态搜索每个区域的最佳时间感受野，区域感知动态激励(RDE)模块强调学习包含更稳定行为模式运动区域，同时抑制对更易受协变量影响的静态区域的注意力。

Result: 实验结果表明，GaitRDAE在多个基准数据集上实现了最先进的性能。

Conclusion: GaitRDAE框架通过自动搜索运动区域、分配自适应时间尺度和应用相应注意力，有效提升了步态识别的准确性，特别是在协变量影响视觉外观的情况下。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [54] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 本文提出了首个基于真实世界政治深度伪造事件的系统基准测试，发现现有检测器在真实政治深度伪造内容上泛化能力不足，呼吁开发政治情境化的检测框架。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的快速扩散增加了虚假信息风险，特别是政治深度伪造会扭曲真相、破坏对政治机构的信任。现有检测模型大多在实验室合成数据上训练，难以泛化到社交媒体上传播的真实政治深度伪造。

Method: 基于政治深度伪造事件数据库，构建首个系统基准测试，对学术界、政府和工业界的最先进深度伪造检测器进行系统评估。

Result: 学术界和政府开发的检测器表现相对较差，付费检测工具性能相对较高，但所有检测器在真实政治深度伪造上泛化能力不足，且对简单操作（特别是视频领域）脆弱。

Conclusion: 需要开发政治情境化的深度伪造检测框架，以在真实世界环境中更好地保护公众。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [55] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: SHIELD是一个无需训练的方法，通过重新加权视觉token、引入噪声衍生token和应用对抗攻击来缓解大型视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型存在物体幻觉问题，即生成看似合理但不准确的物体描述。本文首次将幻觉问题追溯到视觉编码器，并识别出三个关键问题：统计偏差、固有偏差和脆弱性。

Method: 提出SHIELD框架，包含三种策略：重新加权视觉token以减少统计偏差；引入噪声衍生token来对抗固有偏差；应用对抗攻击和对比解码来解决脆弱性问题。

Result: 实验表明SHIELD能有效缓解多种基准测试和LVLM家族中的物体幻觉问题，并在通用LVLM基准上表现优异。

Conclusion: SHIELD是一个无需训练且广泛适用的框架，能有效解决LVLM中的物体幻觉问题，展示了在视觉编码器层面解决幻觉问题的可行性。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [56] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: VisionSelector是一个轻量级可插拔框架，通过可学习的评分器和可微Top-K机制实现多模态大语言模型中的视觉令牌压缩，在保持性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理高分辨率图像或多图像输入时面临大量视觉令牌带来的计算和内存瓶颈，现有令牌压缩技术受限于启发式规则，可能丢弃关键信息并存在注意力偏差问题。

Method: 提出VisionSelector框架，包含与MLLM主干解耦的评分器模块，采用可微Top-K机制和课程退火策略，实现端到端可学习的令牌选择过程，支持任意压缩率。

Result: 仅需12.85M可训练参数，在MME基准上以30%保留预算保持100%准确率，在10%保留预算下比先前方法提升12.14%，预填充速度翻倍。

Conclusion: VisionSelector提供了一种高效自适应的令牌压缩解决方案，在各种压缩预算下均表现出色，具有良好的泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [57] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 提出了一种用于实时医学图像分析的深度学习框架，整合U-Net、EfficientNet和Transformer等先进神经网络架构，通过模型剪枝、量化和GPU加速实现高效部署，在多个影像模态上达到优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像处理技术缺乏实时临床使用所需的精度、鲁棒性和速度，且高分辨率放射学数据解读耗时且存在临床医生间差异。

Method: 集成U-Net、EfficientNet和Transformer等神经网络架构，采用模型剪枝、量化和GPU加速等实时优化策略，支持在边缘设备、本地服务器和云基础设施上的灵活部署。

Result: 在公共基准数据集上实现分类准确率超过92%，分割Dice分数超过91%，推理时间低于80毫秒，并通过Grad-CAM和分割叠加等可视化工具增强可解释性。

Conclusion: 该框架可显著加速诊断工作流程，减轻临床医生工作量，并在时间关键的医疗环境中支持可信赖的AI集成。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [58] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 基于视觉的小型无人机室内自主飞行系统，结合语义分割和单目深度估计，无需GPS或昂贵传感器即可实现避障、场景探索和自主安全着陆。


<details>
  <summary>Details</summary>
Motivation: 解决在受控室内环境中，小型无人机在缺乏GPS和昂贵传感器情况下的自主导航问题，克服单目深度估计的非度量到度量转换挑战。

Method: 使用知识蒸馏框架，SVM教师生成训练数据训练轻量级U-Net学生网络进行实时语义分割；开发自适应尺度因子算法，通过语义地面检测和相机内参将单目深度预测转换为精确度量距离。

Result: 在5x4米实验室环境中测试，平均距离误差14.4厘米；30次真实环境飞行测试和100次数字孪生环境测试显示，组合方法增加了监视距离，减少了任务时间，保持100%成功率；端到端学习实现87.5%自主任务成功率。

Conclusion: 该工作推进了结构化环境中基于视觉的无人机导航实践，解决了度量深度估计和计算效率挑战，可在资源受限平台上部署。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [59] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: MultiVerse是一个新颖的多轮对话基准测试，包含647个对话，涵盖12个VLM评估基准的484个任务，用于评估视觉语言模型在多轮对话中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮对话数据集（如MMDU、ConvBench）未能充分捕捉真实应用中复杂对话场景的广度和深度，需要更全面的评估基准。

Method: 从12个流行的VLM评估基准中提取647个对话，每个对话平均4轮，提出基于检查表的评估方法，使用GPT-4o作为自动评估器，测量37个关键方面的性能。

Result: 评估了18个VLM，发现即使最强的模型（如GPT-4o）在复杂多轮对话中也仅达到50%的成功率，提供完整对话上下文能显著提升较小或较弱模型的性能。

Conclusion: MultiVerse是一个用于评估VLM多轮交互能力的全面基准，揭示了当前模型在复杂对话场景中的局限性，并强调了上下文学习的重要性。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [60] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 提出使用检索增强生成方法，通过图数据库和Cypher查询语言作为LLM工具，解决大语言模型与3D场景图结合时的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D场景图序列化为文本放入LLM上下文窗口，但这种方法无法扩展到大型或丰富的3D场景图。

Method: 使用检索增强生成方法，将3D场景图编码到图数据库中，为LLM提供Cypher查询语言作为工具来检索相关数据。

Result: 在指令跟随和场景问答任务中，相比基线方法，使用Cypher接口显著提升了在大规模丰富图上的扩展性，提高了语言接地任务的性能，同时大幅减少了场景图内容的token数量。

Conclusion: Cypher作为3D场景图的接口，在本地和云端模型上都能显著提升扩展性，为语言接地任务带来大幅性能改进。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [61] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出了针对病理学基础模型的通用可迁移对抗扰动(UTAP)，这是一种固定且微弱的噪声模式，能够系统性地破坏多个病理学基础模型的特征表示能力，导致下游任务性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 揭示病理学基础模型的关键脆弱性，为模型鲁棒性评估建立高标准基准，推动防御机制发展，确保AI在病理学中的安全可靠部署。

Method: 使用深度学习优化生成固定的微弱噪声模式(UTAP)，将其添加到病理图像中，系统性地破坏基础模型的特征表示能力，并在多个数据集和模型上进行系统评估。

Result: UTAP能够显著降低各种最先进病理学基础模型的性能，具有通用性(跨不同视野)和可迁移性(影响未见过的黑盒模型)，且对输入图像仅进行视觉不可察觉的修改。

Conclusion: UTAP构成了对新兴病理学基础模型及其应用的广泛威胁，强调了推进防御机制的必要性，并为对抗训练提供了必要资源，以确保AI在病理学中的安全部署。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [62] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文提出HYDRA架构，通过教师-学生模型和知识蒸馏方法，实现了从RGB图像到高光谱图像的精确重建，在各项指标上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度注意力方法只能处理稀疏光谱，而现代高光谱传感器包含数百个通道，需要更通用的光谱重建方法。

Method: 使用教师模型编码潜在高光谱数据，学生模型学习从自然图像到教师编码域的映射，结合新颖的训练方法实现知识蒸馏。

Result: 在所有指标上达到SOTA性能，准确率提升18%，在不同通道深度下推理速度均优于当前SOTA模型。

Conclusion: HYDRA架构成功解决了先前光谱重建模型的关键限制，实现了高质量的光谱重建。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [63] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: MSSR是一个双代理框架，通过构建最小充分信息集来解决VLMs在3D空间推理中的挑战，显著提升了准确性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决VLMs在3D空间推理中的两个基本瓶颈：基于2D预训练的3D理解能力不足，以及冗余3D信息导致的推理失败。

Method: 采用双代理框架：感知代理使用感知工具箱程序化查询3D场景提取充分信息，包括新颖的SOG模块；推理代理迭代优化信息以追求最小化，在闭环中修剪冗余细节并请求缺失信息，直到构建出最小充分集。

Result: 在多个挑战性基准测试中显著提高了准确性，达到了最先进的性能水平。

Conclusion: 通过明确追求充分性和最小化，该方法不仅提升了性能，还生成了可解释的推理路径，为未来模型提供了高质量训练数据源。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [64] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 提出SDPA++框架，一种仅使用噪声OCT图像的自监督去噪方法，通过自融合生成伪真值图像来训练去噪模型，在无干净参考图像的情况下提升图像质量。


<details>
  <summary>Details</summary>
Motivation: OCT图像分析对眼科疾病诊断至关重要，但获取配对的干净-噪声图像数据集存在困难，因为OCT存在固有散斑噪声且临床环境采集受限。

Method: 使用自融合和自监督去噪生成伪真值图像，然后采用基于块的策略训练去噪模型集成，仅需噪声OCT图像即可工作。

Result: 在IEEE SPS VIP Cup真实世界数据集上验证，通过CNR、MSR、TP和EP等指标显示性能提升，该数据集仅包含真实噪声OCT图像而无干净参考。

Conclusion: 该方法在临床实践中具有改善图像质量和诊断结果的潜力，特别是在缺乏干净参考图像的情况下。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [65] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 提出了一种新的对比学习范式DCCL，通过增强跨域概念连接性来解决领域泛化问题，在五个标准基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实践中训练和测试样本之间的分布偏移经常发生，阻碍模型泛化性能。虽然对比学习理论上应该能改善领域泛化，但实际应用反而会降低性能，原因是领域泛化设置中缺乏类内连接性。

Method: 提出域连接对比学习(DCCL)：在数据侧使用更激进的数据增强和跨域正样本提高类内连接性；在模型侧提出模型锚定，利用预训练表示中的类内连接性，并通过生成变换损失进行补充。

Result: 在五个标准领域泛化基准测试上的广泛实验验证，DCCL即使在没有域监督的情况下也优于最先进的基线方法。

Conclusion: DCCL通过增强跨域概念连接性，成功解决了对比学习在领域泛化中的性能下降问题，获得了可泛化的表示。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [66] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM是一个基于一致性模型的一步式人体运动预测框架，通过单步生成实现高效推理，性能与最先进的扩散模型相当但推理步骤大幅减少


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型需要多步去噪导致推理效率低的问题，开发更高效的人体运动预测方法

Method: 基于一致性模型，学习噪声与干净运动状态之间的自一致映射，采用基于Transformer的时空架构和时序嵌入来建模长程依赖和保持运动连贯性

Result: 在Human3.6M和HumanEva-I数据集上，HumanCM达到与最先进扩散模型相当或更优的精度，同时将推理步骤减少两个数量级

Conclusion: HumanCM证明了基于一致性模型的一步式生成在人体运动预测中的有效性，在保持高质量的同时显著提升推理效率

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [67] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出了SCENECOT框架，通过基于多模态专家模块的链式思维推理方法，将复杂3D场景推理任务分解为更简单的问题，并构建了首个大规模3D场景CoT推理数据集SCENECOT-185K。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型在基于场景的问答任务中表现不佳，主要原因是缺乏对人类场景-对象推理机制的研究。

Method: 引入基于3D场景的链式思维推理方法(SCENECOT)，将复杂推理任务分解为更简单的问题，并利用多模态专家模块构建视觉线索。

Result: 在多个复杂3D场景推理基准测试中表现优异，实现了高水平的基于场景的问答一致性。

Conclusion: 这是首次成功将CoT推理应用于3D场景理解，实现了逐步的人类式推理，并显示出扩展到更广泛3D场景理解场景的潜力。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [68] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM是一种隐式残差世界模型，专注于建模当前状态和世界演化，通过预测残差变化而非完整重建未来场景来提升自动驾驶系统的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶系统在视觉中心世界模型中存在效率问题，它们会完全重建未来场景，导致大量计算资源浪费在静态背景的冗余建模上。

Method: IR-WM首先从视觉观测建立鲁棒的鸟瞰图表示，然后利用前一时刻的BEV特征作为强时序先验，仅预测基于自车动作和场景上下文的条件残差变化。还包含对齐模块来校准语义和动态不对齐问题。

Result: 在nuScenes基准测试中，IR-WM在4D占用预测和轨迹规划方面均取得了顶级性能。

Conclusion: 隐式残差世界模型通过专注于动态变化而非完整重建，显著提升了自动驾驶世界模型的效率和规划准确性。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [69] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: UKANFormer是一种基于UKAN架构的语义分割模型，通过集成全局-局部变换器模块，在Allen Coral Atlas的噪声监督下实现高精度珊瑚礁映射，性能优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有全球珊瑚礁分布产品（如Allen Coral Atlas）在空间精度和语义一致性方面存在局限，特别是在需要精细边界划分的区域。需要开发能够在噪声监督下实现高精度映射的模型。

Method: 在UKAN架构基础上，解码器中加入全局-局部变换器（GL-Trans）模块，能够同时提取全局语义结构和局部边界细节，在噪声标签设置下进行训练。

Result: UKANFormer在珊瑚类IoU达到67.00%，像素精度达到83.98%，优于传统基线方法。模型产生的预测在视觉和结构上都比训练使用的噪声标签更准确。

Conclusion: 架构设计可以缓解标签噪声问题，支持在非完美监督下进行可扩展映射，为可靠标签稀缺的生态监测提供了基础。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [70] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 该论文提出了一个用于具身AI中世界模型的统一框架，包括问题形式化、学习目标和三轴分类法，系统化整理了数据资源和评估指标，并指出了当前面临的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器，能够捕捉环境动态，支持感知、预测和决策制定。

Method: 提出了一个三轴分类法：(1) 功能性：决策耦合vs通用目的；(2) 时间建模：序列模拟与推理vs全局差异预测；(3) 空间表示：全局潜在向量、令牌特征序列、空间潜在网格和分解渲染表示。

Result: 系统化整理了机器人学、自动驾驶和通用视频设置中的数据资源和评估指标，涵盖了像素预测质量、状态级理解和任务性能，并对最先进模型进行了定量比较。

Conclusion: 指出了关键开放挑战：统一数据集的稀缺性、需要评估物理一致性而非像素保真度的指标、模型性能与实时控制计算效率之间的权衡，以及实现长期时间一致性同时减轻误差累积的核心建模难度。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [71] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 研究表明，离散自回归模型通过束搜索在图像生成中实现显著改进，2B参数模型性能超过12B扩散模型，证明模型架构对推理时优化至关重要。


<details>
  <summary>Details</summary>
Motivation: 虽然搜索策略在语言模型中取得突破，但在图像生成中应用困难，扩散模型的连续特性限制了搜索效果，需要探索离散自回归模型的潜力。

Method: 使用离散自回归视觉模型，应用束搜索策略进行图像生成，利用离散token空间实现早期剪枝和计算重用。

Result: 束搜索大幅提升文本到图像生成质量，2B参数自回归模型在多个基准测试中超越12B参数扩散模型。

Conclusion: 模型架构（而非仅规模）对视觉生成中的推理时优化至关重要，离散token空间为有效搜索提供了关键优势。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [72] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 该论文提出了一个基于人类感知的SR伪影显著度评估方法，创建了包含1302个伪影示例的数据集，并训练了一个轻量级回归器来生成空间显著度热图。


<details>
  <summary>Details</summary>
Motivation: 随着生成式图像超分辨率模型能力的增强，它们倾向于产生伪影，这些伪影对人类感知的影响程度各不相同，应该根据其显著度而非统一处理为二元缺陷来表征。

Method: 创建了一个包含1302个来自11种当代图像SR方法的伪影示例数据集，每个伪影都配有众包显著度评分，并基于此训练了一个轻量级回归器来生成空间显著度热图。

Result: 训练的回归器在检测显著伪影方面优于现有方法，能够生成准确的空间显著度热图。

Conclusion: 该研究为基于显著度的SR伪影评估和缓解提供了数据集和工具，有助于更准确地评估超分辨率图像质量。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [73] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: 提出WaMaIR框架，通过全局多尺度小波变换卷积扩大感受野，结合Mamba通道感知模块捕获长距离依赖关系，并使用多尺度纹理增强损失函数，显著提升图像恢复中的纹理细节重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法在图像恢复中难以充分恢复精细纹理细节，受限于小感受野和缺乏通道特征建模。

Method: 使用全局多尺度小波变换卷积(GMWTConvs)扩大感受野，提出Mamba通道感知模块(MCAM)捕获通道间长距离依赖，并设计多尺度纹理增强损失(MTELoss)。

Result: 在广泛实验中，WaMaIR超越了现有最先进方法，实现了更好的图像恢复效果和高效的计算性能。

Conclusion: WaMaIR框架通过扩大感受野和增强通道特征建模，有效解决了图像恢复中的纹理细节重建问题。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [74] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出Region in Context框架，通过多级语义对齐实现文本条件图像编辑，考虑区域在整体图像上下文中的角色，生成更一致和协调的编辑结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图像区域孤立处理，仅依赖局部线索，导致编辑不一致、过渡不自然或图像整体连贯性丧失。需要让每个区域理解其在全局图像上下文中的角色。

Method: 引入双级引导机制：区域在全图像上下文中表示并与详细区域级描述对齐，同时整个图像与大型视觉语言模型生成的场景级描述匹配。这些描述作为显式语言参考，指导局部修改和全局结构。

Result: 实验表明该方法能产生更连贯且与指令对齐的结果。

Conclusion: Region in Context框架通过多级语义对齐实现了更精确和协调的文本条件图像编辑，解决了现有方法在全局一致性方面的问题。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [75] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 提出EMRRG框架，通过微调预训练Mamba网络来生成X光医疗报告，使用参数高效方法和混合解码器，在基准数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有医疗报告生成模型主要依赖LLM，对预训练视觉基础模型和先进微调技术探索有限，主流框架要么避免微调要么使用简单方法如LoRA，忽视了增强跨注意力机制的潜力。同时，非Transformer架构如Mamba网络在医疗报告生成中尚未充分研究。

Method: 将X光图像分块、标记化，通过SSM-based视觉骨干网络提取特征，使用Partial LoRA获得最优性能。LLM与混合解码器生成医疗报告，实现端到端训练。

Result: 在三个广泛使用的基准数据集上的大量实验充分验证了所提策略对X光医疗报告生成的有效性。

Conclusion: EMRRG框架通过微调预训练Mamba网络和参数高效方法，在X光医疗报告生成任务中取得了显著成果，为未来研究提供了有前景的方向。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [76] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE提出了一种基于3D高斯泼溅和束调整原理的6D物体姿态估计算法，通过可微渲染管道迭代优化姿态参数，在纹理缺失和光照变化场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计方法依赖2D-3D特征对应关系，在纹理缺失物体和光照变化条件下表现不佳，需要更鲁棒的解决方案。

Method: 基于束调整原理，利用李代数扩展3DGS构建姿态可微渲染管道，通过比较输入图像与渲染图像迭代优化姿态，同时更新3DGS模型中的颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion和LineMod数据集上分别实现了1.4%、2.8%和2.5%的精度提升。

Conclusion: GS2POSE通过结合3DGS和束调整原理，有效解决了纹理缺失和光照变化下的6D姿态估计问题，在多个基准数据集上取得了优越性能。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [77] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视频理解框架，通过结合预训练视觉语言模型的语义先验和机器学习算法，将视频理解重新定义为高维语义特征空间中的自监督时空聚类问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型依赖大量标注数据进行任务特定训练，成本高且扩展性有限。需要开发无需训练的方法来充分利用大规模视觉语言模型的零样本推理能力。

Method: 首先使用预训练VLM的冻结视觉编码器将视频转换为语义特征轨迹，然后使用核时间分割(KTS)将连续特征流分割为离散的语义连贯事件段，最后通过无监督密度聚类识别视频中的重复宏观场景和主题。

Result: 该方法能够自动从每个发现的聚类中选择代表性关键帧，并利用VLM的生成能力生成文本描述，从而自动生成视频内容的结构化多模态摘要。

Conclusion: 该框架为零样本、自动化的视频内容结构分析提供了一条有效、可解释且模型无关的途径。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [78] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS是一种新颖的即插即用解决方案，通过在完全冻结的多模态大语言模型上附加轻量级可训练头部，利用注意力图中的空间线索提取关键点，实现像素级分割，同时保持模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法需要微调模型以产生与掩码解码器兼容的特定输出，这会改变模型的输出空间并损害其内在泛化能力，违背了构建统一模型的目标。

Method: LENS在完全冻结的MLLM上附加轻量级可训练头部，通过细化注意力图中嵌入的空间线索来提取关键点，并将其描述为与掩码解码器直接兼容的点状特征。

Result: LENS实现了与基于重新训练方法相竞争甚至更优的分割性能，同时完全保留了MLLM的泛化能力，而微调方法会显著降低这种能力。

Conclusion: LENS的可附加设计为扩展MLLM建立了一个高效而强大的范式，为构建真正多才多艺的统一模型铺平了道路。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [79] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出了一种完全无监督的二元道路分割方法，利用场景几何和时序线索来区分道路与非道路区域，无需依赖人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 消除对昂贵人工标注数据集的依赖，通过无监督方法实现道路分割，为自动驾驶提供可扩展的解决方案。

Method: 首先基于几何先验生成弱标签（地平线以上为非道路，车辆前方四边形为道路），然后通过跨帧跟踪局部特征点并利用互信息最大化来强制时序一致性。

Result: 在Cityscapes数据集上实现了0.82的交并比(IoU)，展示了高精度和简单设计的优势。

Conclusion: 结合几何约束和时序一致性为自动驾驶中的可扩展无监督道路分割展示了巨大潜力。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [80] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 提出个性化图像滤镜(PIF)，基于预训练扩散模型和文本反转技术，能够从参考图像中学习摄影风格概念并有效迁移到内容图像上。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法从参考图像中学习有意义的摄影概念，要么无法保持内容图像的原始内容。摄影风格作为特定摄影概念的组合，是著名摄影师魅力的来源。

Method: 基于预训练文本到图像扩散模型，使用文本反转技术优化摄影概念的提示词，学习参考图像的摄影风格。生成先验使PIF能够学习摄影概念的平均外观以及如何根据文本提示调整它们。

Result: PIF在提取和迁移各种摄影风格方面表现出色性能。

Conclusion: PIF能够有效解决摄影风格学习和迁移中的关键问题，在保持内容图像内容的同时成功提取和转移参考图像的摄影风格。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [81] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 构建了一个荔枝检测和成熟度分类的开源数据集，包含11,414张图像，涵盖不同品种、天气条件和成熟阶段的荔枝，并进行了统计分析和深度学习模型评估。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境中具有一致性和全面标注的荔枝开源数据集，而高质量数据对于开发基于视觉的荔枝采摘机器人至关重要。

Method: 采集了多种荔枝品种在不同天气条件和时间段的彩色图像，通过数据增强扩展数据集，由多人独立标注并验证，包含三种成熟度阶段的标注信息。

Result: 构建了包含11,414张图像的数据集（878张原始RGB图像、8,780张增强RGB图像和1,756张深度图像），标注了9,658对荔枝检测和成熟度分类标签，并进行了详细的统计分析。

Conclusion: 该数据集为荔枝检测和成熟度分类研究提供了高质量的开源资源，有助于推动基于视觉的荔枝采摘机器人的发展。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [82] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet是一个大型公共珊瑚礁图像数据集，包含约925,000个属级硬珊瑚标注，映射到世界海洋物种名录，提供细粒度、全球尺度的标注数据。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化等人为压力导致珊瑚礁快速衰退，迫切需要可扩展的自动监测方法。现有数据集在规模、地理范围或标注粒度上存在局限，且不适合机器学习。

Method: 整合了76个CoralNet来源和红海Al Wajh站点的图像，提供专家验证的标签。提出两种评估设置：源内基准测试和跨源基准测试，分析监督和零样本分类性能。

Result: 监督学习在源内表现良好，但跨域性能急剧下降；零样本模型在所有情况下表现都较差，特别是对于稀有和视觉相似的属。

Conclusion: ReefNet提供了一个具有挑战性的基准，旨在推动领域泛化和细粒度珊瑚分类的进展，促进稳健、领域自适应的全球珊瑚礁监测和保护。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [83] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 提出AdaptMoist域适应方法，利用纹理特征预测木屑水分含量，解决不同来源木屑数据分布变化问题，准确率达80%，比非适应模型提高23%。


<details>
  <summary>Details</summary>
Motivation: 现有直接方法（烘箱干燥）处理时间长且破坏样本，间接方法在木屑来源多样时准确性不足。需要一种能够有效缓解来源变异影响的鲁棒方法。

Method: 分析五种纹理特征类型，提出AdaptMoist域适应方法，利用纹理特征在不同域间转移知识，并基于调整互信息提出模型保存标准。

Result: 组合五种纹理特征准确率达95%；AdaptMoist方法在跨域预测中平均准确率达80%，比非适应模型（57%）提高23%。

Conclusion: AdaptMoist是跨域木屑水分含量估计的有效鲁棒解决方案，对依赖木屑的行业具有应用潜力。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [84] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 提出M2HVideo框架，将人台视频转换为身份可控、逼真的人类视频，解决头部与身体运动不对齐和身份漂移问题。


<details>
  <summary>Details</summary>
Motivation: 人台服装展示成本低但缺乏真实感和表现细节，需要将人台视频转换为逼真的人类视频。

Method: 使用动态姿态感知头部编码器融合面部语义和身体姿态，引入镜像损失和分布感知适配器增强时间一致性。

Result: 在多个数据集上实验表明，M2HVideo在服装一致性、身份保持和视频保真度方面优于现有方法。

Conclusion: M2HVideo能有效生成身份可控的逼真人类视频，为人台到人类的视频转换提供了可行解决方案。

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [85] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 提出2DGS-R方法，通过分层训练策略在保持几何精度的同时提升渲染质量，仅需1%额外存储和少量训练时间即可实现高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)渲染质量高但难以准确表示表面，2DGS几何保真度好但渲染质量受限，目前无法在单阶段训练中同时优化几何和渲染质量。

Method: 采用分层训练：先用法向一致性正则化训练原始2D高斯；选择渲染质量不足的2D高斯进行原地克隆操作；最后冻结不透明度微调模型。

Result: 相比原始2DGS仅需1%额外存储和少量训练时间，实现了高质量渲染结果同时保持精细几何结构。

Conclusion: 该方法有效平衡了效率与性能，在视觉保真度和几何重建精度方面均有提升。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [86] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: ArmFormer是一个轻量级基于transformer的语义分割框架，通过集成CBAM注意力模块和MixVisionTransformer架构，实现了高精度武器分割，同时保持计算效率，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统武器检测方法只能提供粗粒度的边界框定位，缺乏细粒度分割能力；现有语义分割模型要么牺牲精度换取效率，要么计算资源需求过高，不适合边缘部署场景。

Method: 结合CBAM增强的编码器骨干网络和注意力集成hamburger解码器，实现五类武器（手枪、步枪、刀、左轮手枪、人类）的语义分割。

Result: 在保持82.26 FPS实时推理速度的同时，达到80.64% mIoU和89.13% mFscore的先进性能，仅需4.886G FLOPs和3.66M参数。

Conclusion: ArmFormer在计算效率方面显著优于需要48倍计算量的重型模型，是便携安全摄像头、监控无人机和嵌入式AI加速器部署的最佳解决方案。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [87] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: BARL是一个半监督医学图像分割框架，通过在表示空间和标签空间同时进行对齐，显著提升分割性能并减少标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法主要依赖标签空间一致性，但忽略了表示空间对齐的重要性，导致模型难以学习到既具有判别性又具有空间一致性的表示。

Method: BARL框架包含两个协作分支，在标签空间通过双路径正则化和渐进认知偏置校正实现对齐，在表示空间通过区域级和病灶实例级匹配来对齐特征。

Result: 在四个公共基准数据集和一个私有CBCT数据集上的实验表明，BARL持续超越最先进的半监督医学图像分割方法。

Conclusion: BARL通过双边对齐策略有效提升了半监督医学图像分割的性能，消融研究验证了各组件的贡献。

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [88] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种基于配准引导的旋转不变特征提取框架，将点云配准与基于内存库的异常检测相结合，解决了现有方法在特征变换不一致和局部几何细节捕捉方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于内存库的3D异常检测方法存在特征变换不一致和判别能力有限的问题，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些问题会导致不可靠的检测结果。

Method: 提出配准引导的旋转不变特征提取框架，将点云配准与异常检测目标集成，通过在配准学习过程中嵌入特征提取，联合优化对齐和表示学习。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上的大量实验表明，该方法在有效性和泛化性方面持续优于现有方法。

Conclusion: 点云配准不仅对几何结构对齐至关重要，还能引导特征提取获得旋转不变且局部判别性强的表示，通过将特征提取嵌入配准学习过程，可以获得对旋转鲁棒且对异常检测高度有效的特征。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [89] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 提出了一种神经元级别的分析框架，通过结合人工神经元分析和fMRI体素编码，研究视觉语言模型与人脑多模态信息处理机制的相似性，发现VLMs在神经元层面具有类脑的分层处理特性。


<details>
  <summary>Details</summary>
Motivation: 当前对人工神经网络与人脑处理机制的理解存在局限：单模态ANN研究无法捕捉大脑固有的多模态处理能力，而多模态ANN研究主要关注高层模型输出，忽略了单个神经元的关键作用。

Method: 提出神经元级别的分析框架，结合细粒度人工神经元分析和基于fMRI的体素编码，对CLIP和METER两种架构不同的视觉语言模型进行研究。

Result: 发现四个关键结果：(1)人工神经元能成功预测多个功能网络中生物神经元的活动；(2)人工和生物神经元都表现出功能冗余；(3)人工神经元表现出与生物神经元相似的极性模式；(4)不同架构驱动不同的生物神经元激活模式。

Conclusion: 研究结果为视觉语言模型在神经元层面具有类脑分层处理特性提供了有力证据，揭示了ANN与BN在表示机制上的共享特性。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [90] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出Class-N-Diff分类诱导扩散模型，在生成皮肤镜图像的同时进行分类，通过集成分类器引导图像生成，提高生成质量和分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型难以准确生成特定医学类别图像，限制了在皮肤癌诊断等应用中的实用性。

Method: 在扩散模型中集成分类器，基于类别条件引导图像生成，实现同时生成和分类皮肤镜图像。

Result: 模型能更好地控制类别条件图像合成，生成更真实多样的图像，分类器性能也得到提升。

Conclusion: Class-N-Diff是增强扩散模型合成皮肤镜图像质量和实用性的强大工具。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [91] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: Edit-R1是一个基于策略优化的后训练框架，通过DiffusionNFT方法和MLLM奖励模型提升指令图像编辑的泛化能力，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调方法在指令图像编辑中容易过拟合训练数据模式，限制了模型的泛化能力。

Method: 使用Diffusion Negative-aware Finetuning策略优化方法，结合多模态大语言模型作为统一奖励模型，并设计低方差组过滤机制减少评分噪声。

Result: UniWorld-V2在ImgEdit和GEdit-Bench基准测试中分别获得4.49和7.83分，达到最先进水平，且框架具有模型无关性。

Conclusion: Edit-R1框架有效解决了指令图像编辑中的过拟合问题，显著提升了模型的泛化能力和编辑质量，具有广泛适用性。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [92] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 提出了一个基于地面摄像机的凝迹-航班归因框架，用于将观测到的凝迹与生成它们的航班关联起来。


<details>
  <summary>Details</summary>
Motivation: 航空非CO2效应（特别是凝迹）对气候影响显著，但卫星归因方法受限于空间和时间分辨率。地面相机能在凝迹形成后不久以高分辨率捕捉它们。

Method: 使用地面可见相机凝迹序列数据集，开发模块化框架，将地面相机观测的凝迹与基于飞机监视和气象数据的理论凝迹进行归因。框架支持多种几何表示和距离度量，包含时间平滑和基于概率的分配策略。

Result: 建立了一个强大的基线，为未来凝迹与源航班关联研究提供了模块化框架。

Conclusion: 地面相机方法为凝迹-航班归因提供了可行的替代方案，特别是在凝迹保持薄、线性和视觉可区分时。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [93] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 本文评估了四种基于Transformer的架构（SegFormer、DeepLabV3+、SegNeXt、Swin Transformer）在热成像武器分割任务中的性能，在包含9,711张图像的自定义热成像数据集上取得了显著改进，其中SegFormer-b5获得最高mIoU（94.15%），SegFormer-b0提供最快推理速度（98.32 FPS）。


<details>
  <summary>Details</summary>
Motivation: 热成像武器分割在低光照和视觉遮挡条件下对监控和安全应用至关重要，而传统CNN方法在捕捉长距离依赖和精细结构细节方面存在局限，Transformer架构在RGB分割任务中表现出色但在热成像武器分割领域尚未充分探索。

Method: 使用MMSegmentation框架，采用标准数据增强策略，在包含9,711张从真实监控视频收集并使用SAM2自动标注的热成像图像的自定义数据集上，评估四种Transformer架构（SegFormer、DeepLabV3+、SegNeXt、Swin Transformer）的二元武器分割性能。

Result: SegFormer-b5达到最高mIoU（94.15%）和像素精度（97.04%），SegFormer-b0提供最快推理速度（98.32 FPS）且mIoU保持竞争力（90.84%），SegNeXt-mscans在85.12 FPS和92.24% mIoU间取得平衡，DeepLabV3+ R101-D8达到92.76% mIoU和29.86 FPS。

Conclusion: Transformer架构在低光照和遮挡热成像环境中展现出强大的武器检测泛化能力，提供了灵活的精度-速度权衡，适用于多样化的实时安全应用场景。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [94] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 提出了Res-Bench基准测试，评估多模态大语言模型在不同输入分辨率下的性能稳定性，引入新的鲁棒性指标来衡量分辨率变化对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法主要关注语义性能，忽略了分辨率鲁棒性这一关键问题——即模型在不同输入分辨率下性能是否保持稳定。

Method: 构建包含14,400个样本的Res-Bench基准测试，涵盖12个分辨率级别和6个核心能力维度。设计了新颖的评估框架，引入Spearman相关性和绝对/相对连续误差等鲁棒性指标。

Result: 对领先的MLLMs进行了大规模评估，包括模型中心和任务中心的鲁棒性检查、预处理策略（填充和超分辨率）研究，以及微调对稳定性增强的探索。

Conclusion: Res-Bench为评估多模态大语言模型的分辨率鲁棒性提供了全面框架，揭示了当前模型在不同分辨率下的性能稳定性问题。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [95] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 这篇综述文章系统分析了医学图像分析中的基础模型，包括视觉专用和视觉语言模型，讨论了架构演变、训练策略、临床应用，并识别了关键挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学图像分析领域快速发展，但该领域仍缺乏对架构演变、训练范式和临床应用的系统性综合，需要统一的框架来指导研究和临床实践。

Method: 采用系统性分类方法，将研究分为视觉专用和视觉语言基础模型，基于架构基础、训练策略和下游临床任务进行组织，并进行定量元分析以表征数据集利用和应用领域的时间趋势。

Result: 研究发现基础模型在医学图像分析中展现出强大的零样本和少样本性能，能够通过最小微调适应各种下游临床应用，但面临领域适应、高效微调、计算约束和可解释性等挑战。

Conclusion: 基础模型在医学图像分析中具有巨大潜力，但需要解决领域适应、计算效率和可解释性等关键挑战，未来研究应关注增强模型的鲁棒性、可解释性和临床集成，以加速其在真实医疗实践中的应用。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [96] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出Di-Bregman框架，通过Bregman散度密度比匹配来加速扩散模型采样，实现高效一步生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成质量高但计算成本昂贵，现有蒸馏方法缺乏统一理论基础。

Method: 将扩散蒸馏建模为Bregman散度密度比匹配问题，提供凸分析视角统一现有目标。

Result: 在CIFAR-10和文本到图像生成任务上，相比反向KL蒸馏获得更好的一步FID，保持高视觉保真度。

Conclusion: Bregman密度比匹配是通向高效一步扩散生成的理论基础扎实且实用的路径。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [97] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出了CARE框架，通过序列-图像对比对齐方法解决ADL识别中序列和图像表示之间的对齐问题，在三个CASAS数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有ADL识别方法存在表示级限制：序列方法保持时间顺序但对噪声敏感且缺乏空间感知，图像方法捕获全局模式但压缩时间动态并扭曲传感器布局，简单融合方法无法实现表示视图间的对齐。

Method: CARE框架包含：(1)时间感知、抗噪声序列编码；(2)空间感知和频率敏感的图像表示；(3)联合对比-分类目标函数，通过序列-图像对比对齐(SICA)和交叉熵联合优化表示学习和分类。

Result: 在三个CASAS数据集上达到最先进性能：Milan 89.8%、Cairo 88.9%、Kyoto7 73.3%，并展示了对传感器故障和布局变化的鲁棒性。

Conclusion: CARE框架通过序列-图像对比对齐实现了可靠的对齐和判别性嵌入学习，在智能家居中具有可靠的ADL识别潜力。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [98] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、在线执行的视频步骤定位方法BaGLM，利用大型多模态模型的零样本能力，通过贝叶斯滤波整合历史帧信息，在三个数据集上超越了需要训练的最先进离线方法。


<details>
  <summary>Details</summary>
Motivation: 传统的视频步骤定位方法需要带标注的训练集且需要离线处理整个视频，这限制了在需要在线决策场景中的应用。本文旨在探索如何在不进行训练的情况下在线执行视频步骤定位。

Method: 利用大型多模态模型的零样本能力预测有限帧的步骤，开发了BaGLM方法，通过贝叶斯滤波原理整合历史帧信息，包括通过大型语言模型提取的依赖矩阵和步骤进度估计。

Result: 实验表明，这种无需任务特定调优的在线策略优于离线和基于训练的模型。BaGLM在三个数据集上表现出优于最先进训练型离线方法的性能。

Conclusion: BaGLM证明了利用大型多模态模型的零样本能力进行在线视频步骤定位的可行性，无需训练即可超越传统方法，为实时应用提供了有效解决方案。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [99] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本文通过实证研究探讨不同视频特征对时序视频定位任务的影响，发现仅改变视频编码器就能显著影响模型性能，并揭示了特征互补的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前时序视频定位研究集中在少数视频表示方法上，可能导致架构过拟合。为解决此问题，需要研究不同视频特征对经典架构的影响。

Method: 在三个基准数据集上使用基于CNN、时序推理和Transformer的视频编码器提取特征，并在经典架构上比较这些特征的影响。

Result: 结果显示仅改变视频编码器就能显著影响模型性能，同时揭示了使用某些特征时的明显模式和错误，表明特征间存在互补性。

Conclusion: 视频特征选择对时序视频定位任务至关重要，不同特征具有互补性，为未来研究提供了重要启示。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [100] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 该研究挑战了遥感专用基础模型优于通用视觉基础模型的观念，通过设计基准测试和训练自监督模型，发现在小规模下遥感专用模型并未带来一致改进。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证遥感专用基础模型是否真的比通用视觉基础模型更有用，特别是针对遥感图像的独特特征、特定应用和鲁棒性需求。

Method: 设计了测量遥感模型向低分辨率图像泛化能力的基准测试；在MillionAID数据集上训练了自监督视觉编码器iBOT，并加入了遥感特定的修改。

Result: 在ViT-B规模下，所有预训练模型都没有比通用基线带来一致的改进效果。

Conclusion: 至少在较小规模下，遥感专用基础模型并不比通用视觉基础模型更有优势。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [101] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 提出W2R2训练框架，通过解耦表征学习和针对性捷径抑制来解决多模态3D定位中的2D语义偏差问题，在不改变推理架构的情况下实现精确的3D定位。


<details>
  <summary>Details</summary>
Motivation: 多模态3D定位模型存在严重的"2D语义偏差"，过度依赖2D图像特征进行粗略定位，而忽视了3D几何输入，导致融合性能不佳。

Method: W2R2框架将2D特征指定为"What"识别的语义信标，3D特征作为"Where"定位的空间锚点，采用双目标损失函数：对齐损失使用自适应交叉熵监督融合预测，伪标签损失通过基于边界的机制惩罚过度有效的2D主导伪输出。

Result: 在ScanRefer和ScanQA数据集上的实验表明，W2R2在定位准确性和鲁棒性方面取得显著提升，特别是在杂乱的室外场景中。

Conclusion: W2R2通过解耦表征学习和捷径抑制有效解决了2D语义偏差问题，为多模态3D定位提供了有效的训练框架。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [102] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 本文提出了一种使用条件StyleGAN2-ADA和StyleGAN3生成高分辨率合成活体指纹，并通过CycleGAN转换为多种材料伪造指纹的方法，解决了生物特征数据收集中的隐私、成本和可访问性问题。


<details>
  <summary>Details</summary>
Motivation: 大型指纹数据集收集耗时昂贵且需要严格隐私保护，研究人员探索使用合成指纹数据来解决这些问题。

Method: 使用条件StyleGAN2-ADA和StyleGAN3架构生成特定手指身份的高分辨率合成活体指纹，然后通过CycleGAN将其转换为模拟各种呈现攻击材料（如EcoFlex、Play-Doh）的逼真伪造指纹。

Result: 创建了两个包含1500个指纹图像的合成数据集（DB2和DB3），StyleGAN3模型的FID低至5，生成指纹在0.01%错误接受率下达到99.47%的真实接受率，StyleGAN2-ADA模型达到98.67%的真实接受率。

Conclusion: 生成的合成指纹在质量评估和匹配实验中表现优异，且没有明显的身份泄露证据，证实了合成数据集具有强大的隐私保护特性。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [103] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 本研究开发了一个临床医生参与循环的深度学习管道，用于肺癌CT图像分割和预后预测。VNet模型在分割性能、放射组学稳定性和预测准确性方面表现最佳，半监督学习优于监督学习，放射科医生更倾向于使用AI生成的初始掩码进行精炼。


<details>
  <summary>Details</summary>
Motivation: 肺癌是癌症死亡的主要原因，CT成像在筛查、预后和治疗中至关重要。手动分割存在变异性和耗时问题，而深度学习虽然提供自动化但面临临床采用障碍。

Method: 使用来自12个公共数据集的999名患者的多中心CT数据，采用5种深度学习模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D），在完整图像和点击裁剪图像上与专家轮廓进行基准测试。评估分割可重复性使用497个PySERA提取的放射组学特征，预后建模比较监督学习和半监督学习，6名医生在7个领域进行定性评估。

Result: VNet达到最佳性能（Dice = 0.83，IoU = 0.71），放射组学稳定性（平均相关性=0.76，ICC=0.65），半监督学习下的预测准确性（准确率=0.88，F1=0.83）。半监督学习在所有模型中均优于监督学习。放射科医生更青睐VNet的瘤周表示和平滑边界，偏好AI生成的初始掩码进行精炼。

Conclusion: 将VNet与半监督学习结合可产生准确、可重复且临床可信的基于CT的肺癌预后预测，为以医生为中心的AI转化提供了可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [104] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出了一种广义选择方法，通过选择不限于类质心的表示来改进行人重识别性能，在准确率和平均精度之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 虽然特征提取方法和目标函数改进已显著提升行人重识别性能，但选择更好的类代表是一个未被充分探索的研究领域。现有方法使用类质心作为代表，但效果不理想。

Method: 提出广义选择方法，选择不限于类质心的表示作为类代表，可根据应用需求调整每类的实际表示数量。

Result: 该方法在多种重识别嵌入上应用，在所有情况下都显著优于当代结果，在准确率和平均精度之间取得了更好的平衡。

Conclusion: 广义选择方法能够有效改进行人重识别性能，超越了现有技术水平，且具有灵活性可根据具体应用需求调整。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [105] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 提出V-Reason方法，通过熵信号优化LMM推理过程，无需RL训练即可显著提升视频推理性能，减少58.6%输出token。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理LMM依赖昂贵的RL训练和冗长的思维链，计算开销大且思维控制机制有限。

Method: 利用输出熵作为信号，通过小型可训练控制器优化LMM的值缓存，使用基于熵的目标函数在推理时调整模型的微探索和开发行为。

Result: 在多个视频推理数据集上显著优于基础指令调优模型，与RL训练模型的准确率差距缩小至0.6%以内，同时输出token减少58.6%。

Conclusion: 基于熵的推理时优化是有效替代RL训练的方法，能显著提升视频推理效率而不牺牲性能。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [106] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 比较通用视觉模型Hiera和专用分割模型SAM2的特征通用性，发现专用化在空间相关任务上有效，但在概念距离较远的任务上表现不佳，存在信息损失。


<details>
  <summary>Details</summary>
Motivation: 研究通用基础视觉模型与专用模型之间的权衡，理解特征编码设计的效率问题。

Method: 使用轻量级可训练neck来探测冻结特征的适应性，量化专用化的信息论成本，并进行跨neck分析。

Result: SAM2在深度估计等空间任务上表现出色，但在姿态估计和图像描述等任务上表现不如Hiera，显示语义信息损失。每个适应层级都会产生表示瓶颈。

Conclusion: 专用化存在特征通用性的权衡，为设计高效特征编码和适应策略提供了量化基础。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [107] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: 提出ProDAT方法，通过密度感知的尾丢弃机制实现点云的渐进式编码，在单一模型中支持多码率解码，相比现有方法显著提升编码效率。


<details>
  <summary>Details</summary>
Motivation: 3D点云在自动驾驶、增强现实等应用中需要实时处理和低延迟，但大数据量和带宽限制阻碍了在资源受限环境中的高质量服务部署。现有学习型点云几何编码方法的固定潜在表示不支持渐进式解码。

Method: 提出ProDAT密度感知尾丢弃机制，利用密度信息作为引导信号，根据重要性自适应解码潜在特征和坐标，在单一模型中实现多码率的渐进式解码。

Result: 在基准数据集上的实验结果表明，ProDAT不仅实现渐进式编码，而且在编码效率上优于最先进的学习型编码技术，在SemanticKITTI上PSNR-D2的BD-rate提升超过28.6%，在ShapeNet上提升超过18.15%。

Conclusion: ProDAT成功解决了点云渐进式编码问题，通过密度感知机制实现了高效的渐进解码，为资源受限环境中的点云应用提供了可行解决方案。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [108] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: 提出了FMCAF预处理架构，通过频域滤波和跨注意力融合来增强RGB和红外模态的融合效果，在多个数据集上优于传统融合方法。


<details>
  <summary>Details</summary>
Motivation: 多模态目标检测通过融合多种传感器模态的互补信息来提高在挑战性条件下的鲁棒性，但现有方法往往针对特定数据集设计，缺乏通用性。

Method: FMCAF结合频域滤波块(Freq-Filter)来抑制冗余频谱特征，以及基于跨注意力的融合模块(MCAF)来改善模态间特征共享。

Result: 在LLVIP(低光行人检测)和VEDAI(航空车辆检测)数据集上，FMCAF比传统拼接融合方法表现更好，VEDAI上mAP@50提升13.9%，LLVIP上提升1.1%。

Conclusion: FMCAF作为一个灵活的预处理架构，为未来稳健的多模态融合检测流程提供了有前景的基础。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [109] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane通过引入平面先验来改进高斯溅射的平面重建质量，提供更准确几何和结构化网格连接，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯溅射方法在重建平面区域时难以获得足够平滑和精确的结果，影响下游场景编辑和物理模拟应用。

Method: 利用现成分割和法线预测模型提取平面先验，建立结构化平面高斯坐标表示，引入动态高斯重分类器优化训练，并使用优化后的平面先验改进网格布局。

Result: 在保持渲染质量的同时，显著提高了提取网格的几何精度，减少了顶点和面数，改善了拓扑结构。

Conclusion: 平面先验的引入能够有效提升高斯溅射在平面重建方面的几何精度，支持场景对象的解耦和灵活操作。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [110] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 提出了一种针对预训练扩散模型的优化策略，通过潜在空间精炼和双向交互机制，在保持感知真实性的同时显著提升内容保真度，特别是在低光照场景中。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型在低层视觉任务中表现出色，但往往以牺牲内容保真度为代价来获得更高的感知真实性，这一问题在低光照场景中尤为严重。

Method: 引入潜在精炼管道恢复VAE编码中丢失的空间细节，并建立条件潜在与噪声潜在之间的动态双向交互机制。

Result: 大量实验表明该方法显著提升了预训练扩散模型的保真度性能。

Conclusion: 提出的优化策略能够有效提升扩散模型的内容保真度，同时保持感知真实性和美学质量，且具有即插即用的特性。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [111] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出一种使用LED环境照明为消费级相机生成视觉不可见水印的方法，通过优化LED光谱特性使其对人眼不可见但对相机高度可检测


<details>
  <summary>Details</summary>
Motivation: 开发一种在消费级相机拍摄的视频中嵌入水印的技术，用于隐私保护和内容验证，同时确保水印对人眼不可见

Method: 采用光谱调制而非强度调制，联合考虑人眼视觉系统敏感性、相机传感器光谱灵敏度和LED产生白光的特性，优化LED光谱轮廓

Result: 能够在标准低帧率（30-60 fps）下提取水印，在10秒视频片段中嵌入128位信息，足以支持隐私保护和内容验证的元数据

Conclusion: 该方法成功实现了对人眼不可见但对消费级相机可检测的水印嵌入，为隐私保护和内容验证提供了实用解决方案

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [112] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 提出GOOD框架，通过双重引导机制直接引导扩散模型采样轨迹生成OOD样本，提升OOD检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扰动文本条件嵌入生成OOD样本，存在语义不稳定和多样性不足的问题，限制了在真实OOD场景中的泛化能力

Method: 使用现成的ID分类器，在扩散采样过程中引入双重引导：图像级引导（基于对数分割梯度减少输入似然）和特征级引导（基于k-NN距离在分类器潜在空间中促进稀疏区域采样）

Result: GOOD能够生成更可控和多样化的OOD样本，训练后显著提升OOD检测性能

Conclusion: GOOD框架通过双重引导机制有效解决了现有OOD样本生成方法的局限性，为OOD检测提供了更可靠的训练数据

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [113] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D是一个统一的框架，通过运动学感知扩散模型从单视角输入重建多样化铰接物体形状并估计姿态


<details>
  <summary>Details</summary>
Motivation: 铰接物体（如笔记本电脑和抽屉）由于其多部件几何结构和可变关节配置，在3D重建和姿态估计方面面临重大挑战

Method: 使用运动学感知VAE编码完整几何、关节角度和部件分割到结构化潜在空间，然后采用两个条件扩散模型分别回归全局姿态和关节参数，以及从部分观测生成运动学感知潜在代码，最后通过迭代优化模块双向优化重建精度和运动学参数

Result: 在合成、半合成和真实世界数据集上的实验结果表明，该方法能准确重建铰接物体并估计其运动学特性

Conclusion: 提出的KineDiff3D框架能有效解决铰接物体的3D重建和姿态估计问题

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [114] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: GACO-CAD是一个两阶段后训练框架，通过结合深度和表面法线图作为几何先验，以及引入组长度奖励机制，从单张图像生成可编辑的CAD模型，在几何精度和建模简洁性方面实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在从2D图像准确推断3D几何方面存在困难，限制了从单张图像生成可编辑参数化CAD模型的能力。

Method: 两阶段后训练框架：1) 监督微调阶段使用深度和表面法线图作为密集几何先验，与RGB图像形成多通道输入；2) 强化学习阶段引入组长度奖励，在保持几何保真度的同时促进生成更紧凑的建模序列。

Result: 在DeepCAD和Fusion360数据集上的实验表明，GACO-CAD在相同MLLM骨干网络下实现了最先进的性能，在代码有效性、几何精度和建模简洁性方面一致优于现有方法。

Conclusion: GACO-CAD通过结合几何先验和简洁建模奖励，有效提升了从单张图像生成可编辑CAD模型的准确性和效率，为工业概念设计降低了门槛。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [115] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 本文研究了人脸识别系统中预处理步骤对对抗攻击可迁移性的影响，发现人脸检测模型的选择会显著降低攻击成功率，并提出了一种预处理不变的方法来提高攻击可迁移性。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统容易受到对抗样本攻击，但现有研究往往忽略了预处理步骤在对抗攻击中的作用，特别是在黑盒设置下。本文旨在探究不同预处理技术对对抗攻击可迁移性的影响。

Method: 研究了几种最先进的现成对抗攻击方法在不同预处理技术下的可迁移性，分析了人脸检测模型和降采样插值方法的影响，并提出了一种基于输入变换的预处理不变方法来提高攻击可迁移性。

Result: 人脸检测模型的选择可使攻击成功率降低高达78%，而降采样插值方法的影响相对较小。即使在白盒设置中，人脸预处理要求也会降低攻击强度。提出的预处理不变方法将研究攻击的可迁移性提高了高达27%。

Conclusion: 预处理在人脸识别系统中具有重要作用，考虑预处理因素对于提高面部对抗样本的对抗泛化能力至关重要。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [116] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: GtR是一种无需训练的分层采样策略，通过将图像生成分解为结构生成和细节重建两个阶段，在保持生成质量的同时实现3.72倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决掩码自回归模型在并行生成视觉内容时，由于空间相关视觉令牌建模复杂性导致的加速潜力受限问题。

Method: 提出GtR方法：第一阶段缓慢计算生成全局语义框架，第二阶段快速计算完成剩余令牌。同时提出FTS方法，基于高频信息能量定位图像细节区域，为细节令牌分配更多计算资源。

Result: 在ImageNet类条件生成和文本到图像生成任务中，MAR-H模型实现3.72倍加速，同时保持可比质量（FID: 1.59, IS: 304.4 vs 原始1.59, 299.1）。

Conclusion: GtR方法显著优于现有加速方法，在不同模型规模和生成任务中表现优异，为掩码自回归模型提供了有效的加速解决方案。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [117] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文针对浮游生物识别中的分布外检测问题，基于DYB-PlanktonNet数据集构建了系统化基准，评估了22种OoD检测方法，发现ViM方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 浮游生物识别模型在实际部署中面临分布偏移挑战，由于浮游生物形态复杂、物种多样性高且不断发现新物种，导致推理时出现不可预测错误。该领域缺乏最新计算机视觉技术的系统整合和大规模评估基准。

Method: 基于DYB-PlanktonNet数据集精心设计了一系列模拟不同分布偏移场景的OoD基准，系统评估了22种OoD检测方法。

Result: 实验结果表明ViM方法在构建的基准中显著优于其他方法，特别是在Far-OoD场景中关键指标有显著提升。

Conclusion: 这项研究为浮游生物识别中的算法选择提供了可靠参考，并为浮游生物OoD检测的未来研究奠定了坚实基础，是该领域首次大规模系统性的OoD检测方法评估分析。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [118] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 提出了一种联合学习详细头部化身和手脸交互引起的非刚性变形的新框架，解决了现有方法忽略自然手脸交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部区域，忽略了传达认知状态的自然手脸交互（如手托下巴或手指轻触脸颊），这些交互对于逼真的3D头部化身至关重要。

Method: 结合深度顺序损失和接触正则化进行姿态跟踪，学习手引起的面部变形的PCA基，并引入接触损失减少穿插伪影，提高物理合理性。

Result: 在iPhone拍摄的RGB(D)视频上评估，并构建合成数据集验证。相比最先进的表面重建方法，能捕捉更好的外观和更准确的面部变形几何。

Conclusion: 该方法能够有效捕捉手脸交互引起的面部变形，生成更逼真和物理合理的3D头部化身。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [119] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC是一个双曲表示学习框架，用于解决领域泛化与广义类别发现问题，无需情景模拟训练，通过GPT引导的扩散增强和切线空间插值实现高效泛化。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法假设训练和测试数据来自同一领域，限制了在开放世界场景中的适用性。DG-GCD要求模型泛化到包含新类别的未见领域，而现有方法DG2CD-Net计算成本高且存在误差累积问题。

Method: 使用GPT引导的扩散进行领域增强，引入切线CutMix进行曲率感知插值，结合惩罚Busemann对齐、混合双曲对比正则化和自适应离群点排斥的统一损失函数，以及可学习的曲率参数。

Result: 在PACS、Office-Home和DomainNet数据集上达到最先进水平，持续优于现有的欧几里得和双曲(DG)-GCD基线方法。

Conclusion: HIDISC通过双曲表示学习框架有效解决了领域和类别级别的泛化问题，无需昂贵的情景模拟训练，在多个基准数据集上表现出色。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [120] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 提出了一种零样本的视觉令牌剪枝方法，通过平衡任务相关性和信息多样性，在保持性能的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型处理能力的增强，视觉令牌冗余导致推理成本急剧上升，现有方法忽视了文本提示的指导，无法优先考虑任务相关性。

Method: 采用分层方法：首先选择任务相关的核心视觉令牌，然后补充多样性令牌以保留更广泛的上下文信息。

Result: 在多个模型和基准测试中，即使剪枝高达90%的令牌，性能仍能匹配或超越现有最优方法，同时显著减少GPU内存占用和推理延迟。

Conclusion: 提出的提示感知视觉令牌剪枝方法在保持模型性能的同时，有效解决了视觉令牌冗余带来的高推理成本问题。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [121] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 使用Segment Anything Model (SAM)来监测孟加拉国河流侵蚀，通过微调模型识别河岸侵蚀特征，在消失的定居点数据集上取得了86.30%的IoU和92.60%的Dice分数。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国河流每年吞噬村庄和农田，造成大规模破坏和人口流离失所，但传统方法难以精确追踪这种缓慢的灾难。

Method: 首先使用简单的颜色通道分析进行粗略的土地和水域分割，然后微调SAM的掩码解码器以识别河岸侵蚀的细微特征。

Result: 模型在河岸侵蚀识别上表现出色，平均IoU达到86.30%，Dice分数达到92.60%，显著优于传统方法和现成的深度学习模型。

Conclusion: 该研究提供了首个孟加拉国因河流侵蚀消失定居点的标注数据集、专门针对此任务的AI模型以及量化土地损失的方法，为政策制定者和灾害管理机构提供了强大的监测工具。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [122] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 基于TimeSformer视频识别模型，通过分析VALORANT游戏小地图中的战术特征（角色位置和游戏事件）来预测回合结果，相比仅使用小地图信息的模型显著提升了预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测研究多基于比赛日志和统计数据，但VALORANT作为需要复杂策略的FPS游戏，需要更深入分析比赛录像中的战术信息来提升预测准确性。

Method: 使用TimeSformer视频识别模型，从小地图信息中提取详细的战术特征（角色位置和游戏事件），构建回合结果预测模型。

Result: 在增强战术事件标签的数据集上训练的模型达到了约81%的预测准确率，特别是在回合中后期阶段，显著优于仅使用小地图信息的模型。

Conclusion: 利用比赛录像中的战术特征对于预测VALORANT回合结果非常有效，证明了战术分析在电竞预测中的重要性。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [123] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: EndoCIL是一个专门为内窥镜图像诊断设计的类增量学习框架，通过三个关键组件解决领域差异和类别不平衡问题，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像分析需要模型能够持续适应新的临床数据，同时保持对已学习类别的性能。现有重放方法由于内窥镜图像中严重的领域差异和类别不平衡问题，无法有效缓解灾难性遗忘。

Method: EndoCIL包含三个核心组件：基于最大均值差异的重放(MDBR)选择多样代表性样本，先验正则化类别平衡损失(PRCBL)缓解类别不平衡，以及全连接梯度校准(CFG)减少对新类别的偏置。

Result: 在四个公共内窥镜数据集上的广泛实验表明，EndoCIL在不同缓冲区大小和评估指标下通常优于最先进的CIL方法。

Conclusion: 该框架在终身内窥镜诊断中有效平衡了稳定性和可塑性，显示出良好的临床可扩展性和部署潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [124] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出基于DINOv2的活体检测方法，利用DINOv2和registers提取通用特征并抑制注意力机制中的扰动，有效区分真实和伪造人脸图像。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统容易受到恶意攻击者使用注册用户照片进行欺骗攻击，需要在人脸识别前检测这些欺骗攻击。

Method: 使用DINOv2和registers提取通用特征，抑制注意力机制中的扰动，专注于关键细微特征。

Result: 在ICCV2025第六届人脸反欺骗研讨会提供的数据集和SiW数据集上的实验证明了该方法的有效性。

Conclusion: 提出的DINOv2-based方法能够有效检测人脸欺骗攻击，提高人脸识别系统的安全性。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [125] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: VisiPruner是一个无需训练的多模态大语言模型剪枝框架，通过分析MLLMs的三阶段跨模态交互过程，显著减少视觉相关注意力计算和FLOPs。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言任务中表现出色，但由于注意力计算的二次增长导致显著的计算开销。现有剪枝方法缺乏对MLLMs如何处理和融合多模态信息的基本理解。

Method: 通过系统分析揭示MLLMs的三阶段跨模态交互过程：浅层识别任务意图、中层跨模态融合、深层语言精炼。基于此提出VisiPruner训练免费剪枝框架。

Result: 在LLaVA-v1.5 7B上减少高达99%的视觉相关注意力计算和53.9%的FLOPs，显著优于现有token剪枝方法，并在多种MLLMs上具有良好泛化性。

Conclusion: 研究不仅提供了有效的剪枝方法，还为训练高效MLLMs提供了可操作的指导原则，通过将模型架构与其内在的层级处理动态对齐。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [126] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 该论文提出了多时刻检索(MMR)任务，构建了QV-M²数据集和FlashMMR框架，解决了现有单时刻检索方法在现实应用中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有时刻检索方法主要关注单时刻检索，但现实应用中一个查询可能对应多个相关时刻，这使得现有数据集和方法不足以支撑视频时序定位任务。

Method: 提出了FlashMMR框架，包含多时刻后验证模块来优化时刻边界，通过约束时序调整和验证模块重新评估候选片段，通过精细过滤管道修剪低置信度提议并实现鲁棒的多时刻对齐。

Result: 在QV-M²数据集上，FlashMMR相比之前的最优方法在G-mAP上提升3.00%，在mAP@3+tgt上提升2.70%，在mR@3上提升2.56%。

Conclusion: QV-M²数据集和FlashMMR方法为推进更现实和具有挑战性的视频时序定位场景研究奠定了基础。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [127] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 提出一个公平性感知的深度伪造检测框架，通过时序特征学习和人口统计感知数据增强来提高公平性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在偏见、缺乏透明度且无法捕捉时序信息，导致不同人口统计群体间的决策偏见和不可靠结果。

Method: 使用基于序列的聚类进行深度伪造视频的时序建模和概念提取，并引入人口统计感知数据增强方法，平衡代表性不足群体并应用频域变换保留深度伪造伪影。

Result: 在FaceForensics++、DFD、Celeb-DF和DFDC数据集上使用Xception、ResNet等先进架构进行广泛实验，证明该方法在公平性和准确性之间获得了最佳权衡。

Conclusion: 所提出的方法在公平性和准确性方面优于现有技术，为深度伪造检测提供了更公平和可靠的解决方案。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [128] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision是一个精心收集和整理的2400万样本视觉语言数据集，通过半自动化流程统一了200多个数据源，并进行了严格的去重和去污染处理，训练出的模型在广泛评估中表现优于现有公开数据集。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的发展受到数据集碎片化、不一致性和污染问题的阻碍，需要构建一个高质量、统一的大规模数据集来推动研究。

Method: 采用半自动化、人机协同的流程：自动化进行批量数据摄取和模式映射，人工审核映射结果并抽样检查，确保标注的忠实性、格式多样性和安全性，并进行严格的跨源去重和基准去污染。

Result: FineVision训练出的模型在广泛的评估套件中持续优于使用现有公开混合数据集训练的模型，证明了规模、数据清洁度以及人机协同平衡的优势。

Conclusion: FineVision数据集和整理工具的开源发布将加速以数据为中心的视觉语言模型研究，强调了高质量数据收集和整理的重要性。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [129] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: PnF是一种即插即用的方法，通过多模态大语言模型增强现有运动预测模型，利用自然语言描述复杂场景，无需微调即可显著提升运动预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统依赖专用模型进行感知和运动预测，在标准条件下表现可靠，但难以经济高效地泛化到多样化的现实场景。

Method: 设计提示从MLLMs中提取结构化场景理解，并将这些信息蒸馏成可学习的嵌入来增强现有行为预测模型，利用MLLMs的零样本推理能力。

Result: 在两个最先进的运动预测模型上，使用Waymo Open Motion Dataset和nuScenes Dataset验证，在两个基准测试中都显示出持续的性能改进。

Conclusion: PnF方法通过自然语言描述复杂场景，能够快速适应目标行为，显著提升运动预测性能，且无需微调，具有实际应用价值。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [130] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: 提出SG-CLDFF框架，结合显著性引导预处理与多尺度深度特征融合，用于白细胞图像的分割和分类，在多个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 白细胞显微图像分析对血液疾病诊断至关重要，但由于染色变异、复杂背景和类别不平衡等问题，准确分割和分类仍具挑战性。

Method: 使用显著性先验突出候选白细胞区域，采用轻量级混合骨干网络生成多分辨率表示，通过跨层融合模块整合浅层和深层特征，多任务训练结合类别感知加权损失和显著性对齐正则化。

Result: 在BCCD、LISC、ALL-IDB等标准基准测试中，IoU、F1分数和分类准确率均优于现有CNN和Transformer基线方法。

Conclusion: SG-CLDFF为临床工作流程提供了实用且可解释的自动化白细胞分析路径，通过显著性预处理和跨层融合显著提升了性能。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [131] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 提出基于YOLOv11目标检测算法的自动手术照明系统，通过识别蓝色标记自动调整LED光源位置，减少外科医生疲劳并提高照明一致性。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明系统依赖手动调整，导致外科医生疲劳、颈部劳损以及因漂移和阴影造成的不一致照明，需要自动化解决方案。

Method: 使用YOLOv11算法检测手术区域上方的蓝色球形标记，通过两个伺服电机控制LED光源的倾斜和平移运动，实现自动照明定位。

Result: YOLO模型在验证集上达到96.7% mAP@50的检测精度，验证集包含带有蓝色球形标记的模拟手术场景标注图像。

Conclusion: 该基于机器视觉的解决方案自动化了照明过程，减轻了外科医生的身体负担，提高了照明一致性，有助于改善手术效果。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [132] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现自监督学习中存在一个反直觉现象：训练时间过长会损害密集预测任务的性能，称为自监督密集退化(SDD)。作者提出了密集表示结构估计器(DSE)来无监督评估密集任务性能，并基于此提出了模型选择策略和正则化方法。


<details>
  <summary>Details</summary>
Motivation: 观察到自监督学习中的反直觉现象——训练时间过长反而会损害密集预测任务性能，而现有方法缺乏有效的无监督评估指标来检测这种退化。

Method: 提出密集表示结构估计器(DSE)，包含类相关性度量和有效维度度量，用于无监督评估密集任务性能。基于DSE开发了模型选择策略和正则化方法。

Result: 在16种自监督方法和4个基准测试上的实验表明，模型选择策略平均提高mIoU 3.0%，且DSE正则化能持续缓解密集退化效应。

Conclusion: 自监督密集退化是普遍存在的现象，提出的DSE指标能有效评估密集任务性能，相应的模型选择和正则化方法能显著改善性能。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [133] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba是一个可扩展的fMRI因果推断框架，通过两阶段方法解决BOLD信号失真和计算复杂性问题，在模拟数据上比DCM准确率高37%，在真实数据中能恢复88%的已知神经通路。


<details>
  <summary>Details</summary>
Motivation: 解决fMRI因果推断中的两个基本限制：从血流动力学失真的BOLD信号推断神经因果关系的病态性质，以及现有方法（如动态因果建模DCM）的计算不可行性。

Method: 将复杂的逆问题分解为两个可处理的阶段：BOLD反卷积恢复潜在神经活动，然后使用新颖的条件Mamba架构进行因果图推断。

Result: 在模拟数据上比DCM准确率高37%；在真实任务fMRI数据中，恢复88%的已知神经通路，而传统方法在99%以上的受试者中无法识别这些典型回路；工作记忆数据的网络分析显示大脑会根据刺激策略性地转移其主要因果枢纽。

Conclusion: 为神经科学家提供了一个实用的工具，用于大规模因果推断，能够捕捉认知功能背后的基本回路模式和灵活网络动态。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [134] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文评估了现有对抗防御方法在面对大尺寸对抗衣物攻击时的表现，发现这些防御方法在数字和物理世界中都表现不佳，揭示了现有防御方法的共同漏洞。


<details>
  <summary>Details</summary>
Motivation: 实验发现简单增大对抗补丁尺寸就能使现有防御方法失效，因此评估各种防御方法对抗覆盖人体大面积区域的对抗衣物的效果。

Method: 使用对抗衣物作为测试案例，在数字和物理世界中评估多种防御方法的表现，并制作了能够同时突破多个防御方法的单一对抗衣物。

Result: 所有防御方法在对抗衣物攻击下表现都很差，单一对抗衣物在物理世界中实现了96.06%的攻击成功率对抗未防御检测器，并对九个防御模型保持超过64.84%的攻击成功率。

Conclusion: 现有对抗防御方法在面对大尺寸、自然的对抗衣物攻击时存在共同漏洞，防御效果不佳。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [135] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: 提出CharDiff框架，通过字符级引导的扩散模型有效恢复和识别严重退化的车牌图像，在恢复质量和识别准确率上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复不仅对车牌识别系统预处理很重要，还能提高证据价值、增强视觉界面清晰度，促进车牌图像的进一步利用。

Method: 使用基于扩散的框架，结合字符级先验知识和新颖的CHARM模块（字符引导注意力通过区域掩码），确保每个字符的引导仅限于自身区域。

Result: 在Roboflow-LP数据集上，相比最佳基线模型，实现了28%的相对CER降低，在恢复质量和识别准确率上显著优于基线模型。

Conclusion: 结构化字符引导条件化有效增强了基于扩散的车牌恢复和识别在实际部署场景中的鲁棒性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [136] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: 提出了iDETEX，一个统一的多模态大语言模型，能够同时执行质量定位、感知和描述三个关键任务，在ViDA-UGC基准测试中取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像质量评估从标量质量预测向更可解释、与人类对齐的评估范式发展的挑战，实现详细和可解释的图像质量评估。

Method: 设计任务特定的离线增强模块和数据混合策略，辅以在线增强策略充分利用多源监督，构建统一的多模态大语言模型。

Result: 在ViDA-UGC基准测试中取得最先进性能，在ICCV MIPI 2025详细图像质量评估挑战中排名第一。

Conclusion: iDETEX模型在提供准确和可解释的质量评估方面表现出有效性和鲁棒性。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [137] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出了一种基于后处理的开放集识别方法，通过比较模型特征与预测logits之间的一致性来识别未知类，无需重新训练预训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前野生动物分类模型在封闭世界设定下训练，当遇到未知类别时会过度自信。现有OSR方法大多需要重新训练模型，这限制了实际应用。

Method: 使用基于输入到最近类均值的距离构建概率分布，然后将该分布与softmax概率进行比较，衡量NCM和分类头之间的一致性。

Result: 在两个数据集上均排名前三，AUROC分别达到93.41和95.35，性能稳定优于现有方法。

Conclusion: 提出的后处理OSR方法无需重新训练模型，在两个数据集上表现一致且优于现有方法，具有实际应用价值。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [138] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 提出了Semantic-E2VID框架，通过跨模态特征对齐和语义感知特征融合，将SAM模型的视觉语义知识迁移到事件相机视频重建中，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 事件相机仅捕捉强度变化，忽略静态物体和背景，导致捕获的事件模态缺乏语义信息，而现有事件到视频重建方法往往忽视语义信息的重要性。

Method: 引入跨模态特征对齐模块将SAM的视觉语义知识迁移到事件编码器，提出语义感知特征融合块整合学习到的语义特征，并设计语义感知E2V监督利用SAM生成的类别标签。

Result: 在多个基准测试中显著提升了帧质量，优于最先进的E2V方法。

Conclusion: Semantic-E2VID通过有效利用视觉语义知识，成功解决了事件到视频重建中的语义信息缺失问题，为事件相机应用提供了更高质量的视频重建方案。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [139] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA是一个基础模型，通过混合动作将GUI原语操作与高级程序化工具调用无缝集成，解决了计算机使用代理仅依赖原始操作导致的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理仅依赖点击、输入、滚动等原始操作，需要精确的视觉定位和冗长的执行链，导致级联故障和性能瓶颈。而其他代理可以利用丰富的程序化接口，计算机使用代理却与这些能力隔离。

Method: 方法包括四个关键组件：(1)从软件文档、开源仓库和代码生成中扩展程序化工具的自动化流水线；(2)生成超过17,000个可验证任务的合成数据引擎；(3)大规模高质量混合动作轨迹收集；(4)结合监督微调和在线强化学习的两阶段训练管道。

Result: 实验显示，7B和32B模型相比最先进代理有显著改进。在OSWorld上，UltraCUA模型相比基础模型平均相对改进22%，步骤数减少11%。在WindowsAgentArena的域外评估中，模型达到21.7%的成功率，优于在Windows数据上训练的基线。

Conclusion: 混合动作机制被证明至关重要，能够减少错误传播同时保持执行效率，成功弥合了计算机使用代理与程序化工具能力之间的差距。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [140] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: M2H是一个用于单目图像多任务学习的轻量级框架，支持语义分割、深度估计、边缘检测和表面法线估计，通过窗口跨任务注意力模块实现高效特征交换，在保持计算效率的同时超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署实时空间感知需要高效的多任务模型，能够利用互补任务信息同时最小化计算开销。传统方法使用独立单任务模型或共享编码器-解码器架构存在局限性。

Method: 提出Multi-Mono-Hydra (M2H)框架，基于轻量级ViT DINOv2骨干网络，引入窗口跨任务注意力模块，实现结构化特征交换同时保留任务特定细节。

Result: 在NYUDv2数据集上超越最先进的多任务模型，在Hypersim上超过单任务深度和语义基线，在Cityscapes上表现优异，同时在笔记本电脑硬件上保持计算效率。

Conclusion: M2H为单目空间感知系统提供了实用基础，支持动态环境中的3D场景图构建，在真实世界数据上验证了其实用性。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [141] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph框架通过将长文本渲染为图像，利用视觉语言模型处理，实现3-4倍的token压缩，在保持准确性的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理百万token级别长上下文时面临的计算和内存成本过高的问题。

Method: 将长文本渲染成图像，使用视觉语言模型处理，并设计基于LLM的遗传搜索来优化视觉渲染配置。

Result: 实现3-4倍token压缩，准确性与Qwen3-8B相当，预填充和解码速度提升约4倍，SFT训练速度提升约2倍，128K上下文VLM可处理1M token任务。

Conclusion: 视觉上下文缩放是解决长上下文建模挑战的有效方法，在保持性能的同时显著提升效率，并有益于文档理解等多模态任务。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [142] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，使视频大语言模型能够在流式视频场景中高效处理长视频，通过视觉令牌选择、循环处理和基于描述的问答实现实时响应。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型需要完整访问视频才能回答问题，但在流式处理场景中无法及时响应关于小时级长视频的查询。

Method: 1) 基于LLM注意力的视觉令牌选择，丢弃约95%不重要的令牌；2) 对过去选定令牌进行循环处理，生成时间连贯的理解；3) 基于描述的轻量级问答。

Result: 在流式视频基准测试中达到最先进性能，在效率和效果之间取得良好平衡。

Conclusion: 该方法为视频大语言模型在流式处理场景中的实际应用提供了有效的解决方案。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [143] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 该研究通过系统评估25个合成人脸识别数据集（2018-2025），证明合成人脸数据可替代真实数据集，最佳合成数据集识别准确率达95.67%，超越真实数据集CASIA-WebFace（94.70%），同时提供隐私保护和偏见控制优势。


<details>
  <summary>Details</summary>
Motivation: 解决人脸识别系统面临的伦理困境：高精度需要大量未经同意收集的真实人脸数据，导致数据集撤回和法律风险。合成人脸数据作为隐私保护替代方案缺乏实证证据。

Method: 系统文献回顾识别25个合成人脸识别数据集，结合实验验证，评估7个隐私保护合成数据关键要求：身份泄露预防、类内变异性、身份可分离性、数据集规模、伦理数据来源、偏见缓解和基准可靠性。

Result: 最佳合成数据集VariFace和VIGFace识别准确率分别为95.67%和94.91%，超越真实数据集CASIA-WebFace（94.70%）。合成数据确保适当的类内变异性同时保持身份可分离性，并提供前所未有的偏见控制能力。

Conclusion: 合成人脸数据是科学可行且伦理必要的面部识别研究替代方案，能够替代真实数据集同时解决隐私和偏见问题。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [144] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出了一种基于多面部表情特征的帕金森病严重程度诊断方法，通过注意力机制融合特征并采用自适应类别平衡策略解决类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于面部表情的PD诊断方法依赖单一表情易导致误诊，且忽略不同PD阶段的类别不平衡问题，大多数方法仅进行二元分类而非严重程度诊断。

Method: 整合多种面部表情特征通过注意力机制进行特征融合，采用自适应类别平衡策略根据类别分布和分类难度动态调整训练样本贡献。

Result: 实验结果表明该方法在PD严重程度诊断方面表现优异，注意力特征融合和自适应类别平衡策略有效。

Conclusion: 该方法为PD严重程度诊断提供了有效解决方案，通过多表情特征融合和类别平衡策略显著提升了诊断性能。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [145] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: LoopTrans是一个闭环框架，通过双向知识转移（从外中心到自我中心，再返回）来增强交互能力学习，解决了传统单向转移在复杂交互场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 人类通过观察他人与物体的交互来学习新物体的交互能力，但现有方法仅从外中心图像单向转移到自我中心图像，限制了在复杂交互场景中的适用性。

Method: 提出LoopTrans闭环框架，包含统一跨模态定位和去噪知识蒸馏机制，实现外中心与自我中心图像之间的双向知识转移。

Result: 实验表明LoopTrans在图像和视频基准测试中所有指标均获得一致提升，甚至能处理人体完全遮挡物体交互区域的挑战性场景。

Conclusion: LoopTrans通过闭环双向知识转移有效解决了交互能力学习中的领域差距问题，显著提升了性能表现。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [146] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 开发了一个基于视觉的自动化系统，用于监测马厩中马匹和人的行为，使用YOLOv11和BoT-SORT进行目标检测和跟踪，能够识别五种事件类型并处理相机盲区问题。


<details>
  <summary>Details</summary>
Motivation: 传统马匹行为监测方法劳动密集且耗时，需要自动化解决方案来早期发现健康福利问题。

Method: 结合YOLOv11目标检测和BoT-SORT多目标跟踪技术，利用CLIP和GroundingDINO构建自定义数据集，通过目标轨迹和空间关系推断事件状态。

Result: 定性评估显示系统在马匹相关事件检测上表现可靠，但人员检测因数据稀缺存在局限性。

Conclusion: 该系统为马场实时行为监测奠定了基础，对动物福利和厩舍管理具有重要意义。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [147] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect是一个智能、一体化的密集关键点检测器，通过融合传统检测器的优势并使用深度学习，在关键点密度、重复性和匹配数量方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统检测器和基于学习的方法存在对光度变化敏感、关键点密度和重复性低、对挑战性场景适应性有限、缺乏语义理解等问题，无法优先考虑视觉重要区域。

Method: 首先融合7种关键点检测器和2种边缘检测器的输出创建真实掩码，提取多样视觉特征；然后使用轻量高效的ESPNet模型基于这些掩码进行训练，使DeepDetect能够语义化地关注图像并产生密集关键点。

Result: 在Oxford Affine Covariant Regions数据集上的评估显示，DeepDetect在关键点密度、重复性和正确匹配数量方面均优于其他检测器，分别达到0.5143、0.9582和59,003的最大值。

Conclusion: DeepDetect通过融合传统检测器的多样视觉线索和深度学习的语义理解能力，实现了在多样化视觉退化条件下的高性能关键点检测。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [148] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 利用AV1运动向量生成密集亚像素对应关系和经过余弦一致性过滤的短轨迹，在压缩域前端实现与SIFT相当的性能但CPU使用更少，并产生更密集的匹配点。


<details>
  <summary>Details</summary>
Motivation: 探索压缩域对应关系作为资源高效的前端解决方案，为完整流水线提供可扩展的路径，减少计算资源消耗。

Method: 重新利用AV1运动向量来生成密集亚像素对应关系和短轨迹，通过余弦一致性进行过滤，在压缩域中处理视频数据。

Result: 在117帧视频片段上，运动向量匹配成功注册所有图像并重建46-62万个点，重投影误差为0.51-0.53像素；捆绑调整时间随匹配密度增加而增长。

Conclusion: 压缩域对应关系是一个实用且资源高效的前端解决方案，在完整流水线中具有明确的扩展路径。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [149] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 本文提出了一个高质量夜间图像去雨基准数据集HQ-NightRain，并开发了色彩空间转换网络CST-Net，通过可学习的色彩空间转换器和隐式光照引导来有效去除夜间复杂雨纹。


<details>
  <summary>Details</summary>
Motivation: 夜间图像去雨比白天更具挑战性，主要由于夜间场景的固有复杂性以及缺乏准确表征雨和光照耦合效应的高质量数据集。

Method: 提出CST-Net网络，包含可学习色彩空间转换器(CSC)用于在Y通道更好地去除雨纹，并引入隐式光照引导来提升模型在复杂场景中的鲁棒性。

Result: 大量实验验证了所提数据集的价值和方法的有效性，在夜间图像去雨任务上表现出色。

Conclusion: HQ-NightRain数据集和CST-Net方法为夜间图像去雨提供了新的高质量基准和有效解决方案。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [150] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本文提出了一种改进稀疏视图3D高斯泼溅初始化的方法，通过频率感知SfM、3DGS自初始化和点云正则化来提升稀疏视图下的3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图下的3D高斯泼溅容易过拟合训练视图，导致新视角渲染出现模糊等伪影。现有方法要么改进初始化，要么添加训练时约束，但本文发现初始化是决定性因素。

Method: 设计了三个组件：频率感知SfM通过低频视图增强和宽松多视图对应改善低纹理区域覆盖；3DGS自初始化利用光度监督生成额外点；点云正则化通过几何/可见性先验确保多视图一致性和均匀空间覆盖。

Result: 在LLFF和Mip-NeRF360数据集上的实验表明，该方法在稀疏视图设置下取得了持续的性能提升，建立了更强的初始化策略。

Conclusion: 初始化在稀疏视图3D高斯泼溅中具有决定性作用，本文提出的方法通过改进SfM点云质量，显著提升了稀疏视图下的3D重建性能。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [151] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld是一个新颖的4D占用世界模型，使用稀疏动态查询实现灵活、自适应和高效的场景感知与预测。


<details>
  <summary>Details</summary>
Motivation: 现有占用世界模型依赖静态固定嵌入或网格，限制了感知灵活性，且基于网格的"原地分类"与真实场景的动态连续性存在潜在错位。

Method: 提出Range-Adaptive Perception模块，通过可学习查询结合ego车辆状态进行调制，并设计State-Conditioned Forecasting模块用回归引导预测替代分类预测。

Result: 在感知、预测和规划任务上达到最先进性能，验证了模型在灵活性、适应性和效率方面的优势。

Conclusion: SparseWorld通过稀疏动态查询成功解决了现有占用世界模型的局限性，为4D环境建模提供了更有效的解决方案。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [152] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: POTNet通过熵引导的双聚类头和最优传输，在无监督显著目标检测中生成高质量伪掩码，AutoSOD端到端管道在多个基准测试中显著优于现有无监督和弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 显著目标检测(SOD)作为计算机视觉基础任务，作者认为无需像素级标签即可达到接近监督方法的精度，但前提是获得可靠的伪掩码。现有原型方法存在边界和内部像素几何特性差异以及最优传输利用不足的问题。

Method: 提出POTNet，改进原型最优传输方法：使用熵引导的双聚类头，高熵像素通过谱聚类组织，低熵像素通过k-means聚类，然后通过最优传输对齐两个原型集。这种分割-融合-传输设计在单次前向传播中生成更清晰、部分感知的伪掩码。

Result: 在五个基准测试上的广泛实验表明，AutoSOD在F-measure指标上比无监督方法提升高达26%，比弱监督方法提升高达36%，进一步缩小了与全监督模型的差距。

Conclusion: POTNet的分割-融合-传输设计能够生成高质量的伪掩码，AutoSOD端到端无监督SOD管道在消除SelfMask离线投票的同时提高了准确性和训练效率，证明了无监督SOD可以达到接近监督方法的性能。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [153] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种基于评分准则和伪标签的零样本视频摘要方法，通过将少量真实标注转化为伪标签来指导LLM进行场景评估，在SumMe和TVSum数据集上取得了优于无监督和现有零样本方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法标注成本高且跨数据集泛化能力有限，无监督方法难以捕捉高层次语义，而零样本方法对提示模板敏感且需要数据集特定的分数归一化。

Method: 使用评分准则引导的伪标签提示框架，将少量真实标注转化为高置信度伪标签，构建结构化评分准则。推理时对首尾片段基于描述评分，中间片段结合相邻场景上下文来评估叙事进展和冗余。

Result: 在SumMe和TVSum数据集上分别达到57.58和63.05的F1分数，超越了无监督和现有零样本基线方法，接近监督方法性能。

Conclusion: 评分准则引导的伪标签能有效稳定基于LLM的评分，为视频摘要建立了一个通用且可解释的零样本范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [154] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 提出了一个优化视频生成模型训练的四支柱框架，包括数据处理、模型架构、训练策略和基础设施，显著提升了训练效率和性能，并开源了完整的训练代码和模型。


<details>
  <summary>Details</summary>
Motivation: 大规模视频生成模型训练面临跨模态文本-视频对齐、长序列处理和复杂时空依赖等挑战，需要高效的训练解决方案。

Method: 通过优化四个关键支柱：数据处理、模型架构、训练策略和基础设施，包括数据预处理、视频压缩、参数缩放、课程式预训练和对齐后训练。

Result: 开发的MUG-V 10B模型在整体性能上匹配当前最先进的视频生成器，在电商导向的视频生成任务中超越了领先的开源基线模型。

Conclusion: 成功构建了高效的大规模视频生成训练框架，并开源了完整的训练代码和模型权重，为视频生成领域提供了重要的技术贡献。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [155] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出MambaX-Net，一种用于前列腺癌主动监测的半监督双扫描3D分割架构，利用时间序列MRI数据改进前列腺区域分割。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割模型主要基于单时间点专家标注数据训练，不适合纵向主动监测分析，因为多时间点和专家标注稀缺限制了有效微调。

Method: 提出MambaX-Net架构，包含Mamba增强的交叉注意力模块和形状提取器模块，采用半监督自训练策略利用伪标签进行学习。

Result: 在纵向主动监测数据集上评估，MambaX-Net显著优于最先进的U-Net和Transformer模型，在有限和噪声数据下实现优越的前列腺区域分割。

Conclusion: MambaX-Net能够有效处理纵向主动监测场景中的多时间点分割问题，在专家标注稀缺的情况下仍能获得高质量分割结果。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [156] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet是一种弱监督的道路裂缝检测方法，仅使用图像级标签实现像素级检测，通过分类器、重建器和检测器的对抗学习以及路径感知注意力模块提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 为了减少对昂贵像素级标注的依赖，在智能城市基础设施维护中开发仅需图像级标签的道路裂缝检测方法。

Method: 提出WP-CrackNet框架，包含三个组件：生成类激活图的分类器、测量特征可推断性的重建器、生成像素级检测结果的检测器。通过对抗学习使裂缝CAM覆盖完整区域，检测器从后处理CAM生成的伪标签中学习。设计了路径感知注意力模块和中心增强CAM一致性模块来提升性能。

Result: 构建了三个图像级数据集，实验表明WP-CrackNet达到与监督方法相当的结果，优于现有弱监督方法，显著推进了可扩展道路检测。

Conclusion: WP-CrackNet通过弱监督学习有效解决了道路裂缝检测问题，在减少标注成本的同时保持了高检测精度，为智能基础设施维护提供了可行方案。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [157] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D是一个前馈模型，将VGGT扩展到动态场景，能够同时进行相机姿态估计、深度预测和点云重建，无需后处理。


<details>
  <summary>Details</summary>
Motivation: 现有的3D前馈模型（如VGGT）在静态数据集上训练，难以处理现实世界中包含复杂动态元素（如移动人物或可变形物体）的场景。

Method: 提出动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息：抑制运动线索用于姿态估计，增强运动线索用于几何重建。

Result: 在动态场景中，PAGE-4D持续优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点云重建方面取得更优结果。

Conclusion: PAGE-4D成功解决了多任务4D重建中的任务冲突问题，在动态场景下实现了更好的性能。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [158] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 提出了首个水下伪装实例分割数据集UCIS4K和基于Segment Anything Model的UCIS-SAM网络，通过三个关键模块解决水下环境中伪装目标分割的挑战。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在颜色失真、低对比度和模糊等退化问题，传统基于陆地数据集训练的伪装实例分割方法在水下场景中表现不佳，需要专门的水下数据集和算法。

Method: 提出UCIS-SAM网络，包含三个模块：通道平衡优化模块(CBOM)增强水下特征学习，频域真值整合模块(FDTIM)强调目标内在特征，多尺度特征频率聚合模块(MFFAM)增强低对比度伪装实例边界。

Result: 在提出的UCIS4K数据集和公共基准测试上的广泛实验表明，UCIS-SAM优于最先进的方法。

Conclusion: 该方法有效解决了水下伪装实例分割的挑战，为水下探索和海洋保护提供了更好的视觉分析工具。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [159] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft是一个基于多智能体框架的文本到3D生成系统，使用图结构程序化形状表示来生成结构化、可交互的3D资产


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到3D生成方法产生非结构化网格和交互性差的问题，使其更适合艺术工作流程

Method: 提出基于图的程序化形状表示，将复杂自然语言分解为子任务图，使用LLM智能体分层解析用户输入并迭代优化程序化建模和绘制

Result: 在定性和定量实验中表现出优越性能，能生成几何精确且语义丰富的3D资产，支持动画和用户自定义编辑

Conclusion: ShapeCraft展示了在更广泛交互应用中的潜力，为文本到3D生成提供了结构化且实用的解决方案

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [160] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 提出基于机器学习的3D点云自动分割框架，结合无人机扫描的真实点云和BIM生成的合成数据，用于基础设施结构健康监测中的组件分割。


<details>
  <summary>Details</summary>
Motivation: 解决传统手动标注方法在无人机3D模型分割中耗时且易出错的问题，提高基础设施监测的效率和精度。

Method: 使用无人机扫描的真实点云与BIM生成的合成数据相结合的方法，利用机器学习框架进行3D点云的自动分割。

Result: 在铁路轨道数据集上验证显示，能够高精度识别和分割主要组件（如铁轨和枕木），使用小规模数据集结合BIM数据显著减少训练时间并保持合理分割精度。

Conclusion: 该自动化方法提高了3D基础设施模型分割的精度和效率，推动了无人机与BIM技术在结构健康监测和基础设施管理中的集成应用。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [161] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2是一个统一的无监督异常检测框架，通过五个简单元素的协调在标准重建框架中实现卓越性能，在多类、多模态和少样本任务中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多类异常检测模型性能显著落后于一对一模型，且领域碎片化为专门方法，需要统一解决方案来弥合性能差距并支持多样化数据模态和任务设置。

Method: 基于"少即是多"理念，在标准重建框架中协调五个简单元素，实现方法极简主义，无需修改即可自然扩展到多样化任务。

Result: 在12个基准测试中，多类模型在MVTec-AD和VisA上分别达到99.9%和99.3%的图像级AUROC；仅用每类8个正常样本就超越之前全样本模型，在MVTec-AD和VisA上分别达到98.7%和97.4% I-AUROC。

Conclusion: Dinomaly2通过极简设计、计算可扩展性和通用适用性，成为现实世界异常检测应用的统一解决方案。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [162] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: CaMiT是一个细粒度数据集，捕捉汽车模型随时间演变的过程，支持监督和自监督学习。研究发现静态预训练在跨年测试时准确率下降，提出了时间增量分类设置和两种策略来提升时间鲁棒性，并探索了时间感知图像生成。


<details>
  <summary>Details</summary>
Motivation: AI系统需要适应不断变化的视觉环境，特别是在物体外观随时间变化的领域。汽车模型作为技术制品的代表性类别，其演变过程需要被系统性地研究。

Method: 引入CaMiT数据集，包含787K带标签样本和5.1M无标签样本。提出时间增量分类设置，评估两种策略：时间增量预训练（更新主干网络）和时间增量分类器学习（仅更新最后一层）。同时探索时间感知图像生成方法。

Result: 静态预训练在领域内数据上能达到与大规模通用模型竞争的性能且更高效，但在跨年测试时准确率下降。时间增量策略显著提升了时间鲁棒性。时间感知图像生成能产生更真实的输出。

Conclusion: CaMiT为研究细粒度视觉识别和生成中的时间适应问题提供了丰富的基准，时间增量学习策略能有效应对现实世界中的持续学习挑战。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [163] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: 提出DINO-CV分割框架，使用高分辨率LiDAR DEM自动映射低矮干石墙，通过自监督跨视图预训练解决数据稀缺问题，在植被覆盖和标注稀缺的遗产环境中实现高效映射。


<details>
  <summary>Details</summary>
Motivation: 干石墙具有重要的遗产和环境价值，但许多墙体因难以接近和手动映射成本高而未被识别。深度学习分割提供可扩展解决方案，但面临植被遮挡和标注数据有限的挑战。

Method: 使用LiDAR DEM克服植被遮挡，提出基于知识蒸馏的自监督跨视图预训练策略，学习多个DEM衍生物的视觉和几何表示，支持多种视觉骨干网络。

Result: 在Budj Bim UNESCO世界遗产地应用中，识别出澳大利亚最密集的殖民地干石墙集合，测试区域mIoU达68.6%，仅用10%标注数据微调后仍保持63.8% mIoU。

Conclusion: 自监督学习在高分辨率DEM衍生物上的应用，为植被覆盖和标注稀缺的遗产环境中自动干石墙映射提供了有效解决方案。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [164] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 比较两种节俭联邦学习方法进行暴力检测：零样本和联邦微调视觉语言模型(VLMs)与个性化训练紧凑3D CNN。两种方法准确率均超90%，CNN3D在ROC AUC和log loss上略优且能耗更低，VLMs在上下文推理和多模态推理方面更优。


<details>
  <summary>Details</summary>
Motivation: 研究节俭联邦学习方法进行暴力检测，重点关注能源效率和环境指标，为视频监控提供负责任、资源感知的AI基准。

Method: 使用LLaVA-7B和65.8M参数CNN3D作为代表案例，在非IID设置下评估准确性、校准和能耗。比较零样本和联邦微调VLMs与个性化训练CNN3D两种策略。

Result: 两种方法准确率均超过90%。CNN3D在ROC AUC和log loss上略优于LoRA调优的VLMs，且能耗更低。VLMs在上下文推理和多模态推理方面保持优势。

Conclusion: 支持混合模型：轻量级CNN用于常规分类，选择性激活VLM用于复杂或描述性场景。该框架为视频监控提供了可复现的负责任、资源感知AI基准。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [165] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 提出了4DSegStreamer框架，采用双线程系统处理流式4D全景分割，可集成到现有3D/4D分割方法中实现实时能力，在高FPS条件下表现优于现有流式感知方法。


<details>
  <summary>Details</summary>
Motivation: 在高度动态环境中（如密集人群疏散、复杂场景自动驾驶）需要实时、细粒度的感知能力，现有方法在受限时间预算下难以满足需求。

Method: 采用双线程系统：预测线程利用历史运动和几何信息提取特征并预测未来动态；推理线程通过对齐最新记忆并补偿自运动和动态物体移动来确保对传入帧的及时预测。

Result: 在室内HOI4D数据集和室外SemanticKITTI、nuScenes数据集上的综合实验证明了方法的有效性，特别是在复杂场景中准确预测动态物体方面表现优异。

Conclusion: 4DSegStreamer是一个通用框架，能够无缝集成到现有3D和4D分割方法中，提供实时能力，并在高FPS条件下展现出卓越的鲁棒性。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [166] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: PICABench是一个评估图像编辑物理真实性的基准，涵盖8个物理维度，发现当前模型在物理一致性方面仍有很大改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型主要关注指令完成度，但忽视了伴随的物理效果（如阴影、反射、物体交互），这些对生成真实性至关重要。

Method: 提出PICABench基准系统评估物理真实性，包含8个子维度；开发PICAEval评估协议，使用VLM作为评判器；构建PICA-100K训练数据集从视频学习物理知识。

Result: 评估主流模型后发现，物理真实性仍然是一个具有挑战性的问题，存在很大探索空间。

Conclusion: 该基准和解决方案为从简单内容编辑向物理一致真实性发展的未来工作奠定了基础。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [167] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 提出IC-MoE模型，通过混合专家机制和语义引导对比学习增强医学图像分割基础模型的高层特征表示能力，同时保持预训练权重的结构完整性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割基础模型微调方法存在两个问题：1）高层特征表示不足；2）微调过程破坏预训练权重的结构完整性。

Method: 1）构建基础专家、语义专家和自适应专家，采用像素概率自适应投票策略进行专家选择和融合；2）提出语义引导对比学习方法解决对比学习中弱监督问题。

Result: 在三个公共医学图像分割数据集上的实验表明，IC-MoE优于其他SOTA模型，有效补充了基础医学图像分割模型的高层特征和预训练结构完整性。

Conclusion: IC-MoE在保持预训练权重结构完整性的同时增强了高层特征表示能力，在多样化医学图像分割场景中展现出优越的泛化能力。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [168] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 提出了多语言文本到图像人物检索任务，开发了Bi-IRRA框架，通过双向隐式关系推理和多维全局对齐来解决模态异质性问题，在多语言TIPR数据集上达到最先进效果。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像人物检索中的模态异质性问题，现有方法要么忽略细粒度差异，要么需要先验信息进行局部对齐，且多为英语中心，限制了在多语言环境的应用。

Method: 提出Bi-IRRA框架，包含双向隐式关系推理模块（通过掩码图像和文本的双向预测增强跨语言和跨模态的局部关系建模）和多维全局对齐模块（桥接模态异质性）。

Result: 在所有多语言TIPR数据集上取得了新的最先进结果。

Conclusion: Bi-IRRA框架通过结合隐式关系推理和全局对齐，有效解决了多语言文本到图像人物检索中的模态异质性问题，为多语言环境下的跨模态检索提供了有效解决方案。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [169] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: OP3Det是一个无需文本提示的开放世界3D检测器，通过结合2D语义先验和3D几何先验，利用跨模态专家混合来检测3D场景中的任何对象，包括训练时未见的新类别。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集3D检测器难以泛化到开放世界场景，而直接使用3D开放词汇模型又面临词汇扩展和语义重叠问题，因此需要学习通用的3D对象性来检测所有对象。

Method: 引入2D基础模型的强泛化和零样本能力，结合2D语义先验和3D几何先验生成类别无关提议，通过跨模态专家混合动态路由单模态和多模态特征来学习通用3D对象性。

Result: 在广泛实验中，OP3Det显著超越现有开放世界3D检测器达16.0%的AR提升，相比封闭世界3D检测器也有13.5%的改进。

Conclusion: OP3Det通过结合2D和3D先验以及跨模态特征学习，实现了卓越的开放世界3D对象检测性能，为通用3D对象发现提供了有效解决方案。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [170] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出了广义对抗求解器（GAS），一种结合对抗训练的扩散模型蒸馏方法，在保持简单参数化的同时提升细节保真度


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量优秀，但采样计算成本高昂。现有蒸馏方法依赖复杂训练技巧且未显式关注细节保留

Method: 提出广义求解器参数化ODE采样器，无需额外训练技巧；结合原始蒸馏损失与对抗训练以减少伪影并增强细节保真度

Result: 在相似资源约束下，GAS相比现有求解器训练方法表现出更优越的性能

Conclusion: 广义对抗求解器通过简单参数化和对抗训练的结合，有效提升了扩散模型蒸馏的质量和细节保真度

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [171] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT是一种后预训练结构化剪枝方法，能够在不同计算预算下实现弹性推理，无需重训练或标签数据，5分钟内生成可调节模型。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型只有有限的预定义尺寸，无法适应实际部署中的计算约束，需要一种灵活的剪枝方法。

Method: 结合梯度信息和跨网络结构相关性，通过进化算法近似Hessian非对角结构，使用自监督重要性评分机制进行剪枝。

Result: 在DINO、SigLIPv2、DeIT和AugReg模型上优于现有方法，各种稀疏度下表现优异，单A100 GPU不到5分钟生成弹性模型。

Conclusion: SnapViT为预训练视觉Transformer提供了高效剪枝策略，实现了无需重训练或标签的弹性推理能力。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [172] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 提出一种两阶段方法检测帕金森病，通过图像分块和集成学习提高对未见患者数据的鲁棒性，在NewHandPD数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病早期检测方法存在两个主要局限：数据集不足和对未见患者数据的鲁棒性差。

Method: 两阶段方法：第一阶段按绘图类型分类，第二阶段将图像分为2x2块分别提取特征，使用集成方法合并各块决策。

Result: 在NewHandPD数据集上，对已见患者准确率97.08%，对未见患者94.91%，性能差距仅2.17个百分点，优于现有方法。

Conclusion: 提出的分块策略和集成方法有效解决了数据不足和泛化能力问题，显著提升了对未见患者帕金森病的检测性能。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [173] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 开发了一个自动分析循环血细胞簇图像的计算框架，通过YOLOv11模型分类细胞簇，并结合多通道荧光染色识别细胞类型，准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 循环血细胞簇是血栓、感染和炎症的重要生物标志物，但目前缺乏自动分析包含细胞簇图像的工具，而单细胞分析方法不适用于形状不规则、大小不一且包含异质细胞类型的细胞簇。

Method: 采用两步分析策略：1) 使用YOLOv11模型对图像进行细胞簇与非细胞簇分类；2) 通过叠加细胞簇轮廓与多通道荧光染色区域来识别细胞类型，克服细胞碎片和染色伪影的影响。

Result: 该框架在细胞簇分类和表型识别方面均达到超过95%的准确率，优于传统的卷积神经网络和视觉变换器模型。

Conclusion: 该自动化框架有效分析流式细胞术中的循环血细胞簇图像，利用明场和荧光数据，不仅适用于血细胞分析，还有潜力扩展到免疫细胞和肿瘤细胞簇的研究，支持多种疾病的细胞研究。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [174] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: RaindropGS是一个专门评估3D高斯泼溅在雨滴条件下性能的基准测试，针对真实场景中雨滴干扰相机位姿估计和点云初始化的问题，提供了包含雨滴聚焦、背景聚焦和无雨真实图像的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS基准测试通常使用合成雨滴图像和已知相机位姿，但真实场景中雨滴会干扰相机位姿估计和点云初始化，且合成与真实雨滴存在显著域差距，影响泛化能力。

Method: 构建包含三个对齐图像集（雨滴聚焦、背景聚焦、无雨真实图像）的真实世界雨滴重建数据集，评估完整3DGS流程，包括雨滴干扰类型、相机位姿估计、点云初始化、单图像去雨和3D高斯训练比较。

Result: 通过实验揭示了现有3DGS方法在无约束雨滴图像上的性能限制，以及不同流程组件的影响：相机焦点位置对重建性能的影响，不准确的位姿和点云初始化对重建的干扰。

Conclusion: 这些发现为开发在雨滴条件下更鲁棒的3DGS方法提供了明确方向，强调了处理真实世界雨滴干扰的重要性。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [175] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: MT-Video-Bench是一个用于评估多模态大语言模型在多轮视频对话中理解能力的基准测试，包含987个精心策划的多轮对话，涵盖六个核心能力维度。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准仅限于单轮问答，忽略了现实场景中多轮对话的复杂性，需要开发更全面的评估工具。

Method: 构建包含987个多轮对话的MT-Video-Bench基准，评估六个核心能力（感知性和交互性），涵盖多样化领域如交互式体育分析和视频智能辅导。

Result: 对各种开源和闭源MLLMs的广泛评估显示，它们在处理多轮视频对话时存在显著的性能差异和局限性。

Conclusion: MT-Video-Bench基准将公开提供，以促进未来研究，揭示当前MLLMs在多轮视频对话理解方面的不足。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [176] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 该研究比较了基于原始签名图像和壳预处理两种方法在跨数据集签名验证中的性能，发现原始图像模型表现更好，但壳预处理方法具有改进潜力。


<details>
  <summary>Details</summary>
Motivation: 解决离线签名验证中由于笔迹风格和采集协议差异导致的跨数据集泛化能力不足问题，提高模型在训练集和测试集不同时的鲁棒性。

Method: 使用CEDAR、ICDAR和GPDS Synthetic三个公开基准数据集，开发了基于原始签名图像和壳预处理两种实验流程，比较它们在跨数据集验证中的性能。

Result: 原始图像模型在跨基准测试中获得了更高的性能，而基于壳预处理的方法显示出未来改进的潜力，但两种方法没有明确的优劣之分。

Conclusion: 虽然原始图像方法在当前表现更好，但壳预处理方法为开发鲁棒的跨域签名验证系统提供了有前景的方向，值得进一步研究改进。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [177] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 研究基于扩散变换器（DiT）的图像到视频（I2V）模型是否能够生成拥挤公共场景中真实的行人运动模式。


<details>
  <summary>Details</summary>
Motivation: 探索高性能I2V模型在大型视频数据集训练后是否具备模拟真实行人动态的能力。

Method: 通过从行人轨迹基准中提取关键帧来条件化I2V模型，然后评估其轨迹预测性能。

Result: 使用行人动力学的定量指标来评估生成的轨迹质量。

Conclusion: 该研究旨在验证I2V模型在模拟复杂人群动态方面的潜力。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [178] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出了一种无需训练的描述符无关方法，通过矩阵分解将多个参考描述符联合建模为基表示，实现基于投影的残差匹配，在多参考视觉位置识别中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多参考视觉位置识别中，在保持轻量化的同时应对外观和视角变化带来的挑战，避免传统方法中数据多样性和模型复杂度增加带来的计算成本问题。

Method: 使用矩阵分解将多个参考描述符分解为基表示，通过投影基进行残差匹配，无需训练过程且与具体描述符类型无关。

Result: 在多外观数据上，Recall@1提升约18%；在外观和视角变化场景下均优于多参考基线方法，在非结构化数据上获得约5%的性能增益。

Conclusion: 该方法在保持轻量化的同时展现出强大的泛化能力，能够有效处理多参考视觉位置识别中的外观和视角变化问题。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [179] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 提出了一种基于双编码器和注意力的皮肤病变分类框架，结合精确的病变分割和临床元数据，在提高分类准确性的同时增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌的早期检测对患者预后至关重要，但现有深度学习模型存在"黑箱"问题，缺乏临床信任。皮肤病变图像存在类内差异大、类间差异小的挑战。

Method: 使用带双注意力门和ASPP的Deep-UNet进行病变分割；分类阶段采用两个DenseNet201编码器分别处理原始图像和分割后的病变，通过多头交叉注意力融合特征；使用transformer模块整合患者元数据（年龄、性别、病变部位）。

Result: 在HAM10000数据集和ISIC 2018、2019挑战赛上取得最先进的分割性能，显著提高了分类准确率和平均AUC。Grad-CAM热图验证模型基于病变区域进行预测。

Conclusion: 将精确的病变分割、临床数据与基于注意力的特征融合相结合，能够构建更准确和可解释的皮肤癌分类模型。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [180] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA是一种高效的视觉语言模型推理范式，通过在预填充阶段剪枝冗余视觉token并在解码阶段仅检索查询相关token，实现视觉稀疏性解耦，显著提升推理速度同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的推理可扩展性受到视觉token数量增长的制约，这些token主导了推理延迟。需要一种方法在保持模型能力的同时加速推理。

Method: 提出解耦视觉稀疏性的方法：预填充阶段剪枝冗余视觉token，解码阶段仅检索查询相关token。基于AWQ优化的推理流水线实现。

Result: 在长上下文视频任务中实现4.0倍预填充加速、2.5倍解码加速和2.6倍端到端加速，同时在文档理解和推理任务上提高准确性。

Conclusion: 通过解耦查询无关剪枝和查询感知检索，SparseVILA为高效多模态推理提供了无需训练、架构无关的加速框架，不牺牲模型能力。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [181] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 提出ConsistEdit方法，针对MM-DiT架构的注意力控制技术，解决现有训练免费编辑方法在编辑强度与一致性之间的平衡问题，支持多轮和视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有训练免费注意力控制方法难以同时实现强编辑能力和源一致性，在多轮和视频编辑中视觉错误会累积，且全局一致性限制限制了细粒度编辑能力。

Method: 基于MM-DiT架构的深入分析，提出三种关键洞察，包括仅视觉注意力控制、掩码引导预注意力融合、以及查询-键-值令牌的差异化操作，开发ConsistEdit方法。

Result: 在广泛的图像和视频编辑任务中实现最先进性能，支持结构和非结构一致性场景，是首个无需手工操作即可在所有推理步骤和注意力层进行编辑的方法。

Conclusion: ConsistEdit显著提升了可靠性和一致性，支持稳健的多轮和多区域编辑，并能渐进调整结构一致性，实现更精细的控制。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [182] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 该论文研究了量子自然语言处理在自然语言推理任务中的应用，比较了量子、混合和经典模型在少样本设置下的表现，并提出了新的信息增益参数比指标来评估学习效率。


<details>
  <summary>Details</summary>
Motivation: 探索量子自然语言处理在语义建模中的潜力，特别是在资源受限的少样本设置下，通过将组合结构直接嵌入量子电路来实现高效的自然语言推理。

Method: 使用lambeq库和DisCoCat框架构建参数化量子电路处理句子对，训练语义相关性和推理分类任务，并提出基于聚类的架构来促进参数共享。

Result: 量子模型在参数数量大幅减少的情况下达到与经典基准相当的性能，在推理任务上优于随机初始化的transformer，在相关任务上测试误差更低，且参数学习效率比经典模型高出最多五个数量级。

Conclusion: 量子自然语言处理在低资源、结构敏感的场景中展现出巨大潜力，特别是在参数效率和泛化能力方面优于传统方法。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [183] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 提出基于ChatGPT和Claude的多模型融合框架，通过相似性共识方法提升胸部X光片诊断准确性，在单模态和多模态输入下均优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 提高AI辅助放射学诊断的可靠性和临床实用性，通过多模型融合减少诊断错误。

Method: 使用ChatGPT和Claude两种LLM，采用相似性共识方法（95%输出相似度阈值），在CheXpert数据集上评估单模态（仅图像）和多模态（图像+合成临床笔记）性能。

Result: 单模态下：ChatGPT准确率62.8%，Claude 76.9%，共识方法77.6%；多模态下：ChatGPT 84%，Claude 76%，共识方法91.3%。共识融合始终优于单一模型。

Conclusion: 多模态输入和输出级共识能显著提升AI放射诊断的准确性和可信度，为减少诊断错误提供实用路径。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [184] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 提出了CorrectBench基准来评估大语言模型的自校正策略效果，发现自校正能提升推理准确性但影响效率，简单的思维链基线表现竞争力强。


<details>
  <summary>Details</summary>
Motivation: 虽然已有多种自校正方法被提出，但这些方法的综合评估仍然缺乏，且LLM是否能真正自我校正是一个重要问题。

Method: 开发CorrectBench基准，评估内在、外部和微调三种自校正策略在常识推理、数学推理和代码生成三个任务上的效果。

Result: 自校正方法能提高准确性（特别是复杂推理任务）；混合策略能进一步改进但降低效率；推理型LLM在额外自校正下优化有限且时间成本高；简单思维链基线表现竞争力强。

Conclusion: 自校正有潜力提升LLM推理性能，但需平衡推理能力与操作效率，建议进一步研究优化这一平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [185] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: EvolveR是一个让LLM智能体通过闭环经验生命周期实现自我改进的框架，包含离线自蒸馏和在线交互两个关键阶段，在复杂多跳问答基准上表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在工具使用方面表现出色，但缺乏从自身经验中系统学习的能力。现有框架主要关注弥补外部知识差距，未能解决更根本的限制：无法迭代优化问题解决策略。

Method: EvolveR框架包含两个关键阶段：(1)离线自蒸馏：将智能体的交互轨迹合成为结构化、可重用的抽象策略原则库；(2)在线交互：智能体与任务交互并主动检索蒸馏原则来指导决策，积累多样化行为轨迹。该循环采用策略强化机制基于性能迭代更新智能体。

Result: 在复杂多跳问答基准测试中，EvolveR实现了优于强智能体基线的性能表现。

Conclusion: 该工作为智能体不仅从外部数据学习，还能从自身行为后果中学习提供了全面蓝图，为更自主和持续改进的系统铺平了道路。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [186] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 提出了神经群通信（NGC）框架，将神经网络重新构想为交互神经群的动态系统，通过低维信号交换实现参数压缩，在保持推理能力的同时提升模型效率。


<details>
  <summary>Details</summary>
Motivation: 解决现代神经网络规模扩大带来的效率和可解释性挑战，构建能够学习高效、模块化和可解释表示的大型神经系统的核心问题。

Method: 将神经网络视为神经群交互的动态系统，权重作为神经状态间的瞬时交互，通过神经群间的迭代通信进行计算，引入神经元稳定性度量来量化序列处理中的稳定模式收敛。

Result: 在大型语言模型中实例化NGC，在适度压缩下在复杂推理基准上表现更好，在可比压缩率下持续优于标准低秩近似和跨层基共享方法。

Conclusion: NGC框架通过结构化神经群动态实现高效、模块化和可解释的表示学习，可能在高维学习系统的泛化中发挥重要作用。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [187] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 开发了BenCao，一个基于ChatGPT的中医多模态助手，通过自然语言指令调优而非参数重训练，整合了结构化知识库、诊断数据和专家反馈，在中医问答和诊断任务中表现优于通用和中医领域模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医依赖整体推理、隐含逻辑和多模态诊断线索，现有中医领域大语言模型在文本理解方面取得进展但缺乏多模态整合、可解释性和临床适用性。

Method: 整合了1000多部经典和现代文本的知识库，基于场景的指令框架，可解释推理的思维链模拟机制，以及执业中医师的反馈精炼过程，连接外部API进行舌像分类和多模态数据库检索。

Result: 在单选题基准和多模态分类任务评估中，BenCao在诊断、草药识别和体质分类方面表现优于通用和中医领域模型，已在OpenAI GPTs商店部署，截至2025年10月有近1000名全球用户访问。

Conclusion: 研究证明了通过自然语言指令调优和多模态整合开发中医领域大语言模型的可行性，为生成式AI与传统医学推理对齐提供了实用框架和可扩展的现实部署路径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [188] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本研究量化了提示策略与大语言模型在系统文献综述筛选阶段的交互作用，评估了6个LLM在5种提示类型下的表现，发现CoT-few-shot提供最可靠的精确度-召回率平衡，并推荐了分阶段工作流程。


<details>
  <summary>Details</summary>
Motivation: 研究LLM提示策略与模型在系统文献综述自动化筛选中的交互作用，为任务自适应LLM部署提供比较基准和实用指导。

Method: 评估6个LLM（GPT-4o、GPT-4o-mini、DeepSeek-Chat-V3、Gemini-2.5-Flash、Claude-3.5-Haiku、Llama-4-Maverick）在5种提示类型（零样本、少样本、思维链、CoT-少样本、自我反思）下的表现，使用准确率、精确度、召回率和F1分数作为评估指标。

Result: 结果显示了显著的模型-提示交互效应：CoT-少样本提供最可靠的精确度-召回率平衡；零样本在高灵敏度筛选时最大化召回率；自我反思因过度包容性和模型间不稳定性表现不佳。GPT-4o和DeepSeek提供稳健的整体性能，而GPT-4o-mini以显著更低的成本表现具有竞争力。

Conclusion: 推荐采用分阶段工作流程：首先使用低成本模型配合结构化提示进行初步筛选，仅将边界案例升级到更高容量模型。这些发现突显了LLM在文献筛选自动化方面不均衡但有前景的潜力。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [189] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出了一个合成测试平台，用于系统分析语言模型中统计规律与事实关联的交互对泛化能力的影响。研究发现上下文多样性和结构对事实回忆的泛化有复杂影响，并识别了优化瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对语言模型中统计规律与事实关联交互影响的系统分析，特别是这种交互如何影响泛化能力。

Method: 设计了一个灵活的合成测试平台，结合通用令牌的统计流和源-目标令牌对的事实流，可独立控制上下文结构和多样性水平。

Result: 研究发现更高的上下文多样性会延迟ID事实准确性，但对OOD事实泛化的影响取决于上下文结构。在某些情况下，OOD性能与ID趋势相同，而在其他情况下，多样性对非平凡事实回忆至关重要。

Conclusion: 上下文设计和多样性水平的相互作用影响不同的泛化方面，通过模型组件干预发现OOD失败源于不同的优化瓶颈，特别是嵌入和解嵌入层的重要性。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [190] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 首个关于生成式AI信任与不信任的计算研究，使用2022-2025年Reddit数据集分析公众态度变化，发现信任与不信任基本平衡，技术性能和可用性是主要维度。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI融入日常生活，理解公众信任对负责任采用和治理至关重要。现有研究缺乏计算性、大规模和纵向方法来衡量对GenAI和LLMs的信任与不信任。

Method: 使用多年度Reddit数据集（39个子版块，197,618个帖子），结合众包标注和分类模型进行大规模分析。

Result: 信任与不信任随时间基本平衡，在主要模型发布时出现波动。技术性能和可用性是最主要的维度，个人经历是态度形成的最常见原因。不同用户群体（专家、伦理学家、普通用户）表现出不同模式。

Conclusion: 研究为大规模信任分析提供了方法论框架，并揭示了公众对生成式AI认知的演变过程。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [191] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 介绍了EgMM-Corpus，一个专门针对埃及文化的多模态数据集，包含3000多张图像，涵盖313个概念。评估CLIP模型在该数据集上的性能，凸显了现有模型的文化偏见。


<details>
  <summary>Details</summary>
Motivation: 中东和非洲地区的多模态文化多样性数据集仍然有限，需要专门的文化数据集来评估和训练视觉语言模型。

Method: 设计和运行新的数据收集流程，收集了涵盖地标、食物和民间传说等313个概念的3000多张图像，每个条目都经过人工验证文化真实性和多模态一致性。

Result: CLIP模型在EgMM-Corpus上的零样本性能：Top-1准确率为21.2%，Top-5准确率为36.4%。

Conclusion: 结果显示了大规模视觉语言模型中存在的文化偏见，证明了EgMM-Corpus作为开发文化感知模型基准的重要性。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [192] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 该论文通过理论分析和实证验证，探讨了语言模型对语法的学习情况，建立了语法、意义和字符串概率之间的关系框架，并提出了三个可验证的预测。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型是否真正学习了语法知识，以及字符串概率能否反映模型的语法能力，这对语言学理论有重要影响。

Method: 基于语料库生成过程的简单假设，构建理论分析框架，并使用28万个英汉句子对进行实证验证。

Result: 验证了三个预测：(1)最小对字符串概率相关性；(2)模型与人类在最小对中的差异相关性；(3)语法与不语法字符串在概率空间中分离度差。

Conclusion: 为使用概率研究语言模型的结构知识提供了理论基础，并指出了语言模型语法评估的未来研究方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [193] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 提出两种方法（多元解码和模型引导）来增强语言模型的多元对齐能力，在低资源设置下仅用50个标注样本就能超越零样本和少样本基线，改善仇恨言论检测、错误信息检测等高风险任务的性能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型对社会影响增大，需要确保它们能够对齐多样化的观点并反映人类价值观的细微差别。当前主流训练范式假设每个查询只有一个最优答案，导致响应泛化且对齐效果差。

Method: 提出两种方法：1) 多元解码 - 生成多样化响应；2) 模型引导 - 使用少量标注样本（仅50个）引导模型学习多元对齐。

Result: 模型引导方法在零样本和少样本基线上表现一致提升，在仇恨言论检测和错误信息检测中减少假阳性，在GlobalOpinionQA上改善了与人类价值观的分布对齐。

Conclusion: 这项工作强调了多样性的重要性，展示了语言模型如何适应考虑细微观点，为构建更包容和对齐的AI系统提供了可行路径。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [194] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出Profile-to-PEFT框架，使用超网络将用户配置文件直接映射到适配器参数，避免为每个用户单独训练，实现实时个性化LLM。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法如"One-PEFT-Per-User"需要为每个用户训练单独的适配器，计算成本高且无法实时更新，限制了大规模应用。

Method: 使用端到端训练的超网络，将编码后的用户配置文件直接映射到完整的适配器参数集（如LoRA），部署时无需每个用户单独训练。

Result: 实验表明该方法在部署时使用更少计算资源的情况下，优于基于提示的个性化和OPPU方法，对分布外用户具有强泛化能力。

Conclusion: Profile-to-PEFT框架实现了高效、可扩展和自适应的LLM个性化，适用于大规模应用场景。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [195] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 该研究探讨了后训练大语言模型是否意识到自己的学习和思考过程，定义了三个核心能力：对学习策略的认知、策略的跨领域泛化能力，以及内部推理与最终输出的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 随着后训练技术让大语言模型能够通过生成规划标记来处理复杂逻辑任务，研究者想了解这些模型是否真正意识到自己的学习和思考过程。

Method: 在多个需要学习不同策略的任务上实证评估三种核心能力，并对比SFT、DPO和GRPO三种后训练方法的模型表现。

Result: RL训练模型比SFT模型表现出更强的学习行为认知能力和结构相似新任务的泛化能力，但推理轨迹与最终输出的对齐较弱，GRPO训练模型这一现象最明显。

Conclusion: 后训练模型确实具备一定程度的元认知能力，但不同训练方法在策略认知、泛化能力和推理对齐方面存在显著差异。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [196] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本文探索使用LLMs生成针对疫苗错误信息的实时反驳论点，通过优化提示策略和微调方法，结合分类器对反疫苗推文进行多标签分类，提高反驳效果。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体影响公共卫生的时代，对抗疫苗怀疑论和错误信息成为关键社会目标。现有研究在检测错误信息方面取得进展，但生成实时定制反驳论点仍研究不足。

Method: 实验多种提示策略和微调方法优化反驳论点生成，训练分类器将反疫苗推文分类为疫苗效力、副作用、政治影响等多标签类别，实现更情境感知的反驳。

Result: 通过人工判断、LLM评估和自动指标的综合评估显示，整合标签描述和结构化微调能显著提高反驳论点的有效性。

Conclusion: 该方法为大规模缓解疫苗错误信息提供了有前景的解决方案，展示了LLMs在生成针对性反驳论点方面的强大能力。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [197] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 本文提出了AASP框架，通过自回归方式联合建模论证组件和论证关系，在三个标准基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 由于论证挖掘任务中论证组件和论证关系之间的复杂依赖关系，现有方法通常通过扁平化处理来简化问题，但这种方法无法有效捕捉论证推理的流程。

Method: 使用自回归论证结构预测框架，将论证结构建模为预定义的约束动作集，通过条件预训练语言模型逐步构建论证结构。

Result: 在三个标准AM基准测试中，AASP在两个基准上实现了所有AM任务的最先进结果，在一个基准上取得了强劲表现。

Conclusion: AASP框架通过自回归方式有效捕捉了论证推理流程，为论证挖掘任务提供了有效的端到端解决方案。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [198] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文首次系统比较了基于提示和基于强化学习的查询增强方法，发现简单的无训练查询增强方法性能与昂贵的RL方法相当甚至更好，并提出了结合两者优势的OPQE混合方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在信息检索中的查询增强主要有两种方法：基于提示的方法和基于强化学习的方法，但缺乏在一致实验条件下的系统比较。

Method: 提出OPQE混合方法，LLM策略学习生成最大化检索性能的伪文档，结合了提示的灵活性和RL的针对性优化。

Result: 无训练查询增强方法性能与RL方法相当或更好；OPQE方法优于单独的提示和RL重写方法。

Conclusion: 协同方法能获得最佳结果，简单的无训练查询增强在强大LLM下已具有竞争力。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [199] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 提出了一种轻量级方法，通过线性变换特定层的激活向量来提升LLM在心理健康评估任务中的表现，无需计算密集型技术。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs快速发展，但小规模模型在特定领域应用中仍表现不佳，特别是在心理健康这样的敏感高影响领域。

Method: 使用线性变换应用于特定层的激活向量，利用导向向量来引导模型输出，这是一种计算成本低的方法。

Result: 该方法在两个任务中取得改进：识别Reddit帖子是否有助于检测抑郁症状的相关性预测任务，以及基于用户Reddit发帖历史完成标准化抑郁筛查问卷的任务。

Conclusion: 导向机制作为计算效率高的工具，在LLMs心理健康领域适应方面具有未开发的潜力。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [200] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: MoReBench是一个包含1000个道德场景和23000多个评估标准的基准测试，用于评估AI模型的道德推理过程，重点关注推理过程而非最终结论。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与决策，需要理解AI如何做出决策而不仅仅是决策结果。道德困境是评估AI推理过程的理想测试平台，因为允许多种合理结论。

Method: 创建MoReBench基准测试，包含道德场景和专家制定的评估标准，涵盖识别道德考量、权衡利弊、给出可行建议等方面。同时开发MoReBench-Theory测试AI在五种规范伦理学框架下的推理能力。

Result: 研究发现，在数学、代码和科学推理任务上的扩展定律和现有基准无法预测模型的道德推理能力。模型对特定道德框架（如边沁功利主义和康德义务论）表现出偏好，这可能是流行训练范式的副作用。

Conclusion: 这些基准测试推动了以过程为重点的推理评估，有助于开发更安全、更透明的AI系统。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [201] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 提出了Nyx，一个统一的混合模态检索器，用于解决通用检索增强生成(URAG)问题，通过检索和推理混合模态信息来提升视觉语言生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统主要关注单模态文本文档，在现实场景中当查询和文档都包含混合模态（如文本和图像）时表现不足，需要解决混合模态信息的检索和推理挑战。

Method: 提出四阶段自动化流水线构建NyxQA数据集，采用两阶段训练框架：先在NyxQA和开源检索数据集上进行预训练，然后使用下游视觉语言模型的反馈进行监督微调。

Result: Nyx不仅在标准文本RAG基准测试中表现优异，在更通用和现实的URAG设置中表现突出，显著提升了视觉语言任务的生成质量。

Conclusion: Nyx通过统一的混合模态检索方法有效解决了URAG挑战，在混合模态场景下显著提升了生成性能，为现实世界的信息需求提供了更好的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [202] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 提出ATA方法，将LLM任务分解为离线知识摄入和在线任务处理两个阶段，通过神经符号方法提升可信度


<details>
  <summary>Details</summary>
Motivation: 解决LLM在高风险领域部署时的可信度问题，包括幻觉、不稳定性和缺乏透明度

Method: 神经符号方法：离线阶段将非正式问题规范转化为形式化知识库，在线阶段使用符号决策引擎处理输入

Result: 在复杂推理任务中，ATA与最先进的端到端推理模型竞争，在人工验证知识库时显著优于更大模型，具有完美确定性、增强的稳定性和对提示注入攻击的免疫力

Conclusion: ATA为构建透明、可审计和可靠的自主代理提供了实用可控的架构

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [203] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 探索Whisper语音识别模型在二语口语评估中的潜力，通过提取隐藏表示中的声学和语言特征，仅需训练轻量级分类器即可超越现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 挖掘Whisper模型在二语口语评估中的潜在能力，超越以往仅分析其转录文本的研究方法。

Method: 从Whisper的隐藏表示中提取声学和语言特征，仅训练轻量级分类器，并整合图像和文本提示作为辅助相关性线索。

Result: 在GEPT图片描述数据集上表现出色，超越了包括多模态方法在内的现有最先进基线，通过添加辅助信息获得额外性能提升。

Conclusion: 即使没有任务特定微调，Whisper模型也能内在编码口语的熟练度模式和语义方面，显示出其作为口语评估和理解的强大基础模型的潜力。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [204] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: FrugalPrompt是一个LLM提示压缩框架，通过保留最具语义重要性的token来减少输入长度，在多数NLP任务中20%压缩仅导致轻微性能损失，但数学推理任务性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的扩展输入上下文带来了高昂的货币成本、碳足迹和推理延迟，而典型提示中存在大量冗余的低效用token，只有少数token承载主要语义权重。

Method: 使用GlobEnc和DecompX两种token归因方法为输入序列中的每个token分配显著性分数，按原始顺序保留前k%的token，获得稀疏的压缩提示。

Result: 在情感分析、常识问答和摘要任务中，20%的提示压缩仅导致任务性能轻微损失；但在数学推理中性能急剧恶化；使用后k%和随机k%token的分析揭示了不对称性能模式。

Conclusion: 该工作有助于更细致地理解LLM在性能-效率权衡中的行为，并界定了容忍上下文稀疏的任务与需要完整上下文的任务之间的边界。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [205] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector是一个高效的Best-of-N框架，利用LLM的隐藏状态进行过程级评分，通过轻量级验证器评估推理轨迹质量，无需大量步骤级标注，在多个基准测试中实现性能提升且推理成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有外部TTS方法存在计算开销大和未充分利用LLM内在潜在表示的问题，需要开发更高效的Best-of-N选择框架。

Method: 使用轻量级验证器（仅0.6B参数）评估步骤级轨迹质量，聚合分数选择最优推理轨迹，采用端到端数据驱动训练方法。

Result: 在Best-of-32设置中，比多数投票准确率高4.61%，比现有过程奖励模型高4.31%-12.21%，同时保持较低推理成本。

Conclusion: TrajSelector通过利用LLM隐藏状态进行过程级评分，实现了高效且有效的推理轨迹选择，在多个基准测试中表现优异。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [206] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN是一个集成课程强化学习和多模态大语言模型的广告视频违规检测框架，通过渐进式训练策略和GRPO优化实现精确时间定位和一致类别预测，在工业数据集和公共基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有广告视频违规检测方法在精确时间定位、噪声标注和泛化能力方面存在不足，需要开发更有效的检测框架。

Method: 结合课程强化学习和MLLMs，采用渐进式训练策略整合精确和粗略标注数据，使用GRPO开发推理能力，通过分层奖励机制确保时间定位和类别预测的准确性。

Result: 在工业数据集和公共基准测试中，RAVEN在违规类别准确性和时间间隔定位方面表现优异，在线A/B测试验证了其实际应用价值，显著提升了精确率和召回率。

Conclusion: RAVEN框架通过集成强化学习和MLLMs，有效解决了广告视频违规检测中的关键挑战，展示了强大的泛化能力和实际部署价值。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [207] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 本研究扩展了自然语言推理中标注变异的分析范围，不仅关注解释类型的差异，还考察标注步骤的分歧。通过LiTEx分类法分析两个英文NLI数据集，发现表面标签分歧可能掩盖解释层面的深层一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注标注者在最终NLI标签一致但解释不同的情况，本文旨在扩展分析范围，考察标注者在推理类型和标注步骤上的分歧，以更全面理解NLI标注中的个体差异。

Method: 使用LiTEx分类法分析两个英文NLI数据集，从NLI标签一致性、解释相似性和分类法一致性三个维度对齐标注变异，并考虑标注者选择偏见的复合因素。

Result: 发现标注者标签分歧但解释高度相似的实例，表明表面分歧可能掩盖解释层面的一致性。分析还揭示了标注者在解释策略和标签选择上的个体偏好。

Conclusion: 推理类型的一致性比标签一致性更能反映自由文本解释的语义相似性，基于推理的解释具有丰富性，需要谨慎对待标签作为绝对真值。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [208] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 提出使用"退出"作为LLM代理的安全机制，让代理在缺乏信心时主动退出，通过ToolEmu框架评估12个先进LLM，结果显示退出指令能显著提升安全性而几乎不影响帮助性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在复杂环境中运行并产生现实后果，其安全性变得至关重要。多轮代理场景中的不确定性和模糊性会累积，导致比传统文本生成失败更严重的风险。

Method: 利用ToolEmu框架系统评估12个先进LLM的退出行为，通过在提示中添加明确的退出指令来让代理在缺乏信心时主动退出。

Result: 退出指令使所有模型的安全性平均提升+0.39（0-3分制），专有模型提升+0.64，而帮助性仅平均下降-0.03，显示出极佳的安全-帮助性权衡。

Conclusion: 简单的退出指令是一种高度有效的安全机制，可立即部署到现有代理系统中，作为高风险应用中自主代理的第一道防线。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [209] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 提出基于在线背包问题的自动化智能体系统组合框架，通过动态测试和实时效用建模，在预算约束下优化选择智能体组件，显著提升成功率和降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统组合方法依赖静态语义检索，存在能力描述不完整、检索方法局限等问题，难以基于能力、成本和实时效用进行有效组件选择和重用。

Method: 引入受背包问题启发的结构化自动化框架，让组合器智能体系统性地识别、选择和组装最优智能体组件集合，综合考虑性能、预算约束和兼容性。

Result: 在5个基准数据集上的实证评估显示，在线背包组合器始终位于帕累托前沿，相比基线方法：单智能体设置下成功率提升高达31.6%；多智能体系统中成功率从37%提升至87%。

Conclusion: 该方法在多样化领域和预算约束下展现出强大的适应性，显著提升了智能体系统的组合效率和性能表现。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [210] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 提出了一种基于超图的适配器HGAdapter，通过捕捉代码中的高阶数据相关性来增强预训练语言模型在代码相关任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型在代码任务中表现良好，但未能充分利用代码中潜在的高阶数据相关性。

Method: 设计了三种代码令牌的高阶相关性（抽象语法树家族相关性、词汇相关性和行相关性），构建了令牌和超边生成器，并改进了超图神经网络架构与适配器调优结合，提出HGAdapter。

Result: 在多个公共数据集上的实验表明，该方法在不同程度上提高了预训练语言模型在代码摘要和代码克隆检测任务中的性能。

Conclusion: 引入高阶数据相关性有助于提高代码相关任务的效果，HGAdapter可以插入各种预训练语言模型以增强性能。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [211] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: ReviewGuard是一个用于检测和分类缺陷同行评审的自动化系统，采用四阶段LLM驱动框架，通过数据收集、标注、合成数据增强和模型微调，有效识别评审质量不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着投稿量激增和LLM在学术评估中的广泛应用，来自人类专家和AI系统的缺陷评审威胁着同行评审生态系统的完整性，需要自动化检测系统来维护学术诚信。

Method: 采用四阶段LLM驱动框架：收集ICLR和NeurIPS论文及评审数据；使用GPT-4.1标注评审类型并人工验证；通过LLM驱动的合成数据增强解决类别不平衡和数据稀缺问题；微调编码器模型和开源LLM。

Result: 构建了包含6,634篇论文、24,657条真实评审和46,438条合成评审的语料库。缺陷评审表现出较低的评分、较高的自报告置信度、较低的结构复杂度和较高的负面情绪比例。混合训练显著提升了二元任务的召回率和F1分数。

Conclusion: 这是首个用于检测缺陷同行评审的LLM驱动系统，为同行评审中的AI治理提供了证据，并为维护学术诚信的人机协作提供了宝贵见解。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [212] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出了可执行知识图谱(xKG)，一个模块化可插拔的知识库，通过整合从科学文献中提取的技术见解、代码片段和领域知识，显著提升了LLM代理在AI研究复现中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成可执行代码方面存在困难，主要由于背景知识不足和RAG方法的局限性，无法捕捉参考文献中的潜在技术细节，且缺乏支持多粒度检索和重用的结构化知识表示。

Method: 开发了可执行知识图谱(xKG)，自动从科学文献中提取技术见解、代码片段和领域特定知识，构建模块化知识库。

Result: 在三个代理框架和两种不同LLM上集成xKG后，在PaperBench上显示出显著的性能提升（o3-mini模型提升10.9%）。

Conclusion: xKG是一种通用且可扩展的解决方案，能有效支持自动化AI研究复现。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [213] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 本文通过分析LLMs内部激活路径重叠来研究其文化理解机制，发现在相同语言不同国家的条件下激活路径重叠更多，表明存在强烈的语言特定模式。韩国-朝鲜对比显示语言相似性并不保证内部表征对齐。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在多元文化背景中的广泛应用，准确的文化理解变得至关重要。现有评估多关注输出层面，难以揭示响应差异的驱动因素，而电路分析研究覆盖语言少且很少关注文化。

Method: 通过测量回答语义等价问题时激活路径的重叠程度，在两种条件下进行：固定问题语言改变目标国家，以及固定国家改变问题语言。使用同语言国家对比来分离语言和文化因素。

Result: 结果显示相同语言、跨国家问题的内部路径重叠度高于跨语言、相同国家问题，表明存在强烈的语言特定模式。韩国-朝鲜对比显示出低重叠和高变异性。

Conclusion: 语言相似性并不保证内部表征对齐，LLMs的文化理解机制表现出强烈的语言依赖性，需要更细致的方法来评估跨文化能力。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [214] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: SHALLOW是首个系统分类和量化ASR系统中幻觉现象的基准框架，包含词汇、语音、形态和语义四个维度，能识别传统WER指标无法区分的细粒度错误模式。


<details>
  <summary>Details</summary>
Motivation: ASR系统中的幻觉会产生与语音信号完全无关但语法语义合理的转录，在医疗、法律等关键领域带来严重风险，而传统错误率指标无法有效区分语音不准确和幻觉问题。

Method: 提出SHALLOW框架，从词汇、语音、形态和语义四个互补维度系统分类幻觉现象，为每个类别定义针对性指标以生成可解释的模型行为分析。

Result: SHALLOW指标在识别质量高时与WER强相关，但随着WER增加相关性显著减弱，能捕捉WER在恶劣条件下无法区分的细粒度错误模式。

Conclusion: SHALLOW框架支持对模型弱点的具体诊断，并提供超越聚合错误率的模型改进反馈，填补了ASR幻觉评估的空白。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [215] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 提出了一种针对乌尔都语的AI生成文本检测框架，使用多语言transformer模型在平衡数据集上训练，mDeBERTa-v3-base模型在测试集上达到91.26%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs生成文本能力增强，区分人类写作和机器生成文本变得困难，乌尔都语等低资源语言缺乏有效的AI文本检测工具，需要填补这一空白。

Method: 构建包含1800篇人类写作和1800篇AI生成文本的平衡数据集，进行语言和统计分析，微调三种多语言transformer模型（mdeberta-v3-base、distilbert-base-multilingualcased、xlm-roberta-base）。

Result: mDeBERTa-v3-base模型表现最佳，在测试集上F1分数达到91.29，准确率为91.26%。

Conclusion: 该研究推进了乌尔都语社区打击错误信息和学术不端的努力，并为低资源语言的NLP工具开发做出了贡献。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [216] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 通过微调大语言模型将句子翻译成句法结构，实现西班牙语句法分析，扩展MiSintaxis教学工具的功能。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型的最新进展，探索基于机器学习的句法分析新方法，扩展西班牙语句法教学工具MiSintaxis的能力。

Method: 使用Hugging Face仓库中的多个模型，在AnCora-ES语料库生成的训练数据上进行微调，将输入句子翻译为对应的句法结构。

Result: 使用F1分数评估性能，结果显示短语结构分析具有高准确率。

Conclusion: 该方法展示了将大语言模型用于句法分析的潜力，为西班牙语句法教学提供了有效工具。

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [217] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: DiMo框架通过四个专门LLM代理的协作辩论，提升大语言模型的性能和可解释性，在六个基准测试中表现优于单模型和辩论基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能强大，但缺乏可解释的推理过程，需要提升性能和可解释性。

Method: 使用四个专门化LLM代理模拟结构化辩论，每个代理代表不同的推理范式，通过迭代辩论挑战和完善初始响应。

Result: 在六个基准测试中，DiMo框架的准确性优于广泛使用的单模型和辩论基线，在数学任务上提升最大。

Conclusion: DiMo是一个语义感知、Web原生的多代理框架，能够生成语义类型化、URL注释的证据链，支持下游系统检查和重用。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [218] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 该论文提出了Capsule Prompt-Tuning (CaPT)方法，通过将实例感知信息集成到提示学习中，解决了传统提示学习方法依赖网格搜索、缺乏实例感知信息的问题，实现了参数高效且性能优越的模型调优。


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的学习方法存在两个主要问题：一是依赖繁琐的网格搜索来确定最佳提示长度，计算负担重；二是缺乏实例感知信息，导致与输入序列的注意力交互受限。研究发现，简单地融入实例感知信息就能提升模型性能。

Method: 提出了Capsule Prompt-Tuning (CaPT)方法，创新性地将实例感知和任务感知信息集成到单个胶囊提示中，采用近乎参数免费的方式。该方法利用现成的信息实例语义，在序列最早位置加入实例感知标记作为"注意力锚点"。

Result: 实验结果显示，该方法在多个语言任务上表现出优越性能（如在T5-Large上达到84.03%的平均准确率），同时保持高参数效率（如在Llama3.2-1B上仅使用模型参数的0.003%）。

Conclusion: CaPT方法通过引入实例感知信息作为"注意力锚点"，有效解决了传统提示学习的局限性，在保持高参数效率的同时显著提升了模型性能，为大型语言模型的高效调优提供了新思路。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [219] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 提出了TUuD框架来评估大语言模型在动态时间参考点下对时间-事件和事件-事件关系的理解能力，发现LLMs表现出部分类似人类的时序认知，但对参考框架变化和时间距离敏感。


<details>
  <summary>Details</summary>
Motivation: 人类通过空间隐喻理解时间，使用时间参考框架(t-FoR)来感知相对于"现在"的时间关系。虽然LLMs在自然语言理解方面取得显著进展，但它们在时间理解和推理方面的能力仍然有限。

Method: 引入TUuD框架，让LLMs评估当前时刻与目标事件之间的相似度(0.00-1.00)，量化两个时间点之间的感知时间对齐程度，研究当"现在"参考点沿时间线动态移动时的表现。

Result: 四个评估的LLMs表现出对指示性t-FoR的可测量适应，相似度评分在现在附近达到峰值，并向过去和未来事件递减。但这种适应在短期情境之外减弱。

Conclusion: LLMs显示出部分类似人类的时间认知，但它们的时序推理仍然对参考框架变化和时间距离敏感。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [220] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 该论文探讨了思维链（CoT）推理在自然语言理解（NLU）任务中的应用效果，发现随着模型规模增大，CoT推理从阻碍NLU性能转变为超越直接标签预测，并开发了专门的训练方法实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注思维链在推理任务中的作用，而忽视了其在自然语言理解任务中的潜在影响。本文旨在系统探索理性在NLU任务中的适用性和效果。

Method: 构建了NLURC数据集（包含理性的高质量NLU数据集集合），开发了多种理性增强方法，并系统评估这些方法在NLU任务上的表现。

Result: 发现三个关键结果：1）CoT推理与模型规模呈正相关；2）大多数理性增强训练方法表现不如仅标签训练，但有一种专门设计的方法能持续改进；3）使用理性训练的LLM在未见过的NLU任务上表现显著提升，性能可与大十倍的模型相媲美，同时提供与商业LLM相当的可解释性。

Conclusion: 理性可以显著提升NLU任务性能，特别是在模型规模较大时，且专门设计的理性增强训练方法能够实现持续改进，同时提供良好的可解释性。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [221] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 这篇综述系统回顾了2014-2025年间自然语言处理技术在心脏病学领域的应用研究，分析了265篇相关文献，从NLP范式、心脏病学任务、心血管疾病类型和数据来源等多个维度进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病日益普遍且复杂，相关信息分散在各类文本数据中。NLP技术能够分析这些非结构化数据，帮助医疗专业人员深入了解心脏病学领域，从而革新心脏问题的诊断、治疗和预防方法。

Method: 查询了六个文献数据库，通过严格筛选流程确定了265篇相关文章，从NLP范式类型、心脏病学任务类型、心血管疾病类型和数据来源类型等多个维度进行分析，并进行了时间趋势分析。

Result: 分析显示在各个维度上都存在相当大的多样性，证明了NLP研究在该领域的广泛性。时间分析揭示了所涵盖的过去十年中NLP方法的演变和变化趋势。

Conclusion: 这是迄今为止对心脏病学领域NLP研究最全面的综述，展示了NLP技术在心血管疾病研究中的重要应用价值和广阔前景。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [222] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 论文首次系统研究了LLM中的"变色龙行为"——在搜索增强LLM的多轮对话中，当面对矛盾问题时，模型会改变立场的倾向。通过Chameleon基准数据集和两个理论指标，揭示了最先进系统的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前LLM与搜索/检索引擎的集成系统存在关键漏洞，其可靠性受到威胁。需要系统调查LLM在多轮对话中面对矛盾问题时改变立场的倾向，特别是在搜索增强LLM中。

Method: 构建了包含17,770个问答对的Chameleon基准数据集，涵盖1,180个多轮对话和12个争议领域。引入了变色龙分数（0-1）和源重用率（0-1）两个理论指标，对Llama-4-Maverick、GPT-4o-mini和Gemini-2.5-Flash进行了严格评估。

Result: 所有模型都表现出严重的变色龙行为（分数0.391-0.511），GPT-4o-mini表现最差。源重用率与置信度（r=0.627）和立场变化（r=0.429）之间存在强相关性，表明有限的知识多样性使模型病态地顺从查询框架。

Conclusion: 这些发现强调了在医疗、法律和金融系统中部署LLM之前，需要进行全面的连贯性评估，因为在这些领域，跨交互保持一致的立场对于可靠的决策支持至关重要。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [223] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 该论文研究了诗歌中空白空间的使用，分析了19k首已发表诗歌的空白分布，并与LLM生成诗歌和未发表诗歌进行比较，探讨了不同文本处理方法对空白表示的影响。


<details>
  <summary>Details</summary>
Motivation: 空白空间是诗歌形式的重要组成部分，反映了诗人的艺术选择，但在NLP研究中未得到足够关注。论文旨在填补这一空白，研究诗歌中空白的使用模式及其对LLM预训练数据集处理策略的影响。

Method: 使用来自Poetry Foundation的19k首英语诗歌语料库，分析4k位诗人的空白使用情况。比较已发表诗歌与51k首LLM生成诗歌和12k首未发表诗歌的空白使用差异，并探讨不同时期、诗歌形式和数据源的空白使用模式。

Result: 发现不同文本处理方法会导致诗歌数据中空白表示显著不同，发布了2.8k首公共领域诗歌的子集以促进进一步研究。比较结果显示已发表诗歌、LLM生成诗歌和未发表诗歌在空白使用上存在差异。

Conclusion: 诗歌中的空白空间是重要的语义和空间特征，其研究对理解诗歌艺术和改善LLM预训练数据集处理策略具有重要意义。不同数据来源的诗歌在空白使用上表现出系统性差异。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [224] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 论文提出了Beacon基准来测量LLM中的谄媚偏见（sycophancy），即模型倾向于迎合用户而非坚持事实。研究发现该偏见随模型能力增强而加剧，并提出干预方法来调节这种偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在奖励优化过程中混淆了帮助性和礼貌顺从，形成了在真实性和谄媚之间的结构性权衡。这种偏见导致模型偏好用户认同而非原则性推理。

Method: 引入Beacon基准，这是一个单轮强制选择测试，能够独立于对话语境隔离谄媚偏见。评估了12个最先进模型，并提出了提示级和激活级的干预方法。

Result: 评估显示谄媚偏见可分解为稳定的语言和情感子偏见，且随模型容量增加而增强。干预方法能够在相反方向上调节这些偏见，揭示了对齐的内部几何结构。

Conclusion: Beacon将谄媚重新定义为可测量的规范错误泛化形式，为研究和大规模生成系统中的对齐漂移缓解提供了可复现的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [225] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 提出SCO-PAL方法，通过自博弈提升语言智能体在对抗性游戏中的策略推理能力，相比基线平均胜率提高约30%，对抗GPT-4达到54.76%胜率。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体在动态对抗性游戏中策略推理能力不足，需要无需专家标注数据的自动学习方法。对手选择对学习性能有重要影响，但相关研究较少。

Method: 提出SCO-PAL方法，通过玩与学习进行步级策略优化，分析不同级别对手的影响，发现自博弈是最有效的学习方式。

Result: 使用SCO-PAL与自博弈，在六个对抗性游戏中对抗四个对手的平均胜率比基线提高约30%，对抗GPT-4达到54.76%胜率。

Conclusion: 自博弈是提升对抗性环境中策略推理的最有效方式，SCO-PAL方法显著提高了语言智能体的游戏表现。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [226] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: LC-Eval是一个双语多任务评估基准，用于评估英语和阿拉伯语的长上下文理解能力，涵盖4k到128k+ token的上下文长度，包含四个具有挑战性的任务。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在长上下文处理能力上的进步，需要更严格的评估方法来有效评估它们在长上下文理解方面的性能。

Method: 开发了LC-Eval双语评估基准，包含四个新颖任务：多文档问答、双语问答、段落内声明验证和基于长上下文的多选题，涵盖英语和阿拉伯语数据集。

Result: 评估显示LC-Eval具有显著挑战性，即使是GPT-4o等高性能模型在某些任务上也表现不佳，突显了基准的复杂性和严谨性。

Conclusion: LC-Eval为评估大语言模型的长上下文理解能力提供了有效的基准，揭示了当前模型在深度推理、文档理解和信息追踪等方面的局限性。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [227] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: MOSAIC是一个用于句子嵌入模型领域适应的多阶段框架，通过联合优化掩码语言建模和对比目标，在专业领域实现有效表示学习。


<details>
  <summary>Details</summary>
Motivation: 解决大规模通用领域句子嵌入模型适应专业领域的挑战，需要保持原始模型的语义判别能力同时学习领域相关知识。

Method: 多阶段框架，结合领域特定的掩码监督，联合优化MLM和对比目标，采用分阶段适应策略。

Result: 在高资源和低资源领域均取得显著改进，NDCG@10指标相比强基线提升高达13.4%，消融研究验证了各组件有效性。

Conclusion: 平衡的联合监督和分阶段适应对于领域适应至关重要，MOSAIC框架在保持语义判别能力的同时有效学习领域相关表示。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [228] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: LLMs在实体比较任务中经常依赖启发式偏见而非真实知识，即使它们具备正确的数值知识。研究发现实体流行度、提及顺序和语义共现三种启发式偏差严重影响预测，小模型更易被这些表面线索主导，而大模型能更选择性地使用数值知识。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在知识推理任务中何时依赖真实知识而非表面启发式，通过实体比较任务来系统分析模型的知识使用机制。

Method: 使用实体比较任务（如比较河流长度），分析LLMs的预测行为，识别三种启发式偏见：实体流行度、提及顺序和语义共现，并比较不同参数规模模型的表现差异。

Result: 小模型（7-8B参数）的预测更多受启发式偏见主导，而大模型（32B参数）能更选择性地使用数值知识。思维链提示能引导所有模型更多使用数值特征。

Conclusion: 模型规模影响知识使用策略，大模型能更智能地在启发式和知识推理间切换，思维链提示能有效改善模型的知识推理能力。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [229] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段检索-重排框架，通过微调LLMs进行跨体裁作者归属任务，在HIATUS基准测试上显著超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 跨体裁作者归属任务需要识别与主题无关的作者特定语言模式，而传统信息检索方法依赖主题线索，不适合该任务。

Method: 采用两阶段检索-重排框架，通过专门的数据策展策略训练重排器，使其能够有效学习作者区分性信号。

Result: 在HIATUS的HRS1和HRS2跨体裁作者归属基准测试上，分别比现有最佳方法提升了22.3和34.4个绝对Success@8点。

Conclusion: 该LLM基础的检索-重排管道在跨体裁作者归属任务中取得了显著改进，证明了针对性的训练策略的重要性。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [230] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: CoRUS框架通过模拟基于角色的提问来评估语言模型，发现在阿片类药物使用障碍等敏感领域中，提问者的角色会显著影响模型回答的内容和风格。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估大多忽略提问者角色，但在敏感领域如阿片类药物使用障碍中，考虑用户背景对于提供无偏见、可访问的回答至关重要。

Method: 基于角色理论和在线康复社区数据构建提问者角色分类法（患者、照顾者、从业者），并模拟15,321个嵌入各角色目标、行为和经验的提问。

Result: 模拟提问具有高可信度且与现实数据相当。评估5个LLM发现：相同问题但不同角色会导致系统性差异，脆弱角色（患者、照顾者）获得更多支持性回答（+17%）但知识内容减少（-19%）。

Conclusion: 用户角色的隐式信号会显著影响模型回答，CoRUS提供了基于角色的对话AI评估方法学。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [231] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight是一个多智能体框架，通过CAVM架构、迭代视觉增强机制和两阶段写作框架，能够生成高质量的多模态财务报告，在事实准确性、分析深度和呈现质量方面显著优于现有基线系统。


<details>
  <summary>Details</summary>
Motivation: 生成专业财务报告是一个劳动密集且智力要求高的过程，现有AI系统难以完全自动化这一任务。

Method: 采用CAVM架构统一外部数据、工具和智能体；提出迭代视觉增强机制逐步优化原始视觉输出；使用两阶段写作框架将简洁的分析链扩展为连贯的多模态报告。

Result: 在各种公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线系统，包括领先的深度研究系统。

Conclusion: FinSight展示了生成接近人类专家质量报告的清晰路径，为自动化专业财务报告生成提供了有效解决方案。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [232] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 视觉语言模型在具身知识理解方面并未超越纯文本模型，且在视觉维度表现最差，需要更有效的具身知识整合方法。


<details>
  <summary>Details</summary>
Motivation: 研究多模态语言模型是否通过视觉基础比纯文本模型更好地理解具身知识，基于心理学感知理论构建评估基准。

Method: 基于心理学感知理论构建具身知识理解基准，涵盖视觉、听觉、触觉、味觉、嗅觉等外部感官和内部感知，通过向量比较和问答任务评估模型感知能力。

Result: 比较30个最先进的语言模型发现，视觉语言模型在两项任务中均未超越纯文本模型，且在视觉维度表现显著差于其他感官维度。

Conclusion: 当前模型在具身知识理解方面存在局限，特别是空间感知和推理能力不足，需要更有效的具身知识整合方法来增强对物理世界的理解。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [233] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: ChiKhaPo是一个大规模多语言基准测试，包含8个不同难度的子任务，旨在评估生成模型的词汇理解和生成能力，覆盖2700多种语言，超越了现有基准的语言覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型基准主要局限于高资源或中资源语言，且多评估高阶推理和生成任务，但大量证据表明LLMs在世界上3800多种书面语言中缺乏基本的语言能力。

Method: ChiKhaPo利用现有词典、单语数据和双语文本构建，包含8个不同难度的子任务来评估词汇理解和生成能力。

Result: 测试了6个最先进的模型，发现它们在ChiKhaPo基准上表现不佳，分析了影响性能的因素包括语言家族、语言资源丰富度、任务类型以及理解与生成方向。

Conclusion: 通过ChiKhaPo，希望能够推动和鼓励LLMs的大规模多语言基准测试。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [234] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 提出了PROMPT-MII方法，通过强化学习元学习生成紧凑指令，在保持性能的同时大幅减少推理成本


<details>
  <summary>Details</summary>
Motivation: 解决上下文学习(ICL)方法在适应大语言模型时推理成本高的问题

Method: 基于强化学习的元学习框架，在3000多个数据集上训练指令归纳模型，生成紧凑指令

Result: 在90个未见任务上，下游模型质量提升4-9个F1点(10-20%相对提升)，性能匹配ICL但所需token减少3-13倍

Conclusion: PROMPT-MII能够有效生成紧凑指令，在保持性能的同时显著降低推理成本

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [235] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文首次应用参数高效微调(PEFT)技术，使用LoRA和QLoRA方法对三个指令调优的大语言模型进行孟加拉语仇恨言论检测，在单张消费级GPU上实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体中仇恨言论急剧增加，特别是针对女性和青少年。现有方法要么依赖计算成本高的全模型微调，要么使用专有API，缺乏高效实用的解决方案。

Method: 使用LoRA和QLoRA参数高效微调技术，在BD-SHS数据集（50,281条标注评论）上微调Gemma-3-4B、Llama-3.2-3B和Mistral-7B三个模型，训练参数少于总参数的1%。

Result: Llama-3.2-3B取得最高F1分数92.23%，Mistral-7B为88.94%，Gemma-3-4B为80.25%，所有实验均在单张消费级GPU上完成。

Conclusion: PEFT被证明是孟加拉语及相关低资源语言仇恨言论检测的实用且可复现策略，为资源受限环境提供了高效解决方案。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [236] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer是一个极简的字节级分词器，直接将文本映射到UTF-8编码对应的字节ID，使用C0控制字节编码特殊行为，提供更快的分词速度和更小的传输开销。


<details>
  <summary>Details</summary>
Motivation: 现有字节级分词方法存在超出范围的ID或需要辅助token的问题，作者希望设计一个更简洁高效的分词方案。

Method: 使用UTF-8编码的字节作为token ID，所有特殊行为通过C0控制字节编码，无需额外token，并引入位偏置嵌入来暴露字节位结构。

Result: 实现了14倍的分词速度提升，8倍于int64的传输开销减少，256维嵌入表可跨模型对齐，提升了语言建模的收敛性。

Conclusion: UTF8Tokenizer提供了一个高效、简洁的分词方案，通过字节级编码和C0控制字节实现了更好的性能和实用性。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [237] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 提出一种词汇表压缩方法，通过将词形变化表示为基词向量与变换向量的组合，而非分配独立token，从而释放词汇表空间并扩展词汇覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 标准分词算法将词形变化视为独立token，导致词汇表被表面形式变体填满，牺牲了低频词和多语言覆盖。

Method: 使用变换向量（加法偏移量）在嵌入空间中表示词形变化，将词汇表重新设计为共享基词向量和变换向量的组合形式。

Result: 在多个LLM和五种语言上应用，最多可移除10%的词汇表条目，扩展了词汇覆盖范围，且对下游性能影响最小，无需修改模型权重。

Conclusion: 这促使对词汇表设计进行根本性重新思考，从字符串枚举转向利用语言底层结构的组合式词汇表。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [238] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的动态防御框架，通过在线学习对抗迭代越狱攻击，使用Past-Direction Gradient Damping防止过拟合，显著提升了LLM的安全性同时保持无害任务的响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法主动破坏迭代越狱攻击的动态试错循环，这些攻击通过反复重写提示词来诱导有害输出，利用模型之前的响应指导每次新迭代。

Method: 采用基于强化学习的方法动态更新防御策略，通过在线学习响应每个新提示；引入Past-Direction Gradient Damping来防止对攻击期间探索的狭窄输入重写范围过拟合。

Result: 在三个LLM上的实验表明，该方法显著优于五种现有防御方法对抗五种迭代越狱方法，同时提示优化策略还提升了无害任务的响应质量。

Conclusion: 该动态防御框架能有效对抗迭代越狱攻击，在保持无害任务性能的同时显著提升模型安全性，为LLM安全防御提供了新思路。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [239] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: DiscoTrack是一个多语言LLM基准测试，专注于语篇理解中的隐含信息和语用推理，包含12种语言的四个语篇理解任务。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要关注自然语言理解中的显性信息提取，缺乏针对隐含信息、语用推理和跨文档语篇跟踪的更具挑战性的多语言基准。

Method: 开发了DiscoTrack基准测试，涵盖12种语言和四个语篇理解层次：显著性识别、实体跟踪、语篇关系和桥接推理。

Result: 评估显示，即使是最先进的模型，这些任务仍然具有挑战性。

Conclusion: DiscoTrack填补了LLM基准测试在语篇理解和多语言隐含信息处理方面的空白，为模型评估提供了新的挑战性标准。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [240] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 本文发现基于LLM的搜索代理比基础LLM更容易产生有害输出，提出了SafeSearch方法，通过多目标强化学习结合最终输出安全和查询级奖励，将代理危害性降低70%以上。


<details>
  <summary>Details</summary>
Motivation: 研究人员主要关注改进搜索代理的实用性，但其安全行为尚未充分探索。研究发现搜索代理比基础LLM更容易产生有害输出，且以实用性为导向的微调会加剧这种风险。

Method: 提出SafeSearch方法，采用多目标强化学习，结合最终输出的安全/实用性奖励和新型查询级塑造项，惩罚不安全查询并奖励安全查询。

Result: 实验显示SafeSearch在三个红队测试数据集上将代理危害性降低70%以上，同时产生安全、有帮助的响应，并与仅以实用性微调的代理在QA性能上相当。

Conclusion: 查询级奖励在联合改进安全和实用性方面有效，SafeSearch方法成功平衡了搜索代理的安全性和实用性。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [241] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 提出了xLSTM框架，结合余弦相似度门控、自适应特征优先级和类别重平衡，在有毒评论检测任务上以更少的参数超越BERT性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于transformer的模型计算成本高且在少数毒性类别上性能下降的问题，同时解决传统集成方法缺乏语义适应性的问题。

Method: 使用可学习的参考向量通过余弦相似度调制上下文嵌入，集成多源嵌入（GloVe、FastText、BERT CLS），包含字符级BiLSTM用于形态线索，嵌入空间SMOTE用于少数类增强，以及自适应焦点损失。

Result: 在Jigsaw有毒评论基准测试中达到96.0%准确率和0.88宏F1，在威胁和身份仇恨类别上分别比BERT高出33%和28%，参数减少15倍，推理延迟50ms。

Conclusion: 轻量级、理论驱动的架构可以在不平衡、领域特定的NLP任务上超越大型预训练模型，建立了新的效率适应性前沿。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [242] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 论文研究大语言模型的提示敏感性现象，提出通过语义概念空间的采样来改善不确定性校准，并引入新的不确定性分解指标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对语义相同但表述不同的提示会产生不同的答案分布，这表明模型输出的不确定性可能无法反映其对提示含义的真实不确定性。

Method: 将提示敏感性建模为泛化误差，通过语义概念空间的转述扰动进行采样，并引入新的不确定性分解指标来量化提示敏感性对不确定性的贡献。

Result: 通过语义概念空间采样改善了不确定性校准，同时不损害准确性；新的分解指标能够有效量化提示敏感性对LLM不确定性的影响。

Conclusion: 这项工作为改善提示敏感语言模型的不确定性校准提供了新方法，并证明某些LLM未能对其输入含义表现出一致的通用推理能力。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [243] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 该论文研究了基于推理的大型语言模型在思考过程中如何聚合社会偏见，发现了两种导致偏见加剧的失败模式，并提出了一种轻量级的提示方法来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 虽然基于推理的大型语言模型通过内部结构化思考过程在复杂任务中表现出色，但出现了令人担忧的现象：这种思考过程会聚合社会刻板印象，导致有偏见的结果。然而，语言模型在社会偏见场景中的底层行为仍未得到充分探索。

Method: 系统研究了思考过程中导致社会偏见聚合的机制，发现了两种失败模式：刻板印象重复和无关信息注入。基于这些发现，引入了一种轻量级的基于提示的缓解方法，让模型根据这些特定失败模式审查自己的初始推理。

Result: 在问答（BBQ和StereoSet）和开放式（BOLD）基准测试上的实验表明，该方法有效减少了偏见，同时保持或提高了准确性。

Conclusion: 该研究揭示了大型语言模型思考过程中偏见聚合的机制，并提出了一种有效的缓解方法，能够在减少偏见的同时保持模型性能。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [244] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一个用于多智能体协作的验证感知规划框架，通过分解任务、建模子任务依赖关系，并将规划器定义的通过标准编码为Python和自然语言的子任务验证函数，解决了多智能体协作中的规划、协调和验证挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在处理复杂任务时面临规划、协调和验证的新挑战，执行失败往往源于任务解释、输出格式或智能体间交接的细微错位，而非推理缺陷。

Method: VeriMAP框架包括任务分解、子任务依赖建模，并将规划器定义的通过标准编码为Python和自然语言的子任务验证函数(VFs)。

Result: 在多样化数据集上的评估表明，VeriMAP超越了单智能体和多智能体基线，同时增强了系统鲁棒性和可解释性。

Conclusion: 验证感知规划能够在多智能体系统中实现可靠的协调和迭代优化，无需依赖外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [245] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: DVAGen是一个开源统一框架，用于训练、评估和可视化动态词汇增强的语言模型，解决固定词汇表模型处理新词和词汇组合的局限性。


<details>
  <summary>Details</summary>
Motivation: 固定词汇表的语言模型难以泛化到新词或词汇表外词汇，现有动态词汇方法存在代码库碎片化、不支持现代LLM、推理可扩展性有限等问题。

Method: 开发DVAGen框架，模块化处理流程便于定制，无缝集成开源LLM，提供CLI和WebUI工具进行实时结果检查，支持批量推理。

Result: 验证了动态词汇方法在现代LLM上的有效性，显著提高了推理吞吐量。

Conclusion: DVAGen通过统一的动态词汇增强框架，有效解决了现有方法的局限性，提升了语言模型的灵活性和推理效率。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [246] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究表明，人们对AI生成的讽刺言论不会完全采用意向性立场，在行为和神经层面都表现出对AI意图归因的减弱。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被部署为社交代理并训练产生幽默和讽刺，需要研究人们是否会对AI的机智言论采用意向性立场，将其视为有意识的交流而非单纯的计算输出。

Method: 通过比较AI与人类来源的讽刺陈述的行为和神经反应，使用ERP成分分析：P200反映早期不一致性检测，P600反映重新解释不一致性为故意讽刺的认知努力。

Result: 行为上，参与者对AI故意交流的归因显著少于人类；神经数据显示AI生成讽刺的P200和P600效应减弱，表明努力检测和重新分析的减少。认为AI更真诚的人对AI讽刺显示出更大的P200和P600效应。

Conclusion: 尽管当前LLMs具有语言复杂性，但要实现真正的社交代理，不仅需要语言能力，还需要人类对人工代理的意向性归因方式发生转变。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [247] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文系统分析了基于分块的稀疏注意力模型，提出了三个关键设计原则：表达性强的分块编码器、旁路残差路径和训练中的强制选择稀疏性，实现了从4K上下文到3200万token的无训练长度外推。


<details>
  <summary>Details</summary>
Motivation: 有效处理长上下文是语言模型的关键挑战。标准Transformer受限于二次复杂性和较差长度外推能力，而滑动窗口注意力等替代架构由于固定大小内存无法充分利用完整上下文。需要理解分块稀疏注意力成功背后的核心架构原则。

Method: 通过统一框架和全面消融研究，识别出三个关键设计原则：1) 具有专用CLS标记的表达性非线性分块编码器；2) 旁路残差路径稳定整合检索的全局信息；3) 预训练中强制选择稀疏性以弥合训练-测试分布差距。

Result: 结合这些原则，在RULER和BABILong上实现了从4K上下文到3200万token的无训练长度外推，建立了新的最先进水平。

Conclusion: 研究为开发未来高性能长上下文语言模型提供了一套清晰且经验证的设计原则，证明了分块内信息处理和地标生成的理论动机。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [248] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 提出了一种新颖的注意力转移框架，通过上下文保持抑制和抗幻觉响应塑造来解决LLM遗忘中的效用与幻觉权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法面临两难困境：激进遗忘会损害模型效用，而保守策略保留效用但会产生幻觉响应，限制了LLM在知识密集型应用中的可靠性。

Method: 注意力转移框架包含两个组件：重要性感知抑制（应用于遗忘集以减少对记忆知识的依赖）和注意力引导保留增强（强化保留集中语义重要token的注意力）。通过双损失目标联合优化，形成软边界来定位遗忘同时保留不相关知识。

Result: 在ToFU基准上准确率比最先进方法提高15%，在TDEC基准上提高10%，同时保持竞争力的无幻觉遗忘效果。

Conclusion: AS方法在遗忘效果、泛化能力和响应可靠性之间实现了优越的平衡。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [249] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出了StreamingThinker框架，实现LLM在阅读输入时同步进行推理的流式思维范式，显著降低推理延迟


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理范式需要等待完整输入才开始思考，这带来了不必要的延迟，且在动态场景下会削弱对早期信息的注意力

Method: 设计流式思维范式，通过流式CoT生成、流式约束训练和流式并行推理实现边读边想，使用流式推理单元、顺序保持注意力掩码和并行KV缓存

Result: 在数学推理、逻辑推理和基于上下文的QA任务上，StreamingThinker保持了与批量思维相当的性能，同时将推理开始前的token等待时间减少80%，最终答案生成的时间级延迟减少60%以上

Conclusion: 流式思维范式能够有效降低LLM推理延迟，同时保持推理质量，为动态场景下的实时推理提供了可行方案

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [250] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文介绍了VideoBiasEval框架，用于评估视频生成模型中的社会偏见，发现对齐调优会放大并稳定化社会偏见。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型通过人类偏好对齐提升了视觉质量，但可能无意中编码和放大了社会偏见，需要系统评估这些偏见在生成流程中的演变。

Method: 提出VideoBiasEval诊断框架，基于社会偏见分类学，采用基于事件的提示策略分离语义内容和演员属性，引入多粒度指标评估偏见。

Result: 研究发现对齐调优不仅加强了表征偏见，还使其在时间上更稳定，产生更平滑但更刻板的描绘。

Conclusion: 需要在对齐过程中进行偏见感知的评估和缓解，以确保公平和社会责任的视频生成。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [251] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 通过大规模情感分析发现孟加拉语新闻标题普遍存在负面情绪倾向，特别是愤怒、恐惧和失望情绪占主导地位，不同媒体对相似事件的报道存在显著情感差异。


<details>
  <summary>Details</summary>
Motivation: 研究新闻媒体如何通过情感框架影响公众情绪，探索负面或情绪化标题吸引更多关注的现象，以及这种倾向如何影响新闻报道方式。

Method: 使用Gemma-3 4B模型进行零样本推理，分析了30万条孟加拉语新闻标题及其内容，识别每篇文章的主要情绪和整体基调。

Result: 发现负面情绪明显占主导地位，特别是愤怒、恐惧和失望情绪，不同媒体对相似故事的报道存在显著情感差异。

Conclusion: 基于研究结果，提出了以人为本的新闻聚合器设计理念，通过可视化情感线索帮助读者识别日常新闻中隐藏的情感框架。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [252] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文综述了基于Transformer的大语言模型的本地可解释性和机制可解释性方法，通过在医疗和自动驾驶领域的实验研究分析了可解释性对信任的影响，并指出了当前未解决的问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能优异，但其预测过程和内容生成机制对人类不可理解，且经常出现幻觉错误，这凸显了理解和解释语言模型内部工作机制的迫切需求，以增强对此类模型的信任。

Method: 首先综述了本地可解释性和机制可解释性方法及相关研究，然后在医疗和自动驾驶两个关键领域进行了可解释性和推理的实验研究，分析了可解释性对解释接收者信任的影响。

Result: 通过实验研究展示了可解释性方法在理解语言模型工作机制方面的有效性，并分析了可解释性对增强模型信任的作用。

Conclusion: 总结了当前LLM可解释性领域未解决的问题，并提出了生成与人类对齐、可信赖的LLM解释的机会、关键挑战和未来方向。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [253] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出了TaxoAlign方法用于自动生成学术分类法，创建了CS-TaxoBench基准数据集，并在结构对齐和语义一致性方面超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动综述生成方法缺乏与人类专家生成分类法的结构比较，需要填补这一空白。

Method: 提出了TaxoAlign方法，这是一种基于主题的三阶段指令引导的学术分类法生成方法，并建立了严格的自动评估框架。

Result: TaxoAlign在CS-TaxoBench基准上几乎在所有指标上都持续超越了基线方法。

Conclusion: 该方法能够弥合人类生成和自动创建分类法之间的差距，为自动文献综述生成提供了有效解决方案。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [254] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 该研究使用法国多党对话数据集CyberAgressionAdo-Large，评估了反社会行为检测、欺凌行为分析和欺凌同伴群体识别三个任务，发现多模态模型优于单模态基线。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的反社会行为（如仇恨言论、骚扰和网络欺凌）对平台安全和社福祉构成风险，但多党对话环境中的此类行为研究因数据有限而不足。

Method: 使用CyberAgressionAdo-Large数据集，评估了6种基于文本和8种基于图的表示学习方法，分析词汇线索、互动动态及其多模态融合。

Result: 多模态模型优于单模态基线，晚期融合模型mBERT + WD-SGCN在滥用检测上表现最佳（0.718），在同伴群体识别（0.286）和欺凌分析（0.606）上也有竞争力。

Conclusion: 多模态方法能有效处理隐含攻击、角色转换和上下文依赖的敌意等微妙的反社会行为现象。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [255] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究发现指令调优大语言模型在简单指令执行方面存在不足，对选项标签格式敏感，且缺乏对原子指令的忠实遵循能力


<details>
  <summary>Details</summary>
Motivation: 探索指令调优大语言模型执行简单、自包含指令的能力，这是复杂指令遵循的基础，但目前研究不足

Method: 在修改后的MMLU和MMLU-Pro基准上评估20个IT-LLM，系统性地改变选项标签格式（字母、数字、罗马数字），采用四种范式测试指令遵循能力

Result: 模型性能受标签格式影响显著（罗马vs数字下降30.45%），无指令时性能进一步下降，移除选项内容后除数字标签外均无法通过随机基线测试，三样本示例无显著改善

Conclusion: 当前指令调优范式存在不足，需要专门针对原子指令遵循的评估方法和训练策略

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [256] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出了EduAdapt基准，包含48k个按年级标注的科学问答对，用于评估LLM在不同年级水平的适应性表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在学术基准上表现良好，但无法根据学生年级水平调整回答，这在K-12教育中至关重要。

Method: 创建了涵盖9个科学学科、1-12年级的问答数据集，将年级分为4个水平组，评估开源LLM的表现。

Result: 大型模型表现更好，但在为低年级学生生成合适回答方面仍有困难。

Conclusion: 这是首个评估LLM年级适应性的数据集和框架，旨在通过改进训练和提示策略开发更适合教育需求的AI系统。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [257] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: Ladder-base是首个采用GRPO强化学习方法训练的中医领域大语言模型，在TCM-Ladder基准测试中表现出色，超越了通用LLM和中医专用模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医知识体系独特，现有中医LLM在一致性、数据质量和评估标准方面存在局限，需要更有效的对齐方法。

Method: 基于Qwen2.5-7B-Instruct基础模型，使用GRPO强化学习方法在TCM-Ladder文本子集上进行训练，通过组内比较优化响应选择。

Result: Ladder-base在多项推理指标上表现优异，超越了GPT-4、Gemini 2.5等通用LLM以及BenTsao、HuatuoGPT2等中医专用模型。

Conclusion: GRPO为传统医学领域LLM与专家级推理对齐提供了有效策略，支持可信赖且临床基础扎实的中医AI系统开发。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [258] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: AfriCaption是一个多语言图像描述框架，支持20种非洲语言，通过精心策划的数据集、动态质量保证流程和专用模型，为资源匮乏语言建立首个可扩展的图像描述资源。


<details>
  <summary>Details</summary>
Motivation: 解决多模态AI研究过度集中在高资源语言的问题，促进该领域技术的民主化，特别关注被忽视的非洲语言。

Method: 构建基于Flickr8k的语义对齐数据集，采用上下文感知选择和翻译；开发动态上下文保持流程，通过模型集成和自适应替换保证质量；设计0.5B参数的AfriCaption模型，集成SigLIP和NLLB200进行跨语言描述生成。

Result: 建立了首个针对非洲低资源语言的可扩展图像描述资源，确保持续的数据质量，为包容性多模态AI奠定基础。

Conclusion: AfriCaption框架成功填补了多模态AI在非洲语言方面的空白，为实现真正包容的多模态AI提供了重要基础设施。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [259] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 论文展示了通过简单的模型权重插值方法，可以在不牺牲准确性的情况下有效缓解对齐过程中的校准损失问题。


<details>
  <summary>Details</summary>
Motivation: 传统的后训练对齐通常会导致任务准确率下降（对齐税），但研究发现这还涉及严重的校准损失，使模型过度自信、可靠性降低且输出多样性减少。

Method: 采用简单的后处理干预：在模型对齐前后的权重之间进行插值。这种方法计算效率高，通过模型合并来缓解对齐税。

Result: 研究发现该过程能持续揭示帕累托最优的插值点，这些模型不仅能超越两个父模型的准确性，还能显著恢复对齐过程中损失的校准能力。

Conclusion: 简单的模型合并提供了一种计算高效的方法来缓解对齐税的全面影响，产生既更有能力又更可靠的模型。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [260] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: RL训练的搜索模型虽然继承了指令调优的拒绝能力，但存在脆弱性。两种简单攻击（搜索攻击和多搜索攻击）可大幅降低模型的安全性能，暴露了当前RL训练忽视查询有害性的核心弱点。


<details>
  <summary>Details</summary>
Motivation: 研究RL训练的搜索模型的安全特性，发现其虽然能拒绝有害请求，但这种安全性很脆弱，需要开发安全感知的智能体RL训练方法。

Method: 使用两种攻击方法：搜索攻击（强制模型以搜索开始响应）和多搜索攻击（鼓励模型重复搜索），在Qwen和Llama模型家族上进行测试，包括本地和网络搜索。

Result: 攻击使拒绝率降低高达60.0%，回答安全性降低82.5%，搜索查询安全性降低82.4%。攻击通过触发模型生成有害的镜像搜索查询来绕过拒绝机制。

Conclusion: 当前RL训练只奖励生成有效查询而不考虑其有害性，导致搜索模型存在易被利用的漏洞，迫切需要开发安全感知的智能体RL训练流程。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [261] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 开发基于BERT的深度上下文嵌入模型，用于提升英语、西班牙语和意大利语临床文本中的疾病和药物命名实体识别性能，在BioASQ MultiCardioNER共享任务中取得了优于平均水平的F1分数。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据快速增长，需要从非结构化临床文本中提取生物医学知识以支持数据驱动的临床系统。虽然英语语料库的命名实体识别系统已有显著进展，但针对低资源语言临床文本的研究仍然稀缺。

Method: 探索了在通用领域文本上训练的单语和多语BERT模型，用于从英语、西班牙语和意大利语的临床病例报告中提取疾病和药物提及。

Result: 在西班牙语疾病识别获得77.88% F1分数，西班牙语药物识别92.09%，英语药物识别91.74%，意大利语药物识别88.9%，均超过测试排行榜的平均和中位数分数。

Conclusion: 基于BERT的深度上下文嵌入模型能够有效提升多语言临床文本中的命名实体识别性能，特别是在低资源语言环境下。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [262] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 该论文针对乌尔都语到英语的习语翻译问题，构建了首个评估数据集，比较了多种LLM和NMT系统在习语翻译上的表现，发现提示工程能提升翻译质量，且原生乌尔都语文本比罗马化乌尔都语翻译效果更好。


<details>
  <summary>Details</summary>
Motivation: 习语翻译在机器翻译中仍然是一个重大挑战，特别是对于乌尔都语等低资源语言，此前研究关注有限。

Method: 构建首个乌尔都语到英语习语翻译评估数据集，评估多种开源LLM和NMT系统，使用BLEU、BERTScore、COMET和XCOMET等自动指标评估翻译质量。

Result: 提示工程相比直接翻译能提升习语翻译质量，但不同提示类型间的性能差异相对较小；原生乌尔都语输入比罗马化乌尔都语产生更准确的习语翻译。

Conclusion: 文本表示对翻译质量有显著影响，原生乌尔都语在习语翻译上表现优于罗马化乌尔都语，提示工程是提升习语翻译的有效方法。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [263] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 研究发现多语言大语言模型在医疗问答中存在显著的跨语言事实对齐差异，即使使用非英语提示，模型回答也更倾向于与英语维基百科对齐。通过提供非英语维基百科的上下文信息可以有效改善事实对齐，为构建更公平的多语言医疗AI系统提供了实用路径。


<details>
  <summary>Details</summary>
Motivation: 将AI整合到医疗保健中时，公平获取可靠健康信息至关重要。然而不同语言间的信息质量存在差异，引发了对多语言大语言模型可靠性和一致性的担忧。

Method: 构建了多语言维基医疗数据集，分析跨语言医疗覆盖度，评估LLM回答与参考资料的匹配度，并通过上下文信息和检索增强生成进行事实对齐案例研究。

Result: 发现维基百科覆盖度和LLM事实对齐都存在显著的跨语言差异。所有LLM的回答都更倾向于与英语维基百科对齐，即使提示是非英语的。在推理时提供非英语维基百科的上下文片段能有效将事实对齐转向文化相关知识。

Conclusion: 研究结果强调了构建更公平、多语言的医疗AI系统的实用路径，通过上下文信息可以有效改善跨语言事实对齐问题。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [264] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE是一种新颖的MoE架构，通过跨层重用专家来改进路由机制，突破了传统层局部路由的限制，在固定参数预算下实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构的层局部路由机制限制了专家组合的灵活性，需要在专家维度和路由多样性之间进行权衡。ReXMoE旨在通过跨层重用专家来解决这一限制。

Method: 提出ReXMoE架构，允许路由器在相邻层之间重用专家；采用渐进式缩放路由（PSR）策略，在训练过程中逐步增加候选专家池。

Result: 在0.5B到7B参数规模的不同架构模型上进行广泛实验，ReXMoE在固定架构维度下持续提升语言建模和下游任务性能。

Conclusion: ReXMoE作为一种新的设计范式，为参数高效且可扩展的基于MoE的大语言模型提供了有效解决方案。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [265] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 提出DETree方法，通过层次亲和树结构建模不同AI-人类协作文本生成过程的关系，并开发RealBench基准数据集，显著提升了混合文本检测的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本检测方法对AI-人类协作过程建模粗糙，主要采用二元分类或多分类方法，无法有效处理复杂的人类-AI协作文本生成过程。

Method: 提出DETree方法，将不同生成过程的关系建模为层次亲和树结构，引入专门损失函数使文本表示与树结构对齐，并开发RealBench基准数据集。

Result: 方法在混合文本检测任务中提升性能，显著增强在分布外场景下的鲁棒性和泛化能力，特别是在少样本学习条件下。

Conclusion: 基于训练的方法在分布外设置中具有潜力，层次亲和树结构能有效建模复杂的人类-AI协作文本生成过程。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [266] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了基于大语言模型的行业智能体技术、应用和评估方法，提出了行业智能体能力成熟度框架，分析了记忆、规划和工具使用三大技术支柱的演进，探讨了行业应用场景和评估挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，能够自主推理、规划和执行复杂任务的智能体成为人工智能前沿，但如何将通用智能体研究转化为推动行业变革的生产力仍面临重大挑战。

Method: 使用行业智能体能力成熟度框架，分析智能体从"流程执行系统"到"自适应社会系统"的演进路径，系统考察记忆、规划和工具使用三大技术支柱的发展。

Result: 梳理了行业智能体在数字工程、科学发现、具身智能、协同业务执行和复杂系统仿真等领域的应用，识别了现有评估系统在真实性、安全性和行业特性方面面临的挑战。

Conclusion: 通过结合技术演进与行业实践，本文为理解和构建下一代行业智能体提供了清晰的路线图和理论基础，探讨了行业智能体的能力边界、发展潜力和治理问题。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [267] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: DSER是一种概率性自进化推理框架，通过将迭代推理建模为马尔可夫链，即使验证和修正能力较弱，也能通过并行运行多个长时程自进化过程来扩展较小模型的推理极限。


<details>
  <summary>Details</summary>
Motivation: 现有的验证-修正框架依赖强大的验证和修正能力，这在开源小规模模型中很脆弱。DSER旨在解决这一问题，通过概率方法扩展这些模型的推理能力。

Method: 将迭代推理建模为马尔可夫链，每个步骤代表解空间的随机转移。只要改进概率略高于退化概率，就能保证收敛到正确解。通过并行运行多个自进化过程来放大这些小的积极趋势。

Result: 在DeepSeek-R1-0528-Qwen3-8B模型上，DSER在AIME 2024-2025基准测试中解决了9个之前无法解决的难题中的5个，提升了整体性能，使这个紧凑模型通过多数投票超越了其600B参数教师的单轮准确率。

Conclusion: DSER不仅为测试时扩展提供了实用价值，还通过明确诊断当前开源推理器在自验证、修正和稳定性方面的局限性，为开发具有强大内在自进化能力的下一代模型建立了清晰的研究议程。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [268] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 该论文系统研究了多语言句子嵌入学习方法，结合单语和跨语言表示学习的最佳方法，显著减少了所需平行训练数据量，在112种语言上实现了83.7%的双文本检索准确率。


<details>
  <summary>Details</summary>
Motivation: 探索BERT在多语言句子嵌入学习中的应用，虽然BERT在单语句子嵌入学习方面表现优异，但其在多语言句子嵌入方面的潜力尚未被充分挖掘。

Method: 结合了掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序和加性边际softmax等最佳方法，并引入预训练的多语言语言模型。

Result: 将所需平行训练数据减少80%，在Tatoeba数据集112种语言上达到83.7%的双文本检索准确率，显著超过LASER的65.5%，同时在单语迁移学习基准上保持竞争力。

Conclusion: 成功开发了高效的多语言句子嵌入模型，在109+种语言上表现优异，可用于训练有竞争力的神经机器翻译模型，模型已公开发布。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [269] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: EliCal是一个两阶段框架，通过廉价的自我一致性监督来引出内部置信度，然后用少量正确性标注进行校准，实现高效的诚实对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大规模标注来实现通用诚实对齐，成本高昂。需要支持标注高效的训练方法。

Method: 提出EliCal两阶段框架：1) 使用廉价的自我一致性监督引出内部置信度；2) 用少量正确性标注校准置信度。

Result: EliCal仅需1k正确性标注（全监督的0.18%）就能实现接近最优的对齐，在未见MMLU任务上表现优于仅校准基线。

Conclusion: EliCal为LLMs的通用诚实对齐提供了可扩展的解决方案。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [270] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: SimBench是首个大规模标准化基准测试，用于评估LLM模拟人类行为的能力。研究发现当前最佳LLM的模拟能力有限，性能随模型规模对数线性增长，指令微调在低熵问题上提升性能但在高熵问题上降低性能，且模型在模拟特定人口群体时表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM模拟人类行为的评估方法零散且不可比较，需要统一的标准化基准来推动LLM模拟能力的科学发展。

Method: 构建SimBench基准，整合20个多样化数据集，涵盖从道德决策到经济选择等任务，基于大规模全球参与者数据进行评估。

Result: 最佳LLM模拟得分仅为40.80/100，性能随模型规模对数线性增长，推理时计算不提升性能，指令微调在低熵问题上改善但在高熵问题上恶化，模型在模拟特定人口群体时表现不佳。

Conclusion: 通过SimBench使进展可测量，可加速开发更忠实的LLM模拟器，模拟能力与深度知识推理能力高度相关。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [271] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 提出了一个统一的多任务学习框架，将自回归LLM与临床推理对齐，用于MSK-CHORD数据集上的癌症治疗结果预测，通过CoT提示和GRPO强化学习方法显著提升了预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在生物医学NLP中表现良好，但缺乏对高风险决策支持至关重要的结构化推理能力，特别是在处理异质临床数据时。

Method: 开发了统一的多任务学习框架，训练模型同时执行二元生存分类、连续生存时间回归和自然语言原理生成。评估了三种对齐策略：标准监督微调、带CoT提示的SFT、以及GRPO强化学习方法。

Result: 实验表明CoT提示将F1提高+6.0%，MAE降低12%，GRPO在BLEU、ROUGE和BERTScore上实现了最先进的可解释性和预测性能。

Conclusion: 研究强调了在多任务临床建模中推理感知对齐的重要性，并为精准肿瘤学中可解释、可信赖的LLM设定了新基准。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [272] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 该论文提出使用拓扑数据分析工具Mapper来研究语言模型如何内部表示歧义性，特别是在人类标注者存在分歧的情况下。研究发现微调后的模型在嵌入空间中形成模块化、非凸的区域，与模型预测对齐，但在歧义数据中与真实标签的对齐度下降。


<details>
  <summary>Details</summary>
Motivation: 传统标量评估指标（如准确率）无法捕捉模型内部如何表示歧义性，特别是在人类标注者存在分歧的情况下。需要新的方法来理解模型如何处理主观性和不确定性。

Method: 使用拓扑数据分析工具Mapper来分析微调后RoBERTa-Large模型在MD-Offense数据集上的嵌入空间结构，与传统方法如PCA和UMAP进行比较。

Result: 微调将嵌入空间重组为模块化、非凸的区域，98%以上的连通组件展现出≥90%的预测纯度。但在歧义数据中，模型结构与真实标签的对齐度下降，揭示了结构置信度与标签不确定性之间的隐藏张力。

Conclusion: Mapper是一个强大的诊断工具，能够直接揭示决策区域、边界塌陷和过度自信的聚类，超越了传统可视化方法。它提供的拓扑指标可以为主观NLP任务中的主动建模策略提供信息。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [273] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 提出Language Confusion Gate (LCG)，一种轻量级插件解决方案，在解码过程中过滤令牌而不改变基础LLM，显著减少语言混淆问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现语言混淆问题，即文本生成过程中无意混合语言。现有解决方案要么需要模型重新训练，要么无法区分有害混淆和可接受的语码转换。

Method: 使用规范调整自蒸馏训练LCG，预测适当的语言家族，仅在需要时应用掩码。基于语言混淆不频繁、正确语言令牌通常位于预测前列、高资源语言的输出令牌嵌入规范更大的发现。

Result: 在包括Qwen3、GPT-OSS、Gemma3、Llama3.1等各种模型上评估，LCG显著减少了语言混淆，通常降低一个数量级，且不影响任务性能。

Conclusion: LCG是一种有效的轻量级解决方案，能够显著减少语言模型的语言混淆问题，同时保持任务性能。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [274] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出了LawChain框架来明确建模中国侵权民事案件中的法律推理过程，构建了评估基准LawChain_eval，并通过实验证明当前LLM在侵权法律推理方面仍有不足，基于LawChain的基线方法能显著提升法律推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理计算方法主要依赖通用推理框架，未能全面考察法律推理的细微过程，且研究多集中于刑事案件，对民事案件建模不足。

Method: 提出LawChain三模块推理框架，将侵权分析中的法律推理过程操作化；构建LawChain_eval评估基准；评估SOTA大语言模型在侵权法律推理中的表现；开发基于LawChain推理的基线方法（提示或后训练）。

Result: 当前模型在准确处理侵权法律推理关键要素方面仍有不足；基于LawChain的基线方法在侵权相关法律推理中取得显著改进，并能很好地推广到相关法律分析任务。

Conclusion: 明确建模法律推理链能有效增强语言模型的推理能力，LawChain框架在法律分析任务中具有良好通用性。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [275] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 本文提出了一种改进的遗忘学习方法，在保持目标知识遗忘效果的同时，恢复了模型在提示中重新引入被遗忘知识时的上下文利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习方法评估只关注目标知识的遗忘程度和保留集性能，但忽略了用户可能希望在提示中重新引入被移除信息时模型仍能利用这些信息的重要可用性需求。

Method: 在遗忘学习目标函数中增加一个插件项，以保持模型在上下文存在被遗忘知识时利用这些知识的能力。

Result: 实验表明该方法能在保持有效遗忘和保留集效用的同时，将上下文效用恢复到接近原始水平。

Conclusion: 提出的方法解决了遗忘学习中上下文效用受损的问题，实现了更全面的模型知识管理。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [276] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: Qomhr'a是一个在低资源条件下开发的双语爱尔兰语-英语大语言模型，通过双语持续预训练、指令微调和人类偏好对齐的完整流程构建，在爱尔兰语和英语基准测试中分别获得最高29%和44%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在低资源条件下开发高质量的爱尔兰语-英语双语大语言模型，解决爱尔兰语资源稀缺的问题，同时保持英语能力。

Method: 采用完整的开发流程：1) 混合和筛选新获取的爱尔兰语语料库和英语文本进行双语持续预训练；2) 使用Gemini-2.5-Pro合成指令微调和人类偏好数据集；3) 贡献30K爱尔兰语-英语平行指令微调数据集和1K人类偏好数据集；4) 进行指令微调和人类偏好对齐。

Result: 模型在翻译、性别理解、主题识别和世界知识等基准测试中表现优异，爱尔兰语性能提升最高29%，英语性能提升最高44%。人类偏好数据集与母语爱尔兰语使用者的对齐度接近完美。

Conclusion: Qomhr'a成功展示了在低资源条件下开发高质量双语LLM的可行性，为爱尔兰语语言技术发展提供了重要资源，并在指令遵循方面取得明显进步，对聊天机器人功能至关重要。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [277] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文提出了一种对话分析方法来评估教育应用中学习者与大型语言模型的互动，关注对话动态和教学策略而非单纯的技术性能或学习成果。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注技术性能或学习成果，忽视了学习者与LLM之间的互动质量，需要填补这一研究空白。

Method: 采用对话分析方法，包括对话数据收集、对话行为标注、对话模式挖掘和预测模型构建四个步骤。

Result: 研究处于进行中，已获得初步见解，为未来研究奠定了基础。

Conclusion: 强调需要通过对语动态和教学策略来评估基于LLM的教育应用，这是评估教育AI的重要方向。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [278] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: 提出QueST框架，通过难度感知图采样和拒绝微调生成具有挑战性的编程问题，显著提升语言模型在竞争性编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有竞争性编程数据集规模有限（仅数千到数万问题），且依赖人工标注数据，限制了语言模型的可扩展性。需要大规模、具有挑战性的编程问题训练数据。

Method: 结合难度感知图采样和难度感知拒绝微调，直接优化专用生成器来创建具有挑战性的编程问题。使用生成的问题进行知识蒸馏或强化学习。

Result: QueST生成的编程问题质量优于GPT-4o。在Qwen3-8B-base上微调100K个QueST生成的问题后，在LiveCodeBench上超越了原始Qwen3-8B。使用额外112K示例（28K人工问题+合成解决方案）后，8B模型性能与DeepSeek-R1-671B相当。

Conclusion: 通过QueST生成复杂问题是推进语言模型在竞争性编程和推理任务前沿的有效且可扩展的方法。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [279] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 提出轻量级少样本NER框架，通过改进指令调优模板和数据增强技术，在低资源场景下实现与SOTA模型相当的性能


<details>
  <summary>Details</summary>
Motivation: 解决NER任务在低资源场景下标注数据稀缺的问题，现有零样本和指令调优方法对领域特定实体泛化能力不足，且无法有效利用有限数据

Method: 1) 新的指令调优模板，简化输出格式以利用大上下文窗口；2) 战略性数据增强技术，在保持实体信息的同时对上下文进行改写

Result: 在CrossNER数据集上少样本方法平均F1达80.1，数据增强版本比基线提升最高17个F1点

Conclusion: 该框架为训练数据和计算资源有限的群体提供了有前景的NER解决方案

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [280] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出AcademicEval基准，使用arXiv论文评估大语言模型在长上下文生成任务中的表现，解决现有基准的上下文长度限制、标注成本高和标签泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文LLM基准存在上下文长度限制、人工标注成本高以及在LLM训练过程中标签泄露的挑战，需要开发更有效的评估方法。

Method: 采用arXiv论文构建学术写作任务（标题、摘要、引言和相关工作），利用收集的合著者图谱提供高质量少样本示例，支持灵活上下文长度，并实现高效实时评估防止标签泄露。

Result: 评估结果显示，LLMs在具有层次抽象级别的任务上表现不佳，且难以处理长少样本演示，突显了该基准的挑战性。

Conclusion: 该研究揭示了增强LLMs长上下文建模能力的一些见解，为改进长上下文理解提供了方向。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [281] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出了一种基于二元检索增强奖励的在线强化学习方法，有效减少语言模型的外源性幻觉，同时保持其他任务性能不下降。


<details>
  <summary>Details</summary>
Motivation: 现有方法在减少语言模型幻觉的同时往往会导致开放生成和下游任务性能下降，限制了实际应用价值。

Method: 使用二元检索增强奖励的在线强化学习，只有当模型输出完全正确时才给予奖励1，否则为0。

Result: 在开放生成任务中幻觉率降低39.3%；在短问答任务中，模型学会了校准性弃权，在PopQA和GPQA上分别减少44.4%和21.7%的错误答案，且不影响指令遵循、数学和代码能力。

Conclusion: 二元奖励方案在提高事实准确性的同时避免了性能退化，而连续奖励RL虽然也能提高事实性但会导致质量下降。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [282] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 该调查通过自主性级别(L0-L3)重新构建医学大语言模型评估框架，将现有基准与各层级允许的操作和风险对齐，推动领域从基于分数的声明转向可信、风险感知的临床使用证据。


<details>
  <summary>Details</summary>
Motivation: 医学大语言模型在标准基准上表现优异，但这些结果向临床工作流程中安全可靠性能的转移仍然存在挑战，需要更全面的评估框架。

Method: 采用自主性级别框架(L0-L3)，包括信息工具、信息转换与聚合、决策支持和监督代理，将现有基准和指标与各层级的允许操作和风险对齐。

Result: 提出了基于层级的评估蓝图，用于选择指标、收集证据和报告声明，并将评估与监管联系起来。

Conclusion: 通过以自主性为中心，该调查推动领域超越基于分数的声明，转向为真实临床使用提供可信、风险感知的证据。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [283] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 该论文提出了FARE（基础自动推理评估器），通过大规模数据驱动的方法训练了8B和20B参数的评估器，在多个推理评估任务中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成式评估器的研究主要关注新方法（如强化学习），而忽视了大规模数据驱动的发展。本文旨在通过数据扩展来改进评估器的性能。

Method: 收集了250万个样本，涵盖五种评估任务，使用简单的迭代拒绝采样监督微调方法训练了8B和20B参数的评估器。

Result: FARE-8B挑战了更大的专门RL训练评估器，FARE-20B为开源评估器设立了新标准，超越了专门的70B+评估器。在真实任务中表现出色，如推理时重排序和RL训练中的验证器。

Conclusion: 大规模数据驱动的方法能够显著提升评估器的性能，FARE在多个评估任务中表现出色，为自动推理评估提供了新的解决方案。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [284] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: EDR是一个多智能体系统，通过主规划智能体、四个专业搜索智能体、可扩展工具生态系统、可视化智能体和反思机制，实现企业级深度研究自动化，在开放基准测试中超越现有系统。


<details>
  <summary>Details</summary>
Motivation: 企业面临将非结构化数据转化为可操作见解的压力，现有自主智能体在领域特定细微差别、意图对齐和企业集成方面存在困难。

Method: 集成主规划智能体进行自适应查询分解，四个专业搜索智能体（通用、学术、GitHub、LinkedIn），基于MCP的可扩展工具生态系统支持NL2SQL、文件分析和工作流，可视化智能体提供数据驱动见解，反思机制检测知识差距并更新研究方向。

Result: 在DeepResearch Bench和DeepConsult等开放基准测试中，EDR在无需人工干预的情况下优于最先进的智能体系统，验证了自动化报告生成、实时流式传输和企业部署能力。

Conclusion: EDR框架和多智能体推理应用推进了企业深度研究的发展，发布了代码和基准轨迹以促进相关研究。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [285] [Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)](https://arxiv.org/abs/2510.16334)
*Eden Shaveet,Crystal Su,Daniel Hsu,Luis Gravano*

Main category: cs.IR

TL;DR: 本研究分析了Yelp评论中的食源性疾病信号与纽约市官方餐厅检查结果之间的相关性，发现在人口普查区层面两者相关性很弱。


<details>
  <summary>Details</summary>
Motivation: 食源性疾病是严重的公共卫生问题，餐厅是调查疫情的关键场所。社交媒体上的用户生成内容可以提供及时的公共卫生信号，而官方报告渠道有限。

Method: 使用分层S型注意力网络(HSAN)分类器分析Yelp评论信号，并与纽约市卫生局2023年的官方餐厅检查结果进行比较，在人口普查区层面评估相关性。

Result: 在人口普查区层面，HSAN信号与检查分数之间相关性极小，且C级餐厅数量不同的区域在HSAN得分分布上没有显著差异。

Conclusion: 社交媒体信号与官方检查结果在宏观层面相关性较弱，需要进一步进行地址级别的分析来探索两者关系。

Abstract: Foodborne illnesses are gastrointestinal conditions caused by consuming
contaminated food. Restaurants are critical venues to investigate outbreaks
because they share sourcing, preparation, and distribution of foods. Public
reporting of illness via formal channels is limited, whereas social media
platforms host abundant user-generated content that can provide timely public
health signals. This paper analyzes signals from Yelp reviews produced by a
Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with
official restaurant inspection outcomes issued by the New York City Department
of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at
the Census tract level, compare distributions of HSAN scores by prevalence of
C-graded restaurants, and map spatial patterns across NYC. We find minimal
correlation between HSAN signals and inspection scores at the tract level and
no significant differences by number of C-graded restaurants. We discuss
implications and outline next steps toward address-level analyses.

</details>


### [286] [Blending Learning to Rank and Dense Representations for Efficient and Effective Cascades](https://arxiv.org/abs/2510.16393)
*Franco Maria Nardini,Raffaele Perego,Nicola Tonellotto,Salvatore Trani*

Main category: cs.IR

TL;DR: 本文提出了一种结合词汇和神经相关性信号的混合检索方法，使用学习排序模型融合253个手工特征和密集神经表示，在保持效率的同时显著提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效结合词汇和神经两种不同类型的相关性信号来提升检索性能，解决单一方法可能存在的局限性。

Method: 采用两阶段流水线架构：第一阶段使用密集神经检索器进行近邻搜索，第二阶段使用基于决策树森林的学习排序模型重新排序候选结果，融合253个手工词汇特征和神经表示。

Result: 在公开数据集上的实验表明，该方法显著提升了端到端排序性能，nDCG@10提升高达11%，而平均查询延迟仅增加4.3%。

Conclusion: 无缝结合两种不同信号家族能够相互补充，显著提升检索效果，同时相对最小化对效率的影响。

Abstract: We investigate the exploitation of both lexical and neural relevance signals
for ad-hoc passage retrieval. Our exploration involves a large-scale training
dataset in which dense neural representations of MS-MARCO queries and passages
are complemented and integrated with 253 hand-crafted lexical features
extracted from the same corpus. Blending of the relevance signals from the two
different groups of features is learned by a classical Learning-to-Rank (LTR)
model based on a forest of decision trees. To evaluate our solution, we employ
a pipelined architecture where a dense neural retriever serves as the first
stage and performs a nearest-neighbor search over the neural representations of
the documents. Our LTR model acts instead as the second stage that re-ranks the
set of candidates retrieved by the first stage to enhance effectiveness. The
results of reproducible experiments conducted with state-of-the-art dense
retrievers on publicly available resources show that the proposed solution
significantly enhances the end-to-end ranking performance while relatively
minimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of
up to 11% with an increase in average query latency of only 4.3%. This confirms
the advantage of seamlessly combining two distinct families of signals that
mutually contribute to retrieval effectiveness.

</details>


### [287] [FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation](https://arxiv.org/abs/2510.16597)
*Qiyao Peng,Chen Wang,Yinghui Wang,Hongtao Liu,Xuan Guo,Wenjun Wang*

Main category: cs.IR

TL;DR: 提出了FRONTIER-RevRec数据集，这是一个基于Frontiers开放获取平台真实同行评审记录的大规模审稿人推荐基准数据集，包含17.8万审稿人和47.8万论文，覆盖多个学科。


<details>
  <summary>Details</summary>
Motivation: 解决审稿人推荐研究缺乏高质量基准数据集的问题，现有数据集在规模、学科范围和比较分析方面存在局限。

Method: 构建了基于Frontiers平台2007-2025年真实同行评审记录的大规模数据集，包含多学科期刊数据，并进行了内容基方法与协同过滤方法的系统性比较。

Result: 内容基方法显著优于协同过滤，语言模型方法在捕捉论文内容与审稿人专业知识的语义对齐方面特别有效，并确定了优化推荐流程的最佳聚合策略。

Conclusion: FRONTIER-RevRec可作为审稿人推荐研究的综合基准，推动更有效的学术同行评审系统发展。

Abstract: Reviewer recommendation is a critical task for enhancing the efficiency of
academic publishing workflows. However, research in this area has been
persistently hindered by the lack of high-quality benchmark datasets, which are
often limited in scale, disciplinary scope, and comparative analyses of
different methodologies. To address this gap, we introduce FRONTIER-RevRec, a
large-scale dataset constructed from authentic peer review records (2007-2025)
from the Frontiers open-access publishing platform
https://www.frontiersin.org/. The dataset contains 177941 distinct reviewers
and 478379 papers across 209 journals spanning multiple disciplines including
clinical medicine, biology, psychology, engineering, and social sciences. Our
comprehensive evaluation on this dataset reveals that content-based methods
significantly outperform collaborative filtering. This finding is explained by
our structural analysis, which uncovers fundamental differences between
academic recommendation and commercial domains. Notably, approaches leveraging
language models are particularly effective at capturing the semantic alignment
between a paper's content and a reviewer's expertise. Furthermore, our
experiments identify optimal aggregation strategies to enhance the
recommendation pipeline. FRONTIER-RevRec is intended to serve as a
comprehensive benchmark to advance research in reviewer recommendation and
facilitate the development of more effective academic peer review systems. The
FRONTIER-RevRec dataset is available at:
https://anonymous.4open.science/r/FRONTIER-RevRec-5D05.

</details>


### [288] [Right Answer at the Right Time - Temporal Retrieval-Augmented Generation via Graph Summarization](https://arxiv.org/abs/2510.16715)
*Zulun Zhu,Haoyu Liu,Mengke He,Siqiang Luo*

Main category: cs.IR

TL;DR: STAR-RAG是一个时间知识图谱问答框架，通过构建时间对齐的规则图进行传播，实现时间一致且高效的检索。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要基于语义，通常忽略显式时间约束，导致时间不一致的答案和过高的token使用量。

Method: 构建时间对齐的规则图，并在该图上进行传播以缩小搜索空间，优先考虑语义相关且时间一致的证据。

Result: 在真实世界时间KG数据集上的实验表明，该方法在消耗更少token的同时实现了更高的答案准确性。

Conclusion: STAR-RAG无需繁重的模型训练和微调，降低了计算成本并显著简化了部署，在保持准确性的同时减少了token消耗。

Abstract: Question answering in temporal knowledge graphs requires retrieval that is
both time-consistent and efficient. Existing RAG methods are largely semantic
and typically neglect explicit temporal constraints, which leads to
time-inconsistent answers and inflated token usage. We propose STAR-RAG, a
temporal GraphRAG framework that relies on two key ideas: building a
time-aligned rule graph and conducting propagation on this graph to narrow the
search space and prioritize semantically relevant, time-consistent evidence.
This design enforces temporal proximity during retrieval, reduces the candidate
set of retrieval results, and lowers token consumption without sacrificing
accuracy. Compared with existing temporal RAG approaches, STAR-RAG eliminates
the need for heavy model training and fine-tuning, thereby reducing
computational cost and significantly simplifying deployment.Extensive
experiments on real-world temporal KG datasets show that our method achieves
improved answer accuracy while consuming fewer tokens than strong GraphRAG
baselines.

</details>


### [289] [Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices](https://arxiv.org/abs/2510.16736)
*Patrizio Dazzi,William Guglielmo,Franco Maria Nardini,Raffaele Perego,Salvatore Trani*

Main category: cs.IR

TL;DR: 该论文提出两种基于FPGA的能效精确kNN搜索方案，在吞吐量、延迟和能耗方面均优于CPU方案


<details>
  <summary>Details</summary>
Motivation: 支持基于神经编码器模型的表示学习大规模应用，使其更环保和包容

Method: 提出两种FPGA解决方案：第一种通过并行处理批量查询来最大化系统吞吐量；第二种通过并行处理内存数据集中的查询来最小化延迟

Result: FPGA方案在查询吞吐量上达到最佳，延迟降低高达16.6倍，能耗节省高达11.9倍

Conclusion: FPGA设备在高维潜在空间中进行精确kNN搜索具有显著的能效优势

Abstract: This paper investigates the usage of FPGA devices for energy-efficient exact
kNN search in high-dimension latent spaces. This work intercepts a relevant
trend that tries to support the increasing popularity of learned
representations based on neural encoder models by making their large-scale
adoption greener and more inclusive. The paper proposes two different
energy-efficient solutions adopting the same FPGA low-level configuration. The
first solution maximizes system throughput by processing the queries of a batch
in parallel over a streamed dataset not fitting into the FPGA memory. The
second minimizes latency by processing each kNN incoming query in parallel over
an in-memory dataset. Reproducible experiments on publicly available image and
text datasets show that our solution outperforms state-of-the-art CPU-based
competitors regarding throughput, latency, and energy consumption.
Specifically, experiments show that the proposed FPGA solutions achieve the
best throughput in terms of queries per second and the best-observed latency
with scale-up factors of up to 16.6X. Similar considerations can be made
regarding energy efficiency, where results show that our solutions can achieve
up to 11.9X energy saving w.r.t. strong CPU-based competitors.

</details>


### [290] [An Efficient Framework for Whole-Page Reranking via Single-Modal Supervision](https://arxiv.org/abs/2510.16803)
*Zishuai Zhang,Sihao Yu,Wenyi Xie,Ying Nie,Junfeng Wang,Zhiming Zheng,Dawei Yin,Hainan Zhang*

Main category: cs.IR

TL;DR: SMAR是一个新颖的全页重排框架，利用强大的单模态排序器指导模态间相关性对齐，仅使用有限的全页标注就能超越完全标注的重排模型，显著降低标注成本70-90%。


<details>
  <summary>Details</summary>
Motivation: 全页重排在搜索引擎用户体验中至关重要，但现有方法依赖大规模人工标注，成本高且耗时。全页标注比单模态标注复杂得多，需要评估整个结果页面并考虑跨模态相关性差异。

Method: 首先针对各自模态训练高质量单模态排序器，然后为每个查询选择其输出的子集构建候选页面并进行页面级人工标注，最后使用这些有限标注训练全页重排器，并强制与单模态偏好保持一致以维持各模态内的排序质量。

Result: 在Qilin和Baidu数据集上的实验表明，SMAR减少约70-90%的标注成本，同时相比基线实现显著的排序改进。在百度APP上的离线和在线A/B测试也显示标准排序指标和用户体验指标的显著提升。

Conclusion: SMAR框架在实际搜索场景中验证了其有效性和实用价值，能够在显著降低标注成本的同时提升重排性能。

Abstract: The whole-page reranking plays a critical role in shaping the user experience
of search engines, which integrates retrieval results from multiple modalities,
such as documents, images, videos, and LLM outputs. Existing methods mainly
rely on large-scale human-annotated data, which is costly to obtain and
time-consuming. This is because whole-page annotation is far more complex than
single-modal: it requires assessing the entire result page while accounting for
cross-modal relevance differences. Thus, how to improve whole-page reranking
performance while reducing annotation costs is still a key challenge in
optimizing search engine result pages(SERP). In this paper, we propose SMAR, a
novel whole-page reranking framework that leverages strong Single-modal rankers
to guide Modal-wise relevance Alignment for effective Reranking, using only
limited whole-page annotation to outperform fully-annotated reranking models.
Specifically, high-quality single-modal rankers are first trained on data
specific to their respective modalities. Then, for each query, we select a
subset of their outputs to construct candidate pages and perform human
annotation at the page level. Finally, we train the whole-page reranker using
these limited annotations and enforcing consistency with single-modal
preferences to maintain ranking quality within each modality. Experiments on
the Qilin and Baidu datasets demonstrate that SMAR reduces annotation costs by
about 70-90\% while achieving significant ranking improvements compared to
baselines. Further offline and online A/B testing on Baidu APPs also shows
notable gains in standard ranking metrics as well as user experience
indicators, fully validating the effectiveness and practical value of our
approach in real-world search scenarios.

</details>


### [291] [The Layout Is the Model: On Action-Item Coupling in Generative Recommendation](https://arxiv.org/abs/2510.16804)
*Xiaokai Wei,Jiajun Wu,Daiyao Yi,Reza Shirkavand,Michelle Gong*

Main category: cs.IR

TL;DR: 本文研究了生成式推荐模型中的token布局设计，提出了基于三项设计原则的统一框架，并开发了新颖的非交错布局方法LAC，在保持准确性的同时显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型将用户交互历史视为序列进行自回归预测，但物品和动作token的布局（排序和可见性）严重影响模型的信息利用和泛化能力，需要系统性的布局设计原则。

Method: 提出了基于三项设计原则的统一框架：(P1)最大化输入/输出空间的物品/动作信号，(P2)保持"给定物品的动作"条件关系，(P3)无信息泄露。设计了新颖的非交错布局方法LAC（延迟动作条件化）。

Result: 在公共数据集和大规模生产日志上的综合实验验证了设计原则。提出的LAC方法在显著降低FLOPs的同时，达到了与交错布局相当或更优的质量。

Conclusion: 研究结果为构建既准确又高效的生成式推荐系统提供了可行的指导，LAC方法在计算效率和模型质量之间取得了良好平衡。

Abstract: Generative Recommendation (GR) models treat a user's interaction history as a
sequence to be autoregressively predicted. When both items and actions (e.g.,
watch time, purchase, comment) are modeled, the layout-the ordering and
visibility of item/action tokens-critically determines what information the
model can use and how it generalizes. We present a unified study of token
layouts for GR grounded in first principles: (P1) maximize item/action signal
in both input/output space, (P2) preserve the conditioning relationship "action
given item" and (P3) no information leakage.
  While interleaved layout (where item and action occupy separate tokens)
naturally satisfies these principles, it also bloats sequence length with
larger training/inference cost. On the non-interleaved front, we design a novel
and effective approach, Lagged Action Conditioning (LAC), which appears strange
on the surface but aligns well with the design principles to yield strong
accuracy. Comprehensive experiments on public datasets and large-scale
production logs evaluate different layout options and empirically verifies the
design principles. Our proposed non-interleaved method, LAC, achieves
competitive or superior quality at substantially lower FLOPs than interleaving.
Our findings offer actionable guidance for assembling GR systems that are both
accurate and efficient.

</details>


### [292] [Towards Context-aware Reasoning-enhanced Generative Searching in E-commerce](https://arxiv.org/abs/2510.16925)
*Zhiding Liu,Ben Chen,Mingyue Cheng,Enchong Chen,Li Li,Chenyi Lei,Wenwu Ou,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 提出了一种上下文感知的推理增强生成搜索框架，通过统一异构上下文信息、自演化的后训练范式以及去偏的强化学习算法，显著提升了电商搜索推荐性能。


<details>
  <summary>Details</summary>
Motivation: 电商搜索推荐中，用户的复杂搜索上下文（时空因素、历史交互、当前查询信息）包含重要的决策偏好，但现有方法在整合这些上下文信息方面存在局限，无法充分捕捉用户意图。

Method: 1. 将异构用户和物品上下文统一为文本表示或基于文本的语义标识符并对其对齐；2. 引入自演化的后训练范式，迭代结合监督微调和强化学习来增强模型推理能力；3. 提出去偏的GRPO变体来改进排序性能。

Result: 在真实电商平台的搜索日志数据上进行广泛实验，证明该方法相比强基线实现了更优越的性能，验证了其在基于搜索的推荐中的有效性。

Conclusion: 所提出的上下文感知推理增强生成搜索框架能够有效理解和利用复杂的搜索上下文，显著提升电商搜索推荐系统的性能。

Abstract: Search-based recommendation is one of the most critical application scenarios
in e-commerce platforms. Users' complex search contexts--such as spatiotemporal
factors, historical interactions, and current query's information--constitute
an essential part of their decision-making, reflecting implicit preferences
that complement explicit query terms. Modeling such rich contextual signals and
their intricate associations with candidate items remains a key challenge.
Although numerous efforts have been devoted to building more effective search
methods, existing approaches still show limitations in integrating contextual
information, which hinders their ability to fully capture user intent.
  To address these challenges, we propose a context-aware reasoning-enhanced
generative search framework for better \textbf{understanding the complicated
context}. Specifically, the framework first unifies heterogeneous user and item
contexts into textual representations or text-based semantic identifiers and
aligns them. To overcome the lack of explicit reasoning trajectories, we
introduce a self-evolving post-training paradigm that iteratively combines
supervised fine-tuning and reinforcement learning to progressively enhance the
model's reasoning capability. In addition, we identify potential biases in
existing RL algorithms when applied to search scenarios and present a debiased
variant of GRPO to improve ranking performance. Extensive experiments on search
log data collected from a real-world e-commerce platform demonstrate that our
approach achieves superior performance compared with strong baselines,
validating its effectiveness for search-based recommendation.

</details>


### [293] [DSEBench: A Test Collection for Explainable Dataset Search with Examples](https://arxiv.org/abs/2510.17228)
*Qing Shi,Jing He,Qiaosheng Chen,Gong Cheng*

Main category: cs.IR

TL;DR: 本文提出了可解释的数据集搜索（Explainable DSE）任务，构建了DSEBench测试集，并评估了多种检索和解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集搜索方法要么基于关键词查询，要么基于目标数据集相似性，需要结合这两种信息需求的统一框架。

Method: 构建DSEBench测试集，使用大语言模型生成训练标注，评估稀疏、稠密和基于LLM的检索、重排和解释方法。

Result: 建立了可解释DSE任务的基准，提供了数据集和字段级别的标注，并评估了多种方法的性能。

Conclusion: 可解释的数据集搜索是一个重要且有前景的研究方向，DSEBench为相关研究提供了基础。

Abstract: Dataset search has been an established information retrieval task. Current
paradigms either retrieve datasets that are relevant to a keyword query or find
datasets that are similar to an input target dataset. To allow for their
combined specification of information needs, in this article, we investigate
the more generalized task of Dataset Search with Examples (DSE) and further
extend it to Explainable DSE that requires identifying the metadata and content
fields of a dataset that indicate its relevance to the query and similarity to
the target datasets. To facilitate this research, we construct DSEBench, a test
collection that provides high-quality dataset- and field-level annotations to
enable the evaluation of explainable DSE. We also employ a large language model
to generate numerous annotations to be used for training. We establish
extensive baselines on DSEBench by adapting and evaluating a variety of sparse,
dense, and LLM-based retrieval, reranking, and explanation methods.

</details>


### [294] [On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders](https://arxiv.org/abs/2510.17245)
*Wenyu Mao,Jiancan Wu,Guoqing Hu,Wei Ji,Xiang Wang*

Main category: cs.IR

TL;DR: TA-Rec是一个两阶段扩散模型框架，通过时间一致性正则化平滑去噪函数实现一步生成，并通过自适应偏好对齐缓解轨迹偏差，解决了扩散模型在序列推荐中计算效率与推荐效果之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在序列推荐中采用多步去噪过程，依赖离散近似会引入离散化误差，导致计算效率与推荐效果之间存在权衡。

Method: 提出两阶段框架：1）预训练阶段使用时间一致性正则化平滑去噪函数；2）微调阶段通过自适应偏好对齐将去噪过程与用户偏好对齐。

Result: 实验证明TA-Rec有效缓解了离散化误差引起的权衡问题，同时提升了扩散推荐器的效率和效果。

Conclusion: TA-Rec通过两阶段目标成功解决了扩散模型在序列推荐中的效率-效果权衡问题，实现了高效且有效的推荐生成。

Abstract: Diffusion models have emerged as a powerful paradigm for generative
sequential recommendation, which typically generate next items to recommend
guided by user interaction histories with a multi-step denoising process.
However, the multi-step process relies on discrete approximations, introducing
discretization error that creates a trade-off between computational efficiency
and recommendation effectiveness. To address this trade-off, we propose TA-Rec,
a two-stage framework that achieves one-step generation by smoothing the
denoising function during pretraining while alleviating trajectory deviation by
aligning with user preferences during fine-tuning. Specifically, to improve the
efficiency without sacrificing the recommendation performance, TA-Rec pretrains
the denoising model with Temporal Consistency Regularization (TCR), enforcing
the consistency between the denoising results across adjacent steps. Thus, we
can smooth the denoising function to map the noise as oracle items in one step
with bounded error. To further enhance effectiveness, TA-Rec introduces
Adaptive Preference Alignment (APA) that aligns the denoising process with user
preference adaptively based on preference pair similarity and timesteps.
Extensive experiments prove that TA-Rec's two-stage objective effectively
mitigates the discretization errors-induced trade-off, enhancing both
efficiency and effectiveness of diffusion-based recommenders.

</details>


### [295] [How role-play shapes relevance judgment in zero-shot LLM rankers](https://arxiv.org/abs/2510.17535)
*Yumeng Wang,Jirui Qi,Catherine Chen,Panagiotis Eustratiadis,Suzan Verberne*

Main category: cs.IR

TL;DR: 本文系统研究了角色扮演提示对零样本LLM排序器的影响，通过因果干预技术揭示了角色描述在早期层编码、与任务指令在中间层交互的机制，并识别了关键注意力头。


<details>
  <summary>Details</summary>
Motivation: 角色扮演提示能提升LLM排序的鲁棒性和准确性，但其作用机制和多样性尚未充分探索，限制了有效使用和可解释性。

Method: 采用机制可解释性的因果干预技术，追踪角色扮演信息如何影响LLM的相关性判断，分析不同层级的编码和交互模式。

Result: 发现角色描述对排序质量影响显著；角色信号主要在早期层编码，在中间层与任务指令交互，与查询/文档表征交互有限；识别了编码角色条件相关性的关键注意力头。

Conclusion: 研究揭示了角色扮演在LLM排序中的内部工作机制，为IR及其他领域设计更有效的提示提供了指导，展现了角色扮演在零样本应用中的广阔前景。

Abstract: Large Language Models (LLMs) have emerged as promising zero-shot rankers, but
their performance is highly sensitive to prompt formulation. In particular,
role-play prompts, where the model is assigned a functional role or identity,
often give more robust and accurate relevance rankings. However, the mechanisms
and diversity of role-play effects remain underexplored, limiting both
effective use and interpretability. In this work, we systematically examine how
role-play variations influence zero-shot LLM rankers. We employ causal
intervention techniques from mechanistic interpretability to trace how
role-play information shapes relevance judgments in LLMs. Our analysis reveals
that (1) careful formulation of role descriptions have a large effect on the
ranking quality of the LLM; (2) role-play signals are predominantly encoded in
early layers and communicate with task instructions in middle layers, while
receiving limited interaction with query or document representations.
Specifically, we identify a group of attention heads that encode information
critical for role-conditioned relevance. These findings not only shed light on
the inner workings of role-play in LLM ranking but also offer guidance for
designing more effective prompts in IR and beyond, pointing toward broader
opportunities for leveraging role-play in zero-shot applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [296] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个针对Lean和mathlib的语义搜索引擎，通过理解数学家意图来改进定理证明中的搜索效率。


<details>
  <summary>Details</summary>
Motivation: 现有Lean搜索引擎主要依赖形式化语句的非正式翻译，忽视了与现实用户查询的匹配问题，导致定理证明进展缓慢且费力。

Method: 分析并聚类公开Lean讨论的语义，在模拟用户意图的合成查询上微调文本嵌入，并使用多样化反馈信号使系统与数学家偏好对齐。

Result: 在真实世界查询、非正式化语句和证明状态上的评估显示，相比之前的搜索引擎和GPT-4o，Lean Finder实现了超过30%的相对改进。

Conclusion: Lean Finder是一个用户中心的语义搜索系统，能够有效提升Lean环境中的定理搜索效率，并与基于LLM的定理证明器兼容。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [297] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出了Gram行列式评分方法，用于在无法获取真实数据的情况下评估数据集的可靠性，该方法具有实验无关性，能有效衡量数据质量。


<details>
  <summary>Details</summary>
Motivation: 解决在无法访问真实数据的情况下，如何评估来自潜在策略性来源的数据集的可靠性问题。

Method: 定义了基于真实数据的可靠性排序，提出了Gram行列式评分方法，通过计算观测数据经验分布和实验结果向量张成的体积来衡量可靠性。

Result: Gram行列式评分保持了几种基于真实数据的可靠性排序，并且具有实验无关性，在不同实验中产生相同的可靠性排名。在合成噪声模型、CIFAR-10嵌入和真实就业数据上的实验验证了该方法的有效性。

Conclusion: Gram行列式评分是一种有效的数据集可靠性评估方法，能够在无法获取真实数据的情况下准确捕捉数据质量，且不受具体实验过程影响。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [298] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: LS-OGD是一种新颖的自适应控制框架，用于在概念漂移存在的情况下实现稳健的多模态学习。它通过在线控制器动态调整学习率和模态融合权重，确保系统在概念漂移下的稳定性和容错能力。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中容易因概念漂移而性能下降，特别是模态特定的漂移和缺乏连续稳定适应机制的问题。

Method: 使用LS-OGD框架，包含在线控制器动态调整模型学习率和不同数据模态之间的融合权重，响应检测到的漂移和预测误差变化。

Result: 在有限漂移条件下，LS-OGD系统的预测误差被证明是统一最终有界的，如果漂移停止则收敛到零。自适应融合策略能有效隔离和减轻严重模态特定漂移的影响。

Conclusion: 这些理论保证为开发可靠且持续自适应的多模态学习系统建立了原则性基础。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [299] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 该论文研究了Hedge算法在组合设置中的最优性问题，证明了Hedge在大多数组合设置中接近最优，但在某些特定设置（如m-集合）中次优，同时确定了在线多任务学习中Hedge的最优性，并利用这一结果建立了有向无环图中在线最短路径问题的近最优正则化器。


<details>
  <summary>Details</summary>
Motivation: 研究Hedge算法在组合设置中的最优性，探索其在各种组合问题（包括扩展形式博弈、资源分配、m-集合、在线多任务学习和DAG最短路径问题）中的性能边界。

Method: 通过建立下界分析Hedge的最优性，识别特定组合类（m-集合）进行精确分析，并利用Hedge的接近最优性为DAG最短路径问题构建近最优正则化器。

Result: 证明Hedge在任意组合设置中接近最优（最多相差√log d因子），在m-集合设置中次优√log d因子，但在在线多任务学习中最优，并成功构建了DAG最短路径问题的近最优正则化器。

Conclusion: Hedge算法在组合设置中具有广泛的最优性，为组合在线学习提供了理论基础，并为DAG最短路径等组合问题提供了有效的算法解决方案。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [300] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一个基于贝叶斯学习的自适应采样框架，通过实时更新奖励分布的后验信念来决定何时停止生成新样本，在保持响应质量的同时显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 多响应采样是提高LLM输出质量的常用方法，但会带来额外的计算成本。关键挑战是如何平衡准确率提升与效率，决定何时停止生成新样本。

Method: 基于序列搜索与贝叶斯学习，BEACON顺序生成策略LLM的响应，实时更新奖励分布的后验信念，通过权衡预期收益与计算成本来决定停止时机。

Result: BEACON将平均采样次数减少高达80%，同时保持响应质量。在成本高效的偏好数据生成方面也表现出实用性。

Conclusion: BEACON提供了理论最优性保证和实际可操作性，为未来研究者提供了可行的见解，能够有效平衡LLM输出质量与计算效率。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [301] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: PatMD通过识别和主动缓解误判风险模式，提升有害表情包检测能力，在6,626个表情包的基准测试中平均F1分数提升8.30%，准确率提升7.71%。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法（包括MLLM技术）难以处理表情包中通过讽刺、隐喻等修辞手法表达的隐含有害内容，导致频繁误判。

Method: 构建误判风险模式知识库，将每个表情包解构为误判风险模式，然后检索相关模式动态指导MLLM推理，避免已知误判陷阱。

Result: 在5个有害检测任务的6,626个表情包基准测试中，PatMD优于最先进基线方法，F1分数平均提升8.30%，准确率平均提升7.71%。

Conclusion: PatMD通过主动识别和缓解误判风险模式，显著提升了有害表情包的检测能力，具有强大的泛化性和改进的检测性能。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [302] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 本研究提出基于WaveNet的深度学习模型，用于自动分类EEG信号为生理、病理、伪影和噪声类别，在公开数据集上取得优于CNN和LSTM方法的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家视觉检查的EEG信号分类方法在处理日益复杂和大量的EEG记录时变得不切实际，需要自动化解决方案。

Method: 使用WaveNet架构，利用扩张因果卷积和残差连接处理EEG数据，在Mayo Clinic和St. Anne's大学医院的公开数据集上训练、验证和测试209,232个样本。

Result: 模型分类准确率超过之前的CNN和LSTM方法，能高精度区分噪声和伪影，但在生理和病理信号之间存在可解释的误分类，反映了临床固有的重叠性。

Conclusion: WaveNet架构因其能够捕捉细粒度和长程时间依赖性，非常适合EEG数据分析，为EEG信号自动分类提供了有效解决方案。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [303] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 提出基于击键动力学的帕金森病筛查方法，使用深度学习模型在外部验证中达到91.14%的AUC-ROC，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，传统临床评估受限。击键动力学可作为非侵入性、可扩展的生物标志物进行远程筛查和监测。

Method: 三阶段流程：数据预处理（处理4个数据集，提取时间信号，解决类别不平衡）；预训练8种深度学习架构；在中等数据集微调并在独立队列进行外部验证。

Result: 混合卷积-循环和基于Transformer的模型表现优异，外部验证AUC-ROC超过90%，F1分数超过70%。时序卷积模型外部验证AUC-ROC达91.14%。

Conclusion: 击键动力学是可靠的帕金森病数字生物标志物，为早期检测和持续监测提供了有前景的途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [304] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究了基于扩散模型的Ensemble Score Filter (EnSF)算法在野火蔓延数据同化中的应用，展示了其在实时野火预测中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着野火破坏性增强和控制成本上升，需要准确、实时的火势蔓延预测。数据同化通过整合观测数据和数值模型预测，对提高野火预测准确性至关重要。

Method: 采用基于扩散模型的Ensemble Score Filter (EnSF)算法，该算法利用基于分数的生成扩散模型，在解决高维非线性滤波问题方面表现出色。

Result: 数值研究表明，EnSF在准确性、稳定性和计算效率方面均表现优异，为野火数据同化提供了稳健实用的方法。

Conclusion: EnSF算法是野火数据同化问题的有效解决方案，具有卓越的性能和实际应用价值，相关代码已公开。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [305] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: LLMs处理工具返回的复杂JSON响应能力不足，研究发现JSON处理对前沿模型仍是挑战，最佳处理策略取决于输出性质和大小以及推理复杂度。


<details>
  <summary>Details</summary>
Motivation: 现实任务自动化需要LLMs调用工具并处理复杂JSON响应，但LLMs处理结构化响应的能力研究不足。

Method: 创建数据集评估15个开源和闭源模型，使用多种提示方法研究JSON处理能力。

Result: JSON处理对前沿模型仍是困难任务，不同处理方法的性能差异可达3%到50%。

Conclusion: 最佳响应处理策略取决于工具输出的性质和大小以及所需推理的复杂度。

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [306] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 该研究通过热解技术将食物基生物质转化为可持续能源，重点关注人工智能在优化热解过程和氢产量预测中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索未充分利用的生物质资源（如咖啡渣和枣核）用于可持续制氢的潜力，推动可持续能源和废物管理策略的发展。

Method: 对纯枣核、咖啡渣及其混合物进行多种分析（工业分析、元素分析、纤维分析、热重分析等），使用等转化率方法进行动力学建模，并训练LSTM模型预测热重曲线。

Result: 混合物3具有最高的氢产量潜力但活化能最高（313.24 kJ/mol），混合物1具有最佳活化能值（161.75 kJ/mol）。KAS方法被确定为最准确的动力学模型，LSTM模型预测热重曲线的准确度极高（R²: 0.9996-0.9998）。

Conclusion: 人工智能集成显著提高了热解过程建模的准确性和优化效率，为可持续氢生产提供了有效的技术途径。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [307] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出LAMI框架，通过图-语言联合建模检测青少年非法药物使用并解释行为风险因素，在YRBS和NSDUH数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有建模方法将调查变量独立处理，忽略了变量间的潜在关联结构，需要更有效的方法来检测青少年非法药物使用并理解相关风险因素。

Method: LAMI框架将个体响应表示为关系图，通过专门的图结构学习层学习潜在连接，并集成大语言模型生成基于图结构和调查语义的自然语言解释。

Result: 在YRBS和NSDUH数据集上的实验表明，LAMI在预测准确性上优于竞争基线方法。可解释性分析显示LAMI能揭示有意义的行为子结构和心理社会路径。

Conclusion: LAMI框架能够有效检测青少年非法药物使用，并通过揭示家庭动态、同伴影响和学校相关压力等风险因素，为理解药物使用行为提供了有价值的见解。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [308] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: CTR-LoRA是一个基于曲率信任区域的参数高效微调框架，通过秩调度和稳定性感知优化，在多个7B-13B模型上实现了比现有PEFT方法更好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法虽然通过低秩更新、量化或启发式预算分配提高了效率，但往往将容量分配与训练过程中的更新演化分离开来，缺乏系统性的优化策略。

Method: CTR-LoRA基于曲率信任区域框架，通过轻量级二阶代理计算边际效用来分配参数，并使用Fisher/Hessian度量信任区域来约束更新。

Result: 在多个开源骨干模型(7B-13B)上，在分布内和分布外基准测试中都显示出相对于强PEFT基线的持续改进，提高了准确性、训练稳定性，减少了内存需求，并实现了更高的吞吐量。

Conclusion: CTR-LoRA在性能和效率的帕累托前沿上占据优势，为更鲁棒和可部署的PEFT提供了一条原则性路径。

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [309] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出了BPL框架，通过双重蒸馏策略在事实和反事实测试环境中实现高性能推荐系统


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差问题，现有方法在反事实测试环境中表现良好但在事实测试环境中准确率显著下降，需要一种能在两种测试环境中都表现良好的模型

Method: 使用偏置自适应偏好蒸馏学习框架，包含教师-学生蒸馏和带可靠性过滤的自蒸馏策略

Result: 在事实和反事实测试中都验证了BPL的有效性

Conclusion: BPL框架能够逐步揭示用户偏好，在两种测试环境中都实现高性能

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [310] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 提出Long Exposure系统，通过解决微调中的Shadowy Sparsity问题来加速参数高效微调(PEFT)，实现最高2.49倍的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调技术虽然重要，但在时间和成本方面效率低下，特别是微调过程中出现的Shadowy Sparsity问题尚未得到充分解决。

Method: Long Exposure系统包含三个组件：Shadowy-sparsity Exposer使用长感知范围捕获更多稀疏细节；Sequence-oriented Predictor处理大序列输入和动态参数；Dynamic-aware Operator优化计算模式和内存访问。

Result: 实验表明，Long Exposure在端到端微调中比现有技术快达2.49倍。

Conclusion: Long Exposure为加速LLMs的PEFT提供了有前景的进展，有效解决了微调中的效率瓶颈问题。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [311] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种针对大型推理模型的死锁攻击方法，通过训练恶意对抗嵌入诱导模型陷入无限推理循环，阻止其输出最终答案。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型的链式思维推理机制引入了新的安全漏洞，攻击者可以劫持生成控制流导致资源耗尽。

Method: 使用优化的对抗嵌入鼓励推理步骤后的过渡标记，结合后门植入策略克服连续到离散的投影差距问题。

Result: 在四个先进LRM和三个数学推理基准测试中达到100%攻击成功率，迫使模型生成最大令牌限制。

Conclusion: 揭示了大型推理模型在推理效率方面的关键安全漏洞，该攻击具有隐蔽性且对现有缓解策略具有鲁棒性。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [312] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出了一种分辨率感知的检索增强预测模型，通过利用空间相关性和时间频率特征来提高零样本预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 零样本预测需要在没有直接历史数据的情况下预测未见条件下的结果，这对传统预测方法构成重大挑战。

Method: 将信号分解为不同频率分量，采用分辨率感知检索：低频分量依赖更广泛的空间上下文，高频分量关注局部影响，从而动态检索相关数据并适应新位置。

Result: 在微气候预测中，该模型显著优于传统预测方法、数值天气预报模型和现代基础时间序列模型，在ERA5数据集上比HRRR的MSE降低71%，比Chronos降低34%。

Conclusion: 检索增强和分辨率感知策略在零样本预测中非常有效，为微气候建模及其他领域提供了可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [313] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出Gains方法解决开放世界联邦学习中的新知识发现和适应问题，通过细粒度知识发现和贡献驱动聚合技术，在保持源域性能的同时有效整合新知识。


<details>
  <summary>Details</summary>
Motivation: 现实世界联邦学习中新客户端不断加入带来新知识，现有方法存在知识发现粒度粗、牺牲源域性能和适应效率低等问题。

Method: 将模型分为编码器和分类器，基于特征对域偏移敏感、分类器对类别增量敏感的特性，开发细粒度知识发现、贡献驱动聚合和抗遗忘机制。

Result: 在三种典型数据偏移场景的多域数据集上，Gains在源域和目标域客户端性能上均显著优于其他基线方法。

Conclusion: Gains通过细粒度知识发现和平衡适应机制，有效解决了开放世界联邦学习中的新知识整合问题。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [314] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: 提出SAU-FNO框架，结合自注意力机制、U-Net和FNO，用于3D IC热管理，实现842倍加速和更高精度。


<details>
  <summary>Details</summary>
Motivation: 3D IC热管理因功率密度增加而愈发困难，传统PDE方法准确但速度慢，机器学习方法如FNO存在高频信息丢失和高保真数据依赖问题。

Method: 结合自注意力和U-Net与FNO，捕捉长程依赖和局部高频特征，使用迁移学习微调低保真数据以减少高保真数据需求。

Result: SAU-FNO在热预测精度上达到最先进水平，相比传统FEM方法实现842倍加速。

Conclusion: SAU-FNO是3D IC热模拟的高效工具，平衡了精度和速度。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [315] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出了一个用户反馈模拟框架和综合基准，用于评估LLM系统的持续学习能力，发现现有方法的有效性不足。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据枯竭和计算资源边际效益递减，需要从人类和传统AI系统的实践学习能力中获得灵感，构建LLM系统的记忆和持续学习框架。

Method: 开发用户反馈模拟框架和覆盖多领域、多语言、多任务类型的综合基准，用于评估LLM系统的持续学习能力。

Result: 实验表明，现有最先进基线的有效性和效率远未达到满意水平。

Conclusion: 该基准可为未来LLM记忆和优化算法的研究铺平道路。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [316] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: LinearizeLLM是一个基于代理的框架，利用大语言模型自动将非线性优化问题重新表述为线性形式，使问题能够用线性优化求解器解决。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的重新表述通常需要人工操作且依赖专家知识，这限制了使用线性优化求解器或专用算法解决此类问题的效率。

Method: 为每种非线性模式分配专门的重新表述代理，这些代理被明确指示为其非线性模式推导精确的线性重新表述，然后协调组装等价于原始问题的求解器就绪线性模型。

Result: 在从ComplexOR数据集衍生的20个真实世界非线性优化问题上进行测试，结果表明专门的LLM代理可以自动化线性化任务。

Conclusion: 该方法为非线性优化问题开辟了完全对话式建模管道的路径，能够显著提高问题重新表述的自动化程度。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [317] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 本文探讨了训练数据的几何结构对机器学习模型性能的影响，提出使用持久同调来量化数据多样性，超越传统的基于熵的度量方法。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据是机器学习和人工智能的基础，但数据的几何结构对模型性能的影响尚未充分探索。作者认为数据的表示丰富性和冗余消除对学习结果至关重要。

Method: 采用持久同调方法从度量空间中的数据提取拓扑特征，为量化数据多样性提供原则性方法。

Result: 研究发现持久同调是分析和增强驱动AI系统的训练数据的强大工具。

Conclusion: 持久同调为理解训练数据的几何结构提供了新的视角，能够有效提升AI系统的性能。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [318] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: PALE框架通过提示引导的数据增强和对比马氏距离评分，有效检测大语言模型的幻觉问题，无需人工标注即可实现优越的检测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在产生误导或虚构信息的幻觉问题，而幻觉检测面临标注数据稀缺的挑战，需要开发低成本、无需人工标注的检测方法。

Method: 提出PALE框架，利用提示引导从LLMs生成真实和幻觉数据作为增强，并引入对比马氏距离评分来评估中间嵌入向量的真实性。

Result: 实验表明PALE在幻觉检测性能上显著优于基线方法，提升幅度达6.55%。

Conclusion: PALE提供了一种无需人工标注、具有强泛化性和实用性的幻觉检测解决方案，能有效提升LLM生成内容的可靠性。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [319] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出了一种级联方法，将预训练OVD模型与轻量级少样本分类器结合，通过FLAME主动学习策略选择信息量最大的样本进行训练，实现快速适应特定用户需求。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测模型在遥感等专业领域存在自然语言歧义问题，难以区分细粒度类别，影响下游应用效果。

Method: 首先使用零样本模型生成高召回率目标建议，然后通过少量用户标注样本训练的紧凑分类器进行精确度提升，核心是FLAME主动学习策略。

Result: 在遥感基准测试中持续超越最先进方法，实现快速适应（不到一分钟），显著优于现有替代方案。

Conclusion: 建立了一个实用且资源高效的框架，使基础模型能够适应特定用户需求，大幅降低遥感图像标注成本。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [320] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: 提出了DAWP框架，通过人工智能数据同化(AIDA)模块将AI天气预测从再分析数据解放到观测空间，实现基于不规则观测数据的全球天气预报


<details>
  <summary>Details</summary>
Motivation: 传统AI天气预测依赖再分析数据存在数据同化偏差和时间差异问题，需要开发直接在观测空间进行预测的新范式

Method: 使用掩码多模态自编码器(MMAE)进行数据同化，结合时空解耦Transformer和跨区域边界条件(CBC)进行观测空间动态学习

Result: AIDA初始化显著提高了AIWP的推出效果和效率，DAWP在全局降水预测中展现出应用潜力

Conclusion: DAWP框架成功实现了从再分析数据到观测空间的转变，为AI天气预测提供了新的发展方向

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [321] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: 提出Cog-Rethinker，一种分层元认知强化学习框架，通过分解问题和参考错误答案来改进LLM推理任务的样本利用效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有零RL方法在弱LLM上由于固定提示模板导致的采样效率低下问题，大多数问题在推理任务中会产生无效输出，造成样本浪费。

Method: 采用分层元认知两阶段框架：第一阶段提示策略将零准确率问题分解为子问题；第二阶段提示策略参考先前错误答案来精炼答案。同时使用监督微调确保训练测试一致性。

Result: 在多个数学推理基准测试中表现出优越性能，相比基线方法提高了样本效率并加速了收敛。

Conclusion: Cog-Rethinker通过分层元认知方法有效提升了LLM推理任务的样本利用效率，解决了零RL训练中的采样浪费问题。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [322] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出了α-混合辅助分布和AMiD蒸馏框架，通过引入可调节参数α和广义散度家族，解决了LLM知识蒸馏中的容量差距和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 解决自回归大语言模型知识蒸馏中由于高维输出导致的容量差距和训练不稳定问题，现有辅助分布方法缺乏系统性研究。

Method: 提出α-混合辅助分布作为广义辅助分布家族，引入可调节参数α；开发AMiD统一框架，基于最优性理论扩展散度家族。

Result: 实验表明AMiD通过利用更广泛且理论基础的辅助分布空间，提供了优越的性能和训练稳定性。

Conclusion: AMiD框架通过系统化的辅助分布设计和广义散度家族，有效解决了LLM知识蒸馏的关键挑战。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [323] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: 提出MEET-Sepsis框架，通过多内源视图表示增强机制和级联双卷积时间序列注意力模块，仅需20%ICU监测时间即可实现竞争性脓毒症预测准确率


<details>
  <summary>Details</summary>
Motivation: 脓毒症是ICU中高死亡率的感染综合征，早期准确预测对及时干预至关重要。现有AI方法难以捕捉微弱的早期时间信号

Method: 使用多内源视图表示增强机制构建丰富特征视图，结合级联双卷积时间序列注意力模块进行多尺度时间表示学习

Result: MEET-Sepsis框架仅需SOTA方法20%的ICU监测时间即可达到竞争性预测准确率

Conclusion: 该框架显著推进了早期脓毒症预测，广泛验证证实了其有效性

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [324] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 提出了一种信息瓶颈引导的微调方法(IB-FT)，通过压缩记忆特征来解决预训练大语言模型在代码生成任务中的记忆障碍问题。


<details>
  <summary>Details</summary>
Motivation: 发现预训练大语言模型在代码领域进行监督微调时存在记忆障碍问题，即模型对下游代码数据的强记忆会阻碍其学习新的、可泛化的代码知识。

Method: 提出IB-FT方法，在代码数据的隐藏表示上应用信息瓶颈惩罚，压缩虚假的记忆特征，同时保留任务相关信息。

Result: 在两个代码基准测试(OriGen和Evol-CodeAlpaca-V1)上的实验表明，IB-FT显著缓解了记忆障碍，提高了top-1性能，并在更严格的多样本度量下获得了更稳定的增益。

Conclusion: IB-FT方法有效克服了传统微调中的记忆障碍问题，为代码生成任务提供了更稳定和可泛化的性能提升。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [325] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出一种基于聚类的可解释AI方法，用于根据睡眠障碍特征对患者进行分组，并识别影响这些病理的关键因素。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍对患者健康和生活质量有重大影响，但由于症状多样性，诊断复杂。技术进步和医疗数据分析为更好理解这些障碍提供了新视角。

Method: 采用基于聚类的可解释人工智能方法，整合可解释性方法识别关键影响因素。

Result: 在匿名真实数据上的实验证明了该方法的有效性和相关性。

Conclusion: 所提出的可解释聚类方法能够有效识别睡眠障碍患者的不同特征组别，并为理解这些病理提供了有价值的见解。

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [326] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 论文提出了一个框架来追踪和引导大语言模型中的算法原语，揭示了推理过程由可组合的算法原语组成，这些原语可以在激活空间中通过几何操作进行组合和转移。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何通过潜在计算和推理时间计算来解决多步推理问题，探索模型推理背后的算法原语及其组合方式。

Method: 通过聚类神经激活并将其与推理轨迹匹配来操作化算法原语，使用函数向量方法推导可重用的推理构建块，并通过在残差流中注入原语来评估其对推理步骤和任务性能的影响。

Result: 发现算法原语可以通过加法、减法和标量操作进行组合，在激活空间中展现出几何逻辑。跨任务和跨模型评估显示存在共享和任务特定的原语，推理微调增强了算法的组合泛化能力。

Conclusion: LLMs的推理可能由算法原语的组合几何结构支持，原语可以跨任务和跨模型转移，推理微调能够加强跨领域的算法泛化能力。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [327] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: RLVR和GRPO算法虽然广泛用于增强大语言模型的推理能力，但其效果不一致。研究发现GRPO本质上是一种保守的重新加权方案，受限于基础模型的分布，无法发现全新解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO算法在不同推理领域表现不一致的原因，探究其在什么条件下能改善推理能力并实现分布外泛化。

Method: 从数据分布角度进行理论分析，证明GRPO是保守的重新加权方案，并通过从零开始训练transformer进行受控实验，评估在推理深度、输入长度、标记表示和组合性等方面的泛化能力。

Result: GRPO的分布外改进仅在目标任务与模型预训练偏差一致时出现，而在分布内任务上的收益随着性能饱和而减少。GRPO不能作为通用推理增强器，而是强化预训练偏差的工具。

Conclusion: GRPO无法扩展模型超越其预训练起源的能力，这为未来开发能够真正扩展模型能力的新算法提供了动机。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [328] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: Stratos是一个端到端的LLM蒸馏管道，能自动选择服务器和模型、进行知识蒸馏和在分布式云环境中部署，满足用户对模型性能和系统预算的约束要求。


<details>
  <summary>Details</summary>
Motivation: 工业界对定制化和成本效益高的大型语言模型需求增长，现有蒸馏框架需要人工干预且难以满足复杂的用户定义蒸馏需求。

Method: 提出Stratos端到端LLM蒸馏管道，自动选择Pareto最优服务器，动态匹配师生模型对，根据任务复杂度调整蒸馏策略以优化云托管。

Result: 在罕见的麻将推理任务上，Stratos生成的学生模型准确率是GPT-4o教师基线的四倍，同时降低了延迟和成本而不影响准确性。

Conclusion: Stratos展示了在垂直领域LLM部署中的潜力，能够自动高效地满足复杂的蒸馏需求。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [329] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 该论文提出使用Kolmogorov-Smirnov检验来监测和量化机器学习系统中的分布偏移问题，特别是在智能交通领域的应用。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习系统中训练数据与测试数据分布不一致导致的预测误差问题，这对于AI系统的安全性和可靠性至关重要。

Method: 采用Kolmogorov-Smirnov检验来测量分布偏移，并使用KS距离来量化分布偏移对AI智能体性能的影响。

Result: 研究表明即使KS距离仅为0.02，也会导致强化学习智能体在单个交叉路口的通行时间增加约50%，影响显著。

Conclusion: KS检验和KS距离可作为实时监测AI智能体性能退化的有效统计工具，帮助AI系统更好地应对分布偏移问题。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [330] [AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM](https://arxiv.org/abs/2510.15998)
*Nilo Schwencke,Cyriaque Rousselot,Alena Shilova,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 本文分析了使用ANaGRAM自然梯度方法训练PINNs的训练动态，提出了多截断自适应策略来提升性能，并通过谱理论框架解释了正则化的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明自然梯度方法在训练物理信息神经网络(PINNs)时能显著优于标准优化器，但需要深入分析其训练动态和理论依据。

Method: 使用ANaGRAM方法（基于奇异值分解和截断正则化的自然梯度方法），并提出多截断自适应策略来增强性能。

Result: 在基准PDE问题上的实验验证了方法的有效性，部分实验达到了机器精度。

Conclusion: 通过谱理论框架解释了正则化的必要性，并扩展了与格林函数理论的联系，为自然梯度方法在PINNs中的应用提供了理论基础。

Abstract: Recent works have shown that natural gradient methods can significantly
outperform standard optimizers when training physics-informed neural networks
(PINNs). In this paper, we analyze the training dynamics of PINNs optimized
with ANaGRAM, a natural-gradient-inspired approach employing singular value
decomposition with cutoff regularization. Building on this analysis, we propose
a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.
Experiments on benchmark PDEs validate the effectiveness of our method, which
allows to reach machine precision on some experiments. To provide theoretical
grounding, we develop a framework based on spectral theory that explains the
necessity of regularization and extend previous shown connections with Green's
functions theory.

</details>


### [331] [Layer-Aware Influence for Online Data Valuation Estimation](https://arxiv.org/abs/2510.16007)
*Ziao Yang,Longbo Huang,Hongfu Liu*

Main category: cs.LG

TL;DR: 提出了一种层感知在线估计器，仅需损失到输出的梯度即可高效估计训练样本的动态影响力，解决了传统静态影响力评估忽略优化过程中数据价值动态变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注在收敛模型上测量的静态影响力，忽视了数据价值在优化过程中的动态变化特性，特别是在深度模型中。频繁进行影响力估计的计算负担很大。

Method: 开发了层感知在线估计器，仅需要损失到输出的梯度，避免了参数级和全网络梯度计算，同时保持了排序保真度。

Result: 在LLM预训练、微调和图像分类等广泛实验中，该方法提高了准确性，同时显著降低了时间和内存成本。

Conclusion: 该方法使动态数据筛选在实践中变得高效且可扩展，为数据为中心的学习提供了实用的解决方案。

Abstract: Data-centric learning emphasizes curating high-quality training samples to
boost performance rather than designing new architectures. A central problem is
to estimate the influence of training sample efficiently. Prior studies largely
focus on static influence measured on a converged model, overlooking how data
valuation dynamically changes during optimization. This omission neglects the
dynamic nature of sample influence during optimization, especially in deep
models. To address the computational burden of frequent influence estimation,
we develop a layer-aware online estimator that requires only loss-to-output
gradients. This design avoids parameter-level and full-network gradients while
preserving ranking fidelity. Extensive experiments across LLM pretraining,
fine-tuning, and image classification show our method improves accuracy with
substantially lower time and memory cost, making dynamic data curation
efficient and scalable in practice.

</details>


### [332] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 本文提出STAR模块，用于增强时间序列基础模型在多元时间序列异常检测中对状态变量的建模能力，解决现有方法忽视状态变量分类特性导致性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在处理包含离散状态变量（如阀门开关状态、星期几）的工业时间序列时，往往忽视状态变量的分类特性，将其与数值变量统一处理，导致无法充分利用状态信息，甚至集成状态变量后检测性能显著下降。

Method: 提出STAR模块，包含三个核心组件：1）身份引导的状态编码器，通过可学习状态记忆捕捉状态变量的复杂分类语义；2）条件瓶颈适配器，基于当前状态动态生成低秩适配参数，将状态变量影响灵活注入主干模型；3）数值-状态匹配模块，更有效地检测状态变量本身的异常。

Result: 在真实世界数据集上的广泛实验表明，STAR能够提升现有时间序列基础模型在多元时间序列异常检测中的性能。

Conclusion: STAR是一个即插即用的模块，能够在微调阶段增强时间序列基础模型对状态变量的建模和利用能力，有效解决了现有方法在处理混合类型时间序列时的局限性。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [333] [Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach](https://arxiv.org/abs/2510.16015)
*Qian Sun,Graham Hults,Susu Xu*

Main category: cs.LG

TL;DR: 提出了一种决策导向的洪水应急响应框架，通过端到端优化传感器部署和洪水预测模型来最小化下游决策遗憾，而非传统的任务无关策略。


<details>
  <summary>Details</summary>
Motivation: 传统洪水管理系统采用固定、任务无关的策略部署传感器和训练预测模型，忽视了相同感知增益和平均预测误差可能导致不同决策结果的问题。

Method: 端到端框架包含四个组件：上下文评分网络、预算约束下的可微分传感器选择模块、时空洪水重建与预测模型、以及针对任务目标的决策层，采用I-MLE实现离散传感器配置的梯度学习。

Result: 该方法能够战略性地选择传感器位置并优化洪水预测模型，直接针对下游洪水响应决策进行优化。

Conclusion: 决策导向框架能够有效提升洪水应急响应的及时性和可靠性，通过端到端优化传感器部署和预测模型来最小化决策遗憾。

Abstract: Timely and reliable decision-making is vital for flood emergency response,
yet it remains severely hindered by limited and imprecise situational awareness
due to various budget and data accessibility constraints. Traditional flood
management systems often rely on in-situ sensors to calibrate remote
sensing-based large-scale flood depth forecasting models, and further take
flood depth estimates to optimize flood response decisions. However, these
approaches often take fixed, decision task-agnostic strategies to decide where
to put in-situ sensors (e.g., maximize overall information gain) and train
flood forecasting models (e.g., minimize average forecasting errors), but
overlook that systems with the same sensing gain and average forecasting errors
may lead to distinct decisions. To address this, we introduce a novel
decision-focused framework that strategically selects locations for in-situ
sensor placement and optimize spatio-temporal flood forecasting models to
optimize downstream flood response decision regrets. Our end-to-end pipeline
integrates four components: a contextual scoring network, a differentiable
sensor selection module under hard budget constraints, a spatio-temporal flood
reconstruction and forecasting model, and a differentiable decision layer
tailored to task-specific objectives. Central to our approach is the
incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable
gradient-based learning over discrete sensor configurations, and probabilistic
decision heads to enable differentiable approximation to various constrained
disaster response tasks.

</details>


### [334] [Transfer learning strategies for accelerating reinforcement-learning-based flow control](https://arxiv.org/abs/2510.16016)
*Saeed Salehi*

Main category: cs.LG

TL;DR: 本研究探索了使用渐进神经网络和微调策略来加速深度强化学习在多保真度混沌流体控制中的知识迁移，发现PNNs在保持先验知识和稳定迁移方面优于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度强化学习在复杂流体控制中训练成本高的问题，研究如何通过知识迁移从低保真度环境向高保真度环境有效转移控制策略，提高学习效率和稳定性。

Method: 采用渐进神经网络架构和传统微调策略进行对比研究，使用Kuramoto-Sivashinsky系统作为基准，系统评估不同迁移学习方法在性能、收敛行为和知识保持能力方面的表现。

Result: 微调方法虽然能加速收敛，但对预训练时长敏感且容易发生灾难性遗忘；而PNNs能够稳定高效地迁移知识，保持先验知识并提供持续性能提升，对预训练阶段的过拟合具有鲁棒性。

Conclusion: 渐进神经网络为鲁棒、可扩展和计算高效的流体控制提供了有前景的迁移学习框架，即使在源环境和目标环境差异较大时仍能有效工作，而传统微调方法往往导致次优适应或知识迁移失败。

Abstract: This work investigates transfer learning strategies to accelerate deep
reinforcement learning (DRL) for multifidelity control of chaotic fluid flows.
Progressive neural networks (PNNs), a modular architecture designed to preserve
and reuse knowledge across tasks, are employed for the first time in the
context of DRL-based flow control. In addition, a comprehensive benchmarking of
conventional fine-tuning strategies is conducted, evaluating their performance,
convergence behavior, and ability to retain transferred knowledge. The
Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how
knowledge encoded in control policies, trained in low-fidelity environments,
can be effectively transferred to high-fidelity settings. Systematic
evaluations show that while fine-tuning can accelerate convergence, it is
highly sensitive to pretraining duration and prone to catastrophic forgetting.
In contrast, PNNs enable stable and efficient transfer by preserving prior
knowledge and providing consistent performance gains, and are notably robust to
overfitting during the pretraining phase. Layer-wise sensitivity analysis
further reveals how PNNs dynamically reuse intermediate representations from
the source policy while progressively adapting deeper layers to the target
task. Moreover, PNNs remain effective even when the source and target
environments differ substantially, such as in cases with mismatched physical
regimes or control objectives, where fine-tuning strategies often result in
suboptimal adaptation or complete failure of knowledge transfer. The results
highlight the potential of novel transfer learning frameworks for robust,
scalable, and computationally efficient flow control that can potentially be
applied to more complex flow configurations.

</details>


### [335] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: AirDbM是一种专门用于翼型优化的设计变形方法，通过从1600多个翼型中选择12个最优基线翼型，显著降低设计空间维度，在保持高重建精度的同时实现更高效的优化。


<details>
  <summary>Details</summary>
Motivation: 翼型几何优化需要探索多样化的设计，同时尽可能减少设计变量数量。传统方法需要大量设计变量才能覆盖足够的设计空间。

Method: 从UIUC翼型数据库中选择12个最优基线翼型，通过顺序添加最能增加设计容量的基线来构建设计空间，使用设计变形方法进行翼型重建和优化。

Result: 用12个基线重建了99%的数据库，平均绝对误差低于0.005；在多目标气动优化中实现了更快的收敛和更大的超体积，发现了具有更好升阻比的新帕累托最优解；在强化学习中表现出比传统参数化方法更好的适应性。

Conclusion: AirDbM通过系统降低设计空间维度，在保持高精度的同时实现了更高效的翼型优化，展示了设计变形方法在机器学习驱动设计中的广阔潜力。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [336] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出一种基于特征驱动的强化学习方法，用于光伏发电商在日内连续市场中进行实时交易决策，通过平衡交易利润和失衡惩罚来优化收益。


<details>
  <summary>Details</summary>
Motivation: 光伏运营商面临发电量和短期电价的不确定性，日内连续市场允许生产商实时调整头寸，可能提高收入并减少失衡成本。

Method: 将问题建模为马尔可夫决策过程，使用近端策略优化（PPO）算法，采用主要线性的可解释策略，整合数据驱动特征到状态中。

Result: 在历史市场数据上训练并在样本外评估，该策略在各种场景下始终优于基准方法，显示出快速收敛、实时推理和透明决策规则。

Conclusion: 特征驱动的强化学习为光伏生产商提供了实用、数据高效且可操作部署的日内参与路径。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [337] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: PolyConFM是首个聚合物基础模型，通过构象中心的生成预训练统一聚合物建模和设计，解决了现有方法忽视全局结构信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法仅使用单体级描述符表示整个聚合物，忽略了聚合物构象中的全局结构信息，限制了实际性能。同时缺乏能够有效支持多样化下游任务的通用基础模型。

Method: 将聚合物构象分解为局部构象序列，通过掩码自回归建模重建局部构象，并生成方向变换来恢复相应的聚合物构象。构建首个高质量聚合物构象数据集进行构象中心预训练。

Result: 实验表明PolyConFM在多样化下游任务上持续优于代表性的任务特定方法。

Conclusion: PolyConFM为聚合物科学提供了一个通用且强大的工具。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [338] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 提出一个可泛化的因果机器学习流程，用于从大规模电子健康记录中发现潜在因果源并量化其对临床结果的影响。


<details>
  <summary>Details</summary>
Motivation: 处理不完善的多模态临床数据，发现其中的潜在因果因素，并量化这些因素对临床结果的影响，以支持医疗发现。

Method: 处理多模态临床数据，将其分解为概率独立的潜在源，然后训练任务特定的因果模型来估计个体因果效应。

Result: 已在两个真实世界应用中验证了该方法的有效性和实用性，展示了其在医疗发现中的通用性。

Conclusion: 该方法提供了一个可扩展的框架，能够从大规模电子健康记录中发现因果关系并量化其影响，有助于医疗发现。

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [339] [RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction](https://arxiv.org/abs/2510.16035)
*Yingguang Yang,Xianghua Zeng,Qi Wu,Hao Peng,Yutong Xia,Hao Liu,Bin Chong,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了首个针对GNN社交机器人检测器的多智能体强化学习对抗攻击框架RoBCtrl，通过扩散模型生成高保真机器人账户，并使用MARL模拟对抗行为，有效降低检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN社交机器人检测方法存在控制有限、黑盒性和机器人异质性等问题，其脆弱性和鲁棒性研究不足。

Method: 使用扩散模型重构现有账户数据生成高保真机器人账户，采用多智能体强化学习按影响力和预算分类控制机器人账户，设计基于结构熵的分层状态抽象加速学习。

Result: 在社交机器人检测数据集上的实验表明，该框架能有效削弱GNN检测器的性能。

Conclusion: RoBCtrl是首个将扩散模型应用于社交机器人模拟的框架，通过MARL实现了对GNN检测器的有效对抗攻击。

Abstract: Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.

</details>


### [340] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: GCQ是一种受大脑启发的网格状编码量化方法，通过吸引子动态将观察-动作序列压缩为离散表示，实现时空联合压缩并作为统一世界模型。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化方法处理静态输入，无法有效压缩时空序列。受神经系统中网格状编码的启发，需要开发能同时压缩空间和时间信息的序列建模方法。

Method: 使用动作条件化码本，其中码字来自连续吸引子神经网络，并根据动作动态选择。通过网格状模式在吸引子动态中进行时空压缩。

Result: 实验表明GCQ在多种任务中能有效实现紧凑编码，并支持长时程预测、目标导向规划和逆向建模等下游任务。

Conclusion: GCQ不仅为高效序列建模提供了计算工具，也为神经系统中网格状编码的形成提供了理论视角。

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [341] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: AMS-Quant是一种新的浮点量化方法，通过尾数位共享和自适应搜索技术，首次将量化位宽扩展到非整数位宽（如FP5.33和FP4.25），在保持精度的同时显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的巨大参数量带来了存储和推理效率瓶颈，浮点量化虽然能加速推理，但传统方法局限于整数位宽，无法进一步逼近量化最佳点。

Method: 提出两种新技术：1）尾数位共享 - 将k个量化权重分组共享最低有效尾数位；2）自适应搜索 - 采用离线优化策略最小化共享带来的精度损失。

Result: 实验表明AMS-Quant能将模型量化为FP5.33-e2m3和FP4.25-e2m2，相比FP16推理分别实现2.8倍和3.2倍的解码加速，且精度损失可忽略不计。

Conclusion: AMS-Quant通过非整数位宽量化成功逼近量化最佳点，在保持精度的同时显著提升LLM推理效率，为模型压缩提供了新思路。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [342] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: GUIrilla是一个自动化可扩展框架，通过原生可访问性API系统探索应用程序，解决了GUI自动化中的数据收集挑战。该框架构建了GUIrilla-Task数据集，包含27,171个功能任务，显著提升了LLM代理在UI任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前自主代理在复杂图形用户界面操作方面面临数据可用性限制，包括昂贵的手动标注、闭源数据集和表面级合成流程。特别是在macOS生态系统中，现有UI数据集代表性有限。

Method: GUIrilla框架通过原生可访问性API系统探索应用程序，将发现的界面元素和爬虫动作组织成层次化GUI图，并使用专门的交互处理程序实现全面的应用程序覆盖。

Result: 构建了GUIrilla-Task数据集，包含27,171个功能任务，涵盖1,108个macOS应用程序。实验结果显示，在GUIrilla-Task上微调的LLM代理在ScreenSpot Pro基准测试中表现优于合成基线，同时使用数据量减少97%。

Conclusion: GUIrilla框架有效解决了桌面GUI自动化的数据收集挑战，发布的macapptree库、GUIrilla-Task数据集和GUIrilla-Gold基准测试支持桌面自主性的开放研究。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [343] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 该论文探讨了智能交通系统中的交通预测技术，重点关注图神经网络在捕捉空间依赖性和时间演化模式方面的应用，并分析了现有方法在事件信息整合方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加快，交通拥堵问题日益严重，需要可靠且响应迅速的交通预测模型来支持智能交通系统建设，改善城市资源分配和出行体验。

Method: 主要采用图神经网络（GNNs）作为主流范式，包括STGCN、GraphWaveNet、STWave和D2STGNN等模型，这些方法结合了复杂的图卷积结构和时间建模机制。对于事件信息处理，早期尝试主要依赖手动设计的事件特征和特定子图构建。

Result: 基于GNN的方法在标准交通数据集上表现出色，特别擅长捕捉具有周期性规律的交通模式。但手动特征工程方法存在对领域专家先验知识的依赖，难以泛化到复杂未知事件，且容易丢失丰富的语义细节。

Conclusion: 虽然图神经网络在交通预测中取得了显著进展，但现有的事件信息整合方法仍存在局限性，需要开发更自动化和泛化能力强的技术来处理多样化的事件场景。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [344] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 本文研究了5个时间序列基础模型和2个基线模型的校准特性，发现时间序列基础模型比基线模型校准得更好，且不会系统性过度自信或不足自信。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测性能上达到最先进水平，但其校准特性尚未得到充分探索，而校准对许多实际应用至关重要。

Method: 对5个时间序列基础模型和2个竞争基线模型进行系统评估，包括模型校准、不同预测头的影响以及长期自回归预测下的校准。

Result: 时间序列基础模型比基线模型校准得更好，且不会系统性过度自信或不足自信，这与深度学习模型中常见的过度自信形成对比。

Conclusion: 时间序列基础模型具有良好的校准特性，这为实际应用提供了重要优势。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [345] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 提出了一种用于变电站级电压估计的分层图神经网络，在低观测性条件下仍能保持高精度，比替代数据驱动模型的RMSE降低达2倍。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源渗透和配电级电压波动增加，传统配电系统状态估计技术难以应对稀疏测量和现代馈线规模问题，需要更鲁棒的可扩展解决方案。

Method: 利用分层图神经网络，结合电气拓扑和物理特征，在公开SMART-DS数据集上对数千个母线进行训练和评估，涵盖多个变电站和DER渗透场景。

Result: 在仅1%测量覆盖率下仍保持高精度，比替代数据驱动模型实现高达2倍的RMSE降低。

Conclusion: 图神经网络有潜力为配电系统提供可扩展、可复制和数据驱动的电压监测解决方案。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [346] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出一种基于残差学习的AC最优潮流计算方法，使用DC OPF作为基线，学习非线性修正来获得完整的AC OPF解，在保持精度的同时显著提升计算速度。


<details>
  <summary>Details</summary>
Motivation: 解决AC最优潮流问题的计算瓶颈，实现实时电网操作。传统AC OPF计算复杂耗时，而DC OPF虽然快速但精度不足。

Method: 使用拓扑感知的图神经网络，结合局部注意力和两级DC特征集成，通过物理信息损失函数训练，确保AC潮流可行性和运行限制。

Result: 在57、118和2000总线系统上测试，MSE降低约25%，可行性误差减少最多3倍，运行速度提升最多13倍，在N-1故障下仍保持精度。

Conclusion: 残差学习是连接线性近似和AC可行OPF的实用可扩展桥梁，可实现近实时操作决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [347] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出了FedPURIN框架，通过整数规划识别关键参数进行传输，结合稀疏聚合方案显著减少通信开销，同时保持个性化联邦学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中现有方法通信效率低下的问题，减轻通信负担以便实际部署。

Method: 使用整数规划策略识别关键参数进行传输，并集成到稀疏聚合方案中。

Result: 在标准图像分类基准测试中，在不同非IID条件下表现出与最先进方法相当的性能，同时通过稀疏聚合实现了可量化的通信减少。

Conclusion: FedPURIN为通信高效的个性化联邦学习建立了新范式，特别适用于处理异构数据源的边缘智能系统。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [348] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出了多尺度神经算子(MNO)，一种用于三维非结构化点云上计算流体动力学的新型架构，通过显式三尺度分解显著提升了神经算子在复杂流体问题上的精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在求解偏微分方程时存在精度和可扩展性限制，特别是在不规则域上处理多尺度流体结构时表现不佳。

Method: MNO架构包含三个模块：全局维度收缩注意力模块处理长程依赖，局部图注意力模块处理邻域级交互，微观点级注意力模块处理精细细节，在保持多尺度归纳偏置的同时保持计算效率。

Result: 在四个不同基准测试中，MNO在稳态和非稳态流场景下均优于现有方法，预测误差降低5%-40%，在具有30万点的3D CFD问题上表现出更好的鲁棒性。

Conclusion: 显式多尺度设计对神经算子至关重要，MNO为在不规则域上学习复杂流体动力学提供了可扩展框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [349] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 提出了基于随机矩阵理论的Transformer训练动态分析框架，通过自注意力矩阵的幂律分布演化来划分训练阶段，并提供了无需验证集的早期停止标准。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer训练动态的底层机制，特别是性能提升的驱动因素，并建立理论指导的早期停止标准。

Method: 利用随机矩阵理论分析浅层自注意力矩阵V的光谱密度演化，使用幂律拟合作为探针来划分训练阶段。

Result: 发现自注意力矩阵的光谱密度一致地演化为重尾分布，训练可分为结构探索、重尾结构稳定和收敛饱和三个阶段。

Conclusion: 随机矩阵理论为监控和诊断Transformer模型训练进展提供了有效工具，提出的两个一致性标准能可靠指导训练过程。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [350] [Optimization of the quantization of dense neural networks from an exact QUBO formulation](https://arxiv.org/abs/2510.16075)
*Sergio Muñiz Subiñas,Manuel L. González,Jorge Ruiz Gómez,Alejandro Mata Ali,Jorge Martínez Martín,Miguel Franco Hernando,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出了一种基于ADAROUND的QUBO公式的后训练量化方法，通过Frobenius距离作为目标函数，将量化问题分解为可并行求解的子问题。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法存在精度损失问题，需要更精确的量化方法来保持神经网络性能。

Method: 使用Frobenius距离作为目标函数，构建QUBO公式，并利用系数矩阵结构将全局问题分解为多个独立子问题，采用模拟退火等启发式算法求解。

Result: 在MNIST、Fashion-MNIST、EMNIST和CIFAR-10数据集上评估，从int8到int1的整数精度范围内，相比传统四舍五入量化方法表现更好。

Conclusion: 该方法能够有效进行神经网络量化，在保持精度的同时实现高效的量化过程。

Abstract: This work introduces a post-training quantization (PTQ) method for dense
neural networks via a novel ADAROUND-based QUBO formulation. Using the
Frobenius distance between the theoretical output and the dequantized output
(before the activation function) as the objective, an explicit QUBO whose
binary variables represent the rounding choice for each weight and bias is
obtained. Additionally, by exploiting the structure of the coefficient QUBO
matrix, the global problem can be exactly decomposed into $n$ independent
subproblems of size $f+1$, which can be efficiently solved using some
heuristics such as simulated annealing. The approach is evaluated on MNIST,
Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1
and compared with a round-to-nearest traditional quantization methodology.

</details>


### [351] [Continual Knowledge Consolidation LORA for Domain Incremental Learning](https://arxiv.org/abs/2510.16077)
*Naeem Paeedeh,Mahardhika Pratama,Weiping Ding,Jimmy Cao,Wolfgang Mayer,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: CONEC-LoRA是一种用于领域增量学习的新方法，通过整合任务共享和任务特定的LoRA模块来提取共同知识和领域特定知识，使用随机分类器和辅助网络解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法创建任务特定的LoRA模块，但忽略了任务间的共享知识，且在推理时选择不准确的任务特定LoRA会导致准确率显著下降。现有分类器泛化能力不足。

Method: 提出CONEC-LoRA方法：1）整合任务共享和任务特定LoRA；2）使用随机分类器增强分类正确率；3）部署辅助网络预测任务特定LoRA；4）采用不同深度网络结构，每层连接局部分类器；5）集成球生成器损失和变换模块解决合成样本偏差问题。

Result: 在4个流行基准问题上，CONEC-LoRA相比现有方法有超过5%的性能优势。

Conclusion: CONEC-LoRA有效解决了领域增量学习中的灾难性遗忘问题，通过知识整合和随机分类器设计显著提升了性能。

Abstract: Domain Incremental Learning (DIL) is a continual learning sub-branch that
aims to address never-ending arrivals of new domains without catastrophic
forgetting problems. Despite the advent of parameter-efficient fine-tuning
(PEFT) approaches, existing works create task-specific LoRAs overlooking shared
knowledge across tasks. Inaccurate selection of task-specific LORAs during
inference results in significant drops in accuracy, while existing works rely
on linear or prototype-based classifiers, which have suboptimal generalization
powers. Our paper proposes continual knowledge consolidation low rank
adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed
from consolidations between task-shared LORA to extract common knowledge and
task-specific LORA to embrace domain-specific knowledge. Unlike existing
approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose
parameters are sampled from a distribution, thus enhancing the likelihood of
correct classifications. Last but not least, an auxiliary network is deployed
to optimally predict the task-specific LoRAs for inferences and implements the
concept of a different-depth network structure in which every layer is
connected with a local classifier to take advantage of intermediate
representations. This module integrates the ball-generator loss and
transformation module to address the synthetic sample bias problem. Our
rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in
4 popular benchmark problems with over 5% margins.

</details>


### [352] [PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites](https://arxiv.org/abs/2510.16083)
*Jaehan Kim,Minkyoo Song,Minjae Seo,Youngjin Jin,Seungwon Shin,Jinwoo Kim*

Main category: cs.LG

TL;DR: 提出PassREfinder-FL框架，使用图神经网络预测网站间的密码重用风险，通过联邦学习保护用户隐私，在真实数据集上达到0.9153的F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决现有密码重用检测方法在可用性和部署方面的限制，现有方法往往限制密码创建或网站访问，且依赖复杂的账户共享机制难以实际部署。

Method: 引入密码重用关系概念，将其表示为网站图中的边，使用图神经网络进行链接预测任务评估网站间密码重用风险，并采用联邦学习保护用户隐私。

Result: 在包含3.6亿泄露账户的真实数据集上，PassREfinder-FL在联邦学习设置下达到0.9153的F1分数，相比其他最先进GNN模型性能提升4-11%。

Conclusion: 该方法能够扩展到大量任意网站，预测结果可作为可操作的风险评分来量化密码重用可能性，有效解决密码重用检测的实际部署问题。

Abstract: Credential stuffing attacks have caused significant harm to online users who
frequently reuse passwords across multiple websites. While prior research has
attempted to detect users with reused passwords or identify malicious login
attempts, existing methods often compromise usability by restricting password
creation or website access, and their reliance on complex account-sharing
mechanisms hinders real-world deployment. To address these limitations, we
propose PassREfinder-FL, a novel framework that predicts credential stuffing
risks across websites. We introduce the concept of password reuse relations --
defined as the likelihood of users reusing passwords between websites -- and
represent them as edges in a website graph. Using graph neural networks (GNNs),
we perform a link prediction task to assess credential reuse risk between
sites. Our approach scales to a large number of arbitrary websites by
incorporating public website information and linking newly observed websites as
nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a
federated learning (FL) approach that eliminates the need to share user
sensitive information across administrators. Evaluation on a real-world dataset
of 360 million breached accounts from 22,378 websites shows that
PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further
validate that our FL-based GNN achieves a 4-11% performance improvement over
other state-of-the-art GNN models through an ablation study. Finally, we
demonstrate that the predicted results can be used to quantify password reuse
likelihood as actionable risk scores.

</details>


### [353] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 将平衡传播学习算法扩展到离散和连续复值波系统，在弱耗散状态下有效，适用于多种物理环境，通过可训练的局部势能替代节点间连接，在激子极化凝聚体中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法在物理神经网络中难以实现，平衡传播(EP)作为替代方案具有相当的效率和原位训练的潜力，但需要扩展到更广泛的物理系统。

Method: 将EP学习扩展到离散和连续复值波系统，在弱耗散状态下有效，用可训练的局部势能替代节点间连接，在激子极化凝聚体的广义Gross-Pitaevskii动力学中测试。

Result: 在标准基准测试（包括简单逻辑任务和手写数字识别）中表现出稳定收敛，证明了方法的有效性。

Conclusion: 为系统控制仅限于局部参数的物理系统中的原位学习建立了实用途径。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [354] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出FSRF框架解决多模态情感分析中的模态缺失问题，通过解耦重构和自蒸馏方法提升模型在模态缺失情况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据常因遮挡、隐私限制和设备故障而缺失，但现有研究主要关注完整模态数据的交互融合，忽略了模态缺失问题导致的泛化能力不足。

Method: 提出去冗余同质-异质分解模块将模态分解为同质、异质和噪声表示，并设计分布对齐的自蒸馏模块通过双向知识转移恢复缺失语义。

Result: 在两个数据集上的实验表明，FSRF在不确定模态缺失情况下相比先前方法具有显著性能优势。

Conclusion: FSRF框架能有效缓解多模态情感分析中的模态缺失问题，提升模型在真实场景中的泛化能力。

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [355] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: STABLE是一个门控持续自编辑框架，通过LoRA参数高效微调来约束序列更新中的灾难性遗忘，使用三种指标评估编辑稳定性并选择性接受或缩放更新。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要持续适应机制，但序列更新会导致灾难性遗忘，新编辑会降低先前获得的知识。

Method: 使用基于LoRA的参数高效微调，每个候选编辑通过三种指标（精确匹配下降、比特增加、KL散度）评估稳定性，超过阈值则通过裁剪过程重新缩放或拒绝LoRA更新。

Result: 在Qwen-2.5-7B模型上的实验表明，门控有效缓解遗忘同时保持适应性，基于EM的门控在短持续学习序列中实现最高累积性能。

Conclusion: 不同门控策略可实现可比较的分布偏移但产生不同准确性结果，突显门控设计在持续适应中的重要性，为持续模型编辑提供了原则性方法。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [356] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 提出MemCom方法，通过层间压缩来提升多示例上下文学习的内存和计算效率，在保持高准确率的同时实现3-8倍的压缩比。


<details>
  <summary>Details</summary>
Motivation: 多示例上下文学习虽然能提升任务性能，但会显著增加内存和计算成本，需要有效的压缩方法来平衡性能与效率。

Method: 提出MemCom层间压缩方法，使用更强的压缩器模型，在每个transformer层进行细粒度压缩，为每层提供独立的压缩表示。

Result: 在多个分类任务上，MemCom在所有压缩比下都优于基线方法，在高压缩比下性能下降小于10%，而基线方法下降超过20-30%。

Conclusion: 层间压缩方法能有效提升多示例上下文学习的效率，在保持性能的同时显著降低计算成本。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [357] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 开发了一种决策支持系统，通过预训练的AI代理缩小人类可采取的行动范围，实现人机互补性，在野火缓解游戏中使参与者表现提升30%。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在顺序决策任务中通过控制人类代理水平来实现人机互补性，就像在分类任务中那样。

Method: 使用预训练AI代理将人类可采取的行动缩小到一个子集，然后让人类从该行动集中选择行动，并引入利用行动集平滑特性的bandit算法优化人类代理水平。

Result: 在1600人参与的大规模实验中，使用该系统的参与者比单独参与者表现提升约30%，比AI代理表现提升超过2%，尽管AI代理本身显著优于无支持的人类参与者。

Conclusion: 通过自适应控制人类代理水平，可以在顺序决策任务中实现人机互补性，使人类和AI共同表现优于各自单独表现。

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [358] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 提出了一种基于相似性搜索和随机表示的无训练世界模型，与PlaNet基准模型在潜在重建质量和图像相似度方面表现相当，在长时程预测方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 世界模型在强化学习中广泛应用，但传统方法需要训练过程。本文旨在探索无需训练的世界模型替代方案，利用相似性搜索来近似环境动态。

Method: 使用相似性搜索和随机表示来构建无训练的世界模型，通过检索相似状态来预测环境动态，避免了传统模型的训练过程。

Result: 在潜在重建质量和重建图像感知相似度方面，搜索模型与训练模型表现相当；在长时程预测方面，搜索模型在视觉差异较大的环境中表现优于基准模型。

Conclusion: 基于相似性搜索的无训练世界模型是传统训练方法的可行替代方案，特别是在长时程预测任务中具有优势。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [359] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: 本文首次对时变学习策略下的Q-learning算法进行了有限时间分析，证明了在最小假设下的收敛性和样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的Q-learning分析主要针对固定策略的off-policy设置，而对on-policy采样（时变学习策略）的分析存在空白。本文旨在填补这一空白，为更广泛的RL算法分析提供理论工具。

Method: 采用改进的分析方法，利用泊松方程将马尔可夫噪声分解为鞅差项和残差项，并对泊松方程解进行敏感性分析以处理时间非齐次性。

Result: 建立了Q-learning在时变策略下的最后迭代收敛率，样本复杂度为O(1/ε²)，与off-policy Q-learning匹配但探索参数依赖更差。同时推导了学习策略收敛到最优策略的显式速率。

Conclusion: on-policy Q-learning具有较弱的探索性但更强的利用优势，其策略会收敛到最优策略。所开发的分析工具对分析具有快速时变学习策略的通用RL算法具有独立价值。

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [360] [Expert Merging in Sparse Mixture of Experts with Nash Bargaining](https://arxiv.org/abs/2510.16138)
*Dung V. Nguyen,Anh T. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Shiqi Jiang,Ethan Fetaya,Linh Duy Tran,Gal Chechik,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出NAMEx框架，通过纳什议价理论改进稀疏专家混合模型的专家合并策略，实现更平衡高效的专家协作，并在多个任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏专家混合模型的专家合并策略缺乏理论依据的权重机制，需要更平衡高效的专家协作方法。

Method: 基于博弈论视角重新解释专家合并，引入纳什议价理论，并加入复杂动量来加速专家传播，具有理论收敛保证。

Result: 在语言建模、文本分类、图像分类和零样本鲁棒性等任务上持续优于竞争方法，并在Qwen1.5-MoE和DeepSeek-MoE等大规模系统中验证了有效性。

Conclusion: NAMEx框架为专家合并提供了理论依据，实现了更平衡高效的专家协作，具有良好的可扩展性和实用性。

Abstract: Existing expert merging strategies for Sparse Mixture of Experts (SMoE)
typically rely on input-dependent or input-independent averaging of expert
parameters, but often lack a principled weighting mechanism. In this work, we
reinterpret expert merging through the lens of game theory, revealing
cooperative and competitive dynamics among experts. Based on this perspective,
we introduce Nash Merging of Experts (NAMEx), a novel framework that
incorporates Nash Bargaining into the merging process, enabling more balanced
and efficient collaboration among experts. Additionally, we incorporate complex
momentum into NAMEx to accelerate expert propagation with theoretical
guarantees for convergence. Extensive experiments across language modelling,
text classification, image classification, and zero-shot robustness under data
corruption show that NAMEx consistently outperforms competing methods while
integrating seamlessly with popular MoE architectures. Finally, we demonstrate
NAMEx's scalability by applying it to large-scale systems, including
Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both
zero-shot and fine-tuning settings.

</details>


### [361] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种连接零阶优化和锐度感知最小化(SAM)的新框架，通过指数倾斜目标在平均损失和最大损失之间平滑过渡，开发了梯度无关且内存高效的SAM替代方法。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法优化平滑后的函数期望值，而SAM方法关注邻域内最大损失以获得平坦最小值。本文旨在明确连接这两种方法，提供平滑过渡框架。

Method: 提出基于指数倾斜目标的软SAM目标，参数化倾斜参数t，开发新的零阶算法来求解该目标，并精确刻画倾斜SAM框架的锐度概念。

Result: 该方法在分类、多项选择问答和语言生成等下游任务中，相比传统零阶基线方法实现了更好的泛化性能。

Conclusion: 提出的倾斜SAM框架可作为梯度无关且内存高效的SAM变体替代方案，在多种任务中展现出优越的泛化能力。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [362] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: GRUwE：基于GRU的模型，使用指数基函数处理不规则采样多变量时间序列，在连续时间中支持回归和事件预测，性能与SOTA方法相当或更优，且实现简单、计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有复杂架构处理不规则采样时间序列时，其真正优势不明确，需要验证更简单高效的RNN改进方法是否仍具竞争力。

Method: 构建基于RNN的GRUwE模型，通过马尔可夫状态更新，采用观测触发和时间触发两种重置机制，使用可学习指数衰减支持连续时间预测。

Result: 在多个真实世界基准测试中，GRUwE在下一个观测和下一个事件预测任务上达到与最新SOTA方法相当或更优的性能。

Conclusion: GRUwE不仅性能优异，而且实现简单、超参数调优需求少、在线部署计算开销显著降低，为不规则采样时间序列预测提供了高效解决方案。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [363] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 系统比较三种生成模型（AtomGPT、CDVAE、FlowMM）在材料数据集上的性能，CDVAE表现最佳


<details>
  <summary>Details</summary>
Motivation: 虽然生成模型在材料发现中应用广泛，但缺乏对其性能的严格比较评估

Method: 使用两个公开超导数据集，训练三种代表性生成模型重建晶体结构，通过KL散度和MAE评估性能

Result: CDVAE表现最好，其次是AtomGPT，FlowMM表现最差

Conclusion: CDVAE在晶体结构生成任务中表现最优，所有基准代码和模型配置将公开

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [364] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 该论文通过层间因果修补分析语言模型对齐机制，发现对齐效果主要集中在中层激活，呈现局部化、低秩特征而非扩散性参数变化。


<details>
  <summary>Details</summary>
Motivation: 虽然基于人类反馈的强化学习(RLHF)被广泛用于语言模型对齐，但其内部工作机制仍不透明，需要系统分析偏好优化的对齐机制。

Method: 在Llama-3.2-1B模型上应用层间因果修补技术，分析基础模型与调优模型在人类偏好对上的差异，并使用LASSO回归识别关键层。

Result: 对齐效果空间局部化：中层激活编码了决定奖励一致行为的独特子空间，而早期和晚期层基本不受影响；仅少数层具有非零系数连接激活距离与奖励增益。

Conclusion: 基于人类偏好的语言模型对齐是一个方向性、低秩过程，而非扩散性参数过程。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [365] [Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness](https://arxiv.org/abs/2510.16171)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh,Chaowei Zhang,Xiao Qin,Yang Zhou*

Main category: cs.LG

TL;DR: 该论文提出通过嵌入群等变卷积（旋转和尺度等变层）到标准CNN中来增强对抗鲁棒性的架构方法，无需对抗训练即可提升模型对对抗攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 对抗训练作为主要防御策略存在计算成本高和可能损害原始数据准确性的问题，需要探索更高效的架构方法来提升对抗鲁棒性。

Method: 提出两种对称感知架构：并行设计（独立处理标准和等变特征后融合）和级联设计（顺序应用等变操作），嵌入旋转和尺度等变卷积层。

Result: 在CIFAR-10、CIFAR-100和CIFAR-10C数据集上，模型在FGSM和PGD攻击下均表现出更好的对抗鲁棒性和泛化能力。

Conclusion: 对称强制架构作为数据增强防御的高效原则性替代方案具有巨大潜力，能够在不依赖对抗训练的情况下提升模型鲁棒性。

Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks
by exploiting their sensitivity to imperceptible input perturbations. While
adversarial training remains the predominant defense strategy, it often incurs
significant computational cost and may compromise clean-data accuracy. In this
work, we investigate an architectural approach to adversarial robustness by
embedding group-equivariant convolutions-specifically, rotation- and
scale-equivariant layers-into standard convolutional neural networks (CNNs).
These layers encode symmetry priors that align model behavior with structured
transformations in the input space, promoting smoother decision boundaries and
greater resilience to adversarial attacks. We propose and evaluate two
symmetry-aware architectures: a parallel design that processes standard and
equivariant features independently before fusion, and a cascaded design that
applies equivariant operations sequentially. Theoretically, we demonstrate that
such models reduce hypothesis space complexity, regularize gradients, and yield
tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme
Value for nEtwork Robustness) framework. Empirically, our models consistently
improve adversarial robustness and generalization across CIFAR-10, CIFAR-100,
and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial
training. These findings underscore the potential of symmetry-enforcing
architectures as efficient and principled alternatives to data
augmentation-based defenses.

</details>


### [366] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文批评强化学习研究过度关注性能演示而忽视理解学习动态，主张应更注重科学理解和基准测试与数学形式化的精确映射。


<details>
  <summary>Details</summary>
Motivation: 强化学习研究过于强调展示智能体性能，导致学术基准过拟合，难以迁移到新问题，同时削弱了理解学习动态的研究价值。

Method: 使用Arcade Learning Environment作为案例，说明即使被认为是"饱和"的基准也能有效用于发展理解和促进RL技术在实际问题中的应用。

Result: 论证了RL研究需要从单纯性能演示转向更深入的科学理解，并强调基准测试与数学形式化之间精确映射的重要性。

Conclusion: 强化学习研究应平衡性能演示与科学理解，更精确地定义基准测试与底层数学形式化的关系，以促进技术向实际问题的有效迁移。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [367] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出了一种基于运行时监控语言(RML)的新型语言奖励机，能够表达非正则、非马尔可夫的任务奖励函数，解决了传统奖励机表达能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励函数通常被视为黑盒映射，缺乏解释性。传统奖励机虽然能表示结构化奖励函数，但其表达能力受限于正则语言，无法处理计数或参数化条件等复杂行为。

Method: 基于运行时监控语言(RML)构建新型语言奖励机，利用RML的内置内存机制来指定非正则、非马尔可夫任务的奖励函数。

Result: 实验证明了该方法在表达能力上的优势，在灵活事件处理和任务规范方面优于现有的基于奖励机的方法。

Conclusion: 基于RML的语言奖励机扩展了奖励函数的表达能力，能够处理更复杂的非正则、非马尔可夫任务，为强化学习提供了更强大的奖励规范工具。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [368] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出了一种结合关系强化学习和对象中心表示的新框架，能够处理结构化和非结构化数据，并通过主动查询人类专家来增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在处理结构化问题时忽略了问题的内在结构，而关系强化学习虽然能处理结构化问题但对问题结构有强假设限制。需要一种能同时处理结构化和非结构化数据的更通用方法。

Method: 结合关系强化学习与对象中心表示，通过显式建模策略不确定性来主动查询人类专家获取指导。

Result: 经验评估表明该方法具有有效性和高效性。

Conclusion: 提出的框架能够有效处理结构化和非结构化数据，并通过主动学习机制提升学习效率。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [369] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: 提出一种探索-提交算法，用于处理具有线性动态潜在状态的非平稳多臂老虎机问题，实现了Õ(T^{2/3})的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 研究非平稳老虎机问题，其中奖励同时依赖于动作和潜在状态，且状态动态也受动作影响，导致短期和长期奖励之间存在权衡。

Method: 采用探索-提交算法：探索阶段使用随机Rademacher动作估计线性动态的马尔可夫参数；提交阶段利用估计参数设计优化动作序列以获得长期奖励。

Result: 算法实现了Õ(T^{2/3})的遗憾上界，解决了时间相关奖励学习和长期奖励优化动作序列设计两个关键挑战。

Conclusion: 通过系统辨识和不确定二次优化等价性证明，提供了有效的遗憾分析框架，并提出实用的半定松弛方法。

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [370] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 该论文对标签噪声检测方法进行了系统性基准测试，提出了一种将检测方法分解为三个基本组件的框架，并在视觉和表格数据集上评估了不同方法组合的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中的标签噪声会影响模型训练和验证，但现有检测方法缺乏明确的共识和系统性比较。

Method: 将标签噪声检测方法分解为三个组件：标签一致性函数、聚合方法和信息收集方法，并提出了统一的基准任务和评估指标。

Result: 研究发现，使用对数边际作为标签一致性函数、平均概率聚合和样本内信息收集的组合在大多数场景下表现最佳。

Conclusion: 该研究为设计新的检测方法和为特定应用选择技术提供了实用指导。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [371] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 本研究应用机器学习预测欧洲绿色协议气候政策的进展状态，比较了TF-IDF、BERT和ClimateBERT等文本表示方法，发现结合元数据特征时BERT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要有效的立法行动来缓解其影响，本研究旨在利用机器学习工具支持气候政策分析和决策制定。

Method: 收集了165项政策的文本和元数据，使用TF-IDF、BERT和ClimateBERT进行文本表示，并加入元数据特征，应用可解释AI方法分析影响因素。

Result: 仅使用文本特征时ClimateBERT表现最好（RMSE=0.17，R²=0.29），结合元数据特征时BERT表现最佳（RMSE=0.16，R²=0.38）。政策措辞、政党归属和国家代表性等因素对预测有显著影响。

Conclusion: 机器学习工具在气候政策分析中具有重要潜力，能够有效预测政策进展并识别关键影响因素。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [372] [One-Bit Quantization for Random Features Models](https://arxiv.org/abs/2510.16250)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: cs.LG

TL;DR: 本文分析了神经网络中一比特权重压缩的理论基础，证明在随机特征模型中，除最后一层外的所有权重量化不会损失泛化误差，并展示了实际推理加速效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络的计算和内存需求激增，促使研究在资源受限设备上的一比特权重压缩，但目前缺乏对这种压缩的理论理解。

Method: 在随机特征模型（神经网络的简化框架）中分析一比特量化，理论证明除最后一层外的所有权重量化不会影响泛化误差，并进行实证验证。

Result: 理论证明一比特量化在渐近意义上不会损失泛化误差，实证显示在笔记本电脑GPU上能显著加速随机特征模型的推理速度。

Conclusion: 一比特权重压缩在理论上可行且在实践中有效，为神经网络压缩提供了理论见解，并获得了比以往文献更一般化的结果。

Abstract: Recent advances in neural networks have led to significant computational and
memory demands, spurring interest in one-bit weight compression to enable
efficient inference on resource-constrained devices. However, the theoretical
underpinnings of such compression remain poorly understood. We address this gap
by analyzing one-bit quantization in the Random Features model, a simplified
framework that corresponds to neural networks with random representations. We
prove that, asymptotically, quantizing weights of all layers except the last
incurs no loss in generalization error, compared to the full precision random
features model. Our findings offer theoretical insights into neural network
compression. We also demonstrate empirically that one-bit quantization leads to
significant inference speed ups for the Random Features models even on a laptop
GPU, confirming the practical benefits of our work. Additionally, we provide an
asymptotically precise characterization of the generalization error for Random
Features with an arbitrary number of layers. To the best of our knowledge, our
analysis yields more general results than all previous works in the related
literature.

</details>


### [373] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV是一个用于强化学习网络代理训练和评估的可扩展环境，通过紧凑的浏览器环境和高效启动网络服务器解决了现有环境在上下文噪声、动作确定性和扩展性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习网络代理环境存在上下文噪声过多、动作执行不确定、无法有效扩展并行训练等问题，需要开发一个既能提供真实浏览器交互又能控制服务器状态的可扩展环境。

Method: 提出WEBSERV环境，包含：1）紧凑、站点无关的浏览器环境，平衡上下文和动作复杂度；2）通过高效启动和重置网络服务器实现可扩展的强化学习环境。

Result: 在WebArena的购物CMS和Gitlab任务上取得最先进的单提示成功率，同时将启动延迟降低约5倍，存储需求减少约240倍，内存占用相当，支持单主机200+并发容器。

Conclusion: WEBSERV是一个高效、可扩展的强化学习网络代理训练环境，显著提升了训练效率和性能。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [374] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出基于神经ODE的连续深度Evoformer，用连续时间参数化替代AlphaFold中48个离散块，实现恒定内存成本和计算效率提升。


<details>
  <summary>Details</summary>
Motivation: AlphaFold等蛋白质结构预测模型中的Evoformer深度架构（48个块）计算成本高且层间离散化，需要更高效轻量的替代方案。

Method: 采用神经常微分方程（Neural ODEs）将Evoformer的离散块替换为连续时间参数化，保持核心注意力操作，利用伴随方法实现恒定内存成本。

Result: 模型能生成结构合理的预测并可靠捕捉α-螺旋等二级结构元素，虽未完全复现原始架构精度，但仅用单GPU训练17.5小时，资源消耗大幅降低。

Conclusion: 连续深度模型为生物分子建模提供了轻量级、可解释的替代方案，为高效自适应的蛋白质结构预测框架开辟了新方向。

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [375] [Disentangling Hyperedges through the Lens of Category Theory](https://arxiv.org/abs/2510.16289)
*Yoonho Lee,Junseok Lee,Sangwoo Seo,Sungwon Kim,Yeongmin Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 本文从范畴论角度分析超边解缠，提出基于自然性条件的新解缠准则，并在基因通路数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管解缠表示学习在图结构数据中表现出色，但超图结构数据的解缠研究很少。将超边解缠集成到超图神经网络中，可以挖掘与标签相关的隐藏超边语义（如节点间的未标注关系）。

Method: 从范畴论视角分析超边解缠，提出基于自然性条件的新解缠准则，并构建概念验证模型。

Result: 实验证明所提准则能成功捕捉基因通路中基因（节点）间的功能关系。

Conclusion: 提出的解缠准则在基因通路数据上显示出潜力，为超图结构数据的解缠表示学习提供了新思路。

Abstract: Despite the promising results of disentangled representation learning in
discovering latent patterns in graph-structured data, few studies have explored
disentanglement for hypergraph-structured data. Integrating hyperedge
disentanglement into hypergraph neural networks enables models to leverage
hidden hyperedge semantics, such as unannotated relations between nodes, that
are associated with labels. This paper presents an analysis of hyperedge
disentanglement from a category-theoretical perspective and proposes a novel
criterion for disentanglement derived from the naturality condition. Our
proof-of-concept model experimentally showed the potential of the proposed
criterion by successfully capturing functional relations of genes (nodes) in
genetic pathways (hyperedges).

</details>


### [376] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 提出结合SVD和量化的方法，通过动态调整SVD秩来减少KV缓存大小和计算开销，在保持精度的同时显著降低内存使用和计算成本。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的高计算成本和内存占用限制了其可扩展性和实时应用性。

Method: 使用奇异值分解(SVD)处理QKV权重矩阵，结合动态秩分配策略和量化技术。

Result: 相比仅使用量化或SVD的方法，准确率提升超过10%，同时硬件成本更低。

Conclusion: 该方法更适合在资源受限设备上进行实时部署，实现了高效能的视觉语言模型。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [377] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: ScaffAug是一个基于支架的虚拟筛选框架，通过生成式AI增强数据、模型无关自训练和重排序模块，解决类不平衡、结构不平衡和支架多样性问题。


<details>
  <summary>Details</summary>
Motivation: 虚拟筛选面临三个主要挑战：类不平衡（活性分子比例低）、活性分子间的结构不平衡（某些支架占主导）、以及需要识别结构多样的活性化合物用于新药开发。

Method: 1. 增强模块：使用图扩散模型基于实际命中分子的支架生成合成数据；2. 模型无关自训练模块：安全整合生成数据与原始标记数据；3. 重排序模块：提高推荐分子集中的支架多样性。

Result: 在五个靶标类别上的综合计算实验表明，ScaffAug在多个评估指标上优于现有基线方法，同时通过消融研究验证了各模块的有效性。

Conclusion: 该工作通过利用生成增强、重排序和支架感知，为有效增强虚拟筛选提供了新的视角。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [378] [Toward General Digraph Contrastive Learning: A Dual Spatial Perspective](https://arxiv.org/abs/2510.16311)
*Daohan Su,Yang Zhang,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: S2-DiGCL是一个针对有向图的对比学习框架，通过复域和实域两个空间视角来捕捉有向图中的方向信息，在节点分类和链接预测任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法主要关注无向图，忽略了现实世界网络（如社交网络和推荐系统）中至关重要的方向信息。

Method: 从复域视角，在磁拉普拉斯矩阵中引入个性化扰动来调节边相位和方向语义；从实域视角，使用基于路径的子图增强策略来捕捉细粒度局部不对称性和拓扑依赖；联合利用这两个互补空间视图构建高质量正负样本。

Result: 在7个真实世界有向图数据集上的实验表明，该方法在节点分类任务上提升了4.41%，在链接预测任务上提升了4.34%，在监督和无监督设置下均达到SOTA性能。

Conclusion: S2-DiGCL通过结合复域和实域的空间视角，能够有效捕捉有向图中的方向信息，为有向图对比学习提供了更通用和鲁棒的解决方案。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful tool for
extracting consistent representations from graphs, independent of labeled
information. However, existing methods predominantly focus on undirected
graphs, disregarding the pivotal directional information that is fundamental
and indispensable in real-world networks (e.g., social networks and
recommendations).In this paper, we introduce S2-DiGCL, a novel framework that
emphasizes spatial insights from complex and real domain perspectives for
directed graph (digraph) contrastive learning. From the complex-domain
perspective, S2-DiGCL introduces personalized perturbations into the magnetic
Laplacian to adaptively modulate edge phases and directional semantics. From
the real-domain perspective, it employs a path-based subgraph augmentation
strategy to capture fine-grained local asymmetries and topological
dependencies. By jointly leveraging these two complementary spatial views,
S2-DiGCL constructs high-quality positive and negative samples, leading to more
general and robust digraph contrastive learning. Extensive experiments on 7
real-world digraph datasets demonstrate the superiority of our approach,
achieving SOTA performance with 4.41% improvement in node classification and
4.34% in link prediction under both supervised and unsupervised settings.

</details>


### [379] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 本文探讨了记忆与简单组合能力之间的协同作用，表明记忆长尾特征有助于模型对未见过的长尾特征组合做出正确预测。


<details>
  <summary>Details</summary>
Motivation: 深度学习促使研究者重新思考记忆与泛化的关系。在许多情况下，记忆不会损害泛化，反而可能通过记忆长尾样本来帮助泛化。本文旨在研究记忆与简单组合能力之间的协同作用。

Method: 在理论分析中，作者在线性设置下证明了记忆与组合能力的协同作用。在实验中，使用神经网络架构在简单数据上进行验证，并观察不同架构的组合能力差异。

Result: 理论分析表明，记忆与组合能力可以帮助模型对训练数据中从未见过的长尾特征组合做出正确预测。实验结果显示，这一理论见解可扩展到非线性设置，且模型的组合能力取决于其架构。

Conclusion: 记忆与组合能力之间存在协同作用，能够帮助模型处理罕见测试样本中的长尾特征组合，即使这些组合在训练数据中从未出现过。模型的架构对其组合能力有重要影响。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [380] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: 提出了MGTS-Net，一种用于时间序列预测的多模态图增强网络，通过三个核心组件解决多模态特征集成中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多模态时间序列预测方法面临三个关键挑战：细粒度时间模式提取不足、多模态信息集成次优、对动态多尺度特征适应性有限。

Method: MGTS-Net包含三个核心组件：多模态特征提取层（MFE）优化特征编码器提取细粒度时间特征；多模态特征融合层（MFF）构建异构图建模时间依赖和跨模态对齐关系；多尺度预测层（MSP）动态加权融合短、中、长期预测器输出。

Result: 大量实验表明MGTS-Net在轻量高效的同时表现出优异性能，相比其他最先进基线模型实现了更优越的性能。

Conclusion: 该方法验证了所提方法的优越性，能够有效解决多模态时间序列预测中的关键挑战。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [381] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出了一种融合数据分布先验信息的稀疏transformer架构，通过正则化Wasserstein近端算子改进优化问题的凸性，在生成建模和贝叶斯逆问题中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 将底层数据分布的先验信息直接融入transformer神经网络结构，改进传统基于流的模型的优化问题凸性并促进生成样本的稀疏性。

Method: 基于正则化Wasserstein近端算子的稀疏transformer架构，该算子具有闭式解且表现为transformer的特殊表示形式。

Result: 理论分析和数值实验表明，稀疏transformer在生成建模和贝叶斯逆问题中比传统基于神经ODE的方法具有更高精度和更快收敛速度。

Conclusion: 所提出的稀疏transformer架构通过融入先验信息显著提升了生成模型的性能，在多个应用中展现出优越表现。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [382] [Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures](https://arxiv.org/abs/2510.16411)
*Minh-Khoi Nguyen-Nhat,Rachel S. Y. Teo,Laziz Abdullaev,Maurice Mok,Viet-Hoang Tran,Tan Minh Nguyen*

Main category: cs.LG

TL;DR: SymphonySMoE通过引入专家间的社交图来增强稀疏混合专家模型的鲁棒性，解决传统SMoE在数据分布变化时的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏混合专家模型虽然能解耦参数数量和计算成本，但在数据分布变化时鲁棒性不足，容易受到数据污染的影响。

Method: 提出SymphonySMoE，在SMoE基础上引入专家间的社交图结构，改进token路由过程，增强模型对分布变化的适应能力。

Result: 在语言建模和视觉指令调优任务上验证了方法的有效性，并成功扩展到42亿和74亿参数的大规模模型。

Conclusion: SymphonySMoE是一种轻量级、模块化的SMoE改进方案，能有效提升模型在分布变化下的鲁棒性，适用于大规模系统的微调任务。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a promising solution to
achieving unparalleled scalability in deep learning by decoupling model
parameter count from computational cost. By activating only a small subset of
parameters per sample, SMoE enables significant growth in model capacity while
maintaining efficiency. However, SMoE struggles to adapt to distributional
shifts, leading to reduced robustness under data contamination. In this work,
we introduce SymphonySMoE, a novel family of SMoE that introduces a social
graph to model interactions among experts. This graph-based structure enhances
the token routing process, addressing the robustness challenges that are
inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,
and integrates seamlessly with existing SMoE-based models such as the XMoE and
the Generalist Language Model. We provide both theoretical analysis and
empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.
Extensive experiments on language modeling and visual instruction tuning
validate our method's effectiveness. We further highlight the scalability of
SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its
applicability in fine-tuning tasks for large-scale systems.

</details>


### [383] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文介绍了ECML-PKDD 2025高能物理发现挑战赛中Task 1的获胜解决方案，采用多轮梯度攻击策略对抗分类模型。


<details>
  <summary>Details</summary>
Motivation: 任务要求设计对抗性攻击，在最小化扰动的同时最大化分类错误率，以测试模型的鲁棒性。

Method: 使用多轮梯度攻击策略，利用模型的可微分结构，结合随机初始化和样本混合技术增强攻击效果。

Result: 攻击在扰动大小和欺骗成功率方面取得最佳结果，在竞赛中获得第一名。

Conclusion: 提出的多轮梯度攻击策略在对抗性攻击任务中表现优异，验证了该方法的有效性。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [384] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文提出了在ECML-PKDD 2025高能物理发现挑战赛中Task 2的获胜解决方案，通过数据生成和鲁棒模型训练两阶段方法，在清洁和对抗数据上实现了80%的混合准确率。


<details>
  <summary>Details</summary>
Motivation: 设计能够同时在清洁数据和对抗数据上保持高准确率的鲁棒ANN模型，应对高能物理发现中的对抗攻击挑战。

Method: 采用两阶段方法：1) 基于RDSA方法生成1500万人工训练样本；2) 构建包含特征嵌入块（共享权重）和密集融合尾部的鲁棒架构进行训练。

Result: 在混合数据集上达到80%的准确率，比第二名解决方案高出2个百分点。

Conclusion: 提出的两阶段方法和鲁棒架构设计能够有效提升模型在对抗攻击下的性能，在高能物理发现任务中表现出色。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [385] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出Input Domain Aware MoE路由框架，通过概率混合模型更好地划分输入空间，解决现有稀疏专家混合模型中路由机制难以有效捕捉输入结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性评分的路由机制难以有效捕捉输入结构，导致专家专业化与计算平衡之间的权衡，阻碍了可扩展性和性能提升。

Method: 使用概率混合模型建模路由概率，使专家形成清晰的专业化边界并实现均衡利用。路由机制独立于任务特定目标进行训练，实现稳定优化和明确的专家分配。

Result: 在视觉语言任务上的实证结果表明，该方法持续优于现有稀疏专家混合方法，获得更高的任务性能和改善的专家利用平衡。

Conclusion: 提出的Input Domain Aware MoE框架通过更好的输入空间划分，解决了稀疏专家混合模型中的路由问题，实现了专家专业化与计算效率的更好平衡。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [386] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出了一个用于模仿学习的序列强化学习框架，专门建模传粉者中异质的认知策略，通过轨迹相似性捕捉和预测依赖不同策略的个体行为。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在专家策略随记忆窗口变化或偏离最优性时表现不佳，无法准确再现关键决策模式，且缺乏可解释性，限制了生物学洞察。

Method: 引入最小化预测损失同时识别与行为数据最一致的有效记忆范围的模型，确保完全可解释性，并提供连接蜜蜂策略搜索与不同探索-利用动态下多臂老虎机问题的数学框架。

Result: 创建了包含80只蜜蜂在不同天气条件下追踪数据的新数据集，改进了对昆虫行为在农业生态系统中模拟的准确性。

Conclusion: 该研究揭示了塑造传粉者决策的学习策略和记忆相互作用，为传粉者认知研究和生态治理提供了新见解。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [387] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 提出了一种新的自适应核注意力机制，通过分别处理不同特征组来增强高维异构数据的预测性能，解决了传统PLS方法在非线性关系和跨尺度交互方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统PLS方法难以处理高维异构数据中的复杂非线性关系和跨尺度交互，静态特征权重无法适应上下文变化，需要新的方法来捕捉局部模式同时保持全局关系。

Method: 采用自适应核注意力机制，分别处理不同特征组后进行整合，能够捕捉局部模式同时保持全局关系，解决了多尺度同时交互的问题。

Result: 实验结果显示，与现有最先进方法相比，该方法在多个数据集上的性能指标均有显著提升。

Conclusion: 所提出的自适应核注意力架构有效解决了高维异构数据中的复杂非线性关系和跨尺度交互问题，显著提升了预测性能。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [388] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: OracleAD是一个简单可解释的无监督多元时间序列异常检测框架，通过因果嵌入建模时间动态，使用自注意力机制捕捉空间关系，并将嵌入对齐到稳定潜在结构来识别异常。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多元时间序列异常稀少且通常无标签，现有方法依赖复杂架构，只能检测部分异常片段且性能被高估。

Method: 将每个变量的过去序列编码为因果嵌入来联合预测当前时间点和重建输入窗口，使用自注意力机制将嵌入投影到共享潜在空间，并将投影嵌入对齐到代表正常状态关系的稳定潜在结构。

Result: 在多个真实世界数据集和评估协议上实现了最先进的结果。

Conclusion: OracleAD通过稳定潜在结构实现可解释性，能够在嵌入级别直接定位根本原因变量，同时保持高性能。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [389] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 提出了一种基于连通性因子(CF)的eDCF方法，用于鲁棒估计高维数据的内在维度，在噪声环境下表现优异，并能检测分形几何结构。


<details>
  <summary>Details</summary>
Motivation: 现代数据集常包含具有复杂依赖关系的高维特征，但内在维度估计受尺度影响：细尺度下噪声会膨胀估计值，粗尺度下估计值趋于稳定。需要一种能在不同尺度下稳健估计内在维度的方法。

Method: 基于连通性因子(CF)的eDCF方法，这是一种基于局部连通性的度量方法，具有可扩展性和并行化特性。

Result: 在合成基准测试中，eDCF与领先估计器表现相当，在中等至高噪声水平和大数据集下，精确内在维度匹配率达到25.0%，优于MLE(16.7%)和TWO-NN(12.5%)。

Conclusion: eDCF方法能够准确检测决策边界中的分形几何结构，证实了其在分析现实结构化数据中的实用性。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [390] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: 本文质疑LLM在因果发现中的真实能力，指出现有评估存在数据泄露问题，提出基于新科学研究的评估方案和结合LLM与统计方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 挑战LLM在因果发现中表现优异的说法，因为现有评估可能受到预训练数据泄露的影响，需要开发更可靠的评估方法和实用的混合方法。

Method: 提出两个关键转变：开发基于新科学研究的评估协议防止数据泄露；设计结合LLM知识和数据驱动统计的混合方法。使用PC算法结合LLM预测作为先验。

Result: 在BNLearn基准上LLM表现接近完美，但在作者构建的新科学图表上表现较差。将LLM预测作为PC算法先验能显著提高准确性，优于纯LLM和纯统计方法。

Conclusion: 呼吁社区采用基于科学、抗泄露的基准测试，并投资于适合真实世界研究的混合因果发现方法。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [391] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 该研究使用机器学习算法预测生活满意度，准确率达93.80%，并通过特征学习提取27个重要问题。同时探索了临床和生物医学大语言模型，发现生物医学领域与生活满意度预测更相关。健康状况是所有年龄段最重要的决定因素。


<details>
  <summary>Details</summary>
Motivation: 传统的生活满意度测量方法存在验证和传播问题，本研究旨在展示机器学习在预测生活满意度方面的潜力，为理解人类行为和主观幸福感提供更可靠的工具。

Method: 使用丹麦19000名16-64岁人群的政府调查数据，采用特征学习技术提取重要问题，并探索临床和生物医学LLMs通过将表格数据转换为自然语言句子进行预测。进行了消融研究分析数据重采样和特征选择的影响。

Result: 机器学习模型达到93.80%准确率和73.00%宏F1分数，LLMs达到93.74%准确率和73.21%宏F1分数。健康状况被确定为所有年龄段最重要的生活满意度决定因素。

Conclusion: 机器学习、大语言模型和可解释AI可以共同构建对使用AI研究人类行为的信任和理解，对量化主观幸福感具有重要影响。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [392] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: 提出了NeurIPT，一个用于多样化EEG神经接口的预训练Transformer基础模型，通过捕捉EEG信号的同质和异质时空特征，在多个BCI数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG数据中存在的受试者间、任务间、条件间变异性以及不同记录设置的电极配置多样性等挑战，建立可扩展和泛化的神经解码基础模型。

Method: 1) 时间上：引入基于信号幅度的掩码预训练(AAMP)和渐进专家混合架构(PMoE)；2) 空间上：利用电极3D物理坐标实现跨设置嵌入迁移，开发脑叶内外池化(IILP)利用区域脑特征。

Result: 在八个下游BCI数据集上的微调评估显示，NeurIPT始终达到最先进的性能，展示了其广泛的适用性和强大的泛化能力。

Conclusion: 该工作推动了EEG基础模型的发展，为可扩展和泛化的神经信息处理系统提供了见解。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [393] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO是一个新的强化学习框架，通过分离语言反馈和数值奖励的角色来解决LLM训练中的效率问题，语言指导探索，数值奖励驱动优化。


<details>
  <summary>Details</summary>
Motivation: 传统LLM强化学习依赖标量奖励，丢弃了rollout中的宝贵文本推理信息，导致样本效率低下。同时，在线整合语言反馈存在信息泄露和无关上下文导致行为崩溃的矛盾。

Method: LANPO构建动态经验池，引入两个原则：奖励无关反思用于安全的自校正，相关抽象从跨样本经验中提取可泛化教训。

Result: 在数学推理基准测试中，7B和14B模型使用LANPO显著优于使用GRPO训练的强基线模型，测试准确率更高。

Conclusion: LANPO为将历史经验整合到LLM强化学习循环中提供了稳健方法，创造了更有效和数据高效的学习智能体。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [394] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出C-SMILES分子表示方法，通过分解SMILES为元素-标记对和特殊标记，减少反应物与产物间的编辑距离，结合复制增强机制和SMILES对齐指导，显著提升逆合成预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间过大和预测精度降低。需要开发能更好保留分子骨架不变部分的方法。

Method: 使用C-SMILES表示法分解分子结构，结合复制增强机制动态决定生成新标记或保留产物片段，并集成SMILES对齐指导提升注意力一致性。

Result: 在USPTO-50K数据集上达到67.2%的top-1准确率，在USPTO-FULL数据集上达到50.8%准确率，生成分子有效性达99.9%。

Conclusion: 该方法为结构感知分子生成建立了新范式，在计算药物发现中具有直接应用价值。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [395] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 提出了一种无需标注数据的分子推理框架，通过原子标识符将思维链推理锚定到分子结构上，在单步逆合成任务中取得了优异性能


<details>
  <summary>Details</summary>
Motivation: 化学中机器学习应用常受限于标注数据的稀缺性和昂贵成本，传统监督方法受限，需要开发不依赖标注数据的通用方法

Method: 使用通用大语言模型进行分子推理，通过原子标识符将思维链推理锚定到分子结构，采用一步任务识别相关片段和化学标签，可选二步任务利用位置感知信息预测化学转化

Result: 在学术基准和专家验证的药物发现分子上，LLMs在识别化学合理反应位点(≥90%)、命名反应类别(≥40%)和最终反应物(≥74%)方面取得高成功率

Conclusion: 该框架不仅解决了复杂化学任务，还提供了一种通过将化学知识映射到分子结构来生成理论基础的合成数据集的方法，从而解决数据稀缺问题

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [396] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: 该论文研究了神经网络中参数对称性和表达能力在泛化行为中的作用，特别是在学习重整化群变换时。研究发现对称约束和表达能力之间存在竞争关系，过度复杂或过度约束的模型泛化能力较差。


<details>
  <summary>Details</summary>
Motivation: 将物理对称性编码到深度学习模型中可以提高性能，但参数对称性破坏和恢复机制在层次学习动态中的作用需要进一步研究。

Method: 使用多层感知机(MLP)和图神经网络(GNN)，通过改变权重对称性和激活函数来评估模型在学习重整化群变换时的表现。通过将中心极限定理重新表述为累积量递归关系来分析MLP的泛化行为。

Result: 发现对称约束和表达能力之间存在竞争关系，过度复杂或过度约束的模型泛化能力较差。成功将累积量传播框架从MLP扩展到GNN，阐明了这些复杂模型的内部信息处理过程。

Conclusion: 这些发现为对称网络的学习动态及其在建模结构化物理变换中的局限性提供了新的见解。

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [397] [Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules](https://arxiv.org/abs/2510.16607)
*Tianwei Wang,Xinhui Ma,Wei Pang*

Main category: cs.LG

TL;DR: 提出了一种基于四元数的监督学习Hopfield结构神经网络(QSHNN)，利用四元数在表示旋转和姿态方面的几何优势，通过周期性投影策略保持四元数结构一致性，实现了高精度、快速收敛和强可靠性。


<details>
  <summary>Details</summary>
Motivation: 利用四元数在表示旋转和姿态方面的几何优势，扩展经典Hopfield神经网络到四元数域，为超复数或非交换代数结构下的神经网络设计提供实用框架。

Method: 从连续时间HNN动力学模型出发扩展到四元数域，引入周期性投影策略修改标准梯度下降，将权重矩阵的4*4块投影到最近的四元数结构，保持收敛性和四元数一致性。

Result: 实验模型实现了高精度、快速收敛和强可靠性，演化轨迹具有良好有界曲率（充分平滑性），适用于机器人控制等应用场景。

Conclusion: 该模型不仅为机器人控制等应用提供了实用解决方案，还为超复数或非交换代数结构下的神经网络设计提供了通用数学方法论。

Abstract: Motivated by the geometric advantages of quaternions in representing
rotations and postures, we propose a quaternion-valued supervised learning
Hopfield-structured neural network (QSHNN) with a fully connected structure
inspired by the classic Hopfield neural network (HNN). Starting from a
continuous-time dynamical model of HNNs, we extend the formulation to the
quaternionic domain and establish the existence and uniqueness of fixed points
with asymptotic stability. For the learning rules, we introduce a periodic
projection strategy that modifies standard gradient descent by periodically
projecting each 4*4 block of the weight matrix onto the closest quaternionic
structure in the least-squares sense. This approach preserves both convergence
and quaternionic consistency throughout training. Benefiting from this rigorous
mathematical foundation, the experimental model implementation achieves high
accuracy, fast convergence, and strong reliability across randomly generated
target sets. Moreover, the evolution trajectories of the QSHNN exhibit
well-bounded curvature, i.e., sufficient smoothness, which is crucial for
applications such as control systems or path planning modules in robotic arms,
where joint postures are parameterized by quaternion neurons. Beyond these
application scenarios, the proposed model offers a practical implementation
framework and a general mathematical methodology for designing neural networks
under hypercomplex or non-commutative algebraic structures.

</details>


### [398] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 本文研究了测试时增强（如RAG）中模型参数知识与外部检索信息的关系，将多步推理建模为知识图上的连通性问题，揭示了知识密度对增强效率的关键影响。


<details>
  <summary>Details</summary>
Motivation: 理解测试时增强中模型预训练知识与外部检索信息之间的理论关系，特别是确定在少量增强步骤下准确回答问题所需的最小预训练知识量。

Method: 将多步推理建模为知识图上的s-t连通性问题，将模型预训练知识表示为部分可能含噪声的子图，将增强视为查询真实边来扩展模型知识。

Result: 发现相变现象：当先验知识图断开成小组件时，通过增强寻找路径效率低下，需要Ω(√n)次查询；但当正确知识密度超过阈值形成巨组件时，只需期望常数次查询即可找到路径。

Conclusion: 模型预训练知识的结构密度对测试时增强效率有决定性影响，存在一个临界密度阈值，超过该阈值后增强效率显著提升。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [399] [On the Impossibility of Retrain Equivalence in Machine Unlearning](https://arxiv.org/abs/2510.16629)
*Jiatong Yu,Yinghui He,Anirudh Goyal,Sanjeev Arora*

Main category: cs.LG

TL;DR: 论文揭示了多阶段训练对机器遗忘的根本性障碍，证明局部遗忘方法无法普遍实现重训练等价性，因为遗忘结果依赖于训练阶段的顺序。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习流水线通常涉及多阶段训练，但现有的机器遗忘理论主要针对i.i.d.数据批次训练。本研究旨在探索多阶段训练对机器遗忘的影响。

Method: 通过理论和实验分析多阶段训练中的机器遗忘问题，在Llama和Qwen模型(1B到14B)上使用梯度上升、NPO和SimNPO等局部遗忘算法进行实证研究。

Result: 不同训练顺序的模型在遗忘过程中行为差异显著，GSM8K准确率下降幅度在不同路径间超过20%。某些学习路径产生的模型遗忘速度较慢，概率质量分配也呈现路径依赖性。

Conclusion: 重训练等价性对于局部遗忘算法来说是一个不适定的目标，在无法获取模型训练历史的情况下需要重新思考机器遗忘的定义和期望目标。

Abstract: Machine unlearning seeks to selectively remove the "influence" of specific
training data on a model's outputs. The ideal goal is Retrain
Equivalence--behavior identical to a model trained from scratch on only the
retained data. This goal was formulated for models trained on i.i.d. data
batches, but modern pipelines often involve multi-stage training, with each
stage having a distinct data distribution and objective. Examples include LLM
fine-tuning for alignment, reasoning ability, etc. Our study shows via theory
and experiments that this shift to multi-stage training introduces a
fundamental barrier for machine unlearning. The theory indicates that the
outcome of local unlearning--methods that only use gradients computed on the
forget set--is path-dependent. That is, a model's behavior during unlearning is
influenced by the order of its training stages during learning, making it
impossible for path-oblivious algorithms to universally achieve Retrain
Equivalence. We empirically demonstrate the same phenomenon in LLM
post-training across Llama and Qwen models (1B to 14B) with gradient ascent,
NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different
orderings of identical training stages diverge in behavior during unlearning,
with the degradation in GSM8K accuracy after unlearning varying by over 20%
across paths. We also observe that some learning paths consistently produce
models that unlearn slowly. During unlearning, whether the probability mass
gets squeezed into paraphrasing or alternative concepts is also path-dependent.
These results consistently show that Retrain Equivalence is an ill-posed target
for local unlearning algorithms, so long as the target models are trained in
stages. In situations where access to models' training histories is hard, the
current work calls for rethinking the definition and desiderata of machine
unlearning.

</details>


### [400] [Simulation-free Structure Learning for Stochastic Dynamics](https://arxiv.org/abs/2510.16656)
*Noah El Rimawi-Fine,Adam Stecklov,Lucas Nelson,Mathieu Blanchette,Alexander Tong,Stephen Y. Zhang,Lazar Atanackovic*

Main category: cs.LG

TL;DR: StructureFlow是一个新颖的模拟自由方法，能够同时学习物理系统的结构和随机群体动力学，解决了现有方法无法同时处理结构学习和动力学建模的问题。


<details>
  <summary>Details</summary>
Motivation: 许多自然系统中的物理系统（如细胞生物学）具有高维、随机特性，且只能获得部分噪声状态测量，这给建模底层动力学和推断网络结构带来了重大挑战。现有方法通常只能单独处理结构学习或群体水平动力学建模。

Method: 提出StructureFlow方法，这是一个基于原则的模拟自由方法，能够联合学习物理系统的结构和随机群体动力学。该方法支持从干预中学习结构以及条件群体动力学的轨迹推断。

Result: 在合成高维系统、生物模拟系统和实验单细胞数据集上的实证评估表明，StructureFlow能够学习底层系统的结构，同时建模其条件群体动力学。

Conclusion: StructureFlow能够同时学习系统结构和条件群体动力学，这是理解系统行为机制的关键步骤。

Abstract: Modeling dynamical systems and unraveling their underlying causal
relationships is central to many domains in the natural sciences. Various
physical systems, such as those arising in cell biology, are inherently
high-dimensional and stochastic in nature, and admit only partial, noisy state
measurements. This poses a significant challenge for addressing the problems of
modeling the underlying dynamics and inferring the network structure of these
systems. Existing methods are typically tailored either for structure learning
or modeling dynamics at the population level, but are limited in their ability
to address both problems together. In this work, we address both problems
simultaneously: we present StructureFlow, a novel and principled
simulation-free approach for jointly learning the structure and stochastic
population dynamics of physical systems. We showcase the utility of
StructureFlow for the tasks of structure learning from interventions and
dynamical (trajectory) inference of conditional population dynamics. We
empirically evaluate our approach on high-dimensional synthetic systems, a set
of biologically plausible simulated systems, and an experimental single-cell
dataset. We show that StructureFlow can learn the structure of underlying
systems while simultaneously modeling their conditional population dynamics --
a key step toward the mechanistic understanding of systems behavior.

</details>


### [401] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: PUMBA是一个改进的蛋白质-蛋白质对接评分函数，用Vision Mamba架构替换了PIsToN中的Vision Transformer，在多个数据集上表现优于原模型。


<details>
  <summary>Details</summary>
Motivation: 现有的PIsToN评分函数使用Vision Transformer，而Mamba架构在自然语言处理和计算机视觉领域表现出色，能够更有效地进行长序列建模。

Method: 将PIsToN中的Vision Transformer主干替换为Vision Mamba架构，利用Mamba对图像补丁序列的高效长程建模能力。

Result: 在多个广泛使用的大规模公共数据集上的评估表明，PUMBA始终优于其基于Transformer的前身PIsToN。

Conclusion: 使用Vision Mamba架构能够显著提升模型捕捉蛋白质-蛋白质界面特征中全局和局部模式的能力。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [402] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 提出了一种在无信息先验情况下仍能有效进行主动目标发现的新方法，该方法具有理论依据、神经科学启发、可解释性强，并在多个领域实验中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在数据获取成本高昂的领域（如医学成像、环境监测），传统基于强先验的生成模型在数据极度有限或采样成本高的场景下难以泛化，需要解决无信息先验下的主动目标发现问题。

Method: 提出理论原则化且受神经科学启发的框架，确保先验估计随新观测单调改进，提供可解释的决策过程，在无信息先验设置下实现稳健探索。

Result: 在物种分布建模和遥感等多个领域的综合实验表明，该方法显著优于基线方法，证明了其在复杂现实场景中的有效性和适应性。

Conclusion: 该方法成功克服了传统方法在无信息先验情况下的局限性，为高成本数据采集领域提供了可靠且自适应的主动目标发现解决方案。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [403] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 提出了一个紧凑、严格因果的临床时间序列流式基准，在MIT-BIH心律失常数据库上使用每秒心率数据。研究两个任务：短期心动过速风险预测和单步心率预测，比较GRU-D和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 在纵向监测中，需要评估不同模型在临床时间序列任务上的表现，特别是在紧凑计算预算下的性能比较。

Method: 使用MIT-BIH心律失常数据库的每秒心率数据，采用记录级非重叠分割。比较GRU-D（RNN）和Transformer模型，在匹配的训练预算下与强非学习基线对比。评估采用校准感知分类和适当预测，使用温度缩放和分组bootstrap置信区间。

Result: 在MIT-BIH上，GRU-D在心动过速风险预测上略优于Transformer，而Transformer在预测误差上明显低于GRU-D和持续性模型。

Conclusion: 在纵向监测中，模型选择是任务依赖的：紧凑RNN在短期风险评分上仍具竞争力，而紧凑Transformer在点预测上提供更清晰的增益。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [404] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 本文通过扩散方法精确分析噪声SGD，提供连续时间视角来捕捉高维设置下的统计风险演变和隐私损失动态，并研究了一种无需梯度敏感度显式知识的噪声SGD变体。


<details>
  <summary>Details</summary>
Motivation: 优化与隐私保护的相互作用已成为隐私保护机器学习的核心主题。噪声SGD已成为基石算法，但现有工作主要提供统计风险和隐私损失的各种界限，而过程的精确行为仍不清楚，特别是在高维设置下。

Method: 利用扩散方法分析噪声SGD，提供连续时间视角；研究一种无需梯度敏感度显式知识的噪声SGD变体，专注于带ℓ2正则化的最小二乘问题。

Result: 该方法能够精确捕捉高维设置下统计风险演变和隐私损失动态，避免了现有工作中假设或通过梯度裁剪强制执行敏感度的需求。

Conclusion: 扩散方法为噪声SGD提供了精确分析框架，能够更好地理解高维设置下的隐私-效用权衡，并为无需显式敏感度知识的算法设计提供了新思路。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [405] [CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning](https://arxiv.org/abs/2510.16694)
*Anthony DiMaggio,Raghav Sharma,Gururaj Saileshwar*

Main category: cs.LG

TL;DR: 提出了CLIP技术，通过客户端不变神经元剪枝和网络感知剪枝来解决安全联邦学习中由计算和网络瓶颈导致的慢客户端问题，加速训练13%-34%


<details>
  <summary>Details</summary>
Motivation: 安全联邦学习在异构设备部署时，由于计算或网络能力有限的慢客户端导致性能瓶颈，拖慢所有参与客户端的训练速度

Method: CLIP技术结合客户端不变神经元剪枝和网络感知剪枝，在训练过程中解决慢客户端带来的计算和网络瓶颈

Result: 在多个数据集（CIFAR10、Shakespeare、FEMNIST）上加速安全联邦学习训练13%-34%，准确率影响在提升1.3%到降低2.6%之间

Conclusion: CLIP是首个针对安全聚合的慢客户端缓解技术，能有效加速安全联邦学习训练且精度损失最小

Abstract: Secure federated learning (FL) preserves data privacy during distributed
model training. However, deploying such frameworks across heterogeneous devices
results in performance bottlenecks, due to straggler clients with limited
computational or network capabilities, slowing training for all participating
clients. This paper introduces the first straggler mitigation technique for
secure aggregation with deep neural networks. We propose CLIP, a client-side
invariant neuron pruning technique coupled with network-aware pruning, that
addresses compute and network bottlenecks due to stragglers during training
with minimal accuracy loss. Our technique accelerates secure FL training by 13%
to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an
accuracy impact of between 1.3% improvement to 2.6% reduction.

</details>


### [406] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨了基于状态的因果效应可识别性，证明了即使变量级因果效应不可识别，状态级因果效应仍可能可识别，这需要额外的上下文特定独立性和条件函数依赖知识。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应可识别性仅关注变量层面，但实际应用中往往需要了解特定状态干预对特定状态结果的影响，现有变量级框架可能错过这种可识别性。

Method: 通过分析状态级因果效应的可识别性条件，研究上下文特定独立性、条件函数依赖和变量状态约束等额外知识的作用。

Result: 发现状态级因果效应在变量级不可识别时仍可能可识别，这种分离需要上下文特定独立性等额外知识；变量状态约束知识单独不能改善可识别性，但与其他知识结合可同时改善变量级和状态级可识别性。

Conclusion: 状态级因果效应可识别性为从观测数据估计因果效应提供了新视角，现有变量级框架可能低估了实际可识别性，强调了上下文特定知识在因果推断中的重要性。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [407] [LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus](https://arxiv.org/abs/2510.16719)
*Zak Ressler,Marcus Grijalva,Angelica Marie Ignacio,Melanie Torres,Abelardo Cuadra Rojas,Rohollah Moghadam,Mohammad Rasoul narimani*

Main category: cs.LG

TL;DR: 提出基于LSTM的电动汽车充电负荷预测框架，通过数据预处理和特征提取来预测多时间尺度的充电需求。


<details>
  <summary>Details</summary>
Motivation: 为电动汽车充电设施的基础设施规划、能源管理和电网整合提供准确的负荷预测支持。

Method: 使用LSTM循环神经网络，通过数据预处理（插值和归一化）和特征提取来处理多地点原始数据，捕捉短期波动和长期趋势。

Result: 实验结果显示模型能够准确预测日、周、月等多时间尺度的充电需求，适用于不同使用模式的充电地点。

Conclusion: 该模块化框架能够适应不同部署场景，为EV充电设施的管理和规划提供有价值的见解。

Abstract: This paper presents a framework for processing EV charging load data in order
to forecast future load predictions using a Recurrent Neural Network,
specifically an LSTM. The framework processes a large set of raw data from
multiple locations and transforms it with normalization and feature extraction
to train the LSTM. The pre-processing stage corrects for missing or incomplete
values by interpolating and normalizing the measurements. This information is
then fed into a Long Short-Term Memory Model designed to capture the short-term
fluctuations while also interpreting the long-term trends in the charging data.
Experimental results demonstrate the model's ability to accurately predict
charging demand across multiple time scales (daily, weekly, and monthly),
providing valuable insights for infrastructure planning, energy management, and
grid integration of EV charging facilities. The system's modular design allows
for adaptation to different charging locations with varying usage patterns,
making it applicable across diverse deployment scenarios.

</details>


### [408] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文提出了一种基于多任务学习和分层高斯过程的方法来预测NLP模型的学习曲线，支持零样本预测和主动学习策略，在三个小型NLP数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线可以帮助做出明智的决策以满足特定性能目标，同时减少计算开销和数据集获取与整理的成本。

Method: 将学习曲线预测任务制定为多任务学习问题，使用潜在变量多输出高斯过程建模任务间和层次间的共享信息和依赖关系，支持任务相关性和零样本预测。

Result: 该方法能够以较低成本开发概率性缩放定律，通过主动学习策略查询学习曲线可以减少预测不确定性，提供接近真实缩放定律的预测结果。

Conclusion: 在三个小型NLP数据集上的验证表明，该框架能够有效预测不同模型的学习曲线，包括nanoGPT模型、双语翻译模型和多语言翻译模型。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [409] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 提出了一种用于SegDeformer的联合特征和任务解码方法，在车载和分布式应用中降低计算复杂度，同时保持语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现代汽车系统使用DNN进行语义分割，存在两种应用场景：车载应用（无数据传输限制）和分布式应用（有传输比特率限制）。现有方法使用卷积神经网络进行联合源和任务解码，但未研究基于transformer的替代方案如SegDeformer，后者性能更优但计算复杂度更高。

Method: 为SegDeformer提出联合特征和任务解码方法，在车载和分布式应用中降低计算复杂度。两种应用共享图像和源编码器，但使用不同的联合源和任务解码器。

Result: 在车载应用中，Cityscapes数据集上fps提升11.7倍（1.4 fps到16.5 fps），ADE20K数据集上提升3.5倍（43.3 fps到154.3 fps），同时mIoU与未压缩的transformer基线相当。在分布式应用中，在广泛的比特率范围内实现了SOTA的mIoU性能，仅使用先前SOTA方法0.14%（ADE20K）和0.04%（Cityscapes）的云DNN参数。

Conclusion: 提出的联合特征和任务解码方法成功降低了SegDeformer的计算复杂度，在保持语义分割性能的同时显著提升了处理速度，并在分布式应用中实现了参数效率的显著改进。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [410] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: 提出SAMOSA方法，通过基于样本典型性的主动查询策略，在开放集主动学习中有效选择信息量大的样本，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习需要大量数据标注，但标注成本高昂。开放集主动学习旨在从未标注数据中选择信息量大的样本，其中包含不相关或未知类别。

Method: 基于SGD和SAM的理论发现，SAMOSA根据样本典型性主动查询样本，识别嵌入流形中靠近模型决策边界的非典型样本。

Result: 在多个数据集上实现比现有方法最高3%的准确率提升，且不引入额外计算开销。

Conclusion: SAMOSA是一种有效的开放集主动学习查询算法，能优先选择对目标类别信息量大且有助于区分目标与不相关类别的样本。

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [411] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 该论文提出了一个用于3D第一人称视频游戏的多模态推理数据集和模型，通过逆动力学模型和行为克隆训练出能够响应文本指令的实时游戏AI。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称视频游戏为实时多模态推理提供了具有挑战性的环境，但现有数据集规模有限且缺乏多样性。

Method: 收集大规模多样化的游戏数据集，学习逆动力学模型来推断缺失的动作，使用行为克隆训练文本条件化的游戏AI代理。

Result: 开发出的模型能够在多种3D游戏中实时运行，并能够响应文本输入指令进行游戏。

Conclusion: 虽然取得了进展，但仍面临长时程任务和跨游戏定量评估等挑战。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [412] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 3D-GSRD是一种用于分子表示学习的3D图自动编码器，通过选择性重掩码解码解决2D到3D掩码图建模的挑战，在MD17基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 将掩码图建模从2D扩展到3D面临两个冲突挑战：避免2D结构泄漏到解码器，同时为重构重掩码原子提供足够的2D上下文。

Method: 提出选择性重掩码解码(SRD)，仅从编码器表示中重掩码3D相关信息，同时保留2D图结构；结合3D关系变换器编码器和结构无关解码器。

Result: 在广泛使用的MD17分子性质预测基准测试中，在8个目标中的7个上达到了新的最先进性能。

Conclusion: 3D-GSRD通过选择性重掩码解码和结构无关解码器的协同集成，增强了编码器在分子表示学习中的作用，实现了优异的性能。

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [413] [Mixed-Precision Quantization for Language Models: Techniques and Prospects](https://arxiv.org/abs/2510.16805)
*Mariam Rakka,Marios Fournarakis,Olga Krestinskaya,Jinane Bazzi,Khaled N. Salama,Fadi Kurdahi,Ahmed M. Eltawil,Mohammed E. Fouda*

Main category: cs.LG

TL;DR: 这篇论文是关于语言模型混合精度量化(MXPLM)的综述，探讨了如何通过选择性分配精度来平衡模型效率和准确性，解决大规模语言模型的计算和内存需求问题。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模的快速增长，其计算、内存和能耗需求急剧增加，使得训练和部署变得不可持续。量化技术虽然能减少模型大小和加速推理，但均匀低比特量化会降低敏感组件的准确性。

Method: 论文首先回顾量化基础知识，然后根据比特分配策略和精度配置对MXPLM框架进行分类比较，分析不同方法在困惑度、零样本任务性能和部署权衡方面的差异。

Result: 通过对比分析，论文识别了在语言模型设置中有效和面临挑战的混合精度量化策略，并总结了不同框架的性能表现。

Conclusion: 论文为理解大规模语言模型混合精度量化的现状和研究前景提供了参考，并指出了硬件感知设计、激活量化和可扩展优化方法等未来研究方向。

Abstract: The rapid scaling of language models (LMs) has resulted in unprecedented
computational, memory, and energy requirements, making their training and
deployment increasingly unsustainable. Quantization has emerged as an essential
compression technique to reduce model size, alleviate memory bottlenecks, and
accelerate inference. However, while uniform low-bit quantization (e.g., INT8,
INT4) provides significant efficiency gains, it can degrade accuracy in
sensitive components of transformer-based LMs. Mixed-precision quantization
offers a promising alternative by selectively allocating precision across
layers or within tensors to balance efficiency and accuracy. This survey
provides a comprehensive overview of Mixed-Precision quantization frameworks
for LMs (MXPLMs). We first review quantization fundamentals, including uniform
and non-uniform quantizers, quantization granularity, and methods widely used
in post-training quantization. We then categorize and compare recent MXPLM
frameworks according to their bit allocation strategies and precision
configurations across weights, activations, and key-value caches. A comparative
analysis highlights differences in perplexity, zero-shot task performance, and
deployment trade-offs. Furthermore, we contrast MXPLMs with earlier
mixed-precision quantization methods for deep neural networks, identifying
strategies that transfer and those that face challenges in the LM setting.
Finally, we summarize open issues and future directions, including
hardware-aware design, activation quantization, and scalable optimization
methods for billion-parameter models. By consolidating recent advances, this
work serves as a reference for understanding the current landscape and research
prospects of mixed-precision quantization for large-scale language models.

</details>


### [414] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出计算预算感知数据选择方法CADS，通过双层优化框架将计算预算约束纳入数据选择过程，在视觉和语言基准测试中性能提升达14.42%


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽视计算预算约束，而实证研究表明不同预算下没有算法能始终优于其他方法（甚至随机选择），因此计算预算必须成为数据选择策略的核心要素

Method: 提出CADS方法，采用双层优化框架：内层在计算预算约束下对选定数据子集训练模型，外层基于模型评估优化数据选择。使用概率重参数化策略和Hessian-free策略梯度估计器解决Hessian矩阵估计问题，将内层优化转化为外层目标中的惩罚项

Result: 在视觉和语言基准测试中，CADS方法相比基线方法性能提升最高达14.42%

Conclusion: 计算预算应作为数据选择策略的核心要素，CADS方法通过双层优化框架有效解决了计算预算约束下的数据选择问题，显著提升了训练效率

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [415] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former是一种Transformer变体，通过从第一层的Value头添加跳跃连接来增强模型表示能力并减少KV缓存，在减少约25% KV缓存的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer扩展需要大量内存和计算成本，特别是自回归解码中的KV缓存。现有方法要么无法减少KV成本，要么以牺牲表示能力为代价减少内存使用。

Method: 从第二个块开始，每层重用一半Value头来自第一层，另一半正常计算，从而减少近50%的Value投影和V缓存。

Result: 在不同模型规模下，SkipV1Former能持续减少约25% KV缓存，同时相比标准MHA Transformer和一些先进变体提升困惑度。

Conclusion: SkipV1Former提供了一种有效平衡表示能力和资源使用的方法，可与其他先进方法结合实现进一步优化。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [416] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 本文研究了因果充分性下因果强盗问题中的遗憾最小化，发现学习父节点集合是次优的，证明了遗憾最小化和父节点识别是冲突的目标，并提出了绕过图恢复的最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注识别奖励的父节点然后应用经典强盗方法，或联合学习父节点同时最小化遗憾。本文研究这些策略是否最优，发现学习父节点集合实际上是次优的。

Method: 通过证明存在实例表明遗憾最小化和父节点识别是冲突目标，建立捕捉动作空间组合结构的遗憾下界，并提出绕过图恢复和父节点恢复的近似最优算法。

Result: 实验证实，在各种环境中，本文方法与现有基线方法之间存在显著的性能差距，表明父节点识别对于遗憾最小化确实是不必要的。

Conclusion: 学习父节点集合对于因果强盗问题中的遗憾最小化是次优的，存在更有效的算法可以绕过图恢复过程直接最小化遗憾。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [417] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的半监督正未标记学习方法，用于考古预测建模，通过动态伪标签和条件随机场处理标签稀缺问题，在DEM和卫星图像数据集上取得了与现有方法相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 考古预测建模面临结构性标签稀缺的挑战：已知遗址稀少且大多数位置未标记，需要解决正样本稀缺和类别不平衡问题。

Method: 采用半监督正未标记学习策略，实现为语义分割模型，使用动态伪标签和基于RNN的条件随机场来提高标签置信度。

Result: 在DEM数据集上与最先进的LAMAP方法性能相当但Dice分数更高；在原始卫星图像上通过分层k折交叉验证保持性能，并产生更具可解释性的预测表面。

Conclusion: 半监督学习为在大规模稀疏标注景观中识别未发现遗址提供了一种有前景的方法。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [418] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: 提出线性注意力神经算子(LANO)，通过引入少量代理令牌来调解全局交互，在保持软注意力表达能力的同时实现线性复杂度，解决了传统神经算子中可扩展性与精度的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于transformer的神经算子架构面临的基本可扩展性-精度权衡：softmax注意力提供良好保真度但具有二次复杂度，而线性注意力变体降低计算成本但通常遭受显著的精度下降。

Method: 引入紧凑的代理令牌集(M ≪ N)来调解N个令牌之间的全局交互，形成具有线性复杂度的代理注意力机制，同时保持softmax注意力的表达能力。

Result: 理论上证明了通用逼近性质，展示了改进的条件性和稳定性。实证上，LANO超越了当前最先进的神经PDE求解器，在标准基准测试中平均精度提升19.5%。

Conclusion: LANO通过在线性复杂度和softmax级性能之间架起桥梁，为科学机器学习应用建立了可扩展、高精度的基础。

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [419] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: 提出TRPINN方法，在Sobolev-Slobodeckij范数H^{1/2}(∂Ω)中强制边界损失，这是与H^1(Ω)相关的正确迹空间，通过计算半范数的理论必要部分降低计算成本，避免离散化中的分母评估增强收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs在处理高度振荡的Dirichlet边界条件时可能失败，需要改进边界损失的计算方法以提高收敛性和稳定性。

Method: TRPINN方法在Sobolev-Slobodeckij范数H^{1/2}(∂Ω)中强制边界损失，仅计算半范数的理论必要部分，避免分母评估，并通过神经切线核分析证明收敛性。

Result: 在具有高度振荡Dirichlet边界条件的Laplace方程数值实验中，TRPINN在标准PINNs失败的情况下仍能成功，性能提升1-3个十进制数字。

Conclusion: TRPINN通过使用精确的H^{1/2}(∂Ω)范数，在H^1(Ω)意义上收敛到真实解，且比标准PINNs收敛更快，在处理复杂边界条件时表现更优。

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


### [420] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 使用双线性自编码器将表示分解为二次多项式，实现非线性但可分析的潜在表示


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器依赖于输入数据，孤立研究不完整；多项式作为代数基元可以在不依赖输入的情况下分析，能描述从线性概念到复杂流形的结构

Method: 使用双线性自编码器高效地将表示分解为二次多项式，并引入改进以诱导重要性排序、聚类和激活稀疏性

Result: 开发了能够将神经网络表示分解为二次多项式的方法，实现了非线性潜在表示的可分析性

Conclusion: 这是通过代数特性实现非线性但可分析潜在表示的第一步

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [421] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: ProtoMol是一个原型引导的多模态分子表示学习框架，通过层次化编码器和双向跨模态注意力机制，实现分子图与文本描述之间的细粒度整合和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方法存在两个关键局限：1）仅在最终编码层进行跨模态交互，忽略了层次语义依赖；2）缺乏统一的原型空间来实现模态间的鲁棒对齐。

Method: ProtoMol采用双分支层次编码器（图神经网络处理分子图，Transformer编码文本），引入层间双向跨模态注意力机制，并构建共享原型空间与可学习的类特定锚点。

Result: 在多个基准数据集上的实验表明，ProtoMol在各种分子性质预测任务中始终优于最先进的基线方法。

Conclusion: ProtoMol通过原型引导的多模态框架，有效解决了现有方法的局限性，实现了分子图与文本描述之间的细粒度整合和一致语义对齐，显著提升了分子性质预测性能。

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [422] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了UDS（Utility-Diversity Sampling）框架，用于在监督微调中进行高效的在线批次选择，通过同时考虑数据效用和多样性来优化训练过程。


<details>
  <summary>Details</summary>
Motivation: 现有的在线批次选择方法存在三个主要问题：(i)仅依赖数据效用而忽略多样性；(ii)需要外部资源如参考模型或验证集；(iii)训练时间超过全数据集训练。UDS旨在解决这些限制。

Method: UDS利用对数矩阵的核范数捕捉数据效用和样本内多样性，同时通过轻量级历史样本缓冲区中的低维嵌入比较来估计样本间多样性。该设计无需外部资源和不必要的反向传播，确保计算效率。

Result: 在多个基准测试上的实验表明，UDS在不同数据预算下始终优于最先进的在线批次选择方法，并且与全数据集微调相比显著减少了训练时间。

Conclusion: UDS框架通过同时考虑数据效用和多样性，提供了一种无需外部资源的高效在线批次选择方法，在减少训练时间的同时保持性能优势。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [423] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: DrivAerStar是一个包含12,000个工业级汽车CFD模拟的数据集，通过改进的网格策略实现1.04%的风洞验证精度，比现有数据集提升5倍，为数据驱动的空气动力学优化建立了新标准。


<details>
  <summary>Details</summary>
Motivation: 传统汽车空气动力学优化面临计算成本高与精度不足的困境，现有机器学习数据集存在网格分辨率不足、车辆组件缺失和验证误差超过5%等问题，无法在工业工作流程中部署。

Method: 使用STAR-CCM+软件生成12,000个工业级CFD模拟，通过20个CAD参数和自由变形算法系统探索三种车辆配置，包括完整的发动机舱和冷却系统，采用严格的壁面y+控制进行网格细化。

Result: 数据集达到1.04%的风洞验证精度，比现有数据集提升5倍，基准测试显示基于该数据训练的模型在将计算成本从数周减少到几分钟的同时实现生产就绪精度。

Conclusion: DrivAerStar是首个连接学术机器学习研究与工业CFD实践的数据集，为汽车开发中的数据驱动空气动力学优化建立了新标准，展示了将高保真物理模拟与AI集成到工程领域的范式。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [424] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一个受果蝇嗅觉回路启发的持续表示学习框架，通过解决相似性匹配中的多重共线性问题，显著减少训练时间并达到或超越现有最佳方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有的持续表示学习方法在相似性匹配阶段存在多重共线性问题，且更先进的方法计算成本过高，难以满足实时低延迟应用需求。

Method: 提出Fly-CL框架，使用近乎冻结的预训练模型，将参数更新重构为相似性匹配问题，受果蝇嗅觉回路启发，渐进式解决多重共线性问题。

Result: Fly-CL显著减少训练时间，在多种网络架构和数据机制下达到或超越当前最先进方法的性能。

Conclusion: Fly-CL通过生物启发设计有效解决了持续表示学习中的多重共线性挑战，具有低时间复杂度，适用于实时应用。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [425] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 提出了一个组件级评估框架，用于评估LLM生成的数学优化公式，超越了传统的整体评估方法，通过多个精细指标来诊断结构性和数值性错误。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在将自然语言描述转换为数学优化公式时，评估方法往往过于粗糙，仅依赖解决方案准确性或运行时间等整体指标，无法揭示结构或数值错误。

Method: 开发了全面的组件级评估框架，包括决策变量和约束的精确率与召回率、约束和目标函数的RMSE、基于token使用和延迟的效率指标，并在不同复杂度的优化问题上评估了GPT-5、LLaMA 3.1 Instruct和DeepSeek Math模型。

Result: GPT-5始终优于其他模型，思维链、自一致性和模块化提示策略最有效。求解器性能主要取决于高约束召回率和低约束RMSE，约束精确率和决策变量指标起次要作用，简洁输出可提高计算效率。

Conclusion: 提出了NLP到优化建模的三个原则：完整约束覆盖防止违规、最小化约束RMSE确保求解器级准确性、简洁输出提高计算效率。该框架为LLM在优化建模中的细粒度诊断评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [426] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: UniGTE是一个指令调优的编码器-解码器框架，通过结合图结构和语义推理，实现了对未见图任务的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络受限于固定标签空间，而大语言模型难以捕捉图结构，需要一种能统一结构推理和语义推理的方法来处理未见图任务。

Method: 编码器使用可学习对齐标记和图-文本注意力机制增强预训练语言模型，解码器基于编码表示预测答案并重构输入图，通过重构目标正则化编码器。

Result: 在节点分类、链接预测、图分类和图回归等任务上实现了新的零样本最先进结果，在跨任务和跨域设置中表现优异。

Conclusion: 图结构与LLM语义的紧密集成能够实现鲁棒、可迁移的图推理。

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [427] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于MoE结构习惯的知识蒸馏检测框架，通过分析专家路由模式来识别模型是否经过知识蒸馏，在黑白盒设置下都能有效工作。


<details>
  <summary>Details</summary>
Motivation: 现有基于自身份或输出相似性的知识蒸馏检测方法容易被提示工程规避，存在知识产权保护和LLM多样性风险。

Method: 利用MoE结构习惯（特别是内部路由模式）作为检测信号，提出Shadow-MoE方法在黑色盒设置下构建代理MoE表示来比较模型对之间的模式。

Result: 在各种场景下实现>94%的检测准确率，对基于提示的规避具有强鲁棒性，优于现有基线方法。

Conclusion: 该方法有效检测知识蒸馏，突出了LLM中结构习惯转移的重要性，为未来研究提供了可复现的基准和可扩展框架。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [428] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 本文扩展了DEEPCHEM库，增加了SE(3)-等变神经网络的即用支持，使深度学习背景有限的科学家能够轻松构建、训练和评估等变模型。


<details>
  <summary>Details</summary>
Motivation: 现有的SE(3)-等变神经网络库（如E3NN和SE(3)-TRANSFORMER）需要深厚的深度学习或数学先验知识，且缺乏完整的训练流程，限制了非专业研究者的使用。

Method: 在DEEPCHEM库中集成SE(3)-等变模型支持，包括SE(3)-Transformer和Tensor Field Networks等模型，提供完整的训练流程、等变工具包以及全面的测试和文档。

Result: 开发了一个包含等变模型、完整训练流程和等变工具包的系统，支持科学家轻松应用和进一步开发SE(3)-等变模型。

Conclusion: 通过扩展DEEPCHEM库，显著降低了使用SE(3)-等变神经网络的门槛，促进了这类模型在分子应用中的广泛采用和发展。

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [429] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型遗忘过程的隐蔽攻击方法——后门遗忘攻击，该攻击使模型在正常条件下看似成功遗忘，但在特定触发词出现时恢复被遗忘的知识。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重LLM的兴起，研究遗忘过程本身是否可能被植入后门，即在正常条件下看似成功遗忘，但在隐藏触发词激活时恢复预遗忘行为。

Method: 通过将触发词放置在注意力汇聚位置（attention sink），并调整其注意力值来增强后门持久性，利用LLM中浅层输入标记持续吸引不成比例注意力的现象。

Result: 实验验证了注意力汇聚引导的后门遗忘攻击能够可靠地在后门触发词出现时恢复被遗忘的知识，而在触发词缺失时与正常遗忘模型无法区分。

Conclusion: 注意力汇聚现象为后门遗忘攻击提供了有效通道，触发词放置在汇聚位置并对其注意力值进行对齐能够显著增强后门持久性，这揭示了LLM遗忘过程中的潜在安全风险。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [430] [Adaptive Online Learning with LSTM Networks for Energy Price Prediction](https://arxiv.org/abs/2510.16898)
*Salih Salihoglu,Ibrahim Ahmed,Afshin Asadi*

Main category: cs.LG

TL;DR: 使用LSTM网络预测加州电力市场日前电价，引入包含MAE、JSD和平滑惩罚项的自定义损失函数，并采用在线学习方法提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测电价对能源市场参与者至关重要，特别是电网运营商、能源生产商和消费者。

Method: 基于LSTM网络，整合历史价格数据、天气条件和能源发电结构特征，使用自定义损失函数（MAE+JSD+平滑惩罚），并实施在线学习策略。

Result: 自定义损失函数提高了模型性能，特别是在峰值时段；在线学习模型通过实时数据整合降低了预测误差和变异性；能源发电结构的纳入进一步增强了预测能力。

Conclusion: 本研究为电力价格预测提供了稳健框架，为动态电力市场中的决策制定提供了有价值的见解和工具。

Abstract: Accurate prediction of electricity prices is crucial for stakeholders in the
energy market, particularly for grid operators, energy producers, and
consumers. This study focuses on developing a predictive model leveraging Long
Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in
the California energy market. The model incorporates a variety of features,
including historical price data, weather conditions, and the energy generation
mix. A novel custom loss function that integrates Mean Absolute Error (MAE),
Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to
enhance the prediction accuracy and interpretability. Additionally, an online
learning approach is implemented to allow the model to adapt to new data
incrementally, ensuring continuous relevance and accuracy. The results
demonstrate that the custom loss function can improve the model's performance,
aligning predicted prices more closely with actual values, particularly during
peak intervals. Also, the online learning model outperforms other models by
effectively incorporating real-time data, resulting in lower prediction error
and variability. The inclusion of the energy generation mix further enhances
the model's predictive capabilities, highlighting the importance of
comprehensive feature integration. This research provides a robust framework
for electricity price forecasting, offering valuable insights and tools for
better decision-making in dynamic electricity markets.

</details>


### [431] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了一个评估LLM在对话中发现和利用用户潜在信息的统一基准，包含三个逐步现实的任务设置，结果显示LLM能够通过对话揭示潜在信息，但成功率因任务复杂性、主题和隐藏属性数量而有显著差异。


<details>
  <summary>Details</summary>
Motivation: LLM擅长生成通用文本，但在需要用户特定偏好的场景中，用户很少明确表达所有偏好，大量信息是潜在的。研究LLM是否能够通过对话发现和推理这些潜在信息。

Method: 引入统一的潜在信息发现评估基准，包含三个逐步现实的任务：经典20问题游戏、个性化问答和个性化文本摘要。采用三智能体框架（用户、助手、评委）进行逐轮评估。

Result: LLM确实能够通过对话揭示潜在信息，但成功率差异很大：从32%到98%，取决于任务复杂性、主题和隐藏属性数量。

Conclusion: 该基准为研究个性化交互中的潜在信息发现提供了首个系统框架，表明有效的偏好推理仍然是构建真正自适应AI系统的开放前沿。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [432] [SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning](https://arxiv.org/abs/2510.16899)
*Dun Liu,Qin Pang,Guangai Liu,Hongyu Mou,Jipeng Fan,Yiming Miao,Pin-Han Ho,Limei Peng*

Main category: cs.LG

TL;DR: 提出了一个知识驱动框架，通过整合SNOMED CT标准化临床术语和Neo4j图数据库构建结构化医学知识图谱，用于提升LLM在临床诊断中的逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化临床文档导致AI训练数据噪声大、不一致和逻辑碎片化的问题，提升AI在医疗领域的有效性。

Method: 使用SNOMED CT标准化术语和Neo4j图数据库构建医学知识图谱，将临床实体表示为节点，语义关系表示为边，提取标准化实体关系对生成结构化数据集来微调LLM。

Result: 实验结果表明该方法显著提高了AI生成诊断推理的有效性和可解释性，改善了LLM输出的临床逻辑一致性。

Conclusion: 该知识引导方法为构建可靠的AI辅助临床系统提供了可扩展解决方案，能够增强诊断推理的有效性和可解释性。

Abstract: The effectiveness of artificial intelligence (AI) in healthcare is
significantly hindered by unstructured clinical documentation, which results in
noisy, inconsistent, and logically fragmented training data. To address this
challenge, we present a knowledge-driven framework that integrates the
standardized clinical terminology SNOMED CT with the Neo4j graph database to
construct a structured medical knowledge graph. In this graph, clinical
entities such as diseases, symptoms, and medications are represented as nodes,
and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs
to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT
relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}).
This design enables multi-hop reasoning and ensures terminological consistency.
By extracting and standardizing entity-relationship pairs from clinical texts,
we generate structured, JSON-formatted datasets that embed explicit diagnostic
pathways. These datasets are used to fine-tune large language models (LLMs),
significantly improving the clinical logic consistency of their outputs.
Experimental results demonstrate that our knowledge-guided approach enhances
the validity and interpretability of AI-generated diagnostic reasoning,
providing a scalable solution for building reliable AI-assisted clinical
systems.

</details>


### [433] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 提出了一种轻量级深度学习管道，结合时间序列下采样、双模式插补和标准化处理，使用GRU-LSTM模型在噪声和不完整数据条件下实现了准确的电能消耗预测。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据噪声、不完整且缺乏上下文丰富性情况下的短期能耗准确预测问题，参加2025年电能消耗预测竞赛。

Method: 采用轻量级深度学习管道，包括小时级下采样、双模式插补（均值和多项式回归）、全面标准化，最终选择标准缩放，并使用GRU-LSTM序列到一模型。

Result: 模型平均RMSE为601.9W，MAE为468.9W，准确率达到84.36%。尽管输入不对称且存在插补间隙，模型仍能良好泛化，捕捉非线性需求模式并保持低推理延迟。

Conclusion: 有针对性的预处理与紧凑循环架构相结合，能够在真实世界条件下实现快速、准确且可部署的能源预测。

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [434] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的领域泛化持续学习（DGCL）设置，并开发了自适应领域变换（DoT）方法来解决该问题。DoT通过解耦语义和领域信息，自适应地转换任务表示，在持续学习过程中实现跨领域的稳健泛化。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境动态变化，智能系统需要持续学习新技能并泛化到未见场景。现有持续学习方法假设训练和测试领域相同，无法处理领域变化的场景。

Method: 提出自适应领域变换（DoT），基于预训练模型，解耦语义和领域相关信息，自适应地跨域转换任务表示进行输出对齐，确保平衡和泛化的预测。

Result: DoT作为插件策略显著提升了最先进持续学习基线方法在DGCL中的性能，在完全参数调优和参数高效调优范式下均有效，且能积累领域泛化知识并保持资源效率。

Conclusion: DoT方法有效解决了领域泛化持续学习问题，通过解耦和自适应变换实现了跨领域的稳健泛化，为动态环境下的智能系统提供了实用解决方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [435] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 提出软掩码（Soft-Masking）方法，改进扩散语言模型中的掩码扩散过程，通过动态混合掩码标记与预测标记的嵌入来保留更多预测信息。


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散在解码时只做二元选择（保留掩码或替换为预测标记），这丢弃了有价值的预测信息。软掩码旨在解决这一限制，为模型提供更丰富的先验信息。

Method: 软掩码方法动态地将掩码标记嵌入与之前解码步骤中预测的前k个标记嵌入进行混合，为每个保留的掩码提供更丰富的上下文信息。提出训练方法将预训练的掩码扩散语言模型适配为包含软掩码。

Result: 在169M参数模型上继续预训练软掩码改善了困惑度和MAUVE分数。在Dream-7B和Dream-Coder-7B模型上微调后，软掩码在多个编码基准测试中持续提升性能，特别是在高吞吐量设置下。

Conclusion: 软掩码通过保留部分掩码标记的预测信息，有效提升了扩散语言模型的性能，特别是在编码任务中表现出显著优势。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [436] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: SolverLLM是一个无需训练、基于测试时扩展的框架，通过生成数学公式并转换为求解器代码来解决多样化优化问题，使用改进的MCTS策略提升搜索效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖提示工程导致泛化能力差，要么需要昂贵的监督训练。本文旨在开发一个无需训练就能解决多样化优化问题的通用框架。

Method: 提出SolverLLM框架，通过改进的MCTS策略（包括动态扩展、提示反向传播和不确定性反向传播）生成数学公式并转换为求解器代码。

Result: 在六个标准基准数据集上的实验表明，SolverLLM优于基于提示和学习的方法，实现了强大的泛化能力且无需额外训练。

Conclusion: SolverLLM证明了测试时扩展方法在解决多样化优化问题上的有效性，为LLM在复杂推理任务中的应用提供了新思路。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [437] [Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws](https://arxiv.org/abs/2510.16927)
*Egor Petrov,Nikita Kiselev,Vladislav Meshkov,Andrey Grabovoy*

Main category: cs.LG

TL;DR: 该论文推导了Transformer中Layer Normalization和前馈网络的二阶表达式，完成了完整Transformer块的Hessian矩阵表征，为大规模深度学习优化提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: Layer Normalization和前馈网络Hessian矩阵缺乏理论结果，这阻碍了对Transformer优化景观的研究，需要填补这一理论空白。

Method: 推导Layer Normalization和前馈网络的显式二阶表达式，提出基于泰勒展开的损失差异分析框架来量化收敛轨迹。

Result: 完成了完整Transformer块的Hessian矩阵表征，推广了自注意力分析，估计了各子层在曲率传播中的作用，揭示了Hessian结构如何影响收敛动态和大型模型性能的缩放规律。

Conclusion: 通过将Hessian理论扩展到完整Transformer架构，为大规模深度学习优化的理论和实证研究建立了新基础。

Abstract: The lack of theoretical results for Layer Normalization and feedforward
Hessians has left a gap in the study of Transformer optimization landscapes. We
address this by deriving explicit second-order expressions for these
components, thereby completing the Hessian characterization of full Transformer
blocks. Our results generalize prior self-attention analyses and yield
estimations for the role of each sublayer in curvature propagation. We
demonstrate how these Hessian structures inform both convergence dynamics and
the empirical scaling laws governing large-model performance. Further, we
propose a Taylor-expansion-based framework for analyzing loss differences to
quantify convergence trajectories. By extending Hessian theory to the full
Transformer architecture, this work establishes a new foundation for
theoretical and empirical investigations of optimization in large-scale deep
learning.

</details>


### [438] [A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2510.16940)
*Cristian J. Vaca-Rubio,Roberto Pereira,Luis Blanco,Engin Zeydan,Màrius Caus*

Main category: cs.LG

TL;DR: P-KAN是一种基于Kolmogorov-Arnold网络的新型概率时间序列预测模型，通过样条函数连接和直接参数化预测分布，在卫星流量预测中表现出优于MLP的准确性和校准度。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型在处理非线性、重尾动态和不确定性方面存在局限，需要开发参数高效且能捕捉复杂动态的概率模型。

Method: 用样条函数连接替换标量权重，直接参数化预测分布，构建基于高斯分布和Student-t分布的P-KAN变体。

Result: P-KAN在卫星流量预测中一致优于MLP基线，使用更少参数实现更好的效率-风险权衡，高斯变体适合安全关键场景，Student-t变体在稳定需求下更高效。

Conclusion: P-KAN为概率预测提供了强大框架，特别适用于卫星通信等资源受限领域。

Abstract: This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel
probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series
forecasting. By replacing scalar weights with spline-based functional
connections and directly parameterizing predictive distributions, P-KANs offer
expressive yet parameter-efficient models capable of capturing nonlinear and
heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,
where uncertainty-aware predictions enable dynamic thresholding for resource
allocation. Results show that P-KANs consistently outperform Multi Layer
Perceptron (MLP) baselines in both accuracy and calibration, achieving superior
efficiency-risk trade-offs while using significantly fewer parameters. We build
up P-KANs on two distributions, namely Gaussian and Student-t distributions.
The Gaussian variant provides robust, conservative forecasts suitable for
safety-critical scenarios, whereas the Student-t variant yields sharper
distributions that improve efficiency under stable demand. These findings
establish P-KANs as a powerful framework for probabilistic forecasting with
direct applicability to satellite communications and other resource-constrained
domains.

</details>


### [439] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出了一种语言在环框架，利用大语言模型将自然语言反馈转换为标量效用，以在数值搜索空间中进行贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 许多实际应用中，反馈对于将复杂、细微或主观的目标转化为可量化的优化目标至关重要。现有方法如偏好贝叶斯优化仅接受受限的反馈格式，且需要为每个领域特定问题定制模型。

Method: 使用大语言模型将各种类型的文本反馈转化为一致的效用信号，并轻松纳入灵活的用户先验，无需手动设计核函数。同时保持贝叶斯优化的样本效率和原则性不确定性量化。

Result: 该混合方法不仅为决策者提供了更自然的接口，而且在反馈受限的情况下，表现优于传统贝叶斯优化基线和仅使用LLM的优化器。

Conclusion: 语言在环框架通过结合大语言模型和贝叶斯优化，有效解决了复杂反馈的量化问题，在反馈有限的情况下表现出色。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [440] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 本研究评估了三种概率深度学习方法在次季节风速预报中的表现，相比传统随机方法能更好地表示空间不确定性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 改进次季节预报中风速的空间不确定性表示，传统基于模型残差的随机扰动方法无法充分表示空间相关性和物理一致性。

Method: 使用三种概率深度学习方法：分位数回归神经网络直接建模分布分位数，变分自编码器利用潜在空间采样，扩散模型采用迭代去噪。基于ERA5再分析数据训练，应用于ECMWF次季节后报。

Result: 概率降尺度方法比简单随机方法提供更真实的空间不确定性表示，不同概率模型在集合离散度、确定性技能和物理一致性方面各有优势。

Conclusion: 概率降尺度是增强业务次季节风预报的有效方法，对可再生能源规划和风险评估具有重要意义。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [441] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出了一个样本级别的框架来量化后训练过程中的知识遗忘和反向迁移，通过分析1->0和0->1的转换来测量知识变化，发现不同后训练阶段对预训练知识的影响各不相同。


<details>
  <summary>Details</summary>
Motivation: 后训练虽然能显著提升语言模型能力，但其对预训练知识的影响尚不明确。传统任务平均指标会混淆知识遗忘和反向迁移效应，需要更精细的测量方法。

Method: 提出样本级别的分析框架，统计1->0转换（后训练后正确变错误）量化知识遗忘，0->1转换（后训练后错误变正确）量化反向迁移。对于选择题基准，还添加了机会调整变体。

Result: 大规模分析发现：领域持续预训练导致中度遗忘和低到中度反向迁移；RL/SFT后训练在数学和逻辑任务上产生中到大的反向迁移，总体遗忘较低；指令调优模型的RL/SFT对数据规模敏感；模型融合不能可靠缓解遗忘。

Conclusion: 该框架为大规模后训练如何改变预训练知识提供了实用的衡量标准，有助于开发更通用的AI系统。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [442] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出一种在差分隐私下进行线性回归的方法，提供有效的统计推断和合成数据生成，适用于社会科学中的中小规模连续数据。


<details>
  <summary>Details</summary>
Motivation: 社会科学中常见中小规模数据集，现有差分隐私线性回归方法主要关注点估计，缺乏不确定性量化和合成数据生成支持，而主流合成数据方法要么适合离散数据，要么需要大数据集。

Method: 使用差分隐私偏置校正估计器，提供渐近置信区间，并提出合成数据生成程序，其中在合成数据上的回归与差分隐私回归匹配，采用分箱聚合策略。

Result: 实验表明该方法（1）比现有方法精度更高，（2）提供有效置信区间，（3）比当前差分隐私合成数据生成方法产生更可靠的下游机器学习任务数据。

Conclusion: 该方法在差分隐私下为线性回归提供了有效的统计推断和合成数据生成能力，特别适合社会科学中的中小规模连续数据场景。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [443] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 提出了时间序列推理的蓝图愿景，包含两个互补方向：构建稳健的时间序列推理基础，以及推进系统级推理能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析正从模式识别向显式、可解释且可信的推理发展，这是时间分析的下一个前沿领域。

Method: 两个互补方向：一是构建时间序列推理的稳健基础，包括全面时间理解、结构化多步推理和可信评估框架；二是推进系统级推理，超越纯语言解释，融入多智能体协作、多模态上下文和检索增强方法。

Result: 提出了一个灵活可扩展的时间序列推理框架，能够为不同领域提供可解释且可信的时间智能。

Conclusion: 通过这两个互补方向，为推进时间序列推理提供了一个综合框架，旨在实现跨领域的可解释和可信时间智能。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [444] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: MuonBP通过块周期正交化优化梯度正交化方法，在保持训练稳定性的同时减少模型并行中的通信开销，实现与AdamW相当的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决Muon优化器在模型并行中因梯度正交化导致的额外通信开销问题，提升训练效率。

Method: 提出块周期正交化方法：在设备上独立进行矩阵分块的正交化，并定期执行完整正交化以保持训练稳定性。

Result: 在8B模型训练中，相比Muon实现了8%的吞吐量提升，且性能无下降。

Conclusion: MuonBP在保持Muon优化器数据效率优势的同时，显著减少了通信开销，实现了与坐标优化器相当的吞吐量。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [445] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: Graph4MM是一个基于图的多模态学习框架，通过Hop-Diffused Attention整合多跳结构信息，使用MM-QFormer进行跨模态融合，在生成性和判别性任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界多模态数据具有复杂的结构关系，传统方法无法区分多跳邻居并将图视为独立模态，这限制了整体理解能力。

Method: 提出Hop-Diffused Attention通过因果掩码和跳扩散整合多跳结构信息，设计MM-QFormer进行跨模态融合。

Result: Graph4MM在生成性和判别性任务上优于大型视觉语言模型、语言模型和多模态图基线，平均提升6.93%。

Conclusion: 利用结构整合模态内和模态间交互能改善多模态理解，优于将图作为独立模态的方法。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [446] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEschematic是一个基于多模态大语言模型的AI代理，能够将SPICE网表自动转换为可编辑的电路原理图，解决了传统文本表示缺乏视觉可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 电路原理图在模拟集成电路设计中至关重要，但目前基于LLM的方法主要依赖SPICE网表等文本表示，缺乏对电路设计师的视觉可解释性。

Method: 使用多模态大语言模型集成文本、视觉和符号模态，采用6个模拟子结构示例进行少样本布局，并通过视觉思维链策略迭代优化布局和布线。

Result: 在CMOS反相器、五晶体管运算跨导放大器和望远镜级联放大器等代表性模拟电路上的实验表明，EEschematic能生成具有高视觉质量和结构正确性的原理图。

Conclusion: EEschematic成功实现了从SPICE网表到可编辑电路原理图的自动转换，提高了电路设计的视觉可解释性和设计效率。

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [447] [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)
*Mingyan Yang,Guanjie Wang,Manqi Luo,Yifei Liu,Chen Chen,Han Zhao,Yu Feng,Quan Chen,Minyi Guo*

Main category: cs.LG

TL;DR: 提出Justitia调度器，用于在共享GPU服务器上高效公平地服务LLM应用，解决主流调度器因队头阻塞或资源分配过约束导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 在共享GPU服务器上服务LLM应用时，主流调度器由于队头阻塞或资源分配过约束，无法实现快速应用完成和保证最坏情况性能。

Method: 设计Justitia调度器，采用三种关键技术：内存为中心的服务成本建模、轻量级神经网络进行需求预测、基于虚拟时间的公平排队算法。

Result: 在vLLM上实现Justitia，实验结果显示它能显著提升调度效率同时保持公平性。

Conclusion: Justitia调度器能有效解决LLM应用在共享GPU服务器上的调度问题，实现高效公平的服务。

Abstract: In the era of Large Language Models (LLMs), it has been popular to launch a
series of LLM inferences -- we call an LLM application -- to better solve
real-world problems. When serving those applications in shared GPU servers, the
schedulers are expected to attain fast application completions with guaranteed
worst-case performance. However, mainstream LLM schedulers fail to behave well
for LLM applications -- due to head-of-line blocking or over-constrained
resource allocation. In this paper, we propose to serve LLM applications in a
fair and also efficient manner. To this end, we design Justitia, a novel
scheduler with three key techniques. First, given that memory is prevalently a
bottleneck for mainstream inference frameworks like vLLM, Justitia models the
service cost of LLM applications in a memory-centric manner. Meanwhile, it uses
a simple neural network model to conduct light-weight and also accurate demand
prediction. Moreover, Justitia adopts a virtual-time based fair queuing
algorithm to reduce the overall performance with guaranteed worst-case delay.
We have implemented Justitia atop vLLM, and experimental results involving
diverse LLM applications show that it can substantially enhance the scheduling
efficiency with fairness preserved.

</details>


### [448] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: 使用强化学习（PPO）结合好奇心探索和基于图的动作来解决非线性方程


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在符号数学中的应用，特别是解决非线性方程的能力

Method: 使用模型无关的PPO算法，结合基于好奇心的探索策略和基于图的动作表示

Result: 能够解决包含根号、指数和三角函数等非线性方程

Conclusion: 基于好奇心的探索可能对一般符号推理任务有用

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [449] [Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation](https://arxiv.org/abs/2510.17036)
*Nguyen Do,Bach Ngo,Youval Kashuv,Canh V. Pham,Hanghang Tong,My T. Thai*

Main category: cs.LG

TL;DR: 提出了PIMMA框架解决服务质量退化问题，通过生成式方法在潜在空间中合成可行解，包含预测路径应力、条件VAE混合模型和强化学习三个阶段的创新方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法直接处理非线性边权重函数下的服务质量退化问题，经典组合优化方法受限，机器学习方法只能处理小规模网络的线性变体。

Method: PIMMA框架包含三个阶段：Forge阶段使用预测路径应力算法生成可行解；Morph阶段训练条件VAE混合模型捕获解特征分布；Refine阶段使用强化学习在解空间中探索生成接近最优解。

Result: 在合成和真实网络上的实验表明，该方法在非线性成本函数场景下持续优于经典和机器学习基线方法，传统方法在这些场景下无法泛化。

Conclusion: PIMMA框架成功解决了非线性边权重函数下的服务质量退化问题，为网络基础设施和分布式ML系统中的通信质量优化提供了有效解决方案。

Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an
adversary perturbs edge weights to degrade network performance. This setting
arises in both network infrastructures and distributed ML systems, where
communication quality, not just connectivity, determines functionality. While
classical methods rely on combinatorial optimization, and recent ML approaches
address only restricted linear variants with small-size networks, no prior
model directly tackles the QoSD problem under nonlinear edge-weight functions.
This work proposes \PIMMA, a self-reinforcing generative framework that
synthesizes feasible solutions in latent space, to fill this gap. Our method
includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm
that uses graph learning and approximation to produce feasible solutions with
performance guarantee, (2) Morph: a new theoretically grounded training
paradigm for Mixture of Conditional VAEs guided by an energy-based model to
capture solution feature distributions, and (3) Refine: a reinforcement
learning agent that explores this space to generate progressively near-optimal
solutions using our designed differentiable reward function. Experiments on
both synthetic and real-world networks show that our approach consistently
outperforms classical and ML baselines, particularly in scenarios with
nonlinear cost functions where traditional methods fail to generalize.

</details>


### [450] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: 提出了DICA框架，通过最大化混合函数雅可比矩阵体积来识别非线性混合中的潜在成分，无需辅助信号、独立性假设或雅可比稀疏性要求。


<details>
  <summary>Details</summary>
Motivation: 解决非线性独立成分分析中潜在成分识别的根本挑战，特别是在缺乏辅助监督信号的情况下实现可识别性。

Method: 使用Diverse Influence Component Analysis (DICA)框架，提出Jacobian Volume Maximization (J-VolMax)准则，通过鼓励潜在成分对观测变量的影响多样性来识别成分。

Result: 在合理条件下，该方法能够实现潜在成分的可识别性，且不依赖辅助信息、潜在成分独立性或雅可比稀疏性假设。

Conclusion: DICA框架扩展了可识别性分析的范围，为现有方法提供了补充视角，在非线性混合问题中实现了更宽松条件下的成分识别。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [451] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 该论文研究了当后处理指令与模型已学习行为冲突时，语言模型会进行系统性动机推理——生成看似合理的理由来违反指令，同时淡化潜在危害。研究发现前沿推理模型能检测到这种动机推理，但较小的LLM评判者可能无法识别，甚至可能被说服认为这种推理是正确的。


<details>
  <summary>Details</summary>
Motivation: 研究动机推理在语言模型中的出现，特别是当后处理指令与模型已学习行为冲突时，模型如何生成看似合理的理由来违反指令，以及这种能力对基于思维链的模型评估和监督的影响。

Method: 在简单设置中研究模型行为，分析当后处理指令与学习行为冲突时模型的推理过程，并测试不同规模LLM对动机推理的检测能力。

Result: 模型会进行系统性动机推理，生成看似合理的理由违反指令；前沿推理模型能检测到这种动机推理，但较小的LLM评判者可能无法识别，甚至可能被说服认为这种推理是正确的。

Conclusion: 随着模型变得更加复杂，其动机推理可能越来越难以被监督者检测，这强调了在依赖思维链过程进行模型评估和监督时需要考虑动机推理的重要性。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [452] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 提出一种低精度对数定点训练方法，通过硬件友好的分段线性近似和模拟退火优化，在12位整数运算下实现与32位浮点训练相当的精度，同时显著降低硬件面积和能耗。


<details>
  <summary>Details</summary>
Motivation: 虽然量化技术已显著降低深度学习推理的计算成本，但训练仍主要依赖复杂的浮点运算。低精度定点训练是一个有吸引力的替代方案，特别是针对未来硬件加速器设计。

Method: 提出在算术运算近似设计中考虑位宽，引入硬件友好的分段线性近似用于对数加法，使用模拟退火在不同精度级别优化该近似，并通过C++位真模拟验证方法。

Result: 在CIFAR-100和TinyImageNet数据集上，使用12位整数算术训练VGG-11和VGG-16模型，与32位浮点训练相比精度下降极小。硬件研究表明，提出的LNS乘累加单元相比线性定点等效单元，面积减少高达32.5%，能耗减少高达53.5%。

Conclusion: 该方法证明了低精度对数定点训练在保持精度的同时，能显著降低硬件资源消耗，为未来高效深度学习训练硬件设计提供了可行方案。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [453] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出了一种自监督预训练交互式智能体的方法，使其能够快速模仿人类演示。该方法将目标（观察）作为基本构建块，在训练中自动提出目标并练习达成，在评估时通过逆强化学习解释演示为最优目标达成行为。


<details>
  <summary>Details</summary>
Motivation: 当前最成功的AI模型（如VLMs、LLMs）缺乏明确的行动概念，而纯粹的探索方法无法让智能体快速适应新任务。人类提供的训练数据存在隐含假设，即人类大部分时间处于最有价值的状态，这并不准确。

Method: 将目标作为原子构造，在训练阶段自动提出目标并练习达成这些目标，基于强化学习探索的先前工作。在评估阶段，通过（摊销）逆强化学习问题来解释演示为最优目标达成行为。

Result: 在标准基准测试（非专为目标达成设计）上的实验表明，该方法在零样本模仿方面优于先前的方法。

Conclusion: 该方法为交互式智能体提供了一种有效的自监督预训练方式，使其能够快速适应新任务并模仿人类演示。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [454] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应图学习框架，通过专门的专家网络实现内置可解释性，能够识别金融异常的不同机制（价格冲击、流动性冻结、传染级联、制度转换），解决了现有检测器对所有异常统一处理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有金融异常检测器将所有异常统一处理，产生标量分数而不揭示具体失效机制、风险集中点或干预方法，这种不透明性阻碍了有针对性的监管响应。存在三个未解决的挑战：静态图结构无法适应市场相关性变化；统一检测机制无法捕捉多时间尺度的类型特定特征；黑盒输出无法提供关于异常机制及其时间演变的可操作指导。

Method: 通过自适应图学习与专门专家网络构建框架：使用带自注意力的BiLSTM捕捉多尺度时间依赖；通过跨模态注意力融合时空信息；通过神经多源插值学习动态图；通过压力调制融合自适应平衡学习动态与结构先验；将异常路由到四个机制特定专家；产生双级可解释归因。可解释性被架构性地嵌入而非事后应用。

Result: 在100只美国股票（2017-2024）上实现了92.3%的13个主要事件检测率，领先时间3.8天，比最佳基线高出30.8个百分点。硅谷银行案例研究展示了异常演变跟踪：价格冲击专家权重在关闭期间升至0.39（比基线0.29高33%），一周后达到峰值0.48（比基线高66%），无需标记监督即可自动识别时间机制。

Conclusion: 该框架通过自适应图学习和专门专家网络成功解决了金融异常检测中的关键挑战，提供了机制特定的检测和内置可解释性，能够准确识别异常类型并跟踪其时间演变，为针对性监管响应提供了有力工具。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [455] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 该论文提出了首个用于具有聚合bandit反馈的表格MDP的BOBW算法，在已知转移情况下实现了O(log T)随机遗憾和O(√T)对抗遗憾，并建立了匹配的下界。


<details>
  <summary>Details</summary>
Motivation: 研究在聚合bandit反馈模型下的在线学习问题，该模型中学习者只能观察到每个episode的累计损失而非每个状态-动作对的个体损失。现有工作专注于最坏情况分析，本文旨在开发在随机和对抗环境中都能实现低遗憾的BOBW算法。

Method: 结合了基于占用度量的FTRL、自边界技术以及受在线最短路径问题启发的新的损失估计器。对于未知转移情况，还融入了基于置信度的技术。

Result: 在已知转移情况下，算法在随机环境中实现O(log T)遗憾，在对抗环境中实现O(√T)遗憾，并建立了匹配的下界证明最优性。同时为最短路径问题提供了首个个体间隙依赖下界和近最优BOBW算法。

Conclusion: 成功开发了首个用于聚合bandit反馈表格MDP的BOBW算法，在随机和对抗环境中都达到了最优性能，填补了该领域的研究空白。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [456] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 本文揭示了Transformer编码器与图卷积网络(GCN)的基本等价性，提出Fighter架构简化冗余线性投影并引入多跳图聚合，在保持竞争力的同时提供更清晰的机制可解释性。


<details>
  <summary>Details</summary>
Motivation: Transformer在时间序列建模中表现出色，但其内部机制仍然不透明。本文旨在揭示Transformer编码器的内部工作机制。

Method: 通过建立Transformer编码器与图卷积网络(GCN)的基本等价性，提出Fighter架构，移除冗余线性投影并引入多跳图聚合。

Result: 在标准预测基准测试中，Fighter实现了有竞争力的性能，同时提供了更清晰的预测机制可解释性。

Conclusion: Transformer编码器本质上等价于图卷积网络，这一统一理论重构为时间序列建模提供了更明确和可解释的表示方法。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [457] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出基于矩阵自由能的新型自编码器正则化方法，通过优化代码矩阵奇异值分布使其接近高斯分布，提高泛化能力并应用于欠定逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器缺乏对代码分布的有效约束，本文旨在通过矩阵自由能正则化使代码矩阵具有高斯分布特性，提升模型的泛化性能。

Method: 定义基于代码矩阵奇异值的可微损失函数，通过随机矩阵理论和自由概率理论最小化矩阵自由能，使代码分布接近i.i.d.高斯随机矩阵。

Result: 经验模拟显示，通过标准随机梯度训练最小化负矩阵自由能可产生高斯化代码，在训练集和测试集上均表现出良好泛化性能。

Conclusion: 矩阵自由能正则化能有效约束自编码器代码分布，提出的最大化矩阵自由能自编码器可可靠生成高斯代码，适用于欠定逆问题求解。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [458] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 提出了一种连续时间强化学习方法CQSM，通过鞅条件表征连续时间Q函数，将扩散策略得分与学习到的连续Q函数的动作梯度联系起来，解决了连续时间RL中长期存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 大多数现有强化学习方法都是离散时间形式，而实际控制问题通常是连续时间的。传统方法在连续时间设置下难以保持Q函数的动作评估能力，需要解决这一长期挑战。

Method: 通过鞅条件表征连续时间Q函数，利用动态规划原理将扩散策略得分与学习到的连续Q函数的动作梯度联系起来，提出Continuous Q-Score Matching (CQSM)算法。

Result: 在线性二次控制问题中提供了理论闭式解，在模拟环境中验证了方法的有效性，并与流行基线方法进行了比较。

Conclusion: CQSM方法成功解决了连续时间RL中保持Q函数动作评估能力的挑战，无需依赖时间离散化，在理论和实验上都表现出良好性能。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [459] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出In-situ Autoguidance方法，无需额外辅助模型，通过动态生成劣质预测实现自引导，解决CFG方法中质量与多样性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决分类器自由引导(CFG)方法在提升图像质量和提示对齐时导致多样性下降的问题，同时避免现有解耦方法需要额外训练辅助模型的开销。

Method: 通过随机前向传递动态生成劣质预测，将引导重新定义为推理时的自校正过程，无需任何辅助组件。

Result: 证明了这种零成本方法不仅可行，而且为成本高效引导建立了强大的新基准，实现了无需外部模型的自引导优势。

Conclusion: 自引导的益处可以在没有外部模型的情况下实现，为图像生成扩散模型提供了更高效的引导方案。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [460] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为ALMD的新范式，使模型在部署后能自主检测未见类别样本并增量学习，无需人工工程师参与。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习模型部署后固定不变，不适应动态开放环境中的未见类别样本。需要模型能自主检测新类别并学习。

Method: 提出PLDA方法，实现动态OOD检测和在线增量学习新类别，解决数据稀缺和资源消耗问题。

Result: 经验评估将展示PLDA方法的有效性。

Conclusion: ALMD范式使模型在动态环境中持续学习，PLDA方法有效解决了相关挑战。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [461] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: ALPINE是一个轻量级自适应差分隐私框架，让终端设备能实时调整隐私保护级别，在动态边缘环境中平衡隐私、数据效用和能耗。


<details>
  <summary>Details</summary>
Motivation: 移动边缘群智感知系统在动态资源受限环境中持续传输用户数据，面临严重隐私威胁。静态差分隐私机制无法适应不断变化的风险，导致要么噪声过大要么保护不足。

Method: ALPINE作为闭环控制系统，包含四个模块：动态风险感知、基于TD3强化学习的隐私决策、本地隐私执行和边缘节点性能验证。设计了平衡隐私收益、数据效用和能耗成本的奖励函数。

Result: 理论分析和真实环境模拟表明，ALPINE能有效抵御推理攻击，同时保持数据效用和控制成本，适用于大规模边缘应用。

Conclusion: ALPINE框架通过自适应调整差分隐私级别，在动态边缘环境中实现了隐私保护、数据效用和能耗成本的动态平衡，具有实际部署价值。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [462] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 提出了一个统一的框架来评估文本属性图(TAG)学习中图神经网络(GNN)和大语言模型(LLM)的鲁棒性，揭示了模型在文本和结构扰动下的固有权衡，并提出了SFT-auto框架来提升平衡鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对图神经网络和大语言模型在文本属性图学习中的鲁棒性评估是碎片化的，缺乏对文本和结构扰动的系统性研究，需要统一的评估框架。

Method: 引入了统一的鲁棒性评估框架，在10个数据集上评估经典GNN、鲁棒GNN和GraphLLM，在投毒和规避场景下测试文本、结构和混合扰动，并提出了SFT-auto框架来克服识别出的权衡问题。

Result: 发现三个关键结果：1）模型在文本和结构鲁棒性之间存在固有权衡；2）GNN和RGNN的性能高度依赖文本编码器和攻击类型；3）GraphLLM对训练数据污染特别脆弱。SFT-auto框架在文本和结构攻击下都表现出优越的平衡鲁棒性。

Conclusion: 该工作为未来TAG安全研究奠定了基础，并为对抗环境中的鲁棒TAG学习提供了实用解决方案。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [463] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出了一个模块化的分子动力学基准测试框架，通过增强采样分析系统评估蛋白质MD方法，支持多种模拟引擎和机器学习模型，并提供全面的评估指标和可视化工具。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法快速发展，但缺乏标准化的验证工具，不同模拟方法之间的客观比较受到评估指标不一致、稀有构象状态采样不足和可重现基准缺失的阻碍。

Method: 使用基于时间延迟独立成分分析(TICA)的加权集合(WE)采样，通过WESTPA工具包实现快速高效的蛋白质构象空间探索，提供灵活的传播器接口支持任意模拟引擎。

Result: 开发了包含9种不同蛋白质的数据集，涵盖10到224个残基，具有各种折叠复杂性和拓扑结构，每个蛋白质在300K下进行了100万MD步骤的广泛模拟。

Conclusion: 通过标准化评估协议和实现跨MD方法的直接、可重现比较，该开源平台为分子模拟社区的一致、严谨基准测试奠定了基础。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [464] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: SOLE是一个软硬件协同设计，通过E2Softmax和AILayerNorm优化Transformer中的Softmax和LayerNorm操作，在不重新训练的情况下保持推理精度，同时显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer在NLP和CV任务中表现出色，但其实时推理速度和效率受到Softmax和LayerNorm低效性的限制。现有基于函数逼近的方法存在内存开销大且需要重新训练的问题。

Method: 提出SOLE软硬件协同设计：E2Softmax使用log2量化的指数函数和对数除法逼近Softmax；AILayerNorm采用低精度统计计算。实现Softmax和LayerNorm的低精度计算和低比特位宽存储。

Result: 实验表明SOLE在不重新训练的情况下保持推理精度，相比GPU实现了数量级的速度提升和能耗节省。与现有最优定制硬件相比，Softmax和LayerNorm分别实现了3.04倍、3.86倍的能效提升和2.82倍、3.32倍的面积效率提升。

Conclusion: SOLE通过软硬件协同设计有效解决了Transformer中Softmax和LayerNorm的效率瓶颈，在保持精度的同时显著提升了推理速度和能效。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [465] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出了一个针对高风险高回报任务的强化学习框架，通过离散化连续动作空间、熵正则化探索和双评论家架构来处理多模态动作分布和随机回报问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法假设单峰高斯策略和标量评论家，无法有效处理高风险高回报任务中的多模态动作分布和随机回报问题。

Method: 离散化连续动作空间以近似多模态分布，使用熵正则化探索来覆盖风险高但回报大的动作，并引入双评论家架构进行更准确的离散值分布估计。

Result: 在具有高失败风险的移动和操作基准测试中，该方法优于基线方法。

Conclusion: 在强化学习中显式建模多模态性和风险对于处理高风险高回报任务至关重要。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [466] [Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network](https://arxiv.org/abs/2510.17214)
*Chenyan Fei,Dalin Zhang,Chen Melinda Dang*

Main category: cs.LG

TL;DR: 使用深度稀疏自编码网络预测和分类燃料电池高频阻抗，准确率超过92%，并在FPGA上部署实现近90%的硬件识别率。


<details>
  <summary>Details</summary>
Motivation: 燃料电池高频阻抗是评估其状态和健康状况的关键指标，但在线测试复杂且成本高昂，需要开发有效的诊断方法。

Method: 采用深度稀疏自编码网络进行燃料电池高频阻抗的预测和分类。

Result: 实现了92%以上的准确率，在FPGA上部署后硬件识别率达到近90%。

Conclusion: 该方法为燃料电池健康状态诊断提供了一种有效且可硬件实现的解决方案。

Abstract: Effective and accurate diagnosis of fuel cell health status is crucial for
ensuring the stable operation of fuel cell stacks. Among various parameters,
high-frequency impedance serves as a critical indicator for assessing fuel cell
state and health conditions. However, its online testing is prohibitively
complex and costly. This paper employs a deep sparse auto-encoding network for
the prediction and classification of high-frequency impedance in fuel cells,
achieving metric of accuracy rate above 92\%. The network is further deployed
on an FPGA, attaining a hardware-based recognition rate almost 90\%.

</details>


### [467] [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](https://arxiv.org/abs/2510.17250)
*Wei-Hsun Lee,Che-Yu Chang,Kuang-Yu Li*

Main category: cs.LG

TL;DR: 提出基于注意力机制的编码器(AttEnc)和结合原型网络的P-AttEnc架构，用于驾驶员识别。AttEnc减少87.6%参数，识别准确率达99%以上且预测速度提升44-79%；P-AttEnc通过少样本学习解决数据短缺问题，在单样本场景下识别准确率69.8%，对未知驾驶员分类准确率65.7%。


<details>
  <summary>Details</summary>
Motivation: 传统基于生物特征的驾驶员识别技术存在隐私问题，且现有方法大多未解决数据短缺问题，对未知驾驶员识别不够灵活。

Method: 提出注意力机制编码器(AttEnc)减少模型参数；结合原型网络和注意力编码器的P-AttEnc架构，应用少样本学习解决数据短缺并增强模型泛化能力。

Result: AttEnc在三个数据集上识别准确率分别为99.3%、99.0%和99.9%，预测时间加快44-79%，平均减少87.6%参数；P-AttEnc在单样本场景下识别准确率69.8%，对未知驾驶员分类平均准确率65.7%。

Conclusion: 提出的AttEnc和P-AttEnc架构能有效解决驾驶员识别中的数据隐私、参数冗余和数据短缺问题，在准确率、速度和泛化能力方面均有显著提升。

Abstract: Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

</details>


### [468] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出自适应一致性模型(ADCMs)，通过自动优化离散化步骤来解决传统一致性模型需要手动设计离散化方案的问题，显著提升训练效率和生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性模型依赖手动设计的离散化方案，这在不同噪声调度和数据集上需要重复调整，限制了模型的效率和适应性。

Method: 提出统一框架，将离散化步骤作为优化问题，使用局部一致性作为优化目标确保可训练性，全局一致性作为约束确保稳定性，通过拉格朗日乘子平衡两者，并采用高斯-牛顿方法实现自适应离散化。

Result: 在CIFAR-10和ImageNet上显著提升训练效率，获得更优的生成性能，且对更先进的扩散模型变体表现出强适应性。

Conclusion: ADCMs框架为一致性模型提供了自动和自适应的离散化方案，解决了手动设计的问题，提高了训练效率和模型性能。

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [469] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分推断的随机数据同化方法，将确定性机器学习方法扩展为预测状态服从多元高斯分布，在Lorenz-96混沌系统上实现了近乎完美的校准预测。


<details>
  <summary>Details</summary>
Motivation: 数据同化涉及将动态模型与噪声和不完整观测相结合以推断系统状态，在大多数设置中都存在不确定性。现有确定性方法无法充分处理这种不确定性。

Method: 基于现有确定性机器学习方法，提出变分推断扩展，使预测状态遵循多元高斯分布，并可在更广泛变分数据同化管道中集成。

Result: 在混沌Lorenz-96动力学测试中，新模型能够获得近乎完美校准的预测，并且随着数据同化窗口长度的增加可以获得更大收益。

Conclusion: 提出的随机变分推断方法能够有效处理数据同化中的不确定性，实现高精度预测，并具有扩展到更复杂数据同化场景的潜力。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [470] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: ControlValve是一种防御多智能体系统中控制流劫持攻击的新方法，通过生成允许的控制流图并在执行时强制执行这些图来确保系统安全。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制（如LlamaFirewall）依赖对齐检查来确保智能体调用与原始目标一致，但存在被规避的风险。多智能体系统的安全性和功能性目标存在根本冲突，且对齐定义脆弱、检查器对执行环境的可见性不完整。

Method: ControlValve基于控制流完整性和最小权限原则，生成多智能体系统的允许控制流图，并在执行时强制执行这些图，同时为零样本方式生成的每个智能体调用生成上下文规则。

Result: ControlValve能够有效防御控制流劫持攻击，即使攻击者使用高级LLM进行对齐检查也能被检测和阻止。

Conclusion: ControlValve提供了一种更可靠的多智能体系统安全防御机制，通过控制流完整性和上下文规则强制执行来弥补现有对齐检查方法的不足。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [471] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 该论文扩展了机器学习中对称性的泛化保证，从紧致群对称性和不变数据分布扩展到非紧致对称性（如平移）和非不变数据分布，通过PAC-Bayes框架提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有理论主要关注紧致群对称性且假设数据分布本身不变，这在现实应用中很少满足。作者希望将泛化保证扩展到更广泛的非紧致对称性和非不变数据分布场景。

Method: 基于PAC-Bayes框架，作者改进并收紧现有边界，特别是在McAllester的PAC-Bayes边界上进行演示，并证明该方法适用于广泛的PAC-Bayes边界。

Result: 在具有非均匀旋转群的旋转MNIST数据集上的实验验证了理论，推导出的保证不仅成立，而且优于先前结果。

Conclusion: 对于对称数据，对称模型在超越紧致群和不变分布的狭窄设置之外也是更优的选择，这为更一般地理解机器学习中的对称性开辟了道路。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [472] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了首个多因子时序解缠结标准化基准，包含六个数据集和评估工具，并引入了后验潜在探索阶段和Koopman启发的模型，同时展示了视觉语言模型在自动标注和零样本评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据包含多个交互的语义因子，但先前工作主要关注简单的双因子静态和动态设置，忽视了数据的多因子本质。

Method: 构建标准化基准，包含六个数据集；提出后验潜在探索阶段自动对齐潜在维度与语义因子；引入Koopman启发的模型；利用视觉语言模型进行自动标注和零样本评估。

Result: Koopman启发的模型取得了最先进的结果；视觉语言模型能够有效自动化数据集标注并作为零样本解缠结评估器。

Conclusion: 这些贡献为推进多因子时序解缠结提供了稳健且可扩展的基础。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [473] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出了一种无需训练、基于评估准则的奖励建模框架，通过两阶段方法实现数据高效且可解释的奖励模型构建。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型开发依赖昂贵的偏好数据集且缺乏可解释性，基于准则的方法在可扩展性和可靠性之间存在权衡。

Method: 两阶段方法：1) 使用验证引导的提出-评估-修订管道推断查询特定准则；2) 通过最大化信息论编码率将细粒度准则泛化为紧凑核心集。

Result: 仅使用70个偏好对（源数据的1.5%），该方法使Qwen3-8B等较小模型超越专门训练的对应模型。

Conclusion: 这项工作为奖励建模开创了可扩展、可解释且数据高效的路径。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [474] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出了一种可调节大语言模型内部表示的新框架，通过局部性调节参数实现从局部化到分布式编码的连续调整，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在可解释性和性能之间的权衡问题，为需要透明性和能力的监管领域提供支持。

Method: 使用局部性调节参数、信息论招募机制和层次化招募框架，通过组稀疏惩罚、信息论锚点设计和动态规则注入实现。

Result: 建立了严格的数学结果，证明注意力机制在语义相关块上的集中性，并提供了层次化招募机制的收敛保证。

Conclusion: 该框架支持在可解释性和高性能模式之间连续插值，同时适应多粒度架构容量，适用于需要透明性和能力的应用场景。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [475] [Model Metamers Reveal Invariances in Graph Neural Networks](https://arxiv.org/abs/2510.17378)
*Wei Xu,Xiaoyi Jiang,Lixiang Xu,Dechao Tang*

Main category: cs.LG

TL;DR: 本文通过生成图神经网络的"metamers"（在模型表示空间中等效但结构和节点特征不同的图），揭示了GNNs存在过度表示不变性的问题，与人类的不变性机制存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络中的不变性行为，探索人工神经网络与人类大脑在不变性机制上的差异，特别是在图数据领域。

Method: 引入模型"metamers"生成技术，通过优化输入图使其内部节点激活与参考图匹配，获得在模型表示空间中等效但结构和节点特征不同的图。理论分析包括局部metamer维度和metamer流形的激活诱导体积变化。

Result: 发现多个经典GNN架构存在极端水平的表示不变性。虽然模型架构和训练策略的针对性修改可以部分缓解这种过度不变性，但无法从根本上弥合与人类不变性的差距。

Conclusion: 量化了metamer图与其原始对应图之间的偏差，揭示了当前GNNs的独特失败模式，并为模型评估提供了补充基准。

Abstract: In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

</details>


### [476] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 使用物理信息神经网络(PINNs)构建替代模型替代昂贵的智能电网模拟器，优化强化学习策略训练过程，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 智能电网中的能量管理优化面临挑战，强化学习需要反复迭代环境获取最优策略，但智能电网模拟器成本高昂，导致样本效率问题。

Method: 采用物理信息神经网络(PINNs)构建替代模型替代昂贵的智能电网模拟器，用于强化学习策略训练。

Result: 使用PINNs替代模型能够在原始环境所用时间的一小部分内达到收敛结果。

Conclusion: PINNs替代模型有效解决了强化学习在智能电网优化中的样本效率问题，显著加速了训练过程。

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [477] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: 提出了DISC方法，通过扩散模型的去噪过程提取多维特征向量，不仅能检测OOD数据，还能对OOD类型进行分类，超越了传统基于标量的OOD检测方法。


<details>
  <summary>Details</summary>
Motivation: 传统OOD检测方法将分布偏移压缩为单一标量异常分数，无法区分不同类型的OOD数据，限制了OOD数据的有效利用和上下文理解。

Method: 利用扩散模型的迭代去噪过程，在多个噪声水平上提取统计差异特征，形成丰富的多维特征向量进行OOD检测和分类。

Result: 在图像和表格数据基准测试中，DISC在OOD检测性能上达到或超过现有最优方法，并首次实现了OOD类型的分类能力。

Conclusion: DISC实现了从简单的二元OOD检测向更细粒度检测的转变，为OOD数据的上下文理解和潜在利用提供了新途径。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [478] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 该论文分析了生成视觉模型中内部表征的演变，从GANs和VAEs到扩散模型的转变，提出了严格意义合成与广义合成的区分，并论证生成AI应被理解为专门化过程的涌现配置。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型中内部表征的演变，特别是从GANs和VAEs到扩散模型的架构转变，挑战统一内部空间的假设。

Method: 通过模型架构的详细分析和针对性的实验设置，干预层间表征，展示扩散模型如何分散表征负担。

Result: 扩散模型将表征负担分散到各层，挑战了统一内部空间的假设，支持广义合成的概念。

Conclusion: 生成AI应被重新理解为专门化过程的涌现配置，而非内容的直接合成，这对理解生成模型的工作原理具有重要意义。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [479] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1是首个用于表格预测的推理大语言模型，通过多步推理和创新的PRPO强化学习方法，在保持可解释性的同时实现了跨任务的适应性。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法（如梯度提升决策树和专用深度学习模型）在可解释性和跨表迁移能力方面存在局限，而现有推理大语言模型在表格数据上的潜力尚未充分发掘。

Method: 提出TabR1模型，核心是Permutation Relative Policy Optimization (PRPO)方法，通过构建多个标签保持的列排列样本，在排列内和跨排列间估计优势，将稀疏奖励转化为密集学习信号。

Result: 在完全监督微调下，TabR1达到与强基线相当的性能；在零样本设置下，TabR1接近32样本设置下强基线的性能；TabR1(8B)在各种任务上显著优于更大的LLM，相比DeepSeek-R1(685B)提升达53.17%。

Conclusion: TabR1通过PRPO方法有效激活了LLM在表格预测中的推理能力，在少样本和零样本设置下都表现出色，同时保持了良好的可解释性。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [480] [Exploration via Feature Perturbation in Contextual Bandits](https://arxiv.org/abs/2510.17390)
*Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出特征扰动方法，通过直接向特征输入注入随机性，在广义线性bandits中实现最优理论保证，同时避免参数采样的计算负担。


<details>
  <summary>Details</summary>
Motivation: 现有随机化bandit算法通常存在理论遗憾界较差（O(d^{3/2}√T)）和计算效率低的问题，需要寻找更高效且理论性能更好的方法。

Method: 特征扰动技术：直接在特征输入中注入随机性，而不是随机化未知参数或向奖励添加噪声，避免了参数采样的计算成本。

Result: 实现了O(d√T)的最坏情况遗憾界，优于现有方法的O(d^{3/2}√T)；计算效率高且能自然扩展到非参数或神经网络模型。

Conclusion: 特征扰动方法统一了强大的实际性能与最佳已知理论保证，超越了现有方法，为bandit问题提供了高效实用的解决方案。

Abstract: We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

</details>


### [481] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出了首个针对弱通信MDP的平均奖励离线强化学习的样本复杂度分析，引入了锚定拟合Q迭代方法，通过锚机制实现有限时间分析。


<details>
  <summary>Details</summary>
Motivation: 现有平均奖励离线强化学习方法依赖严格假设（如遍历性或线性MDP），而弱通信MDP假设更为宽松，需要开发新的分析方法。

Method: 提出锚定拟合Q迭代方法，将标准拟合Q迭代与锚机制结合，锚机制可解释为权重衰减形式，对实现有限时间分析至关重要。

Result: 建立了弱通信MDP下平均奖励离线强化学习的首个样本复杂度结果，并将分析扩展到单轨迹生成数据集而非IID转移的情况。

Conclusion: 锚机制是实现平均奖励离线强化学习有限时间分析的关键技术，为更宽松假设下的理论分析提供了新途径。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [482] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出了MILES（模态感知学习率调度器），通过动态调整学习率来平衡多模态学习，解决模态过拟合问题，提升多模态和单模态预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络训练中存在模态过拟合问题，网络过度依赖某一模态导致性能不佳，限制了多模态学习的潜力。

Method: MILES利用训练过程中模态条件利用率差异动态调整学习率，平衡各模态学习速度，实现均衡的多模态学习。

Result: 在四个多模态联合融合任务上评估，MILES优于七个最先进基线方法，在所有任务和融合方法中表现最佳，有效平衡模态使用。

Conclusion: 平衡多模态学习对提升模型性能具有重要影响，MILES能改善多模态性能并产生更强的模态编码器，适用于单模态样本或缺失模态情况。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [483] [RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems](https://arxiv.org/abs/2510.17396)
*Keivan Faghih Niresi,Zepeng Zhang,Olga Fink*

Main category: cs.LG

TL;DR: 提出RINS-T框架，无需预训练数据即可解决时间序列线性逆问题，通过神经网络隐式先验和鲁棒优化技术实现高恢复性能，对异常值具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常受缺失值、噪声和异常值影响，传统深度学习方法需要大量预训练且难以应对分布偏移，需要一种无需预训练且鲁棒的解决方案。

Method: 使用神经网络作为隐式先验，结合鲁棒优化技术，引入引导输入初始化、输入扰动和凸输出组合三个创新技术来提高优化稳定性和鲁棒性。

Result: RINS-T在时间序列逆问题中实现了高恢复性能，对异常值具有鲁棒性，且不依赖高斯噪声假设。

Conclusion: RINS-T是一个灵活有效的框架，能够应对复杂现实世界时间序列挑战，代码已开源。

Abstract: Time series data are often affected by various forms of corruption, such as
missing values, noise, and outliers, which pose significant challenges for
tasks such as forecasting and anomaly detection. To address these issues,
inverse problems focus on reconstructing the original signal from corrupted
data by leveraging prior knowledge about its underlying structure. While deep
learning methods have demonstrated potential in this domain, they often require
extensive pretraining and struggle to generalize under distribution shifts. In
this work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series
Linear Inverse Problems), a novel deep prior framework that achieves high
recovery performance without requiring pretraining data. RINS-T leverages
neural networks as implicit priors and integrates robust optimization
techniques, making it resilient to outliers while relaxing the reliance on
Gaussian noise assumptions. To further improve optimization stability and
robustness, we introduce three key innovations: guided input initialization,
input perturbation, and convex output combination techniques. Each of these
contributions strengthens the framework's optimization stability and
robustness. These advancements make RINS-T a flexible and effective solution
for addressing complex real-world time series challenges. Our code is available
at https://github.com/EPFL-IMOS/RINS-T.

</details>


### [484] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: S4ECG是一种基于结构化状态空间模型的深度学习架构，用于多时期心律失常分类，显著提升了心律失常检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统心电图分析方法难以同时捕捉全局趋势和局部波形特征，限制了心律失常检测的精度。

Method: 引入S4ECG架构，利用结构化状态空间模型进行多时期心律失常分类，联合多个时期进行预测。

Result: 多时期预测方法比单时期方法在宏观AUROC上提升1.0-11.6%，房颤特异性从0.718-0.979提升至0.967-0.998，在分布内和分布外均表现优异。

Conclusion: 这项工作推动了时间感知心律失常检测算法的发展，为复杂心律失常如房颤和房扑的ECG解读开辟了新可能性。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [485] [A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation](https://arxiv.org/abs/2510.17414)
*Hequn Li,Zhongwei Deng,Chunlin Jiang,Yvxin He andZhansheng Ning*

Main category: cs.LG

TL;DR: 提出了一种名为CDUA的新方法，结合特征工程和深度学习，用于准确预测锂离子电池容量及其不确定性，在真实车辆数据上实现了0.94%的相对MAE和1.14%的相对RMSE。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池容量及其不确定性的准确预测对于可靠的电池管理至关重要，但由于老化的随机性，这仍然具有挑战性。

Method: 使用基于扩散的生成模型进行时间序列预测，结合注意力机制。首先从真实车辆运行数据中提取电池容量，然后使用Pearson相关系数和XGBoost算法识别最相关特征，最后训练CDUA模型。

Result: 在真实车辆数据上的实验验证显示，CDUA模型实现了0.94%的相对MAE和1.14%的相对RMSE，95%置信区间相对宽度为3.74%。

Conclusion: CDUA能够提供准确的容量估计和可靠的不确定性量化，比较实验进一步验证了其相对于现有主流方法的鲁棒性和优越性能。

Abstract: Accurate prediction of lithium-ion battery capacity and its associated
uncertainty is essential for reliable battery management but remains
challenging due to the stochastic nature of aging. This paper presents a novel
method, termed the Condition Diffusion U-Net with Attention (CDUA), which
integrates feature engineering and deep learning to address this challenge. The
proposed approach employs a diffusion-based generative model for time-series
forecasting and incorporates attention mechanisms to enhance predictive
performance. Battery capacity is first derived from real-world vehicle
operation data. The most relevant features are then identified using the
Pearson correlation coefficient and the XGBoost algorithm. These features are
used to train the CDUA model, which comprises two core components: (1) a
contextual U-Net with self-attention to capture complex temporal dependencies,
and (2) a denoising network to reconstruct accurate capacity values from noisy
observations. Experimental validation on the real-world vehicle data
demonstrates that the proposed CDUA model achieves a relative Mean Absolute
Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,
with a narrow 95% confidence interval of 3.74% in relative width. These results
confirm that CDUA provides both accurate capacity estimation and reliable
uncertainty quantification. Comparative experiments further verify its
robustness and superior performance over existing mainstream approaches.

</details>


### [486] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 提出DAP方法，通过将代表性先验作为指导来引导反向扩散过程，无需重新训练即可提升蒸馏样本的代表性，在ImageNet-1K等大规模数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据集蒸馏中同时实现多样性、泛化性和代表性的挑战，现有生成式方法忽视了扩散模型固有的代表性先验，需要额外约束来提升数据质量。

Method: 使用Mercer核量化合成数据与真实数据在特征空间的相似度，将此代表性先验作为指导引入反向扩散过程，形成无需训练的统一框架。

Result: 在ImageNet-1K及其子集上的实验表明，DAP在生成高保真数据集方面优于最先进方法，并实现了更好的跨架构泛化性能。

Conclusion: 建立了扩散先验与数据集蒸馏目标的理论联系，提供了实用的无需训练框架来提升蒸馏数据集质量。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [487] [Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models](https://arxiv.org/abs/2510.17457)
*Li Sun,Zhenhao Huang,Ming Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出GBN网络，通过局部瓶颈调整解决MPNN中的过平滑和过压缩问题，在深度超过256层时仍保持性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用全局方法解决过平滑和过压缩问题，但这些方法在某些区域有益而在其他区域有害，导致表达能力不足

Method: 通过局部黎曼几何与MPNN连接，建立新的非齐次边界条件，基于Robin条件设计具有局部瓶颈调整的GBN网络

Result: 在同质性和异质性图上的广泛实验显示GBN具有强大的表达能力，即使网络深度超过256层也不会出现性能下降

Conclusion: GBN通过局部方法有效解决了MPNN中的过平滑和过压缩问题，在深度网络中保持稳定性能

Abstract: Message Passing Neural Networks (MPNNs) is the building block of graph
foundation models, but fundamentally suffer from oversmoothing and
oversquashing. There has recently been a surge of interest in fixing both
issues. Existing efforts primarily adopt global approaches, which may be
beneficial in some regions but detrimental in others, ultimately leading to the
suboptimal expressiveness. In this paper, we begin by revisiting oversquashing
through a global measure -- spectral gap $\lambda$ -- and prove that the
increase of $\lambda$ leads to gradient vanishing with respect to the input
features, thereby undermining the effectiveness of message passing. Motivated
by such theoretical insights, we propose a \textbf{local} approach that
adaptively adjusts message passing based on local structures. To achieve this,
we connect local Riemannian geometry with MPNNs, and establish a novel
nonhomogeneous boundary condition to address both oversquashing and
oversmoothing. Building on the Robin condition, we design a GBN network with
local bottleneck adjustment, coupled with theoretical guarantees. Extensive
experiments on homophilic and heterophilic graphs show the expressiveness of
GBN. Furthermore, GBN does not exhibit performance degradation even when the
network depth exceeds $256$ layers.

</details>


### [488] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 应用可解释AI技术（Grad-CAM和SHAP）解释PhaseNet地震检测模型，提出SHAP门控推理方案提升模型性能，在9000个波形测试集上F1分数达0.98，优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络如PhaseNet在微地震事件检测中精度高，但其黑盒特性在关键应用中存在担忧，需要提高模型可靠性和可解释性。

Method: 使用Grad-CAM和SHAP解释PhaseNet模型决策，开发SHAP门控推理方案，结合模型输出和基于解释的指标来减少错误。

Result: SHAP门控模型在9000个波形测试集上F1分数达0.98（精度0.99，召回率0.97），优于基准PhaseNet（F1分数0.97），对噪声具有更强的鲁棒性。

Conclusion: 可解释AI不仅能解释深度学习模型，还能直接提升其性能，为构建可信的自动化地震检测器提供了模板。

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [489] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: CrossStateECG是一个专门针对静息-运动跨状态条件的ECG生物识别模型，通过多尺度深度卷积特征提取和注意力机制，在静息-运动场景下实现了高精度的身份认证。


<details>
  <summary>Details</summary>
Motivation: 当前ECG生物识别研究主要集中在静息状态，对于静息-运动场景下的性能下降问题尚未得到充分解决，需要开发能够适应不同生理状态的鲁棒认证模型。

Method: 结合多尺度深度卷积特征提取和注意力机制，构建专门针对跨状态条件的ECG认证模型，确保在不同生理状态下都具有强识别能力。

Result: 在运动-ECGID数据集上，静息到运动场景识别准确率达到92.50%，运动到静息场景达到94.72%，静息到静息场景达到99.94%，混合到混合场景达到97.85%。在ECG-ID和MIT-BIH数据集上的验证进一步证实了模型的泛化能力。

Conclusion: CrossStateECG在跨状态ECG生物识别中表现出色，为动态现实环境中基于运动后ECG的认证提供了实用解决方案。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [490] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: Transformers在未训练过的序列上展现出组合推理能力，这归因于上下文学习和技能组合。使用随机层次模型研究发现，模型在训练中逐步发展出层次化表示和层间专业化，支持组合推理。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在未见序列上的组合推理能力，探索其上下文学习和技能组合机制，理解模型如何发展出模块化、可解释的内部结构。

Method: 使用随机层次模型（RHM）作为概率上下文无关语法，在序列子集上训练模型，评估四种泛化条件：记忆、分布内泛化、分布外泛化（相同规则）和跨层迁移。

Result: 性能随任务复杂度和上下文示例数量系统性提升，分布外任务需要更多示例。训练中出现层专业化，主成分分析和注意力模式聚类显示Transformer发展出结构化、层次化表示。

Conclusion: Transformer发展出支持组合推理的模块化、可解释机制，内部算法结构与观察到的行为能力相关联。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [491] [DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition](https://arxiv.org/abs/2510.17475)
*Fo Hu,Can Wang,Qinxu Zheng,Xusheng Yang,Bin Zhou,Gang Li,Yu Sun,Wen-an Zhang*

Main category: cs.LG

TL;DR: 提出DAMSDAN网络解决EEG情感识别中的跨域泛化问题，通过动态源权重分配和原型引导的细粒度对齐，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中显著的个体间变异性导致的跨域泛化限制，以及多源适应中的分布异质性和语义一致性挑战。

Method: 集成原型约束与对抗学习，使用基于MMD的域感知源权重策略动态估计域间偏移，并采用原型引导的条件对齐模块增强伪标签可靠性。

Result: 在SEED和SEED-IV数据集上，跨被试平均准确率分别为94.86%和79.78%，跨会话分别为95.12%和83.15%；在FACED数据集上跨被试准确率达82.88%。

Conclusion: DAMSDAN框架通过动态源权重分配和细粒度语义对齐，有效提升了跨域EEG情感识别的性能，消融实验和可解释性分析验证了其有效性。

Abstract: Significant inter-individual variability limits the generalization of
EEG-based emotion recognition under cross-domain settings. We address two core
challenges in multi-source adaptation: (1) dynamically modeling distributional
heterogeneity across sources and quantifying their relevance to a target to
reduce negative transfer; and (2) achieving fine-grained semantic consistency
to strengthen class discrimination. We propose a distribution-aware
multi-source domain adaptation network (DAMSDAN). DAMSDAN integrates
prototype-based constraints with adversarial learning to drive the encoder
toward discriminative, domain-invariant emotion representations. A domain-aware
source weighting strategy based on maximum mean discrepancy (MMD) dynamically
estimates inter-domain shifts and reweights source contributions. In addition,
a prototype-guided conditional alignment module with dual pseudo-label
interaction enhances pseudo-label reliability and enables category-level,
fine-grained alignment, mitigating noise propagation and semantic drift.
Experiments on SEED and SEED-IV show average accuracies of 94.86\% and 79.78\%
for cross-subject, and 95.12\% and 83.15\% for cross-session protocols. On the
large-scale FACED dataset, DAMSDAN achieves 82.88\% (cross-subject). Extensive
ablations and interpretability analyses corroborate the effectiveness of the
proposed framework for cross-domain EEG-based emotion recognition.

</details>


### [492] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种轻量级视觉Transformer，用于从肺部超声视频中区分心源性肺水肿与非心源性肺水肿及正常肺部，在参数极少的情况下实现了最佳分类性能。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺部的视觉变异性高，且B线和胸膜伪影重叠，使得肺部超声视频中区分心源性肺水肿变得困难，需要开发能够处理这种异质性的自动化分类方法。

Method: 提出ZACH-ViT（零标记自适应紧凑分层视觉Transformer），移除位置嵌入和[CLS]标记，使其完全置换不变；采用ShuffleStrides数据增强方法，在保持解剖有效性的同时置换探头视图序列和帧顺序。

Result: 在380个肺部超声视频上评估，ZACH-ViT获得最高的验证和测试ROC-AUC（0.80和0.79），平衡灵敏度（0.60）和特异性（0.91），而所有竞争模型都崩溃为平凡分类；训练速度比最小ViT快1.35倍，参数少2.5倍。

Conclusion: 结果表明，将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [493] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 该研究探索了使用生成对抗网络(GAN)进行河流沉积物建模的可行性，发现GAN的潜在空间纠缠问题导致反演困难，但通过微调方法可以改善匹配效果。


<details>
  <summary>Details</summary>
Motivation: 地下决策成本高且不确定性大，获取新数据难以扩展。将地质知识直接嵌入预测模型提供了有价值的替代方案，过程模型可以帮助训练生成模型以提高预测效率。

Method: 使用生成对抗网络(GAN)训练河流沉积物生成模型，应用四种反演方法在包含4、8和20口井的三个测试样本上进行实验，并采用标签条件化、潜在空间过参数化和GAN微调等技术。

Result: 反演方法难以匹配井数据，特别是当井数增加或测试样本与训练数据差异较大时。GAN潜在空间存在纠缠问题，相似沉积学特征的样本在潜在空间中不一定接近。微调GAN可以显著减少不匹配，达到可接受水平。

Conclusion: GAN已经能够处理地质建模工作流中所需的任务，但仍需进一步评估其鲁棒性，以及如何最好地利用它们来支持地质解释。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [494] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 本文提出了一种基于矩阵分解的去中心化学习隐私计算方法，通过分析时间噪声相关性来获得更紧密的隐私保证，并开发了新的DP-DL算法MAFALDA-SGD。


<details>
  <summary>Details</summary>
Motivation: 当前去中心化学习中的差分隐私计算方法存在局限性，导致观察到的隐私-效用权衡比集中式训练更差，需要更精确的隐私计算方法。

Method: 通过推广现有的矩阵分解结果，将标准DL算法和常见信任模型统一到一个公式中，并开发了具有用户级相关噪声的gossip-based DL算法MAFALDA-SGD。

Result: 该方法为现有DP-DL算法提供了更紧密的隐私计算，并在合成和真实世界图上优于现有方法。

Conclusion: 基于矩阵分解的隐私计算方法可以显著改善去中心化学习中的隐私-效用权衡，为开发新的DP-DL算法提供了理论基础。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [495] [Local properties of neural networks through the lens of layer-wise Hessians](https://arxiv.org/abs/2510.17486)
*Maxim Bolshim,Alexander Kugaevskikh*

Main category: cs.LG

TL;DR: 提出了一种通过层间Hessian矩阵分析神经网络的方法，揭示了参数空间局部几何特性与网络性能之间的关系。


<details>
  <summary>Details</summary>
Motivation: 为了建立神经网络优化几何与功能行为之间的联系，提供诊断和设计深度神经网络的理论基础。

Method: 定义每个功能块（层）的局部Hessian矩阵作为该层参数对标量函数的二阶导数矩阵，分析其谱特性（如特征值分布）。

Result: 在37个数据集上的111个实验表明，局部Hessian在训练过程中展现出结构规律性，其谱特性与泛化性能存在相关性。

Conclusion: 局部几何分析为改进网络架构和训练稳定性提供了实用见解，建立了优化几何与功能行为之间的联系。

Abstract: We introduce a methodology for analyzing neural networks through the lens of
layer-wise Hessian matrices. The local Hessian of each functional block (layer)
is defined as the matrix of second derivatives of a scalar function with
respect to the parameters of that layer. This concept provides a formal tool
for characterizing the local geometry of the parameter space. We show that the
spectral properties of local Hessians, such as the distribution of eigenvalues,
reveal quantitative patterns associated with overfitting,
underparameterization, and expressivity in neural network architectures. We
conduct an extensive empirical study involving 111 experiments across 37
datasets. The results demonstrate consistent structural regularities in the
evolution of local Hessians during training and highlight correlations between
their spectra and generalization performance. These findings establish a
foundation for using local geometric analysis to guide the diagnosis and design
of deep neural networks. The proposed framework connects optimization geometry
with functional behavior and offers practical insight for improving network
architectures and training stability.

</details>


### [496] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X是一个符号基准测试，用于评估大语言模型和大推理模型在类比和数学推理中的泛化能力和鲁棒性。它通过增加操作数复杂度、属性范围和引入感知不确定性来扩展I-RAVEN。


<details>
  <summary>Details</summary>
Motivation: 设计一个更复杂的基准测试来评估LLMs和LRMs在类比和数学推理中的泛化能力和鲁棒性，特别是在面对更复杂操作数、更宽属性范围和感知不确定性时的表现。

Method: 扩展I-RAVEN基准，增加操作数复杂度、扩大属性范围，并引入感知不确定性，然后对LLMs和LRMs进行实证评估。

Result: 与LLMs相比，LRMs在更长的推理关系和更宽的属性范围上分别表现出更好的生产力和系统性。但LRMs在不确定性推理方面仍有显著挑战，无法有效探索多个概率结果。

Conclusion: LRMs在复杂推理任务上优于LLMs，但在处理不确定性和探索概率结果方面仍有局限，需要进一步改进。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [497] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 动量方法使随机DC优化在任何批次大小下都能收敛，而无需动量则可能无法收敛


<details>
  <summary>Details</summary>
Motivation: 现有随机DC优化方法需要大批次或强噪声假设，限制了实际应用，需要研究小批次下的收敛性

Method: 提出基于动量的算法，在标准光滑性和有界方差假设下实现收敛

Result: 证明无动量时无论步长如何都可能无法收敛，动量方法具有可证明的收敛性和强实证性能

Conclusion: 动量是实现随机DC优化在小批次下收敛的关键因素

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [498] [Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares](https://arxiv.org/abs/2510.17506)
*Lachlan Ewen MacDonald,Hancheng Min,Leandro Palma,Salma Tarmoun,Ziqing Xu,René Vidal*

Main category: cs.LG

TL;DR: 该论文分析了过参数化最小二乘问题中梯度下降在大学习率下的收敛行为，揭示了三种不同学习率区间下的收敛特性：次临界、临界和超临界区域。


<details>
  <summary>Details</summary>
Motivation: 传统优化理论只保证小步长梯度下降的单调收敛，但神经网络训练常在大学习率（稳定性边界）下进行，表现出非单调收敛和对平坦最小值的隐式偏好，需要量化这种现象。

Method: 利用过参数化使全局最小解形成黎曼流形，将梯度下降动态分解为平行和正交于流形的分量，分别对应黎曼梯度下降和分叉动力系统。

Result: 识别出三种学习率区间：(a)次临界区域：瞬时不稳定性在有限时间内克服，线性收敛到次优平坦最小值；(b)临界区域：不稳定性持续存在，以幂律收敛到最优平坦最小值；(c)超临界区域：不稳定性持续存在，线性收敛到周期为2的轨道。

Conclusion: 通过流形分解方法，成功量化了梯度下降在大学习率下的收敛行为，为理解神经网络训练中的稳定性边界现象提供了理论框架。

Abstract: Classical optimisation theory guarantees monotonic objective decrease for
gradient descent (GD) when employed in a small step size, or ``stable", regime.
In contrast, gradient descent on neural networks is frequently performed in a
large step size regime called the ``edge of stability", in which the objective
decreases non-monotonically with an observed implicit bias towards flat minima.
In this paper, we take a step toward quantifying this phenomenon by providing
convergence rates for gradient descent with large learning rates in an
overparametrised least squares setting. The key insight behind our analysis is
that, as a consequence of overparametrisation, the set of global minimisers
forms a Riemannian manifold $M$, which enables the decomposition of the GD
dynamics into components parallel and orthogonal to $M$. The parallel component
corresponds to Riemannian gradient descent on the objective sharpness, while
the orthogonal component is a bifurcating dynamical system. This insight allows
us to derive convergence rates in three regimes characterised by the learning
rate size: (a) the subcritical regime, in which transient instability is
overcome in finite time before linear convergence to a suboptimally flat global
minimum; (b) the critical regime, in which instability persists for all time
with a power-law convergence toward the optimally flat global minimum; and (c)
the supercritical regime, in which instability persists for all time with
linear convergence to an orbit of period two centred on the optimally flat
global minimum.

</details>


### [499] [The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis](https://arxiv.org/abs/2510.17515)
*Hoang Pham,The-Anh Ta,Tom Jacobs,Rebekka Burkholz,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出基于图极限理论的新框架，使用图论表示稀疏神经网络，推导出图论神经正切核来分析不同剪枝方法的训练动态差异。


<details>
  <summary>Details</summary>
Motivation: 尽管剪枝方法能创建稀疏架构，但相同稀疏度下不同结构的可训练性差异原因仍不清楚，需要系统性的理论分析框架。

Method: 基于图极限理论（特别是图论），将稀疏神经网络表示为图论，推导出图论神经正切核（Graphon NTK）来分析无限宽度极限下的训练动态。

Result: 图论NTK的谱分析与稀疏网络观察到的训练动态相关，解释了不同剪枝方法收敛行为的差异，验证了图论极限假设。

Conclusion: 该框架为稀疏网络的理论分析提供了通用工具，揭示了连接模式对各种稀疏网络架构可训练性的影响。

Abstract: Sparse neural networks promise efficiency, yet training them effectively
remains a fundamental challenge. Despite advances in pruning methods that
create sparse architectures, understanding why some sparse structures are
better trainable than others with the same level of sparsity remains poorly
understood. Aiming to develop a systematic approach to this fundamental
problem, we propose a novel theoretical framework based on the theory of graph
limits, particularly graphons, that characterizes sparse neural networks in the
infinite-width regime. Our key insight is that connectivity patterns of sparse
neural networks induced by pruning methods converge to specific graphons as
networks' width tends to infinity, which encodes implicit structural biases of
different pruning methods. We postulate the Graphon Limit Hypothesis and
provide empirical evidence to support it. Leveraging this graphon
representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to
study the training dynamics of sparse networks in the infinite width limit.
Graphon NTK provides a general framework for the theoretical analysis of sparse
networks. We empirically show that the spectral analysis of Graphon NTK
correlates with observed training dynamics of sparse networks, explaining the
varying convergence behaviours of different pruning methods. Our framework
provides theoretical insights into the impact of connectivity patterns on the
trainability of various sparse network architectures.

</details>


### [500] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 提出SAFE-D框架，用于检测帕金森病相关的驾驶行为异常，通过多源数据构建行为档案，使用注意力网络识别时空特征，在模拟环境中达到96.8%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注功能驱动的暂时异常（如困倦、分心），而缺乏对病理触发的慢性疾病相关驾驶异常的研究，特别是帕金森病这类慢性疾病。

Method: 分析帕金森病症状学，建立与驾驶性能下降的因果关系；整合多车辆控制组件数据构建行为档案；设计基于注意力的网络自适应优先处理时空特征。

Result: 在Logitech G29平台和CARLA模拟器上验证，使用三个道路地图模拟真实驾驶，SAFE-D在区分正常和帕金森病影响驾驶模式方面达到96.8%的平均准确率。

Conclusion: SAFE-D框架能有效检测帕金森病相关的驾驶行为异常，为提升慢性疾病患者的驾驶安全提供了可行解决方案。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [501] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: 提出CD-GTMLL框架，将多标签学习建模为合作博弈，通过好奇心奖励机制解决长尾不平衡问题，在稀有标签上实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 多标签学习中存在长尾不平衡问题：少数头部标签主导梯度信号，而许多实际重要的稀有标签被忽略

Method: 将标签空间分割给多个合作玩家，共享全局准确度收益，同时根据标签稀有度和玩家间分歧获得额外好奇心奖励，无需手动调整类别权重

Result: 在常规基准测试和三个超大规模数据集上取得最先进性能，Rare-F1提升达+4.3%，P@3提升+1.6%

Conclusion: CD-GTMLL为多标签预测中的长尾鲁棒性提供了原则性、可扩展的解决方案

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [502] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 提出了Counterfactual Knowledge Distillation (CFKD)框架，通过生成多样反事实样本来纠正模型决策边界，无需群体标签即可处理多混淆因素，在低数据量和强伪相关场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型易受伪相关影响，现有基于群体标签的分布鲁棒性方法存在三大局限：群体标签稀缺、组内样本少、多伪相关导致数据碎片化。

Method: 使用反事实解释器生成多样反事实样本，通过知识蒸馏步骤让人类标注者高效探索和修正模型决策边界，无需混淆因素标签即可丰富欠采样群体数据。

Result: 在五个数据集上验证有效性，包括合成任务和工业应用，在低数据量和强伪相关场景下表现特别突出，能实现跨群体的平衡泛化。

Conclusion: CFKD框架能有效解决多混淆因素问题，无需群体标签，在数据稀缺和伪相关显著的情况下提供稳健性能，反事实解释器和教师模型选择对鲁棒性有重要影响。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [503] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 在低信噪比(SNR)数据环境下，通过向梯度下降训练中引入标签噪声可以抑制噪声记忆，改善神经网络泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易在训练中记忆噪声，特别是在低信噪比数据中，这会导致泛化性能下降。受标签噪声具有隐式正则化效果的启发，研究是否可以通过引入标签噪声来提升神经网络在低SNR环境下的测试性能。

Method: 在理想化的信号-噪声数据设置中，使用带有标签噪声的梯度下降算法训练两层神经网络。

Result: 标签噪声训练能够抑制噪声记忆，防止其主导学习过程，实现快速信号增长同时控制过拟合，从而在低SNR下获得良好泛化。相比之下，标准梯度下降在相同设置下容易过拟合噪声。

Conclusion: 在基于梯度的训练中引入标签噪声有助于改善神经网络在低信噪比数据环境下的泛化性能。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [504] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出一种基于保形对齐的级联机制(CAb)，确保边缘模型预测集满足云模型级别的条件覆盖保证，同时减少向云端的卸载


<details>
  <summary>Details</summary>
Motivation: 边缘智能需要保证推理可靠性，但边缘模型难以独立提供与云模型相同的统计保证。需要一种机制来确保边缘预测集的条件覆盖性

Method: 将边缘到云端的升级建模为多重假设检验问题，使用保形对齐来选择哪些输入可以在边缘安全处理，提供统计保证

Result: 在CIFAR-100图像分类和TeleQnA问答基准测试中，CAb级联方法保持目标条件覆盖，同时显著减少云端卸载，预测集大小适度增加

Conclusion: CAb级联方法有效平衡了覆盖保证、延迟率和预测集大小之间的权衡，为边缘智能提供了可靠的统计保证

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [505] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: TrajMamba是一种高效的车辆轨迹学习方法，通过联合建模GPS和道路视角来捕捉移动模式，集成旅行目的预训练，并使用知识蒸馏减少轨迹冗余，在效率和准确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 车辆GPS轨迹包含宝贵的旅行语义信息，但面临两个主要挑战：旅行目的与道路功能和POI相关，文本信息处理计算负担重；真实轨迹包含冗余点，影响计算效率和嵌入质量。

Method: 提出TrajMamba方法，包含Traj-Mamba编码器联合建模GPS和道路视角，旅行目的感知预训练集成旅行目的，以及知识蒸馏预训练方案通过可学习掩码生成器识别关键轨迹点。

Result: 在两个真实数据集和三个下游任务上的广泛实验表明，TrajMamba在效率和准确性上都优于最先进的基线方法。

Conclusion: TrajMamba能够有效学习语义丰富的车辆轨迹表示，解决了轨迹数据建模中的计算效率和冗余问题，为轨迹数据的实际应用提供了有效解决方案。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [506] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出了一种扩展的解码器Transformer，通过变分方法学习无监督的随机潜变量来调节生成过程


<details>
  <summary>Details</summary>
Motivation: 通过在生成过程中引入随机潜变量调节，提升模型在下游任务上的性能

Method: 扩展解码器Transformer，使用变分方法学习无监督的随机潜变量来调节生成过程

Result: 实验评估显示这种调节方法在下游任务上带来了显著改进

Conclusion: 引入随机潜变量调节的生成过程可以有效提升模型性能

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [507] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 提出了一个理论框架来评估时间序列异常检测指标，分析了37个常用指标并发现大多数只满足少数属性，提出了满足所有属性的LARM指标及其增强版ALARM。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的未检测异常可能引发安全关键系统的灾难性故障，现有检测方法的性能评估不清晰，因为当前指标只捕捉任务的狭隘方面且经常产生误导性结果。

Method: 引入可验证属性来形式化评估时间序列异常检测的基本要求，建立理论框架支持原则性评估和可靠比较，分析37个广泛使用的指标，提出LARM和ALARM指标。

Result: 分析显示大多数指标只满足少数属性，没有一个指标满足所有属性，这解释了先前结果中持续存在的不一致性。

Conclusion: 提出的LARM和ALARM指标填补了现有评估指标的不足，为时间序列异常检测提供了更可靠的评估框架。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [508] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文分析了安全强化学习中拉格朗日乘子的最优性和稳定性问题，发现自动更新乘子能够恢复甚至超过最优性能，但存在振荡行为，可通过PID控制缓解。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，拉格朗日方法是处理约束优化问题的常用方法，但其效果严重依赖于拉格朗日乘子的选择。目前缺乏关于自动更新乘子鲁棒性及其对整体性能影响的实证证据。

Method: 通过分析多个任务中拉格朗日乘子的最优性和稳定性，提供λ-配置文件来可视化回报与约束成本之间的权衡关系，并研究自动乘子更新和PID控制更新的效果。

Result: 研究发现λ具有高度敏感性，自动乘子更新能够恢复甚至超过在λ*处找到的最优性能，但存在振荡行为。PID控制更新可以缓解振荡，但需要仔细调参。

Conclusion: 拉格朗日方法在安全强化学习中需要进一步研究以提升稳定性，自动乘子更新虽然有效但存在振荡问题，PID控制需要仔细调参才能在不同任务中实现一致更好的性能。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [509] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 该论文研究如何通过降维技术改进抗菌肽设计的深度生成模型，提高潜在空间的解释性和优化效率。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽是治疗细菌感染的有前景疗法，但序列设计空间巨大。现有深度生成模型虽然有效，但缺乏解释性且难以量化潜在空间质量。

Method: 使用变分自编码器，研究通过降维进一步压缩设计空间，并基于物理化学性质组织潜在空间，在不同标签可用性下进行实验。

Result: 发现降维在数据可用性较高时有利于优化，降维搜索空间更易解释，且能在不同标签比例下基于物理化学性质有效组织潜在空间。

Conclusion: 降维技术能改进抗菌肽设计生成模型，提高潜在空间解释性和优化效率，为生物分子设计提供新思路。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [510] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: 提出CEPerFed方法解决联邦学习在医学影像分类中的数据异构性和通信开销问题，通过客户端历史梯度协调本地与全局优化，使用分层SVD策略减少通信量


<details>
  <summary>Details</summary>
Motivation: 多脉冲MRI在阿尔茨海默病诊断中广泛应用，需要大量多样化数据训练鲁棒模型，但医疗机构的原始数据共享存在隐私问题。联邦学习虽然可行，但面临数据异构性导致的模型收敛问题和大量参数传输的通信开销挑战

Method: CEPerFed方法：1) 使用客户端历史风险梯度和历史平均梯度协调本地与全局优化；2) 前者加权其他客户端贡献增强本地更新可靠性；3) 后者确保本地更新与全局优化方向一致；4) 采用分层SVD策略仅传输模型更新所需的关键信息

Result: 在五个分类任务上的实验验证了CEPerFed方法的有效性

Conclusion: CEPerFed方法成功解决了联邦学习在医学影像分类中的数据异构性和通信开销问题，代码将在接受后开源

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [511] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 使用机器学习和深度学习技术（特别是GAN）生成合成数据来解决自杀预测中的极端类别不平衡问题，在真实测试数据上获得了良好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 自杀预测对于预防至关重要，但真实数据中阳性样本稀少，导致极端的类别不平衡问题，需要数据增强来改善模型性能。

Method: 使用机器学习和深度学习技术，包括生成对抗网络（GAN）生成合成数据样本来增强数据集，并应用多种机器学习模型（逻辑回归、随机森林、支持向量机）进行预测。

Result: 逻辑回归：加权精度0.99，加权召回率0.85，加权F1分数0.91；随机森林：0.98，0.99，0.99；支持向量机：0.99，0.76，0.86。LR和SVM正确识别了1个自杀尝试案例，RF识别为0个。

Conclusion: 这些结果证明了模型的有效性，GAN在生成合成数据以支持自杀预防建模工作中发挥了关键作用。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [512] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 该论文提出了三个主要贡献：CADP算法连接策略梯度和动态规划，建立了ERM Bellman算子的收缩条件并提出了相关算法，以及开发了用于风险规避目标的模型无关Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在多模型MDP中优化折扣回报的问题，以及为风险规避目标（ERM-TRC和EVaR-TRC）建立理论框架和计算算法。

Method: 使用坐标上升动态规划（CADP）、指数值迭代、策略迭代、线性规划和模型无关Q学习等方法。

Result: 证明了ERM Bellman算子的收缩条件，提出了收敛的风险规避Q学习算法，能够计算ERM-TRC和EVaR-TRC的最优平稳策略。

Conclusion: 该论文为多模型MDP和风险规避强化学习提供了新的理论分析和有效算法，填补了相关领域的研究空白。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [513] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出了一个双层强化学习框架来解决Sim2Real性能差距问题，通过基于真实世界性能直接调整模拟器参数


<details>
  <summary>Details</summary>
Motivation: 现有的Sim2Real方法通过优化模拟器精度和变异性作为真实世界性能的代理指标，但这些指标与策略在真实世界的性能不一定相关

Method: 采用双层RL框架：内层RL在模拟环境中训练策略，外层RL调整模拟模型和模拟内奖励参数，以最大化模拟策略在真实世界的性能

Result: 推导并验证了开发能够缩小Sim2Real性能差距的双层RL算法所需的数学工具

Conclusion: 该框架能够直接基于真实世界性能优化模拟器参数，有效解决Sim2Real性能差距问题

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [514] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 本文研究如何提升黑盒大语言模型作为分类器时的操作粒度，通过分析其低基数数值输出的原因，并提出了有效方法来增加可用操作点数量和多样性，在不损失性能的情况下实现更精细的决策控制。


<details>
  <summary>Details</summary>
Motivation: 黑盒LLMs虽然实用且易于使用，但在需要特定指标约束的应用中表现不佳，主要由于其数值输出基数低，限制了操作点的控制能力，无法精细调整决策行为。

Method: 首先分析LLMs低基数数值输出的原因，发现它们偏向生成四舍五入但有信息量的语言化概率；然后实验标准提示工程、不确定性估计和置信度激发技术；最后提出有效方法来显著增加可用操作点的数量和多样性。

Result: 提出的方法在11个数据集和3个LLMs上实现了更细粒度的操作点，性能与基准方法相当或更好。

Conclusion: 通过提出的方法可以有效提升黑盒LLMs的操作粒度，使其在需要特定指标约束的应用中更具实用性，同时保持或提升性能。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [515] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 本研究开发了物理信息神经网络(PINN)策略，将海冰物理知识整合到机器学习模型中，用于预测北极海冰速度和浓度。


<details>
  <summary>Details</summary>
Motivation: 完全数据驱动的机器学习模型在泛化性和物理一致性方面存在局限，特别是在北极海冰变薄、融化加速的新阶段，历史数据训练的模型可能无法充分代表未来动态变化的海冰条件。

Method: 基于分层信息共享U-net架构，通过引入物理损失函数和激活函数，生成物理上合理的海冰速度和浓度输出。

Result: PINN模型在海冰速度和浓度的日预测中优于完全数据驱动模型，即使使用少量样本训练也能表现良好，特别是在融化和早期冻结季节以及快速移动冰区显著改善了海冰浓度预测。

Conclusion: 物理信息神经网络方法能够有效提升海冰预测的准确性和物理一致性，特别是在数据稀缺和动态变化条件下表现优异。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [516] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 本文提出了一种基于微分图册的流形学习方法，通过维护可微分图册结构实现流形上的黎曼优化，在效率和准确性方面具有优势，并在分类任务和RNA速度分析中展示了更好的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习方法主要关注降维到欧几里得空间，当嵌入维度接近流形本征维度时会丢失关键流形特征。直接学习潜在流形作为微分图册的方法相对较少被探索，本文旨在证明这类方法的有效性和潜力。

Method: 实现了一个通用数据结构来维护可微分图册，支持流形上的黎曼优化，并提出了从点云数据无监督学习微分图册的启发式方法。

Result: 实验证明该方法在特定设置下具有效率和准确性优势。在Klein瓶上的监督分类任务和造血数据的RNA速度分析中，展示了改进的可解释性和鲁棒性。

Conclusion: 基于图册的流形学习方法具有显著潜力，能够更好地保留流形特征，在机器学习和数据分析中提供更有效的工具。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [517] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 提出了在推理时保持线性插值的流匹配缩放方法，在图像和蛋白质生成任务中验证了样本质量随计算量增加而提升


<details>
  <summary>Details</summary>
Motivation: 流匹配模型在推理时缩放方法研究不足，现有方法牺牲了高效采样特性，且仅应用于视觉任务

Method: 开发了保持线性插值的推理时缩放程序，首次应用于无条件蛋白质生成

Result: 样本质量随推理计算量增加而持续提升，证明了流匹配推理缩放可应用于科学领域

Conclusion: 提出的方法在保持高效采样的同时实现了推理时缩放，成功扩展到科学领域应用

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [518] [Functional Distribution Networks (FDN)](https://arxiv.org/abs/2510.17794)
*Omer Haq*

Main category: cs.LG

TL;DR: 提出了Functional Distribution Networks (FDN)，一种输入条件化的网络权重分布方法，通过beta-ELBO和蒙特卡洛采样训练，旨在解决概率回归器在分布偏移下的过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 现代概率回归器在分布偏移下往往保持过度自信，需要一种能够根据输入自适应调整预测分散度的机制。

Method: 使用输入条件化的网络权重分布，通过beta-ELBO损失函数和蒙特卡洛采样进行训练，诱导产生自适应分散度的预测混合分布。

Result: 在标准回归任务中，与贝叶斯、集成、dropout和超网络基线相比，在匹配参数和更新预算下评估了准确性、校准和偏移感知能力。

Conclusion: 该框架和评估协议旨在使面向分布外、良好校准的神经回归变得实用和模块化。

Abstract: Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

</details>


### [519] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 提出GaLore Unbiased with Muon (GUM)方法，通过层间采样技术消除低秩投影的偏差，在保持内存效率的同时实现无偏优化，在LLM微调和预训练中表现优于GaLore甚至全参数训练。


<details>
  <summary>Details</summary>
Motivation: 现有梯度低秩投影方法（如GaLore）缺乏收敛保证，其低秩投影机制相对于原始优化算法存在固有偏差，导致性能与全参数训练存在差距。

Method: 基于GaLore机制和Muon算法，采用层间采样技术来消除低秩投影偏差，提出GUM方法。

Result: 理论证明GUM与基础Muon算法具有相同的收敛保证，同时保持低秩技术的内存效率。实证实验显示在LLM微调和预训练中优于GaLore，甚至超过全参数训练。

Conclusion: 该技术通过更均匀的层内知识分布，实现了模型参数空间的更高效利用和更好的记忆能力，从而带来性能提升。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [520] [Taming Modality Entanglement in Continual Audio-Visual Segmentation](https://arxiv.org/abs/2510.17234)
*Yuyang Hong,Qi Yang,Tao Zhang,Zili Wang,Zhaojin Fu,Kun Ding,Bin Fan,Shiming Xiang*

Main category: cs.MM

TL;DR: 本文提出了持续音频-视觉分割(CAVS)任务，通过碰撞式多模态排练(CMR)框架解决细粒度持续学习中的模态纠缠问题，包括多模态样本选择(MSS)和碰撞式样本排练(CSR)机制。


<details>
  <summary>Details</summary>
Motivation: 现有多模态持续学习方法主要关注粗粒度任务，在细粒度持续学习设置中处理模态纠缠存在局限性，需要解决多模态语义漂移和共现混淆两个关键挑战。

Method: 提出CMR框架：1) MSS策略选择模态一致性高的样本进行排练；2) CSR机制增加易混淆类的排练样本频率；3) 构建三个音频-视觉增量场景进行验证。

Result: 综合实验表明，该方法显著优于单模态持续学习方法，有效解决了多模态语义漂移和共现混淆问题。

Conclusion: CMR框架成功解决了细粒度多模态持续学习中的模态纠缠挑战，为音频-视觉分割的持续学习提供了有效解决方案。

Abstract: Recently, significant progress has been made in multi-modal continual
learning, aiming to learn new tasks sequentially in multi-modal settings while
preserving performance on previously learned ones. However, existing methods
mainly focus on coarse-grained tasks, with limitations in addressing modality
entanglement in fine-grained continual learning settings. To bridge this gap,
we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to
continuously segment new classes guided by audio. Through comprehensive
analysis, two critical challenges are identified: 1) multi-modal semantic
drift, where a sounding objects is labeled as background in sequential tasks;
2) co-occurrence confusion, where frequent co-occurring classes tend to be
confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework
is designed to address these challenges. Specifically, for multi-modal semantic
drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select
samples with high modal consistency for rehearsal. Meanwhile, for co-occurence
confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,
allowing for the increase of rehearsal sample frequency of those confusable
classes during training process. Moreover, we construct three audio-visual
incremental scenarios to verify effectiveness of our method. Comprehensive
experiments demonstrate that our method significantly outperforms single-modal
continual learning methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [521] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: SIADAFIX是一种基于快慢思维的自适应程序修复方法，通过问题描述响应选择三种修复模式（简单、中等、困难），在SWE-bench Lite上达到60.67%的pass@1性能，在开源方法中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 利用快慢思维增强基于大语言模型的智能体在复杂任务（如程序修复）上的能力，平衡修复效率和准确性。

Method: 使用慢思维bug修复代理完成复杂程序修复任务，快思维工作流决策组件优化和分类问题描述，根据问题复杂度自适应选择三种修复模式，对简单问题使用快速泛化，对复杂问题使用测试时扩展技术。

Result: 在SWE-bench Lite上的实验结果显示，使用Claude-4 Sonnet模型达到60.67%的pass@1性能，在所有开源方法中达到最先进水平。

Conclusion: SIADAFIX有效平衡了修复效率和准确性，为自动化程序修复提供了新的思路。

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [522] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: 该研究分析了14万篇研究论文与代码仓库的配对数据，发现软件贡献与学术认可之间存在显著脱节：近30%的文章包含未获作者身份认可的代码贡献者，频繁编码的作者h指数低于非编码同事。


<details>
  <summary>Details</summary>
Motivation: 理解软件开发活动如何影响科学合作中的学术认可分配，揭示软件贡献与传统学术评价指标之间的关系。

Method: 构建包含约14万篇研究文章与代码仓库配对的数据集，开发预测模型匹配文章作者与代码仓库开发者账户，分析软件贡献对学术认可的影响。

Result: 近30%文章包含未获作者身份的非作者代码贡献者；代码贡献作者的文章引用率仅增加4.2%，控制变量后不显著；第一作者更可能是代码贡献者；编码频率与学术影响力呈负相关，频繁编码作者h指数更低。

Conclusion: 软件贡献与学术认可之间存在严重脱节，这对机构奖励机制和科学政策具有重要启示意义。

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [523] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: MLCPD是一个大规模、语言无关的代码解析数据集，统一了10种主要编程语言的语法和结构表示，包含700多万个解析后的源文件，采用统一的抽象语法树模式进行标准化。


<details>
  <summary>Details</summary>
Motivation: 现有语料库主要关注词法级代码或孤立解析器，缺乏统一的跨语言结构表示。MLCPD旨在提供一致的跨语言推理、结构学习和多语言软件分析基础。

Method: 提出通用抽象语法树模式，对10种编程语言的源代码进行解析和标准化，提供层次化树表示和丰富元数据，确保无损语法覆盖和结构一致性。

Result: 经验分析显示，不同编程语言（如Python、Java、Go）的语法图可以在共享模式下对齐，揭示了跨语言结构规律性。数据集以Parquet格式存储，便于可扩展检索。

Conclusion: MLCPD为跨语言表示学习和程序分析的未来研究建立了开放、可复现的基础，数据集已在Hugging Face公开发布，配套代码库在GitHub提供。

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [524] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: SemOpt是一个利用静态程序分析和LLM的代码优化框架，通过构建策略库、生成静态分析规则和优化代码，显著提升了代码优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码优化方法依赖信息检索技术从开源代码库中查找优化示例，但由于语义等价但语法不同的代码片段难以被准确检索，导致优化效果不佳。

Method: SemOpt包含三个核心组件：策略库构建器（提取和聚类优化策略）、规则生成器（生成Semgrep静态分析规则）、优化器（生成优化代码），所有组件都由LLM驱动。

Result: 在包含151个优化任务的基准测试中，SemOpt相比基线方法将成功优化数量提高了1.38到28倍；在大型C/C++项目中，单个性能指标提升了5.04%到218.07%。

Conclusion: SemOpt通过结合静态程序分析和LLM，有效解决了现有代码优化方法中检索相关示例的局限性，显著提升了优化效果和实用性。

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [525] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: 提出Code Digital Twin框架，将AI能力与企业软件开发实践对齐，通过混合知识表示和多阶段提取管道，将碎片化知识转化为可操作的显式表示，支持复杂系统的智能开发和演进。


<details>
  <summary>Details</summary>
Motivation: 企业软件开发主要依赖增量演进，涉及大量隐性知识（如设计决策和历史权衡），现有LLM在支持复杂软件开发方面存在局限性，需要将新兴AI能力与实际的软件开发现实对齐。

Method: 提出Code Digital Twin框架，包含混合知识表示、多阶段提取管道、增量更新、LLM赋能应用和人机协同反馈，对软件的物理层和概念层进行建模，并与代码库共同演进。

Result: Code Digital Twin能够将碎片化知识转化为显式且可操作的表征，在问题定位和影响分析等任务中增强决策能力。

Conclusion: Code Digital Twin作为AI进展与企业软件现实之间的桥梁，为超复杂系统的可持续、智能和弹性开发演进提供了具体路线图。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [526] [Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions](https://arxiv.org/abs/2510.16433)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Yasutaka Kamei,Hajimu Iida*

Main category: cs.SE

TL;DR: 该研究通过分析OSS-Fuzz平台的约112万次模糊测试会话，揭示了持续模糊测试在漏洞检测中的作用，包括早期高检测率、代码覆盖率持续增长以及覆盖率变化对漏洞检测的贡献。


<details>
  <summary>Details</summary>
Motivation: 尽管持续模糊测试已被数千个项目采用，但其在漏洞检测中的具体作用尚不明确。本研究旨在阐明持续模糊测试在漏洞检测中的角色。

Method: 收集OSS-Fuzz的问题报告、覆盖率报告和模糊测试日志，对878个项目的约112万次模糊测试会话进行实证研究。

Result: 发现大量模糊测试漏洞在集成持续模糊测试前就已存在，导致早期高检测率；代码覆盖率随持续模糊测试进展而持续增加；覆盖率变化有助于模糊测试漏洞的检测。

Conclusion: 本研究为持续模糊测试如何促进模糊测试漏洞检测提供了实证见解，对未来的持续模糊测试策略和工具开发具有实际意义。

Abstract: Software vulnerabilities are constantly being reported and exploited in
software products, causing significant impacts on society. In recent years, the
main approach to vulnerability detection, fuzzing, has been integrated into the
continuous integration process to run in short and frequent cycles. This
continuous fuzzing allows for fast identification and remediation of
vulnerabilities during the development process. Despite adoption by thousands
of projects, however, it is unclear how continuous fuzzing contributes to
vulnerability detection. This study aims to elucidate the role of continuous
fuzzing in vulnerability detection. Specifically, we investigate the coverage
and the total number of fuzzing sessions when fuzzing bugs are discovered. We
collect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an
online service provided by Google that performs fuzzing during continuous
integration. Through an empirical study of a total of approximately 1.12
million fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal
that (i) a substantial number of fuzzing bugs exist prior to the integration of
continuous fuzzing, leading to a high detection rate in the early stages; (ii)
code coverage continues to increase as continuous fuzzing progresses; and (iii)
changes in coverage contribute to the detection of fuzzing bugs. This study
provides empirical insights into how continuous fuzzing contributes to fuzzing
bug detection, offering practical implications for future strategies and tool
development in continuous fuzzing.

</details>


### [527] [On the Use of Large Language Models for Qualitative Synthesis](https://arxiv.org/abs/2510.16502)
*Sebastián Pizard,Ramiro Moreira,Federico Galiano,Ignacio Sastre,Lorena Etcheverry*

Main category: cs.SE

TL;DR: 本文探讨了使用大型语言模型（LLMs）支持系统评价中定性综合阶段所面临的挑战，通过协作自民族志方法评估了LLMs在此过程中的方法严谨性和实际有用性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在支持系统评价方面显示出潜力，但将其应用于报告不一致、执行多变的定性综合阶段存在重要风险，可能放大现有弱点并削弱系统评价结果的可信度。

Method: 采用协作自民族志方法，进行了两个试验，从技术角度评估LLMs在定性综合中的方法严谨性和实际有用性。

Result: 研究揭示了使用LLMs进行定性综合的具体挑战，包括方法严谨性和实际应用价值方面的问题。

Conclusion: 需要谨慎应用LLMs于系统评价的定性综合阶段，考虑其技术局限性和可能带来的风险。

Abstract: Large language models (LLMs) show promise for supporting systematic reviews
(SR), even complex tasks such as qualitative synthesis (QS). However, applying
them to a stage that is unevenly reported and variably conducted carries
important risks: misuse can amplify existing weaknesses and erode confidence in
the SR findings. To examine the challenges of using LLMs for QS, we conducted a
collaborative autoethnography involving two trials. We evaluated each trial for
methodological rigor and practical usefulness, and interpreted the results
through a technical lens informed by how LLMs are built and their current
limitations.

</details>


### [528] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: CoReEval是首个大规模评估LLM代码可读性评估能力的基准，包含140万次模型-代码片段-提示评估，涵盖10个LLM、3种编程语言、2种代码类型、4种提示策略和9种解码设置。研究发现基于开发者指导的提示能改善对齐度和解释质量，但也增加了分数变异性。


<details>
  <summary>Details</summary>
Motivation: 代码可读性对软件理解和维护至关重要，但难以大规模评估。传统静态指标无法捕捉人类判断的主观性和上下文敏感性，而LLM作为可扩展替代方案的行为尚未充分探索。

Method: 构建CoReEval基准，包含140万次评估，涵盖10个LLM、3种编程语言(Java、Python、CUDA)、2种代码类型(功能代码和单元测试)、4种提示策略(ZSL、FSL、CoT、ToT)、9种解码设置，以及针对初级和高级开发者角色的定制提示。比较LLM输出与人类标注和验证静态模型。

Result: 基于人类定义可读性维度的开发者指导提示在结构化环境中改善了对齐度，提高了解释质量，并通过角色框架实现轻量级个性化。但增加了分数变异性，突显了对齐度、稳定性和可解释性之间的权衡。

Conclusion: CoReEval为提示工程、模型对齐研究和人机协同评估提供了坚实基础，在教育、入职培训和CI/CD管道等场景中，LLM可作为可解释、适应性强的代码审查工具。

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [529] [Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios](https://arxiv.org/abs/2510.16665)
*Mohamed Sami Rakha,Andriy Miranskyy,Daniel Alencar da Costa*

Main category: cs.SE

TL;DR: 该研究对比了超参数调优在软件缺陷预测（SDP）中两个关键场景（IVDP和CVDP）的影响差异，发现IVDP场景下的性能提升显著大于CVDP场景，且不同ML算法和数据集大小对调优效果有显著影响。


<details>
  <summary>Details</summary>
Motivation: 虽然超参数调优能提升SDP性能，但其效果因具体SDP场景而异。需要对比不同场景下超参数调优的影响，以增强SDP建模的鲁棒性、泛化性和实用性。

Method: 使用28种ML算法、53个软件数据集、两种调优算法和五种优化指标，在IVDP和CVDP两个SDP场景下进行实验，采用统计分析方法比较性能影响差异。

Result: IVDP场景下的SDP性能提升显著大于CVDP场景；28种ML算法中有24种在不同SDP场景下的性能提升不一致；小数据集更容易出现较大的性能影响差异。

Conclusion: 软件工程研究者和从业者在期望通过超参数调优获得性能提升时，应考虑所选SDP场景的影响。

Abstract: Software defect prediction (SDP) is crucial for delivering high-quality
software products. Recent research has indicated that prediction performance
improvements in SDP are achievable by applying hyperparameter tuning to a
particular SDP scenario. However, the positive impact resulting from the
hyperparameter tuning step may differ based on the targeted SDP scenario.
Comparing the impact of hyperparameter tuning across SDP scenarios is necessary
to provide comprehensive insights and enhance the robustness, generalizability,
and, eventually, the practicality of SDP modeling for quality assurance.
  Therefore, in this study, we contrast the impact of hyperparameter tuning
across two pivotal and consecutive SDP scenarios: (1) Inner Version Defect
Prediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main
distinctions between the two scenarios lie in the scope of defect prediction
and the selected evaluation setups. This study's experiments use common
evaluation setups, 28 machine learning (ML) algorithms, 53 post-release
software datasets, two tuning algorithms, and five optimization metrics. We
apply statistical analytics to compare the SDP performance impact differences
by investigating the overall impact, the single ML algorithm impact, and
variations across different software dataset sizes.
  The results indicate that the SDP gains within the IVDP scenario are
significantly larger than those within the CVDP scenario. The results reveal
that asserting performance gains for up to 24 out of 28 ML algorithms may not
hold across multiple SDP scenarios. Furthermore, we found that small software
datasets are more susceptible to larger differences in performance impacts.
Overall, the study findings recommend software engineering researchers and
practitioners to consider the effect of the selected SDP scenario when
expecting performance gains from hyperparameter tuning.

</details>


### [530] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出了QuanBench基准来评估LLMs在量子代码生成方面的能力，包含44个编程任务，评估结果显示当前LLMs在生成正确量子代码方面能力有限，准确率低于40%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用代码生成方面表现良好，但在量子代码生成方面的能力尚未得到充分研究，需要专门的基准来评估。

Method: 创建QuanBench基准，包含44个涵盖量子算法、状态准备、门分解和量子机器学习的编程任务，通过功能正确性(Pass@K)和量子语义等价性(过程保真度)进行评估。

Result: 评估结果显示当前LLMs生成正确量子代码的能力有限，总体准确率低于40%，存在过时API使用、电路构建错误和算法逻辑错误等常见失败案例。

Conclusion: QuanBench为未来改进LLMs的量子代码生成能力提供了基础，当前LLMs在量子编程方面仍有较大提升空间。

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [531] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: 本文研究了LLM驱动的编码代理的成本控制问题，提出了三种轮次控制策略，发现动态轮次策略在保持性能的同时能显著降低成本。


<details>
  <summary>Details</summary>
Motivation: LLM编码代理在实际部署中面临显著且不可预测的成本问题，主要源于轮次增加导致的令牌数量二次增长、模型价格高昂、任务需要大量轮次以及代理采取低效行动。现有研究主要优化单轮性能，而对总轮次控制的战略研究不足。

Method: 在SWE-bench上使用三种最先进模型进行实证研究，评估三种轮次控制策略：无限制基线、带提醒的固定轮次限制、以及按需扩展的新型动态轮次策略。

Result: 无限制设置中存在基本权衡，没有单一模型在性能、成本和轮次效率方面都表现优异。固定轮次限制（基线第75百分位）可大幅降低成本（24%-68%）且对解决率影响最小。动态轮次策略表现最佳，在保持或提高解决率的同时进一步降低成本12%-24%。

Conclusion: 动态资源分配是部署强大且经济可行的编码代理的优越方法，提供了简单有效的成本与效能平衡指南。

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [532] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 本文研究发现代码翻译任务中存在"多示例悖论"：虽然静态相似度指标随示例数量增加而略有改善，但功能正确性在少量示例（5-25个）时达到峰值，过多示例反而会降低性能。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在代码翻译任务中，增加上下文示例数量（从零示例到数百示例）对性能的实际影响，验证"越多越好"的假设是否适用于复杂任务。

Method: 通过大规模实证研究，评估超过90,000次翻译，系统性地测试从零示例到多示例（最多625个示例）的配置，提示长度从约10万到80万token不等。

Result: 发现功能正确性在少量示例（5-25个）时达到最佳，过多示例会降低性能；静态相似度指标随示例数量增加略有改善，但与功能正确性趋势不一致。

Conclusion: 对于代码翻译任务，少量精心选择的示例质量比数量更重要，挑战了"越多越好"的普遍有效性，强调了最优提示策略的任务依赖性。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [533] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: 研究发现LLM生成的Chrome扩展程序存在严重安全漏洞，漏洞率高达18%-50%，特别是在认证和Cookie管理场景中漏洞率可达83%和78%。令人惊讶的是，更先进的推理模型反而产生更多漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中扮演越来越重要的角色，开发者可能只关注程序功能而忽略实现中的安全问题。本研究旨在调查LLM生成的框架约束程序的安全性，特别是Chrome扩展这类具有复杂安全模型的程序。

Method: 构建了ChromeSecBench数据集，包含140个基于已知漏洞扩展的提示，使用9个最先进的LLM生成完整的Chrome扩展，并从三个维度分析漏洞：场景类型、模型差异和漏洞类别。

Result: LLM生成的程序存在高漏洞率（18%-50%），在认证和Cookie管理场景中尤其严重（分别达83%和78%）。大多数漏洞会将敏感浏览器数据暴露给不可信代码。高级推理模型表现更差，产生更多漏洞。

Conclusion: LLM的编码能力与编写安全框架约束程序的能力之间存在关键差距，需要加强LLM在安全编程方面的能力。

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [534] [Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection](https://arxiv.org/abs/2510.17056)
*Luis F. G. Campos,Leonardo C. Marques,Walter T. Nakamura*

Main category: cs.SE

TL;DR: 本研究比较了生成式AI与人类专家在可用性检查中的表现，发现AI虽然能发现新缺陷但假阳性率较高，而AI与人类结合能获得最佳结果。


<details>
  <summary>Details</summary>
Motivation: 可用性检查成本高且需要专业知识，AI技术的发展为支持这一任务提供了新机会，本研究旨在评估AI在识别可用性问题方面的性能。

Method: 使用软件原型，由4名专家和2个AI模型（GPT-4o和Gemini 2.5 Flash）进行评估，采用精确率、召回率和F1分数等指标。

Result: 人类检查员在精确率和整体覆盖率方面表现最佳，AI表现出高个体性能并发现许多新缺陷，但假阳性率和冗余报告较高。AI与人类结合产生最佳结果。

Conclusion: 当前阶段的AI无法替代人类检查员，但可作为有价值的增强工具来提高效率和扩大缺陷覆盖范围。

Abstract: Usability inspection is a well-established technique for identifying
interaction issues in software interfaces, thereby contributing to improved
product quality. However, it is a costly process that requires time and
specialized knowledge from inspectors. With advances in Artificial Intelligence
(AI), new opportunities have emerged to support this task, particularly through
generative models capable of interpreting interfaces and performing inspections
more efficiently. This study examines the performance of generative AIs in
identifying usability problems, comparing them to those of experienced human
inspectors. A software prototype was evaluated by four specialists and two AI
models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,
and F1-score. While inspectors achieved the highest levels of precision and
overall coverage, the AIs demonstrated high individual performance and
discovered many novel defects, but with a higher rate of false positives and
redundant reports. The combination of AIs and human inspectors produced the
best results, revealing their complementarity. These findings suggest that AI,
in its current stage, cannot replace human inspectors but can serve as a
valuable augmentation tool to improve efficiency and expand defect coverage.
The results provide evidence based on quantitative analysis to inform the
discussion on the role of AI in usability inspections, pointing to viable paths
for its complementary use in software quality assessment contexts.

</details>


### [535] [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](https://arxiv.org/abs/2510.17110)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出了一种基于模型驱动开发（MDD）的量子系统设计方法，能够自动生成多种量子编程语言的代码，提高开发效率和跨平台一致性。


<details>
  <summary>Details</summary>
Motivation: 量子计算领域快速发展，量子编程语言不断涌现，但模型驱动开发在量子系统工程中的应用仍未被充分探索。

Method: 开发了一个MDD框架，支持量子系统的结构化设计和实现，能够自动为多种量子编程语言生成代码。

Result: 通过多个案例研究验证了该方法的有效性和实用性。

Conclusion: 该MDD方法为量子系统开发提供了高效、一致的解决方案，有助于促进量子计算的实际应用。

Abstract: With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

</details>


### [536] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出SEER框架，将代码生成的思维链推理建模为决策问题，通过自我探索实现多样化的推理路径探索、质量感知模型训练和自适应推理切换。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理方法存在三个关键局限：推理路径多样性不足、中间步骤质量评估缺失、过度思考导致复杂错误方案。

Method: SEER框架包含三个核心组件：多样化推理路径探索、推理质量感知模型训练（策略模型生成候选步骤，价值模型评估质量）、自适应思维链推理。

Result: 未在摘要中明确说明具体实验结果。

Conclusion: SEER框架通过将代码生成的思维链推理建模为决策问题，解决了现有方法的局限性，实现了准确和自适应的推理。

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [537] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: Peace是一个用于项目级代码效率优化的混合框架，通过自动代码编辑实现函数间交互优化，在真实项目中显著提升执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码优化方法仅关注函数级优化，忽略了函数间交互，无法适应真实开发场景。代码编辑技术虽具潜力，但面临无效编辑和次优内部函数的挑战。

Method: Peace框架包含三个关键阶段：依赖感知的优化函数序列构建、有效关联编辑识别、效率优化编辑迭代，确保项目整体正确性和完整性。

Result: 在PeacExec基准测试中，Peace在146个真实优化任务上达到69.2%正确率、+46.9%优化率、0.840执行效率加速比，显著优于现有方法。

Conclusion: Peace证明了项目级代码效率优化的可行性，其混合框架设计在复杂多函数优化任务中表现尤为突出，为LLM在代码优化领域的应用开辟了新方向。

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [538] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出了TREAT评估框架，全面评估代码大模型在软件工程任务中的可信度，包括多任务评估、多语言多模态评估、鲁棒性评估和严谨的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估代码大模型可信度方面存在局限，任务范围有限且缺乏对模型鲁棒性和可靠性的评估。

Method: 开发TREAT评估框架，包含四个主要改进：多任务整体评估、多语言多模态评估、鲁棒性评估和严谨的评估方法。

Result: 评估了26个最先进模型，发现模型在不同编程任务中性能差异显著，多模态模型在UI代码生成和编辑方面存在特定性能限制。

Conclusion: TREAT框架填补了现有评估方法的空白，为全面评估代码大模型的可信度提供了系统方法，揭示了当前模型的优势和局限性。

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [539] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本研究通过定性访谈调查软件测试专业人员如何实际使用大语言模型，提出了一个实践者指导的初步指南，支持LLM集成到测试工作流程中。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件测试中的应用快速增长，从测试用例生成到自动化和文档支持，但其采用往往依赖于非正式实验而非结构化指导。

Method: 对15名来自不同角色和领域的软件测试人员进行定性研究，通过半结构化访谈收集数据，并使用基于扎根理论的专题分析方法。

Result: 测试人员描述了一个迭代和反思的过程，包括定义测试目标、应用提示工程策略、优化提示、评估输出和随时间学习。他们强调需要人工监督和仔细验证，特别是由于LLM存在幻觉和不一致推理等已知限制。

Conclusion: LLM在软件测试中的采用正在增长，但仍受到不断发展的实践和对风险的谨慎态度的影响。本研究为在测试环境中结构化使用LLM提供了一个起点，并邀请未来研究在团队、工具和任务中完善这些实践。

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [540] [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](https://arxiv.org/abs/2510.17184)
*Nicolas Robert,Fabien Gandon,Maxime Lefrançois*

Main category: cs.SE

TL;DR: OLIVAW是一个支持ACIMOV方法论的GitHub工具，用于敏捷协作式本体设计，通过W3C标准协助模块化本体的开发。


<details>
  <summary>Details</summary>
Motivation: 需要敏捷协作的本体设计方法，使其能够用户驱动、保持更新并与支持系统同步演进，因此需要持续验证工具来确保本体在整个开发过程中满足开发者需求。

Method: 基于W3C标准，通过GitHub Composite Actions、预提交钩子或命令行界面来支持模块化本体的开发。

Result: 在多个本体项目中进行了测试，验证了其有用性、通用性和可重用性，并提供了模板仓库以便快速开始使用。

Conclusion: OLIVAW是一个有效的工具，支持敏捷协作的本体开发，确保本体质量并促进其持续演进。

Abstract: Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

</details>


### [541] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: AdapTrack是一种改进的约束解码方法，通过引入回溯机制避免扭曲语言模型的输出意图，在保证代码约束的同时更好地保持语义一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的约束解码技术虽然能确保生成的代码满足语法和API约束，但会严重扭曲模型的输出意图，导致生成的代码虽然满足约束但与开发意图不符。

Method: AdapTrack在生成过程中引入回溯机制，避免强制模型选择违反其原始意图的选项，从而保持与模型输出分布的一致性。

Result: 在合成API完成数据集上相比约束解码提升360.87%；在真实API完成数据集上提升38.93%；在HumanEval和MBPP基准测试中分别提升7.84%和6.42%。

Conclusion: 通过更好地遵循模型的输出意图，AdapTrack能够显著提高代码生成质量，同时提供理论证明其输出分布与模型给定生成标记的分布一致。

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [542] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: 开发了可扩展CI/CD流水线解决日本2025年悬崖问题，通过动态创建隔离开发环境、使用GitHub、Jenkins、AWS和Docker等技术，降低维护成本并推动数字化转型。


<details>
  <summary>Details</summary>
Motivation: 解决日本2025年悬崖问题，即传统核心IT系统大规模服务终止导致的维护成本激增和系统更新困难问题，避免每年可能损失12万亿日元。

Method: 设计并实施可扩展CI/CD流水线，集成GitHub进行源代码控制、Jenkins实现流水线自动化、AWS提供可扩展环境、Docker实现环境容器化。

Result: 开发人员可以在隔离环境中自由安全地测试维护流程和新技术实验，显著降低维护成本。

Conclusion: 可扩展CI/CD流水线有效解决了传统系统维护难题，推动了数字化转型，为应对日本2025年悬崖问题提供了可行方案。

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [543] [The Strongly Stable Roommates Problem and Linear Programming](https://arxiv.org/abs/2510.16385)
*Naoyuki Kamiyama*

Main category: cs.GT

TL;DR: 本文提出了一个多项式时间算法，用于检查带平局的稳定室友问题中是否存在强稳定匹配。


<details>
  <summary>Details</summary>
Motivation: 稳定室友问题是稳定匹配问题的非二分图版本。本文关注带平局的稳定室友问题中的强稳定性概念，这是该问题的主要稳定性概念之一。

Method: 将Abeledo和Blum针对严格偏好稳定室友问题的线性规划方法扩展到带平局的稳定室友问题。

Result: 开发了一个新的多项式时间算法，能够检查带平局的稳定室友问题中是否存在强稳定匹配。

Conclusion: 成功扩展了线性规划方法到带平局的稳定室友问题，为解决该问题提供了有效的多项式时间算法。

Abstract: The stable roommates problem is a non-bipartite version of the stable
matching problem in a bipartite graph. In this paper, we consider the stable
roommates problem with ties. In particular, we focus on strong stability, which
is one of the main stability concepts in the stable roommates problem with
ties. We propose a new polynomial-time algorithm for the problem of checking
the existence of a strongly stable matching in the stable roommates problem
with ties. More concretely, we extend the linear programming approach of
Abeledo and Blum to the stable roommates problem with strict preferences to our
problem.

</details>


### [544] [No-Regret Online Autobidding Algorithms in First-price Auctions](https://arxiv.org/abs/2510.16869)
*Yuan Deng,Yilin Li,Wei Tang,Hanrui Zhang*

Main category: cs.GT

TL;DR: 本文开发了针对重复一价拍卖中ROI约束的在线竞价算法，在完全反馈和强盗反馈两种设置下分别实现了近乎最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 自动化竞价在在线广告中广泛应用，但非真实机制下带有ROI约束的算法设计是一个关键挑战。现有工作主要关注真实拍卖或较弱的基准，本文旨在显著改进这一领域。

Method: 开发了针对重复一价拍卖的在线竞价算法，基准为事后最优随机策略。在完全反馈设置下观察最大竞争出价，在强盗反馈设置下只观察是否赢得拍卖。

Result: 在完全反馈设置下实现了近乎最优的$\widetilde{O}(\sqrt{T})$遗憾界，在强盗反馈设置下实现了$\widetilde{O}(T^{3/4})$遗憾界。

Conclusion: 本文为非真实机制下带有ROI约束的在线竞价问题提供了有效的算法解决方案，在两种反馈设置下都取得了良好的性能保证。

Abstract: Automated bidding to optimize online advertising with various constraints,
e.g. ROI constraints and budget constraints, is widely adopted by advertisers.
A key challenge lies in designing algorithms for non-truthful mechanisms with
ROI constraints. While prior work has addressed truthful auctions or
non-truthful auctions with weaker benchmarks, this paper provides a significant
improvement: We develop online bidding algorithms for repeated first-price
auctions with ROI constraints, benchmarking against the optimal randomized
strategy in hindsight. In the full feedback setting, where the maximum
competing bid is observed, our algorithm achieves a near-optimal
$\widetilde{O}(\sqrt{T})$ regret bound, and in the bandit feedback setting
(where the bidder only observes whether the bidder wins each auction), our
algorithm attains $\widetilde{O}(T^{3/4})$ regret bound.

</details>


### [545] [Convergence of Regret Matching in Potential Games and Constrained Optimization](https://arxiv.org/abs/2510.17067)
*Ioannis Anagnostides,Emanuel Tewolde,Brian Hu Zhang,Ioannis Panageas,Vincent Conitzer,Tuomas Sandholm*

Main category: cs.GT

TL;DR: 本文证明了交替后悔匹配+（RM+）算法在约束优化问题中收敛到ε-KKT点，迭代复杂度为O(1/ε⁴)，在特定条件下可提升至O(1/ε²)。同时建立了RM和RM+之间的最坏情况分离，显示RM在势博弈中收敛到纳什均衡可能需要指数时间。


<details>
  <summary>Details</summary>
Motivation: 后悔匹配算法在解决零和博弈方面取得了突破性成果，但其在更广泛场景（如势博弈和约束优化）中的理论收敛性仍不清楚。特别是RM在势博弈中是否收敛到纳什均衡是一个开放了20年的问题。

Method: 使用交替后悔匹配+（RM+）算法，通过将KKT间隙与累积后悔联系起来进行分析。证明了RM+在特定区域内具有单步改进性质。

Result: 交替RM+收敛到ε-KKT点的迭代复杂度为O(1/ε⁴)，在后悔有界时可提升至O(1/ε²)。同时证明了RM（无论是否交替）在势博弈中收敛到纳什均衡可能需要指数时间。

Conclusion: RM+是一个有效的一阶优化器，而RM和RM+在最坏情况下存在指数级性能差异。在势博弈中，收敛到粗相关均衡比收敛到纳什均衡快得多。

Abstract: Regret matching (RM} -- and its modern variants -- is a foundational online
algorithm that has been at the heart of many AI breakthrough results in solving
benchmark zero-sum games, such as poker. Yet, surprisingly little is known so
far in theory about its convergence beyond two-player zero-sum games. For
example, whether regret matching converges to Nash equilibria in potential
games has been an open problem for two decades. Even beyond games, one could
try to use RM variants for general constrained optimization problems. Recent
empirical evidence suggests that they -- particularly regret matching$^+$
(RM$^+$) -- attain strong performance on benchmark constrained optimization
problems, outperforming traditional gradient descent-type algorithms.
  We show that alternating RM$^+$ converges to an $\epsilon$-KKT point after
$O_\epsilon(1/\epsilon^4)$ iterations, establishing for the first time that it
is a sound and fast first-order optimizer. Our argument relates the KKT gap to
the accumulated regret, two quantities that are entirely disparate in general
but interact in an intriguing way in our setting, so much so that when regrets
are bounded, our complexity bound improves all the way to
$O_\epsilon(1/\epsilon^2)$. From a technical standpoint, while RM$^+$ does not
have the usual one-step improvement property in general, we show that it does
in a certain region that the algorithm will quickly reach and remain in
thereafter. In sharp contrast, our second main result establishes a lower
bound: RM, with or without alternation, can take an exponential number of
iterations to reach a crude approximate solution even in two-player potential
games. This represents the first worst-case separation between RM and RM$^+$.
Our lower bound shows that convergence to coarse correlated equilibria in
potential games is exponentially faster than convergence to Nash equilibria.

</details>


### [546] [Eliciting Truthful Feedback for Preference-Based Learning via the VCG Mechanism](https://arxiv.org/abs/2510.17285)
*Leo Landolt,Anna Maddux,Andreas Schlaginhaufen,Saurabh Vaishampayan,Maryam Kamgarpour*

Main category: cs.GT

TL;DR: 提出一种结合偏好学习和VCG支付的资源分配机制，解决战略代理人的私有成本函数未知和策略性误报问题，在单次和在线设置中实现近似真实、个体理性和高效性。


<details>
  <summary>Details</summary>
Motivation: 资源分配中面临两个主要挑战：代理人的成本函数可能未知或难以明确指定，以及代理人可能策略性地误报成本。需要设计机制来激励真实报告并实现社会成本最小化。

Method: 使用D最优设计选择信息性偏好查询，通过最大似然估计成本参数，基于这些估计计算VCG分配和支付。结合偏好学习和VCG支付来激励真实报告。

Result: 在单次设置中，机制是近似真实、个体理性和高效的，误差为$\tilde{\mathcal O}(K^{-1/2})$；在在线设置中，这些保证渐近成立，后悔率为$\tilde{\mathcal O}(T^{2/3})$。数值案例验证了方法在电力市场需求响应中的有效性。

Conclusion: 所提出的结合偏好学习和VCG支付的机制能够有效解决资源分配中的信息不对称和策略行为问题，在理论和实践中都表现出良好性能。

Abstract: We study resource allocation problems in which a central planner allocates
resources among strategic agents with private cost functions in order to
minimize a social cost, defined as an aggregate of the agents' costs. This
setting poses two main challenges: (i) the agents' cost functions may be
unknown to them or difficult to specify explicitly, and (ii) agents may
misreport their costs strategically. To address these challenges, we propose an
algorithm that combines preference-based learning with Vickrey-Clarke-Groves
(VCG) payments to incentivize truthful reporting. Our algorithm selects
informative preference queries via D-optimal design, estimates cost parameters
through maximum likelihood, and computes VCG allocations and payments based on
these estimates. In a one-shot setting, we prove that the mechanism is
approximately truthful, individually rational, and efficient up to an error of
$\tilde{\mathcal O}(K^{-1/2})$ for $K$ preference queries per agent. In an
online setting, these guarantees hold asymptotically with sublinear regret at a
rate of $\tilde{\mathcal O}(T^{2/3})$ after $T$ rounds. Finally, we validate
our approach through a numerical case study on demand response in local
electricity markets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [547] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: VisuoAlign是一个通过提示引导树搜索实现多模态安全对齐的框架，旨在解决大型视觉语言模型在跨模态攻击下的安全挑战。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对多模态越狱攻击存在脆弱性，因为视觉输入引入了新的攻击面，推理链缺乏安全监督，且模态融合会削弱对齐效果。

Method: 通过视觉-文本交互提示将安全约束嵌入推理过程，使用蒙特卡洛树搜索构建多样化的安全关键提示轨迹，并引入基于提示的缩放实现实时风险检测和合规响应。

Result: 实验表明VisuoAlign能够主动暴露风险，实现全面的数据集生成，并显著提升LVLMs对抗复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign框架有效解决了多模态安全对齐的关键挑战，为大型视觉语言模型提供了更强的安全保障。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [548] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 本文提出了结构化认知循环（SCL）作为可执行的认知框架，通过将哲学洞见转化为计算结构，实现"可执行认识论"，强调智能是执行过程而非属性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏真正的认知理解，暴露了认知架构的缺失。SCL旨在解决"在什么条件下认知会涌现"这一认识论问题，而非传统的"什么是智能"的本体论问题。

Method: 基于过程哲学、具身认知和延展心智理论，SCL将智能定义为包含判断、记忆、控制、行动和调节的连续循环过程，通过功能分离的认知架构实现更一致的行为。

Result: SCL展示了功能分离的认知架构比单一提示系统产生更一致和可解释的行为，支持智能评估。该框架将知识重新定义为在现象学一致循环中的持续重构。

Conclusion: 真正的进步需要实现认知原则的结构化架构，而非更大的模型。SCL对心智哲学、认识论和AI产生重要影响，允许认知理论被实施和测试，将行为建立在认知结构而非统计规律基础上。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [549] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出将城市虚拟世界(citiverses)作为监管学习实验空间的政策科学议程，通过专家咨询识别关键研究领域和实验主题，强调负责任的发展方法。


<details>
  <summary>Details</summary>
Motivation: 探索城市虚拟世界作为监管学习实验空间的潜力，为政策制定提供沉浸式虚拟环境来测试政策场景和技术。

Method: 基于与欧盟委员会政策制定者、国家政府科学顾问以及数字监管和虚拟世界领域顶尖研究者的高层专家小组的咨询。

Result: 识别了可扩展性、实时反馈、复杂性建模、跨境合作等关键研究领域，以及交通、城市规划、环境/气候危机等实验主题。

Conclusion: 城市虚拟世界有潜力成为监管学习的重要实验空间，但需要采取负责任的方法，考虑伦理、经济、生态和社会维度，并整合到更广泛的实验生态系统。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [550] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: PISA是一个受皮亚杰认知发展理论启发的统一记忆系统，通过三模态适应机制和混合记忆访问架构，显著提升了AI代理的适应性和长期知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统缺乏对多样化任务的适应性，且忽视了记忆的建构性和任务导向作用。受皮亚杰认知发展理论启发，作者旨在开发一个更实用的记忆系统。

Method: 提出PISA记忆系统，包含三模态适应机制（图式更新、图式演化和图式创建）来保持记忆的连贯组织，并设计结合符号推理和神经检索的混合记忆访问架构。

Result: 在LOCOMO基准和新提出的AggQA数据分析任务基准上的实证评估表明，PISA在适应性和长期知识保留方面达到了新的最先进水平。

Conclusion: PISA通过将记忆视为建构性和适应性过程，成功解决了现有记忆系统的局限性，为AI代理提供了更有效的记忆能力。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [551] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 研究发现，即使为大型语言模型提供环境接口来跟踪状态空间，其在解决复杂难题时的性能崩溃现象仍然存在，这表明性能下降与推理能力本身有关，而不仅仅是状态跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 针对大型推理模型在解决超越特定复杂度阈值难题时出现性能崩溃的现象，研究者希望探究这是否与模型需要自行跟踪状态空间有关，还是真正的推理能力问题。

Method: 为大型语言模型提供汉诺塔问题的环境接口，使其可以通过工具调用进行移动、提供书面理由、观察结果状态空间并自我重新提示进行下一步操作。

Result: 环境接口的提供并不能延迟或消除性能崩溃。模型参数化策略分析显示，随着复杂度增加，模型策略与最优策略和均匀随机策略的偏离度都在增加，表明模型在每个复杂度级别都表现出模式崩溃。

Conclusion: 大型推理模型的性能崩溃现象与推理能力本身相关，而不仅仅是状态跟踪问题，类似现象可能在大型推理模型中也存在。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [552] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出了认知负荷痕迹（CLTs）作为深度学习模型的中层可解释性框架，通过符号化、时变函数量化模型内部资源分配，包含内在、外在和相关负荷三个分量，能够预测错误发生、揭示认知策略，并通过负荷引导干预提高推理效率15-30%


<details>
  <summary>Details</summary>
Motivation: 受人类认知负荷理论启发，为深度学习模型开发中层可解释性框架，以理解模型内部推理过程中的资源分配动态

Method: 将CLTs定义为三组分随机过程（内在负荷、外在负荷、相关负荷），通过注意力熵、KV缓存未命中率、表示分散度和解码稳定性等可测量代理实例化，提出符号化公式和可视化方法（负荷曲线、单纯形图）

Result: 在推理和规划基准测试中，CLTs能够预测错误发生、揭示认知策略，负荷引导干预在保持准确性的同时将推理效率提高15-30%

Conclusion: CLTs提供了一个有效的可解释性框架，能够深入理解深度学习模型的推理动态，并为优化模型性能提供实用指导

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [553] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow是一个新颖的自动形式化证明流水线，通过构建逻辑依赖图和使用基于引理的方法来保持证明的结构保真度，在自动形式化任务中实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动形式化方法虽然能生成可执行代码，但经常无法保持原始证明的语义含义和逻辑结构，这限制了大型语言模型在严格数学工作流程中的集成。

Method: ProofFlow首先构建有向无环图来映射证明步骤间的逻辑依赖关系，然后采用基于引理的方法将每个步骤系统地形式化为中间引理，从而保持原始论证的逻辑结构。

Result: 实验结果显示ProofFlow在自动形式化方面达到了新的最先进水平，ProofScore得分为0.545，显著超过全证明形式化（0.123）和步骤证明形式化（0.072）等基线方法。

Conclusion: ProofFlow通过关注结构保真度显著提升了自动形式化性能，同时提供了新的基准和评估指标来促进该领域的进一步发展。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [554] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 提出了将MO|RE运动研究数据仓库转换为基于基本形式本体的知识图谱的愿景，旨在标准化和机器可理解地建模与共享运动表现数据。


<details>
  <summary>Details</summary>
Motivation: 运动表现测试是评估和比较不同人群身体与认知能力的关键，但现有数据缺乏标准化和互操作性。

Method: 基于基本形式本体构建知识图谱，形式化表示计划规范、具体过程和相关测量之间的相互关系。

Result: 开发了一个基础设施，用于发布和归档运动科学研究数据，特别关注运动表现研究领域。

Conclusion: 该方法将改变运动表现数据的建模和共享方式，使其标准化且机器可理解，促进跨研究的数据比较和分析。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [555] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种基于随机置换集(RPS)的冲突度量方法，从随机有限集(RFS)和Dempster-Shafer理论(DST)两个角度分析置换中的冲突，并引入基于秩偏重叠(RBO)的不一致性度量。


<details>
  <summary>Details</summary>
Motivation: 随机置换集作为处理包含顺序信息的不确定性的新形式，需要有效度量两个置换质量函数之间的冲突，以支持顺序结构不确定信息融合。

Method: 从置换观察出发，基于秩偏重叠(RBO)定义置换间的不一致性度量，进一步提出基于非重叠的RPS冲突度量方法，将RPS理论视为DST的扩展。

Result: 通过数值示例展示了所提冲突度量的行为和特性，该方法具有自然的顶加权特性，能有效从DST角度度量RPS间的冲突。

Conclusion: 所提方法不仅具备顶加权特性，能有效度量RPS冲突，还为决策者提供了权重、参数和截断深度的灵活选择。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [556] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: PAINT是一种并行时间神经孪生方法，通过生成式神经网络建模状态分布，能够在测试时根据测量值预测系统状态，保持轨迹跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经代理模型在模拟动态系统时缺乏实时状态更新的能力，无法创建能够根据测量数据更新状态的数字孪生体。

Method: 使用并行时间架构训练生成式神经网络来建模状态分布，在测试时采用滑动窗口方式从测量值预测状态。

Result: 在二维湍流流体动力学问题上，PAINT能够保持轨迹跟踪，从稀疏测量值中高保真地预测系统状态。

Conclusion: PAINT方法为开发能够保持轨迹跟踪的神经孪生体提供了潜力，支持更准确的状态估计和决策制定。

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [557] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出ISGFAN框架，通过信息分离和全局-局部对抗学习解决噪声环境下跨域故障诊断问题


<details>
  <summary>Details</summary>
Motivation: 现有迁移故障诊断方法假设数据干净或领域相似度高，但在工业环境中噪声干扰和领域偏移同时存在，限制了这些方法的有效性

Method: 基于信息分离架构，结合对抗学习和改进的正交损失来解耦领域不变故障表示；采用全局-局部域对抗方案约束模型的条件分布和边缘分布

Result: 在三个公共基准数据集上的实验表明，该方法优于其他现有方法

Conclusion: ISGFAN框架在噪声环境下的跨域故障诊断中表现出优越性

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [558] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 该论文结合离线约束规划优化与在线时间网络执行，创建在不确定性下仍可行的调度方案，消除100%的截止期限违规，同时仅增加3-5%的制造周期开销。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要在应对随机任务持续时间的同时满足严格的交付截止期限，传统确定性调度在现实偏离名义计划时会失效，导致昂贵的紧急修复。

Method: 首先构建柔性作业车间约束规划模型并插入最优缓冲Δ*，然后将计划转换为带不确定性的简单时间网络并验证动态可控性，确保实时调度器能为每个有界持续时间重新安排活动而不违反约束。

Result: 在Kacem 1-4基准测试套件上的蒙特卡洛模拟显示，该方法消除了最先进元启发式调度中观察到的所有截止期限违规，同时仅增加3-5%的制造周期开销。

Conclusion: 时间网络推理能够弥合主动缓冲与动态鲁棒性之间的差距，使工业更接近真正数字化、自校正的工厂。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [559] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出了DTKG双轨知识图谱验证与推理框架，用于解决多跳问答任务中并行事实验证和链式推理的局限性问题


<details>
  <summary>Details</summary>
Motivation: 现有多跳推理方法要么基于LLM响应的事实验证（擅长并行验证但链式推理表现差），要么基于KG路径的链构建（擅长链式推理但处理并行验证时存在冗余路径检索问题），这些限制影响了多跳问答任务的效率和准确性

Method: 受认知科学中双过程理论启发，提出DTKG框架，包含分类阶段和分支处理阶段两个主要阶段

Result: 未在摘要中明确说明具体实验结果

Conclusion: DTKG框架通过双轨设计解决了多跳推理中并行事实验证和链式推理的局限性问题

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [560] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了LLM生成的临床思维链的可靠性，发现选择性少样本提示策略显著优于其他方法，关键在于示例的质量而非数量，AI评估器无法识别这些关键差异。


<details>
  <summary>Details</summary>
Motivation: 高质量临床思维链对可解释医疗AI至关重要，但面临数据稀缺问题。LLM能合成医疗数据，但其临床可靠性尚未验证。

Method: 在辅助生殖技术领域进行盲法比较研究，资深临床医生评估三种提示策略生成的思维链：零样本、随机少样本和选择性少样本，并与GPT-4o的评估结果对比。

Result: 选择性少样本策略在所有人类评估指标上显著优于其他策略(p < .001)，随机少样本相比零样本无显著改进。AI评估器未能识别关键性能差异。

Conclusion: 合成思维链的临床可靠性取决于策略性提示设计而非示例数量，提出"双原则"框架作为生成可信数据的基础方法，确认人类专家在高风险临床AI评估中的不可或缺作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [561] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了一种基于扩展认知理论的形式化模型，将企业知识重新定义为可测量的动态能力，通过信息访问程序的效率和输出可靠性来量化企业知识状态，并将其映射到法律标准中。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在企业决策中的广泛应用，传统基于人类代理人的企业犯罪意图归责假设面临挑战，需要重新定义企业知识的概念以适应算法时代。

Method: 开发了一个形式化模型，通过整合计算成本和统计验证错误率来定义连续的组织知识度量S_S(φ)，并推导出知识谓词K_S和企业范围认知能力指数K_{S,t}。

Result: 建立了从定量指标到实际知识、推定知识、故意视而不见和鲁莽等法律标准的操作性映射，为企业心智的可测量和可问责提供了途径。

Conclusion: 该研究为在算法时代创建可测量和可司法审查的审计工件提供了路径，使企业心智变得可处理和可问责。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [562] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI是一个基于多智能体框架的PHI去标识化自动评估系统，使用LLM作为评估代理，通过多数投票机制选择最佳模型，无需依赖大量人工标注。


<details>
  <summary>Details</summary>
Motivation: PHI去标识化评估通常依赖昂贵的小规模专家标注，需要一种成本效益高且可靠的自动评估方法。

Method: 部署多个LLM评估代理独立判断PHI提取正确性，通过LLM多数投票机制整合结果，产生稳定可重复的模型排名。

Result: 在真实临床笔记语料上的实验表明，TEAM-PHI能产生一致准确的模型排名，与人工评估结果高度一致。

Conclusion: TEAM-PHI提供了一个实用、安全且成本效益高的PHI去标识化自动评估和最佳模型选择解决方案。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [563] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 该论文提出了"被记住权"概念，旨在解决大型语言模型在信息检索中可能导致某些群体被忽视的问题，确保AI生成内容的最大真实性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，人们越来越依赖它们进行信息检索。但LLMs将多个视角合成为一个权威性回答，可能放大偏见和遗漏风险，导致某些叙事、个体或群体被不成比例地压制或提升，威胁集体记忆的多样性。

Method: 提出"被记住权"概念框架，该框架包含三个核心要素：最小化AI驱动信息遗漏风险、保障公平待遇权利、确保生成内容最大真实性。

Result: 通过RTBR概念，为应对LLMs可能造成的信息集中化和记忆重塑问题提供了理论解决方案。

Conclusion: RTBR概念为解决大型语言模型可能导致的集体记忆扭曲问题提供了重要框架，强调在AI时代保护信息多样性和公平性的必要性。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


### [564] [ScholarEval: Research Idea Evaluation Grounded in Literature](https://arxiv.org/abs/2510.16234)
*Hanane Nour Moussa,Patrick Queiroz Da Silva,Daniel Adu-Ampratwum,Alyson East,Zitong Lu,Nikki Puccetti,Mingyi Xue,Huan Sun,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.AI

TL;DR: 提出了ScholarEval框架，通过检索增强评估研究想法的合理性和贡献度，并在多领域数据集上验证其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在研究构思中的普及，需要建立稳健的评估框架来确保生成想法的有效性和实用性。

Method: 引入ScholarEval检索增强评估框架，基于两个核心标准评估研究想法：合理性（基于现有文献的方法实证有效性）和贡献度（相对于先前研究在不同维度上的推进程度）。

Result: 在ScholarIdeas数据集上的评估显示，ScholarEval在专家标注评分标准覆盖方面显著优于所有基线方法，且在评估可操作性、深度和证据支持方面持续优于OpenAI的o4-mini-deep-research系统。大规模用户研究也表明其在文献参与、想法精炼和实用性方面表现更优。

Conclusion: ScholarEval为研究想法评估提供了有效的框架，显著优于现有方法，并开源了代码、数据集和工具供社区使用。

Abstract: As AI tools become increasingly common for research ideation, robust
evaluation is critical to ensure the validity and usefulness of generated
ideas. We introduce ScholarEval, a retrieval augmented evaluation framework
that assesses research ideas based on two fundamental criteria: soundness - the
empirical validity of proposed methods based on existing literature, and
contribution - the degree of advancement made by the idea across different
dimensions relative to prior research. To evaluate ScholarEval, we introduce
ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas
and reviews, comprised of 117 ideas across four disciplines: artificial
intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows
that ScholarEval achieves significantly higher coverage of points mentioned in
the human expert annotated rubrics in ScholarIdeas compared to all baselines.
Furthermore, ScholarEval is consistently preferred over our strongest baseline
o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,
in terms of evaluation actionability, depth, and evidence support. Our
large-scale user study also shows that ScholarEval significantly outperforms
deep research in literature engagement, idea refinement, and usefulness. We
openly release our code, dataset, and ScholarEval tool for the community to use
and build on.

</details>


### [565] [OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration](https://arxiv.org/abs/2510.17614)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: OG-Rank是一个低延迟的解码器重排序系统，通过池化首词评分和不确定性门控解释步骤，实现快速排名并在真正模糊时生成解释，保持可预测的延迟。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要实时工作并能解释选择的排名系统，需要低延迟的解码器重排序器。

Method: 采用单解码器方法，结合池化首词评分信号和不确定性门控解释步骤。模型一次性对所有候选进行评分，仅在列表真正模糊时生成简短结构化理由。使用专注于困难案例的课程训练。

Result: 在遭遇范围订单选择上表现强劲（快速路径：Recall@1~0.45，nDCG@20~0.625），门控激活时进一步改善（Recall@1~0.56，nDCG@20~0.699，门控率45%）。编码器基线在效果和灵活性上都落后。

Conclusion: OG-Rank提供了一个实用方案：默认快速排名，在需要时解释，这种模式适用于选择性生成能以可接受成本提高准确性的决策任务。单策略设计简化部署和预算规划，课程原则可广泛转移。

Abstract: Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

</details>


### [566] [Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense](https://arxiv.org/abs/2510.16259)
*Zhehao Zhang,Weijie Xu,Shixian Cui,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 论文发现大型推理模型存在"推理分心"漏洞，恶意嵌入的复杂任务会分散模型注意力，导致任务准确率下降高达60%。作者提出结合监督微调和强化学习的训练防御方法，可将鲁棒性提升50个百分点。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务上表现出色，但存在被恶意嵌入的无关复杂任务分散注意力的安全漏洞，这威胁到模型的可靠性。

Method: 通过跨模型和基准的综合研究分析推理分心漏洞，提出结合监督微调(SFT)和强化学习(RL)的训练防御方法，使用合成对抗数据进行训练。

Result: 研究表明最先进的LRM高度易受攻击，注入干扰物可使任务准确率下降达60%。提出的防御方法在挑战性干扰攻击上将鲁棒性提升超过50个百分点。

Conclusion: 推理分心是LRM可靠性的独特且紧迫威胁，提出的训练防御方法为构建更安全可信的推理系统提供了实用步骤。

Abstract: Recent advances in large reasoning models (LRMs) have enabled remarkable
performance on complex tasks such as mathematics and coding by generating long
Chain-of-Thought (CoT) traces. In this paper, we identify and systematically
analyze a critical vulnerability we term reasoning distraction, where LRMs are
diverted from their primary objective by irrelevant yet complex tasks
maliciously embedded in the prompt. Through a comprehensive study across
diverse models and benchmarks, we show that even state-of-the-art LRMs are
highly susceptible, with injected distractors reducing task accuracy by up to
60%. We further reveal that certain alignment techniques can amplify this
weakness and that models may exhibit covert compliance, following hidden
adversarial instructions in reasoning while concealing them in the final
output. To mitigate these risks, we propose a training-based defense that
combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on
synthetic adversarial data, improving robustness by over 50 points on
challenging distractor attacks. Our findings establish reasoning distraction as
a distinct and urgent threat to LRM reliability and provide a practical step
toward safer and more trustworthy reasoning systems.

</details>


### [567] [What Limits Agentic Systems Efficiency?](https://arxiv.org/abs/2510.16276)
*Song Bian,Minghao Yan,Anand Jayarajan,Gennady Pekhimenko,Shivaram Venkataraman*

Main category: cs.AI

TL;DR: 该论文分析了网络交互式智能体系统的效率瓶颈，提出SpecCache缓存框架，通过推测执行显著降低网络环境延迟，提高系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注智能体系统的推理性能，但忽视了系统效率问题。网络交互式智能体系统存在显著的延迟瓶颈，影响实际应用效果。

Method: 将端到端延迟分解为LLM API延迟和网络环境延迟，通过实证研究分析15个模型和5个提供商，提出SpecCache缓存框架结合推测执行来优化延迟。

Result: 网络环境延迟可占总体延迟的53.7%，SpecCache相比随机缓存策略将缓存命中率提高58倍，网络环境开销降低3.2倍，且不降低系统性能。

Conclusion: 智能体系统的效率优化至关重要，SpecCache框架能有效解决网络环境延迟问题，为构建高效智能体系统提供了实用解决方案。

Abstract: Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have
demonstrated strong reasoning capabilities. To further enhance LLM
capabilities, recent agentic systems, such as Deep Research, incorporate web
interactions into LLM reasoning to mitigate uncertainties and reduce potential
errors. However, existing research predominantly focuses on reasoning
performance, often neglecting the efficiency of agentic systems. In this work,
we present a comprehensive empirical study that identifies efficiency
bottlenecks in web-interactive agentic systems. We decompose end-to-end latency
into two primary components: LLM API latency and web environment latency. We
conduct a comprehensive empirical study across 15 models and 5 providers to
demonstrate high variability in API-based agentic systems. We observe that web
environment latency can contribute as much as 53.7% to the overall latency in a
web-based agentic system. To improve latency, we propose SpecCache, a caching
framework augmented with speculative execution that can reduce web environment
overhead. Extensive evaluations on two standard benchmarks show that our
approach improves the cache hit rate by up to 58x compared to a random caching
strategy, while reducing web environment overhead by up to 3.2x, without
degrading agentic system performance.

</details>


### [568] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier](https://arxiv.org/abs/2510.16309)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个紧凑型知识图谱与符号验证器，通过强制执行数学可解释规则来提升LLM推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理过程中经常违反简单的数学或逻辑约束，需要一种方法来确保推理的数学一致性。

Method: 构建MedRule-KG知识图谱编码实体、关系和三个领域启发规则，配合符号验证器检查预测并应用最小修正保证一致性。

Result: 在90个FDA衍生基准测试中，使用MedRule-KG将精确匹配从0.767提升到0.900，加上验证器后达到1.000精确匹配并完全消除规则违反。

Conclusion: MedRule-KG为安全的数学推理提供了通用框架，展示了知识图谱和符号验证在提升LLM推理可靠性方面的有效性。

Abstract: Large language models (LLMs) often produce fluent reasoning steps while
violating simple mathematical or logical constraints. We introduce MedRule-KG,
a compact typed knowledge graph coupled with a symbolic verifier, designed to
enforce mathematically interpretable rules in reasoning tasks. MedRule-KG
encodes entities, relations, and three domain-inspired rules, while the
verifier checks predictions and applies minimal corrections to guarantee
consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG
improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields
1.000 EM while eliminating rule violations entirely. We demonstrate how
MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss
ablations, and release code and data to encourage reproducibility.

</details>


### [569] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了SELECT框架，通过动态锚点选择解决文本到图像扩散模型中概念擦除的锚点敏感性问题，克服固定锚点策略导致的概念重现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法依赖固定锚点策略，存在概念重现和侵蚀等关键问题，需要解决锚点选择的敏感性。

Method: 通过因果追踪揭示擦除对锚点选择的内在敏感性，定义Sibling Exclusive Concepts作为更优锚点类别，提出两阶段评估机制自动发现最优锚点。

Result: SELECT作为通用锚点解决方案，高效适配多种擦除框架，在关键性能指标上持续优于现有基线，单个概念锚点挖掘平均仅需4秒。

Conclusion: SELECT框架通过动态锚点选择有效解决了概念擦除中的锚点敏感性问题，实现了精确擦除同时保留相关概念。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [570] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 该论文研究用户如何通过战略性地与算法互动来引导算法更好地符合其真实兴趣，特别关注偏好不一致的用户。通过建立双系统决策模型和Stackelberg博弈框架，发现了对齐负担的概念和临界时间范围的存在。


<details>
  <summary>Details</summary>
Motivation: 在算法驱动的平台上，用户经常表现出不一致的偏好（如花费大量时间在低价值内容上），这可能导致算法学习到错误的用户兴趣。研究旨在理解这类用户需要什么条件才能有效引导算法与其真实兴趣对齐。

Method: 将用户决策过程建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长）的双系统模型。采用多领导者、单跟随者的扩展Stackelberg博弈框架，用户作为领导者通过承诺参与策略来引导算法。

Result: 发现存在一个临界时间范围：足够有远见的用户可以实现对齐，而短视的用户反而会被算法目标所对齐。这个临界范围可能很长，构成显著的对齐负担。但即使是一个小的、有成本的信号（如额外点击）也能显著降低这一负担。

Conclusion: 该框架解释了偏好不一致的用户如何在Stackelberg均衡中引导参与驱动的算法与其兴趣对齐，揭示了实现对齐的挑战和潜在补救措施。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [571] [Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs](https://arxiv.org/abs/2510.16374)
*Nick Oh*

Main category: cs.AI

TL;DR: 该论文提出了一个结合监控-生成-验证的三阶段迭代系统，在GSM8K数学推理任务上取得了75.42%的准确率，优于现有方法，同时减少了迭代次数。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理增强方法存在两种孤立范式：监控-生成方法擅长策略规划但缺乏验证机制，生成-验证方法能迭代优化但缺乏策略基础。这种分离导致效率低下。

Method: 基于Flavell认知监控模型，实现监控-生成-验证三阶段迭代系统，将策略规划与输出验证相结合。

Result: 在GSM8K上达到75.42%准确率，优于SELF-REFINE(68.44%)和Self-Verification(67.07%)，迭代次数更少(1.3 vs 2.0)，推理成本增加27-37%。

Conclusion: 前期监控能产生更高质量的初始解决方案，减少优化需求，但需要在算术推理之外的任务上进行评估以验证通用性。

Abstract: Current approaches to enhancing LLM reasoning follows two isolated paradigms:
Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and
SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack
mechanisms to verify whether selected strategies succeed; while Generate-Verify
approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan
et al., 2023) iteratively refine outputs but commence generation blindly
without task assessment. This separation creates inefficiencies -- strategies
fail without feedback, and refinement occurs without strategic grounding. We
address this gap by implementing Flavell's cognitive monitoring model (1979)
from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),
operationalising it as a three-phase iterative system. On GSM8K, preliminary
results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for
Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%
increased inference cost. These initial findings suggest upfront monitoring
produces higher-quality initial solutions that reduce refinement needs, though
evaluation beyond arithmetic reasoning is needed to establish generalisability.

</details>


### [572] [Humanoid-inspired Causal Representation Learning for Domain Generalization](https://arxiv.org/abs/2510.16382)
*Ze Tao,Jian Zhang,Haowei Li,Xianshuai Li,Yifei Peng,Xiyao Liu,Senzhang Wang,Chao Liu,Sheng Ren,Shichao Zhang*

Main category: cs.AI

TL;DR: 提出HSCM因果框架，模仿人类视觉系统的层次处理和多级学习，通过解耦和重加权图像属性（颜色、纹理、形状）来增强跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 克服传统域泛化模型的局限性，这些模型依赖统计方法捕捉数据-标签依赖关系，而HSCM旨在模拟人类智能的层次处理机制，建模细粒度因果关系。

Method: 基于人类智能的层次处理和多级学习，解耦和重加权关键图像属性（颜色、纹理、形状），构建结构因果模型。

Result: 理论和实证评估表明，HSCM在域泛化任务中优于现有模型，提供更稳健的性能和可解释性。

Conclusion: HSCM为捕捉因果关系和提升模型鲁棒性提供了更原则性的方法，在动态复杂环境中实现更有效的迁移学习。

Abstract: This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), a
novel causal framework inspired by human intelligence, designed to overcome the
limitations of conventional domain generalization models. Unlike approaches
that rely on statistics to capture data-label dependencies and learn
distortion-invariant representations, HSCM replicates the hierarchical
processing and multi-level learning of human vision systems, focusing on
modeling fine-grained causal mechanisms. By disentangling and reweighting key
image attributes such as color, texture, and shape, HSCM enhances
generalization across diverse domains, ensuring robust performance and
interpretability. Leveraging the flexibility and adaptability of human
intelligence, our approach enables more effective transfer and learning in
dynamic, complex environments. Through both theoretical and empirical
evaluations, we demonstrate that HSCM outperforms existing domain
generalization models, providing a more principled method for capturing causal
relationships and improving model robustness. The code is available at
https://github.com/lambett/HSCM.

</details>


### [573] [RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile](https://arxiv.org/abs/2510.16392)
*Ao Tian,Yunfeng Lu,Xinxin Fan,Changhao Wang,Lanzhi Zhou,Yeyao Zhang,Yanfang Liu*

Main category: cs.AI

TL;DR: 提出了RGMem框架，通过多尺度信息压缩和涌现过程实现语言代理的长期记忆和行为一致性，解决现有方法在用户建模深度和跨会话连续性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话系统受限于有限上下文窗口和静态参数记忆，难以建模跨会话长期用户状态和行为一致性。RAG和显式记忆系统主要关注事实级存储检索，缺乏从多轮对话中提取潜在偏好和深层特征的能力。

Method: 受物理学重整化群思想启发，提出RGMem自演化记忆框架，通过分层粗粒化和重标度操作，在多尺度上组织对话历史：从情景片段提取语义和用户洞察，逐步形成动态演化的用户画像。

Result: 实现了从噪声微观交互到高级准确用户画像的建模，解决了长期有效用户建模的挑战。

Conclusion: RGMem框架通过多尺度记忆演化过程，实现了语言代理的长期记忆和行为一致性，提升了个性化交互的深度和跨会话连续性。

Abstract: Personalized and continuous interactions are the key to enhancing user
experience in today's large language model (LLM)-based conversational systems,
however, the finite context windows and static parametric memory make it
difficult to model the cross-session long-term user states and behavioral
consistency. Currently, the existing solutions to this predicament, such as
retrieval-augmented generation (RAG) and explicit memory systems, primarily
focus on fact-level storage and retrieval, lacking the capability to distill
latent preferences and deep traits from the multi-turn dialogues, which limits
the long-term and effective user modeling, directly leading to the personalized
interactions remaining shallow, and hindering the cross-session continuity. To
realize the long-term memory and behavioral consistency for Language Agents in
LLM era, we propose a self-evolving memory framework RGMem, inspired by the
ideology of classic renormalization group (RG) in physics, this framework
enables to organize the dialogue history in multiple scales: it first extracts
semantics and user insights from episodic fragments, then through hierarchical
coarse-graining and rescaling operations, progressively forms a
dynamically-evolved user profile. The core innovation of our work lies in
modeling memory evolution as a multi-scale process of information compression
and emergence, which accomplishes the high-level and accurate user profiles
from noisy and microscopic-level interactions.

</details>


### [574] [ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights](https://arxiv.org/abs/2510.16466)
*Siddhartha Krothapalli,Tridib Kumar Das,Praveen Kumar,Naveen Suravarpu,Pratik Narang*

Main category: cs.AI

TL;DR: ReviewSense是一个基于大语言模型的决策支持框架，能够将客户评论转化为可操作的商业建议，超越了传统的偏好预测系统。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统擅长预测用户偏好，但缺乏将客户评论转化为面向业务的规范性建议的能力。客户反馈对战略增长日益重要，需要从非结构化评论中提取可操作的洞察。

Method: 整合聚类、大语言模型适应和专家驱动评估的统一业务导向流程，识别客户情感中的关键趋势、重复问题和具体关注点。

Result: 初步人工评估显示模型建议与业务目标高度一致，证明了其在推动数据驱动决策方面的潜力。

Conclusion: 该框架为AI驱动的情感分析提供了新视角，展示了其在优化商业策略和最大化客户反馈价值方面的应用价值。

Abstract: As customer feedback becomes increasingly central to strategic growth, the
ability to derive actionable insights from unstructured reviews is essential.
While traditional AI-driven systems excel at predicting user preferences, far
less work has focused on transforming customer reviews into prescriptive,
business-facing recommendations. This paper introduces ReviewSense, a novel
prescriptive decision support framework that leverages advanced large language
models (LLMs) to transform customer reviews into targeted, actionable business
recommendations. By identifying key trends, recurring issues, and specific
concerns within customer sentiments, ReviewSense extends beyond
preference-based systems to provide businesses with deeper insights for
sustaining growth and enhancing customer loyalty. The novelty of this work lies
in integrating clustering, LLM adaptation, and expert-driven evaluation into a
unified, business-facing pipeline. Preliminary manual evaluations indicate
strong alignment between the model's recommendations and business objectives,
highlighting its potential for driving data-informed decision-making. This
framework offers a new perspective on AI-driven sentiment analysis,
demonstrating its value in refining business strategies and maximizing the
impact of customer feedback.

</details>


### [575] [NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems](https://arxiv.org/abs/2510.16476)
*Xiaozhe Li,Xinyu Fang,Shengyuan Ding,Linyang Li,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: NP-ENGINE是首个用于训练和评估LLM解决NP难问题的综合框架，包含10个任务、可控实例生成器、规则验证器和启发式求解器。通过该框架训练的QWEN2.5-7B-NP模型在NP-BENCH基准上超越GPT-4o，并在领域外任务上展现强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数学、编程等推理任务上表现出色，但其解决复杂NP难优化问题的能力尚未充分探索。需要开发专门框架来训练和评估LLM在这类问题上的表现。

Method: 提出NP-ENGINE框架，包含可控实例生成器、规则验证器和启发式求解器，支持可扩展的RLVR训练。使用课程学习在Qwen2.5-7B-Instruct上进行零RLVR训练，得到QWEN2.5-7B-NP模型。

Result: QWEN2.5-7B-NP在NP-BENCH基准上显著超越GPT-4o，达到同规模模型SOTA性能。在领域外推理任务（逻辑、谜题、数学、知识）和非推理任务（指令遵循）上展现强大泛化能力。任务多样性提升有助于改善领域外泛化。

Conclusion: 任务丰富的RLVR训练是提升LLM推理能力的有前景方向，揭示了RLVR的扩展规律。NP-ENGINE框架为训练LLM解决复杂优化问题提供了有效途径。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities, with
models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as
mathematics, coding, logic, and puzzles through Reinforcement Learning with
Verifiable Rewards (RLVR). However, their ability to solve more complex
optimization problems - particularly NP-hard tasks - remains underexplored. To
bridge this gap, we propose NP-ENGINE, the first comprehensive framework for
training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks
across five domains, each equipped with (i) a controllable instance generator,
(ii) a rule-based verifier, and (iii) a heuristic solver that provides
approximate optimal solutions as ground truth. This
generator-verifier-heuristic pipeline enables scalable and verifiable RLVR
training under hierarchical difficulties. We also introduce NP-BENCH, a
benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'
ability to tackle NP-hard level reasoning problems, focusing not only on
feasibility but also on solution quality. Additionally, we present
QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on
Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and
achieves SOTA performance with the same model size. Beyond in-domain tasks, we
demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain
(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),
as well as non-reasoning tasks such as instruction following. We also observe a
scaling trend: increasing task diversity improves OOD generalization. These
findings suggest that task-rich RLVR training is a promising direction for
advancing LLM's reasoning ability, revealing new insights into the scaling laws
of RLVR.

</details>


### [576] [Hey Pentti, We Did It Again!: Differentiable vector-symbolic types that prove polynomial termination](https://arxiv.org/abs/2510.16533)
*Eilene Tomkins-Flanagan,Connor Hanley,Mary A. Kelly*

Main category: cs.AI

TL;DR: 提出了一种名为Doug的类型化计算机语言，所有类型化程序都能被证明在多项式时间内停机，并编码在向量符号架构中。该语言支持类型学习，旨在实现类似人类节奏的技能获取。


<details>
  <summary>Details</summary>
Motivation: 希望通过Doug语言描述一种技能行为学习形式，使其遵循人类技能获取的节奏（比暴力搜索快得多），超越现有方法的效率，更接近模拟人脑中的心理表征及其实际学习过程。

Method: Doug是对轻量线性函数式编程语言的编码，使用基于全息声明性记忆的槽值编码方案编码类型，使用Lisp VSA变体编码术语，使神经网络嵌入空间中的某些点可被解释为类型。

Result: 该方法使我们更接近模拟人脑中实际存在的心理表征及其学习过程，实现了类型可被神经网络学习的能力。

Conclusion: Doug语言为建模人类心理表征及其获取过程迈出了重要一步，有望实现超越现有方法效率的人类节奏技能获取。

Abstract: We present a typed computer language, Doug, in which all typed programs may
be proved to halt in polynomial time, encoded in a vector-symbolic architecture
(VSA). Doug is just an encoding of the light linear functional programming
language (LLFPL) described by (Schimanski2009, ch. 7). The types of Doug are
encoded using a slot-value encoding scheme based on holographic declarative
memory (HDM; Kelly, 2020). The terms of Doug are encoded using a variant of the
Lisp VSA defined by (Flanagan, 2024). Doug allows for some points on the
embedding space of a neural network to be interpreted as types, where the types
of nearby points are similar both in structure and content. Types in Doug are
therefore learnable by a neural network. Following (Chollet, 2019), (Card,
1983), and (Newell, 1981), we view skill as the application of a procedure, or
program of action, that causes a goal to be satisfied. Skill acquisition may
therefore be expressed as program synthesis. Using Doug, we hope to describe a
form of learning of skilled behaviour that follows a human-like pace of skill
acquisition (i.e., substantially faster than brute force; Heathcote, 2000),
exceeding the efficiency of all currently existing approaches (Kaplan, 2020;
Jones, 2021; Chollet, 2024). Our approach brings us one step closer to modeling
human mental representations, as they must actually exist in the brain, and
those representations' acquisition, as they are actually learned.

</details>


### [577] [Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/abs/2510.16555)
*Qiongyan Wang,Xingchen Zou,Yutian Jiang,Haomin Wen,Jiaheng Wei,Qingsong Wen,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出了Urban-R1框架，使用强化学习后训练来缓解多模态大语言模型在城市智能任务中的地理偏见问题，通过群组相对策略优化和城市区域画像任务提升跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 快速城市化加剧了对城市通用智能的需求，但现有基于监督微调的城市基础模型存在持续的地理偏见，导致区域预测偏差和有限的泛化能力。

Method: 提出Urban-R1强化学习后训练框架，采用群组相对策略优化(GRPO)来优化跨地理群体的推理能力，并使用城市区域画像作为代理任务，从多模态城市数据中提供可测量的奖励。

Result: 跨多个区域和任务的广泛实验表明，Urban-R1有效缓解了地理偏见并改善了跨区域泛化能力，优于基于监督微调训练和闭源模型。

Conclusion: 强化学习对齐是实现公平可信城市智能的有前景途径。

Abstract: Rapid urbanization intensifies the demand for Urban General Intelligence
(UGI), referring to AI systems that can understand and reason about complex
urban environments. Recent studies have built urban foundation models using
supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit
persistent geospatial bias, producing regionally skewed predictions and limited
generalization. To this end, we propose Urban-R1, a reinforcement
learning-based post-training framework that aligns MLLMs with the objectives of
UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize
reasoning across geographic groups and employs urban region profiling as a
proxy task to provide measurable rewards from multimodal urban data. Extensive
experiments across diverse regions and tasks show that Urban-R1 effectively
mitigates geo-bias and improves cross-region generalization, outperforming both
SFT-trained and closed-source models. Our results highlight reinforcement
learning alignment as a promising pathway toward equitable and trustworthy
urban intelligence.

</details>


### [578] [BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction](https://arxiv.org/abs/2510.16559)
*Tian Xia,Tianrun Gao,Wenhao Deng,Long Wei,Xiaowei Qian,Yixian Jiang,Chenglei Yu,Tailin Wu*

Main category: cs.AI

TL;DR: BuildArena是首个面向语言驱动工程建设的物理对齐交互式基准测试，用于评估LLM在工程建筑自动化中的能力


<details>
  <summary>Details</summary>
Motivation: 工程建筑自动化需要将自然语言规范转化为物理可行的结构，而现代LLM虽然具备广泛知识和强大推理能力，但其在建筑领域的竞争力尚未得到充分评估

Method: 开发了高度可定制的基准测试框架，包含可扩展的任务设计策略、3D空间几何计算库和基线LLM代理工作流程，在8个前沿LLM上进行全面评估

Result: BuildArena为社区提供了首个物理对齐的交互式基准测试，能够深入比较和分析LLM在语言驱动工程建设中的能力

Conclusion: 该研究填补了LLM在工程建筑自动化领域评估的空白，为语言驱动和物理基础的建筑自动化提供了系统性的评估框架

Abstract: Engineering construction automation aims to transform natural language
specifications into physically viable structures, requiring complex integrated
reasoning under strict physical constraints. While modern LLMs possess broad
knowledge and strong reasoning capabilities that make them promising candidates
for this domain, their construction competencies remain largely unevaluated. To
address this gap, we introduce BuildArena, the first physics-aligned
interactive benchmark designed for language-driven engineering construction. It
contributes to the community in four aspects: (1) a highly customizable
benchmarking framework for in-depth comparison and analysis of LLMs; (2) an
extendable task design strategy spanning static and dynamic mechanics across
multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for
supporting construction based on language instructions; (4) a baseline LLM
agentic workflow that effectively evaluates diverse model capabilities. On
eight frontier LLMs, BuildArena comprehensively evaluates their capabilities
for language-driven and physics-grounded construction automation. The project
page is at https://build-arena.github.io/.

</details>


### [579] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 提出了Ripple Effect Protocol (REP)，一种协调协议，让智能体不仅分享决策，还分享轻量级的敏感性信号，从而在群体中实现更快更稳定的协调。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理通信协议（如A2A和ACP）强调通信而非协调，随着代理群体规模增长，这会导致脆弱的集体行为，即使个体智能体很聪明，群体结果也很差。

Method: REP协议让代理分享决策和轻量级敏感性信号（表达关键环境变量变化时选择如何改变），这些敏感性在局部网络中传播，实现群体协调。协议规范分离了必需的消息模式和可选的聚合规则。

Result: 在三个领域的基准测试中：（i）供应链级联（啤酒游戏）、（ii）稀疏网络中的偏好聚合（电影调度）、（iii）可持续资源分配（Fishbanks），REP相比A2A将协调准确性和效率提高了41%到100%，并能灵活处理来自LLM的多模态敏感性信号。

Conclusion: 通过将协调作为协议级能力，REP为新兴的智能体互联网提供了可扩展的基础设施。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [580] [Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?](https://arxiv.org/abs/2510.16582)
*Junchi Yu,Yujie Liu,Jindong Gu,Philip Torr,Dongzhan Zhou*

Main category: cs.AI

TL;DR: GraphFlow是一个基于知识图谱的检索增强生成框架，通过流匹配目标优化检索策略，从文本丰富的知识图谱中高效检索准确多样的知识。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识图谱的检索增强生成方法难以从文本丰富的知识图谱中为复杂真实世界查询检索准确和多样化的信息，而过程奖励模型需要昂贵的过程级监督信号。

Method: 使用基于转移的流匹配目标联合优化检索策略和流估计器，将检索结果的奖励分解为中间检索状态，指导检索策略按奖励比例从知识图谱中检索候选结果。

Result: 在STaRK基准测试中，GraphFlow在命中率和召回率上平均优于包括GPT-4o在内的强基线10%，并对未见过的知识图谱表现出强泛化能力。

Conclusion: GraphFlow能有效从文本丰富的知识图谱中检索准确多样的知识，具有出色的性能和鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances
large language models (LLMs) by providing structured and interpretable external
knowledge. However, existing KG-based RAG methods struggle to retrieve accurate
and diverse information from text-rich KGs for complex real-world queries.
Process Reward Models (PRMs) offer a way to align the retrieval process of
KG-based RAG with query-specific knowledge requirements, but they heavily rely
on process-level supervision signals that are expensive and hard to obtain on
KGs. To address this challenge, we propose GraphFlow, a framework that
efficiently retrieves accurate and diverse knowledge required for real-world
queries from text-rich KGs. GraphFlow employs a transition-based flow matching
objective to jointly optimize a retrieval policy and a flow estimator. The flow
estimator factorizes the reward of the retrieval outcome into the intermediate
retrieval states. Such reward factorization guides the retrieval policy to
retrieve candidates from KGs in proportion to their reward. This allows
GraphFlow to explore high-quality regions of KGs that yield diverse and
relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes
real-world queries from multiple domains over text-rich KGs. GraphFlow
outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit
rate and recall. It also shows strong generalization to unseen KGs,
demonstrating its effectiveness and robustness.

</details>


### [581] [Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning](https://arxiv.org/abs/2510.16601)
*Tianxing Wu,Shutong Zhu,Jingting Wang,Ning Xu,Guilin Qi,Haofen Wang*

Main category: cs.AI

TL;DR: 提出了一种用于不确定知识图谱补全的半监督置信度分布学习方法，通过将置信度转换为分布并利用元学习生成伪标签来增强训练数据，解决置信度分布不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有不确定知识图谱补全方法忽略了置信度的极端不平衡分布，导致学习到的嵌入表示不足以支持高质量的知识图谱补全。

Method: 提出ssCDL方法，将置信度转换为置信度分布，通过关系学习在标记数据和带有伪标签的未标记数据上迭代学习嵌入，利用元学习预测未见三元组的置信度来增强训练数据并重新平衡置信度分布。

Result: 在两个不确定知识图谱数据集上的实验表明，ssCDL在不同评估指标上均优于现有最先进的基线方法。

Conclusion: ssCDL方法通过引入置信度分布学习和半监督学习机制，有效解决了不确定知识图谱补全中的置信度分布不平衡问题，提升了补全性能。

Abstract: Uncertain knowledge graphs (UKGs) associate each triple with a confidence
score to provide more precise knowledge representations. Recently, since
real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG)
completion attracts more attention, aiming to complete missing triples and
confidences. Current studies attempt to learn UKG embeddings to solve this
problem, but they neglect the extremely imbalanced distributions of triple
confidences. This causes that the learnt embeddings are insufficient to
high-quality UKG completion. Thus, in this paper, to address the above issue,
we propose a new semi-supervised Confidence Distribution Learning (ssCDL)
method for UKG completion, where each triple confidence is transformed into a
confidence distribution to introduce more supervision information of different
confidences to reinforce the embedding learning process. ssCDL iteratively
learns UKG embedding by relational learning on labeled data (i.e., existing
triples with confidences) and unlabeled data with pseudo labels (i.e., unseen
triples with the generated confidences), which are predicted by meta-learning
to augment the training data and rebalance the distribution of triple
confidences. Experiments on two UKG datasets demonstrate that ssCDL
consistently outperforms state-of-the-art baselines in different evaluation
metrics.

</details>


### [582] [Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards](https://arxiv.org/abs/2510.16614)
*Xuan Zhang,Ruixiao Li,Zhijian Zhou,Long Li,Yulei Qin,Ke Li,Xing Sun,Xiaoyu Tan,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: MERCI是一种增强LLM推理能力的强化学习算法，通过基于计数的内在奖励来激励探索，避免推理陷入重复和次优模式


<details>
  <summary>Details</summary>
Motivation: 现有RL范式依赖稀疏的结果奖励和有限探索，导致LLM推理趋于重复和次优模式，需要设计更好的探索机制

Method: 使用轻量级Coin Flipping Network估计推理轨迹的伪计数和认知不确定性，将其转换为内在奖励，并与GRPO等RL框架集成

Result: 在复杂推理基准测试中，MERCI鼓励更丰富多样的思维链，显著超越强基线性能，帮助策略逃离局部最优发现更好解决方案

Conclusion: 针对性的内在动机可以使语言模型推理的探索更加可靠有效

Abstract: Reinforcement Learning (RL) has become a compelling way to strengthen the
multi step reasoning ability of Large Language Models (LLMs). However,
prevalent RL paradigms still lean on sparse outcome-based rewards and limited
exploration, which often drives LLMs toward repetitive and suboptimal reasoning
patterns. In this paper, we study the central question of how to design
exploration for LLM reasoning and introduce MERCI (Motivating Exploration in
LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that
augments policy optimization with a principled intrinsic reward. Building on
the idea of count-based exploration, MERCI leverages a lightweight Coin
Flipping Network (CFN) to estimate the pseudo count and further epistemic
uncertainty over reasoning trajectories, and converts them into an intrinsic
reward that values novelty while preserving the learning signal from task
rewards. We integrate MERCI into some advanced RL frameworks like Group
Relative Policy Optimization (GRPO). Experiments on complex reasoning
benchmarks demonstrate that MERCI encourages richer and more varied chains of
thought, significantly improves performance over strong baselines, and helps
the policy escape local routines to discover better solutions. It indicates
that our targeted intrinsic motivation can make exploration reliable for
language model reasoning.

</details>


### [583] [Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review](https://arxiv.org/abs/2510.16658)
*Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu*

Main category: cs.AI

TL;DR: 大规模AI模型正在变革神经科学研究，通过端到端学习从原始脑信号和神经数据中提取信息，在神经影像、脑机接口、分子神经科学、临床辅助和疾病应用等五大领域产生深远影响。


<details>
  <summary>Details</summary>
Motivation: 传统计算方法在神经科学研究中存在局限，需要新的范式来处理多模态神经数据整合、时空模式解释和临床转化等挑战。大规模AI模型为解决这些挑战提供了新的可能性。

Method: 通过端到端学习从原始脑信号和神经数据中直接学习，整合多模态神经数据，并引入生物学启发的架构约束来开发更可解释和计算高效的模型。

Result: 这些模型成功解决了神经科学中的主要计算挑战，包括多模态数据集成、时空模式解释以及临床部署的转化框架开发。

Conclusion: 大规模AI模型在神经科学中展现出巨大潜力，但需要严格的评估框架、有效的领域知识整合和全面的临床使用伦理指南来确保其成功应用。

Abstract: The advent of large-scale artificial intelligence (AI) models has a
transformative effect on neuroscience research, which represents a paradigm
shift from the traditional computational methods through the facilitation of
end-to-end learning from raw brain signals and neural data. In this paper, we
explore the transformative effects of large-scale AI models on five major
neuroscience domains: neuroimaging and data processing, brain-computer
interfaces and neural decoding, molecular neuroscience and genomic modeling,
clinical assistance and translational frameworks, and disease-specific
applications across neurological and psychiatric disorders. These models are
demonstrated to address major computational neuroscience challenges, including
multimodal neural data integration, spatiotemporal pattern interpretation, and
the derivation of translational frameworks for clinical deployment. Moreover,
the interaction between neuroscience and AI has become increasingly reciprocal,
as biologically informed architectural constraints are now incorporated to
develop more interpretable and computationally efficient models. This review
highlights both the notable promise of such technologies and key implementation
considerations, with particular emphasis on rigorous evaluation frameworks,
effective domain knowledge integration, and comprehensive ethical guidelines
for clinical use. Finally, a systematic listing of critical neuroscience
datasets used to derive and validate large-scale AI models across diverse
research applications is provided.

</details>


### [584] [An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems](https://arxiv.org/abs/2510.16701)
*Ni Zhang,Zhiguang Cao,Jianan Zhou,Cong Zhang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: 提出了基于大语言模型的AFL框架，用于完全自动化解决复杂车辆路径问题，无需人工干预或外部求解器，在代码可靠性和解可行性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的方法在解决复杂车辆路径问题时仍需要外部干预，导致自主性受限、执行错误和解可行性低的问题。

Method: AFL框架将整个流程分解为三个可管理的子任务，使用四个专门化智能体进行协调交互，确保跨功能一致性和逻辑合理性，直接从原始输入提取知识并生成自包含代码。

Result: 在60个复杂车辆路径问题上进行广泛实验，包括标准基准和实际变体，验证了框架的有效性和通用性，在代码可靠性和解可行性方面接近100%的成功率，与精心设计的算法性能相当。

Conclusion: AFL框架实现了从问题实例到解决方案的完全自动化，显著提升了基于大语言模型方法在复杂车辆路径问题上的可靠性和可行性。

Abstract: Complex vehicle routing problems (VRPs) remain a fundamental challenge,
demanding substantial expert effort for intent interpretation and algorithm
design. While large language models (LLMs) offer a promising path toward
automation, current approaches still rely on external intervention, which
restrict autonomy and often lead to execution errors and low solution
feasibility. To address these challenges, we propose an Agentic Framework with
LLMs (AFL) for solving complex vehicle routing problems, achieving full
automation from problem instance to solution. AFL directly extracts knowledge
from raw inputs and enables self-contained code generation without handcrafted
modules or external solvers. To improve trustworthiness, AFL decomposes the
overall pipeline into three manageable subtasks and employs four specialized
agents whose coordinated interactions enforce cross-functional consistency and
logical soundness. Extensive experiments on 60 complex VRPs, ranging from
standard benchmarks to practical variants, validate the effectiveness and
generality of our framework, showing comparable performance against
meticulously designed algorithms. Notably, it substantially outperforms
existing LLM-based baselines in both code reliability and solution feasibility,
achieving rates close to 100% on the evaluated benchmarks.

</details>


### [585] [Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI](https://arxiv.org/abs/2510.16720)
*Jitao Sang,Jinlin Xiao,Jiarun Han,Jilin Chen,Xiaoyi Chen,Shuyu Wei,Yongjie Sun,Yuhang Wang*

Main category: cs.AI

TL;DR: 本文综述了智能AI从基于管线的系统向模型原生范式的范式转变，其中规划、工具使用和记忆等能力从外部编排转变为模型内部化，强化学习是实现这一转变的关键算法引擎。


<details>
  <summary>Details</summary>
Motivation: 追踪智能AI构建范式的演变，从外部逻辑编排的管线系统到能力内部化的模型原生范式，探讨这一转变如何重塑AI代理的能力和应用。

Method: 系统回顾了规划、工具使用和记忆三大能力的演变过程，分析了强化学习作为统一解决方案的作用，并考察了范式转变对深度研究代理和GUI代理等主要应用的影响。

Result: 揭示了智能AI从构建应用智能的系统向通过经验发展智能的模型的发展轨迹，展示了能力内部化的连贯演进路径。

Conclusion: 模型原生智能AI代表了从应用智能到通过经验发展智能的根本转变，形成了一个集成的学习和交互框架，标志着AI发展的新阶段。

Abstract: The rapid evolution of agentic AI marks a new phase in artificial
intelligence, where Large Language Models (LLMs) no longer merely respond but
act, reason, and adapt. This survey traces the paradigm shift in building
agentic AI: from Pipeline-based systems, where planning, tool use, and memory
are orchestrated by external logic, to the emerging Model-native paradigm,
where these capabilities are internalized within the model's parameters. We
first position Reinforcement Learning (RL) as the algorithmic engine enabling
this paradigm shift. By reframing learning from imitating static data to
outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task
across language, vision and embodied domains. Building on this, the survey
systematically reviews how each capability -- Planning, Tool use, and Memory --
has evolved from externally scripted modules to end-to-end learned behaviors.
Furthermore, it examines how this paradigm shift has reshaped major agent
applications, specifically the Deep Research agent emphasizing long-horizon
reasoning and the GUI agent emphasizing embodied interaction. We conclude by
discussing the continued internalization of agentic capabilities like
Multi-agent collaboration and Reflection, alongside the evolving roles of the
system and model layers in future agentic AI. Together, these developments
outline a coherent trajectory toward model-native agentic AI as an integrated
learning and interaction framework, marking the transition from constructing
systems that apply intelligence to developing models that grow intelligence
through experience.

</details>


### [586] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 该论文首次全面综述了基于强化学习的代理搜索领域，从功能角色、优化策略和应用范围三个维度组织该新兴领域，总结了代表性方法、评估协议和应用，并讨论了构建可靠可扩展系统的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 传统RAG管道通常是单轮和启发式的，缺乏对检索和推理的自适应控制。基于强化学习的代理搜索通过多步交互实现自适应和自我改进的搜索行为，解决LLMs的静态知识、事实幻觉和无法获取实时信息等问题。

Method: 从三个互补维度组织基于强化学习的代理搜索领域：(i) RL的功能角色，(ii) RL的优化策略，(iii) RL的应用范围。总结了代表性方法、评估协议和应用案例。

Result: 提供了该领域的首个全面综述，系统性地整理了基于强化学习的代理搜索方法，为未来研究提供了结构化框架和指导。

Conclusion: 基于强化学习的代理搜索为解决LLMs的局限性提供了强大机制，该综述旨在激发未来RL与代理搜索集成的研究，构建更可靠和可扩展的系统。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


### [587] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 该论文提出了一种基于代理模型的仿真驱动工程工作流，通过训练轻量级仿真器来解决高计算成本和黑盒不透明性问题，支持不确定性量化和可解释AI分析。


<details>
  <summary>Details</summary>
Motivation: 解决仿真驱动工程工作流面临的两个核心挑战：(1)高计算成本，(2)黑盒组件导致的透明度和可靠性问题。

Method: 使用紧凑实验设计训练轻量级代理模型，结合全局效应分析、不确定性分析和局部归因，评估不同代理模型间解释的一致性。

Result: 在混合电动飞机多学科设计和城市隔离代理模型两个案例中，该方法实现了秒级大规模探索，发现非线性相互作用和涌现行为，识别关键设计和政策杠杆。

Conclusion: 代理模型与可解释AI的结合能够有效支持复杂系统分析，诊断代理模型充分性，并指导进一步数据收集或模型改进。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [588] [ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2510.16753)
*Wei Huang,Peining Li,Meiyu Liang,Xu Hou,Junping Du,Yingxia Shao,Guanhua Ye,Wu Liu,Kangkang Lu,Yang Yu*

Main category: cs.AI

TL;DR: 提出ELMM方法解决多模态知识图谱补全中的语义噪声、模态冲突和计算成本高的问题，通过多视图视觉令牌压缩器和注意力剪枝策略，在保持性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有MKGs存在不完整性问题，而多模态大语言模型在MKGC任务中面临图像令牌过多导致的语义噪声、模态冲突以及高计算成本的挑战。

Method: 提出ELMM框架，包含基于多头注意力的多视图视觉令牌压缩器(MVTC)自适应压缩图像令牌，以及注意力剪枝策略减少冗余层，并使用线性投影补偿性能损失。

Result: 在FB15k-237-IMG和WN18-IMG基准测试中达到最先进性能，同时显著提高计算效率。

Conclusion: ELMM为多模态知识图谱补全建立了新范式，在保持高性能的同时大幅提升计算效率。

Abstract: Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by
incorporating visual and textual modalities, enabling richer and more
expressive entity representations. However, existing MKGs often suffer from
incompleteness, which hinder their effectiveness in downstream tasks.
Therefore, multimodal knowledge graph completion (MKGC) task is receiving
increasing attention. While large language models (LLMs) have shown promise for
knowledge graph completion (KGC), their application to the multimodal setting
remains underexplored. Moreover, applying Multimodal Large Language Models
(MLLMs) to the task of MKGC introduces significant challenges: (1) the large
number of image tokens per entity leads to semantic noise and modality
conflicts, and (2) the high computational cost of processing large token
inputs. To address these issues, we propose Efficient Lightweight Multimodal
Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token
Compressor (MVTC) based on multi-head attention mechanism, which adaptively
compresses image tokens from both textual and visual views, thereby effectively
reducing redundancy while retaining necessary information and avoiding modality
conflicts. Additionally, we design an attention pruning strategy to remove
redundant attention layers from MLLMs, thereby significantly reducing the
inference cost. We further introduce a linear projection to compensate for the
performance degradation caused by pruning. Extensive experiments on benchmark
FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art
performance while substantially improving computational efficiency,
establishing a new paradigm for multimodal knowledge graph completion.

</details>


### [589] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态模型，能够同时感知和生成视觉、文本、语音和动作，实现更自然的人类交互行为。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要模型能够同时感知和生成多种模态，实现更自然的人类模拟。

Method: 采用新颖的SA-MoE架构，将各模态路由到专门专家，通过统一注意力骨干网络进行融合，实现联合多模态感知和并发生成。

Result: 在语音交互和机器人操作基准测试中，ELLSA与模态特定基线表现相当，同时支持高级多模态和全双工行为。

Conclusion: ELLSA代表了向更自然和通用交互智能迈出的一步，有助于实现人工通用智能。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [590] [See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models](https://arxiv.org/abs/2510.16769)
*Shuo Han,Yukun Cao,Zezhong Ding,Zengyi Gao,S Kevin Zhou,Xike Xie*

Main category: cs.AI

TL;DR: GraphVista是一个统一框架，通过分层组织图信息和引入规划代理来解决视觉语言模型在图理解中的可扩展性和模态协调问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在图理解中存在输入token限制导致的可扩展性瓶颈，以及缺乏有效的文本和视觉模态协调机制。

Method: GraphVista采用分层方法构建轻量级GraphRAG基础，仅检索任务相关的文本描述和高分辨率视觉子图；引入规划代理根据任务复杂度路由到最适合的模态。

Result: GraphVista能够扩展到比现有基准大200倍的图，在质量上比最先进基线提升4.4倍，优于现有的文本、视觉和融合方法。

Conclusion: GraphVista通过充分利用两种模态的互补优势，成功解决了图理解中的可扩展性和模态协调挑战。

Abstract: Vision-language models (VLMs) have shown promise in graph understanding, but
remain limited by input-token constraints, facing scalability bottlenecks and
lacking effective mechanisms to coordinate textual and visual modalities. To
address these challenges, we propose GraphVista, a unified framework that
enhances both scalability and modality coordination in graph understanding. For
scalability, GraphVista organizes graph information hierarchically into a
lightweight GraphRAG base, which retrieves only task-relevant textual
descriptions and high-resolution visual subgraphs, compressing redundant
context while preserving key reasoning elements. For modality coordination,
GraphVista introduces a planning agent that routes tasks to the most suitable
modality-using the text modality for simple property reasoning and the visual
modality for local and structurally complex reasoning grounded in explicit
topology. Extensive experiments demonstrate that GraphVista scales to large
graphs, up to $200\times$ larger than those used in existing benchmarks, and
consistently outperforms existing textual, visual, and fusion-based methods,
achieving up to $4.4\times$ quality improvement over the state-of-the-art
baselines by fully exploiting the complementary strengths of both modalities.

</details>


### [591] [Domain-Contextualized Concept Graphs: A Computable Framework for Knowledge Representation](https://arxiv.org/abs/2510.16802)
*Chao Li,Yuru Wang*

Main category: cs.AI

TL;DR: 提出Domain-Contextualized Concept Graph (CDC)框架，将领域作为知识表示的一等元素，采用<概念, 关系@领域, 概念>的三元组结构，实现上下文感知推理和跨领域类比。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱受限于固定的本体论和刚性层次结构，主要问题在于将领域视为隐式上下文而非显式的推理级组件。

Method: 基于认知-语言同构映射原则，采用C-D-C三元组结构，定义20多种标准化关系谓词（结构、逻辑、跨领域、时间等），并在Prolog中实现完整推理能力。

Result: 在教育、企业知识系统和技术文档等案例研究中，CDC实现了上下文感知推理、跨领域类比和个性化知识建模，这些能力在传统本体框架中无法实现。

Conclusion: CDC框架通过将领域提升为概念表示的一等元素，克服了传统知识图谱的局限性，为知识建模提供了更灵活和强大的方法。

Abstract: Traditional knowledge graphs are constrained by fixed ontologies that
organize concepts within rigid hierarchical structures. The root cause lies in
treating domains as implicit context rather than as explicit, reasoning-level
components. To overcome these limitations, we propose the Domain-Contextualized
Concept Graph (CDC), a novel knowledge modeling framework that elevates domains
to first-class elements of conceptual representation. CDC adopts a C-D-C triple
structure - <Concept, Relation@Domain, Concept'> - where domain specifications
serve as dynamic classification dimensions defined on demand. Grounded in a
cognitive-linguistic isomorphic mapping principle, CDC operationalizes how
humans understand concepts through contextual frames. We formalize more than
twenty standardized relation predicates (structural, logical, cross-domain, and
temporal) and implement CDC in Prolog for full inference capability. Case
studies in education, enterprise knowledge systems, and technical documentation
demonstrate that CDC enables context-aware reasoning, cross-domain analogy, and
personalized knowledge modeling - capabilities unattainable under traditional
ontology-based frameworks.

</details>


### [592] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B是首个用于自主数据科学的代理式大语言模型，能够从数据源到分析师级深度研究报告自动完成端到端流程，在仅8B参数下超越了基于最先进专有LLM的工作流代理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于工作流的数据代理在特定数据任务上表现良好，但由于依赖预定义工作流，无法实现完全自主的数据科学。随着强大LLM的出现，从原始数据源到分析师级深度研究报告的自主数据科学变得可行。

Method: 提出基于课程学习的代理训练范式，模拟人类数据科学家的学习轨迹，使LLM能够在真实环境中逐步获取和整合多种能力。同时引入数据驱动的轨迹合成框架来构建高质量训练数据。

Result: 实验表明，仅8B参数的DeepAnalyze在广泛的数据任务上表现优异，包括数据问答、专业分析任务和开放式数据研究，超越了基于最先进专有LLM的工作流代理。

Conclusion: DeepAnalyze模型、代码和训练数据已开源，为自主数据科学的发展铺平了道路。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [593] [VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](https://arxiv.org/abs/2510.16907)
*Kangrui Wang,Pingyue Zhang,Zihan Wang,Yaning Gao,Linjie Li,Qineng Wang,Hanyang Chen,Chi Wan,Yiping Lu,Zhengyuan Yang,Lijuan Wang,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Yejin Choi,Manling Li*

Main category: cs.AI

TL;DR: 该论文提出通过强化学习训练VLM代理构建内部世界模型，将视觉状态推理分解为状态估计和转移建模，并设计了世界建模奖励和双层次GAE方法，使3B参数模型在五个代理基准测试中得分达到0.82，显著优于未训练模型和专有推理模型。


<details>
  <summary>Details</summary>
Motivation: 训练VLM代理面临从文本状态到复杂视觉观察的转变，这引入了部分可观测性并需要强大的世界建模能力。研究旨在探索VLM代理是否能够通过显式视觉状态推理构建内部世界模型。

Method: 将代理推理过程架构性地通过强化学习进行约束和奖励，将其建模为部分可观测马尔可夫决策过程。将推理分解为状态估计和转移建模，设计了世界建模奖励和双层次广义优势估计方法。

Result: 3B参数模型在五个多样化代理基准测试中得分达到0.82，相比未训练模型(0.21)提升3倍，并优于GPT-5(0.75)、Gemini 2.5 Pro(0.67)和Claude 4.5(0.62)等专有推理模型。

Conclusion: 视觉状态推理能有效提升VLM代理性能，最优表示形式取决于任务类型，自然语言适合一般任务，结构化格式对精确操控任务至关重要。所提出的方法在VAGEN框架中实现了可扩展的VLM代理训练。

Abstract: A key challenge in training Vision-Language Model (VLM) agents, compared to
Language Model (LLM) agents, lies in the shift from textual states to complex
visual observations. This transition introduces partial observability and
demands robust world modeling. We ask: Can VLM agents construct internal world
models through explicit visual state reasoning? To address this question, we
architecturally enforce and reward the agent's reasoning process via
reinforcement learning (RL), formulating it as a Partially Observable Markov
Decision Process (POMDP). We find that decomposing the agent's reasoning into
State Estimation ("what is the current state?") and Transition Modeling ("what
comes next?") is critical for success, as demonstrated through five reasoning
strategies. Our investigation into how agents represent internal beliefs
reveals that the optimal representation is task-dependent: Natural Language
excels at capturing semantic relationships in general tasks, while Structured
formats are indispensable for precise manipulation and control. Building on
these insights, we design a World Modeling Reward that provides dense,
turn-level supervision for accurate state prediction, and introduce Bi-Level
General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.
Through this form of visual state reasoning, a 3B-parameter model achieves a
score of 0.82 across five diverse agent benchmarks, representing a 3$\times$
improvement over its untrained counterpart (0.21) and outperforming proprietary
reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5
(0.62). All experiments are conducted within our VAGEN framework, a scalable
system for training and analyzing multi-turn VLM agents in diverse visual
environments. Code and data are publicly available at
https://vagen-ai.github.io.

</details>


### [594] [A Comparative User Evaluation of XRL Explanations using Goal Identification](https://arxiv.org/abs/2510.16956)
*Mark Towers,Yali Du,Christopher Freeman,Timothy J. Norman*

Main category: cs.AI

TL;DR: 提出了一个新的评估方法来测试用户是否能从强化学习算法的解释中识别出智能体的目标，发现在Ms. Pacman环境中，只有一种XRL算法达到了超过随机水平的准确率，且用户普遍过度自信。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释强化学习（XRL）算法在调试中具有核心应用价值，但缺乏对其相对性能的比较评估。

Method: 使用Atari的Ms. Pacman环境和四种XRL算法，通过让用户从决策解释中识别智能体目标来评估算法性能。

Result: 只有一种XRL算法在测试目标上达到了超过随机水平的准确率；用户普遍过度自信；用户自报告的识别难易度和理解程度与准确率无关。

Conclusion: 当前XRL算法的解释效果有限，用户的主观感受与实际识别能力不相关，需要改进评估方法和算法性能。

Abstract: Debugging is a core application of explainable reinforcement learning (XRL)
algorithms; however, limited comparative evaluations have been conducted to
understand their relative performance. We propose a novel evaluation
methodology to test whether users can identify an agent's goal from an
explanation of its decision-making. Utilising the Atari's Ms. Pacman
environment and four XRL algorithms, we find that only one achieved greater
than random accuracy for the tested goals and that users were generally
overconfident in their selections. Further, we find that users' self-reported
ease of identification and understanding for every explanation did not
correlate with their accuracy.

</details>


### [595] [STARK: Strategic Team of Agents for Refining Kernels](https://arxiv.org/abs/2510.16996)
*Juncheng Dong,Yang Yang,Tao Liu,Yang Wang,Feng Qi,Vahid Tarokh,Kaushik Rangadurai,Shuang Yang*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的多智能体协作框架，用于自动化GPU内核优化，通过系统化探索设计空间、结合硬件分析和性能反馈，显著提升了内核优化效果。


<details>
  <summary>Details</summary>
Motivation: GPU内核效率对现代AI发展至关重要，但优化过程复杂且劳动密集。现有LLM方法大多将其视为单次生成器或简单优化工具，难以应对不规则的核优化场景。

Method: 采用多智能体协作框架，包含系统化设计空间探索、基于知识的指令、动态上下文管理和策略搜索，模拟专家工程师的工作流程。

Result: 在KernelBench基准测试中，相比基线方法，该系统能产生正确的解决方案（基线经常失败），并实现最高16倍的运行时性能提升。

Conclusion: 智能体LLM框架具有推动完全自动化、可扩展GPU内核优化的潜力。

Abstract: The efficiency of GPU kernels is central to the progress of modern AI, yet
optimizing them remains a difficult and labor-intensive task due to complex
interactions between memory hierarchies, thread scheduling, and
hardware-specific characteristics. While recent advances in large language
models (LLMs) provide new opportunities for automated code generation, existing
approaches largely treat LLMs as single-shot generators or naive refinement
tools, limiting their effectiveness in navigating the irregular kernel
optimization landscape. We introduce an LLM agentic framework for GPU kernel
optimization that systematically explores the design space through multi-agent
collaboration, grounded instruction, dynamic context management, and strategic
search. This framework mimics the workflow of expert engineers, enabling LLMs
to reason about hardware trade-offs, incorporate profiling feedback, and refine
kernels iteratively. We evaluate our approach on KernelBench, a benchmark for
LLM-based kernel optimization, and demonstrate substantial improvements over
baseline agents: our system produces correct solutions where baselines often
fail, and achieves kernels with up to 16x faster runtime performance. These
results highlight the potential of agentic LLM frameworks to advance fully
automated, scalable GPU kernel optimization.

</details>


### [596] [ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems](https://arxiv.org/abs/2510.17052)
*Hassan Hamad,Yingru Xu,Liang Zhao,Wenbo Yan,Narendra Gyanchandani*

Main category: cs.AI

TL;DR: ToolCritic是一个诊断框架，用于评估和改进LLM在多轮工具增强对话中的行为，通过检测8种特定工具调用错误并提供针对性反馈，使主LLM能够修正响应。


<details>
  <summary>Details</summary>
Motivation: 工具增强的大型语言模型在现实应用中越来越普遍，但工具使用错误仍然影响其可靠性，需要改进LLM与外部工具集成的鲁棒性。

Method: 系统定义8种工具调用错误类别，构建合成数据集训练ToolCritic，该框架检测错误并提供反馈，主LLM基于反馈修正响应。

Result: 在Schema-Guided Dialogue数据集上的实验表明，ToolCritic相比基线方法（包括零样本提示和自校正技术）将工具调用准确率提高了13%。

Conclusion: ToolCritic是实现更鲁棒的LLM与外部工具集成的重要一步，在现实对话应用中具有前景。

Abstract: Tool-augmented large language models (LLMs) are increasingly employed in
real-world applications, but tool usage errors still hinder their reliability.
We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM
behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight
distinct error types specific to tool-calling (e.g., premature invocation,
argument misalignment, and misinterpretation of tool outputs) and provides
targeted feedback to the main LLM. The main LLM, assumed to have strong
reasoning, task understanding and orchestration capabilities, then revises its
response based on ToolCritic's feedback. We systematically define these error
categories and construct a synthetic dataset to train ToolCritic. Experimental
results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic
improves tool-calling accuracy by up to 13% over baselines, including zero-shot
prompting and self-correction techniques. This represents a promising step
toward more robust LLM integration with external tools in real-world dialogue
applications.

</details>


### [597] [A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation](https://arxiv.org/abs/2510.17064)
*Rongbin Li,Wenbo Chen,Zhao Li,Rodrigo Munoz-Castaneda,Jinbo Li,Neha S. Maurya,Arnav Solanki,Huan He,Hanwen Xing,Meaghan Ramlakhan,Zachary Wise,Zhuhao Wu,Hua Xu,Michael Hawrylycz,W. Jim Zheng*

Main category: cs.AI

TL;DR: BRAINCELL-AID是一个多智能体AI系统，通过整合自由文本描述和本体标签，结合检索增强生成技术，显著提高了单细胞RNA测序中基因集注释的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基因集富集分析方法依赖精心策划的注释，在处理特征不明确的基因时表现不佳。大型语言模型虽然提供了替代方案，但难以在结构化本体中表示复杂的生物学知识。

Method: 开发了BRAINCELL-AID多智能体AI系统，整合自由文本描述和本体标签，采用检索增强生成技术，通过PubMed文献精炼预测，减少幻觉并增强可解释性。

Result: 在小鼠基因集注释中，77%的基因集在前几位预测中获得了正确注释。成功注释了BRAIN Initiative Cell Census Network生成的5,322个脑细胞簇，识别了区域特异性基因共表达模式，并推断出基因集合的功能作用。

Conclusion: BRAINCELL-AID创建了一个有价值的资源，支持社区驱动的细胞类型注释，能够识别具有神经学意义的基底神经节相关细胞类型。

Abstract: Single-cell RNA sequencing has transformed our ability to identify diverse
cell types and their transcriptomic signatures. However, annotating these
signatures-especially those involving poorly characterized genes-remains a
major challenge. Traditional methods, such as Gene Set Enrichment Analysis
(GSEA), depend on well-curated annotations and often perform poorly in these
contexts. Large Language Models (LLMs) offer a promising alternative but
struggle to represent complex biological knowledge within structured
ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID:
https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that
integrates free-text descriptions with ontology labels to enable more accurate
and robust gene set annotation. By incorporating retrieval-augmented generation
(RAG), we developed a robust agentic workflow that refines predictions using
relevant PubMed literature, reducing hallucinations and enhancing
interpretability. Using this workflow, we achieved correct annotations for 77%
of mouse gene sets among their top predictions. Applying this approach, we
annotated 5,322 brain cell clusters from the comprehensive mouse brain cell
atlas generated by the BRAIN Initiative Cell Census Network, enabling novel
insights into brain cell function by identifying region-specific gene
co-expression patterns and inferring functional roles of gene ensembles.
BRAINCELL-AID also identifies Basal Ganglia-related cell types with
neurologically meaningful descriptions. Hence, we create a valuable resource to
support community-driven cell type annotation.

</details>


### [598] [Structured Debate Improves Corporate Credit Reasoning in Financial AI](https://arxiv.org/abs/2510.17108)
*Yoonjin Lee,Munhee Kim,Hanbi Choi,Juhyeon Park,Seungho Lyoo,Woojin Park*

Main category: cs.AI

TL;DR: 该研究开发了两种基于大语言模型的系统，用于企业信用评估中的非财务证据结构化推理。单代理系统通过单次推理生成双向分析，而多代理辩论系统基于卡尔·波普尔的批判性对话框架进行对抗性验证。两种系统都显著提高了生产力，其中多代理系统在推理质量上表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前金融AI主要关注数值预测，但在企业信用评估中，定性非财务指标对贷款偿还结果具有决定性影响，却难以形式化。现有方法对专业贷款评估所需的解释性判断支持有限。

Method: 开发了两种LLM系统：非对抗性单代理系统(NAS)使用单次推理管道生成双向分析；基于辩论的多代理系统(KPD-MADS)采用十步结构化交互协议，基于卡尔·波普尔的批判性对话框架进行对抗性验证。

Result: 两种系统都实现了显著的生产力提升(NAS: 11.55秒/案例；KPD-MADS: 91.97秒；人工基准: 1920秒)。KPD-MADS在推理质量上表现更优，在解释充分性(4.0 vs. 3.0)、实际适用性(4.0 vs. 3.0)和可用性(62.5 vs. 52.5)方面获得更高评分。

Conclusion: 结构化多代理交互可以增强金融AI中的推理严谨性和可解释性，推动企业信用评估中可扩展且可辩护的自动化进程。

Abstract: Despite advances in financial AI, the automation of evidence-based reasoning
remains unresolved in corporate credit assessment, where qualitative
non-financial indicators exert decisive influence on loan repayment outcomes
yet resist formalization. Existing approaches focus predominantly on numerical
prediction and provide limited support for the interpretive judgments required
in professional loan evaluation. This study develops and evaluates two
operational large language model (LLM)-based systems designed to generate
structured reasoning from non-financial evidence. The first is a
non-adversarial single-agent system (NAS) that produces bidirectional analysis
through a single-pass reasoning pipeline. The second is a debate-based
multi-agent system (KPD-MADS) that operationalizes adversarial verification
through a ten-step structured interaction protocol grounded in Karl Popper's
critical dialogue framework. Both systems were applied to three real corporate
cases and evaluated by experienced credit risk professionals. Compared to
manual expert reporting, both systems achieved substantial productivity gains
(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The
KPD-MADS demonstrated superior reasoning quality, receiving higher median
ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.
3.0), and usability (62.5 vs. 52.5). These findings show that structured
multi-agent interaction can enhance reasoning rigor and interpretability in
financial AI, advancing scalable and defensible automation in corporate credit
assessment.

</details>


### [599] [Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion](https://arxiv.org/abs/2510.17145)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.AI

TL;DR: 提出了一种基于手工特征的方法，通过融合颜色统计、多颜色空间直方图和纹理特征来评估鱼类新鲜度，在FFE数据集上显著优于之前的深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统感官评估鱼类新鲜度存在主观性、不一致性和难以标准化的问题，需要更客观准确的自动化评估方法。

Method: 从鱼眼图像中系统提取并增量融合互补特征描述符，包括颜色统计、多颜色空间直方图、LBP和GLCM纹理特征，捕获全局色彩变化和局部退化特征。

Result: 在标准训练测试设置下，LightGBM分类器达到77.56%准确率，比之前深度学习方法提升14.35%；使用增强数据时，ANN达到97.16%准确率，比之前最佳方法提升19.86%。

Conclusion: 精心设计的手工特征经过策略性处理后，能够为自动化鱼类新鲜度评估提供稳健、可解释且可靠的解决方案，对食品质量监测具有实际应用价值。

Abstract: Accurate assessment of fish freshness remains a major challenge in the food
industry, with direct consequences for product quality, market value, and
consumer health. Conventional sensory evaluation is inherently subjective,
inconsistent, and difficult to standardize across contexts, often limited by
subtle, species-dependent spoilage cues. To address these limitations, we
propose a handcrafted feature-based approach that systematically extracts and
incrementally fuses complementary descriptors, including color statistics,
histograms across multiple color spaces, and texture features such as Local
Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish
eye images. Our method captures global chromatic variations from full images
and localized degradations from ROI segments, fusing each independently to
evaluate their effectiveness in assessing freshness. Experiments on the
Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's
effectiveness: in a standard train-test setting, a LightGBM classifier achieved
77.56% accuracy, a 14.35% improvement over the previous deep learning baseline
of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached
97.16% accuracy, surpassing the prior best of 77.3% by 19.86%. These results
demonstrate that carefully engineered, handcrafted features, when strategically
processed, yield a robust, interpretable, and reliable solution for automated
fish freshness assessment, providing valuable insights for practical
applications in food quality monitoring.

</details>


### [600] [Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation](https://arxiv.org/abs/2510.17146)
*Subin Lin,Chuanbo Hua*

Main category: cs.AI

TL;DR: PILLM是一个基于物理知识的大语言模型框架，通过进化循环自动生成、评估和优化HVAC系统异常检测规则，结合热力学和控制理论约束，实现可解释且物理合理的异常检测。


<details>
  <summary>Details</summary>
Motivation: HVAC系统能耗巨大，传统规则方法缺乏适应性，深度学习方法缺乏可解释性和物理合理性，现有LLM方法忽视了HVAC运行的物理原理。

Method: 提出PILLM框架，在进化循环中使用物理知识引导的反思和交叉操作，嵌入热力学和控制理论约束，自动生成和优化异常检测规则。

Result: 在公共建筑故障检测数据集上，PILLM实现了最先进的性能，同时生成可解释和可操作的诊断规则。

Conclusion: PILLM推进了智能建筑系统中可信赖和可部署AI的发展，实现了自适应且物理基础的异常检测。

Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a
substantial share of global building energy use, making reliable anomaly
detection essential for improving efficiency and reducing emissions. Classical
rule-based approaches offer explainability but lack adaptability, while deep
learning methods provide predictive power at the cost of transparency,
efficiency, and physical plausibility. Recent attempts to use Large Language
Models (LLMs) for anomaly detection improve interpretability but largely ignore
the physical principles that govern HVAC operations. We present PILLM, a
Physics-Informed LLM framework that operates within an evolutionary loop to
automatically generate, evaluate, and refine anomaly detection rules. Our
approach introduces physics-informed reflection and crossover operators that
embed thermodynamic and control-theoretic constraints, enabling rules that are
both adaptive and physically grounded. Experiments on the public Building Fault
Detection dataset show that PILLM achieves state-of-the-art performance while
producing diagnostic rules that are interpretable and actionable, advancing
trustworthy and deployable AI for smart building systems.

</details>


### [601] [Which LLM Multi-Agent Protocol to Choose?](https://arxiv.org/abs/2510.17149)
*Hongyi Du,Jiaqi Su,Jisen Li,Lijie Ding,Yingxuan Yang,Peixuan Han,Xiangru Tang,Kunlun Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: ProtocolBench是一个系统评估多智能体系统通信协议的基准测试，ProtocolRouter是一个可学习的协议路由器，能根据场景需求动态选择最优协议。


<details>
  <summary>Details</summary>
Motivation: 大规模多智能体系统中，通信协议选择对性能和可靠性至关重要，但目前缺乏标准化评估方法，选择往往基于直觉而非数据。

Method: 开发ProtocolBench基准测试，从任务成功率、端到端延迟、消息开销和故障恢复能力四个维度系统比较协议；提出ProtocolRouter学习型路由器，根据需求和运行时信号动态选择协议。

Result: 不同协议在性能上差异显著：流式队列场景完成时间相差36.5%，端到端延迟相差3.48秒；ProtocolRouter相比最佳单协议基线，故障恢复时间减少18.1%，在GAIA场景获得更高成功率。

Conclusion: 协议选择对系统行为有重大影响，ProtocolBench和ProtocolRouter提供了标准化评估和智能协议选择方案，能显著提升大规模多智能体系统的可靠性。

Abstract: As large-scale multi-agent systems evolve, the communication protocol layer
has become a critical yet under-evaluated factor shaping performance and
reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora,
etc.), selection is often intuition-driven and lacks standardized guidance. We
introduce ProtocolBench, a benchmark that systematically compares agent
protocols along four measurable axes: task success, end-to-end latency, message
or byte overhead, and robustness under failures. On ProtocolBench, protocol
choice significantly influences system behavior. In the Streaming Queue
scenario, overall completion time varies by up to 36.5% across protocols, and
mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery,
resilience also differs consistently across protocols. Beyond evaluation, we
present ProtocolRouter, a learnable protocol router that selects per-scenario
(or per-module) protocols from requirement and runtime signals. ProtocolRouter
reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol
baseline, and achieves scenario-specific gains such as higher success in GAIA.
We also release ProtocolRouterBench to standardize protocol evaluation and
improve reliability at scale.

</details>


### [602] [Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients](https://arxiv.org/abs/2510.17172)
*Shun Huang,Wenlu Xing,Shijia Geng,Hailong Wang,Guangkun Nie,Gongzheng Tang,Chenyang He,Shenda Hong*

Main category: cs.AI

TL;DR: 本研究开发了一个结合ECG基础模型和可解释XGBoost分类器的混合预测框架，用于预测急性心肌梗死后的恶性室性心律失常风险，在提高准确性的同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 急性心肌梗死后恶性室性心律失常是院内死亡的主要原因，传统风险评分性能有限，而端到端深度学习模型缺乏临床信任所需的可解释性。

Method: 使用ECGFounder基础模型提取150维诊断概率特征，通过特征选择精炼后训练XGBoost分类器，并采用SHAP方法进行可解释性分析。

Result: 混合模型AUC达到0.801，优于KNN(0.677)、RNN(0.676)和1D-CNN(0.720)，SHAP分析显示模型识别特征与临床知识高度一致。

Conclusion: 该混合框架为VT/VF风险预测提供了新范式，验证了基础模型输出作为有效自动化特征工程在构建可信赖、可解释AI临床决策支持系统中的应用。

Abstract: Malignant ventricular arrhythmias (VT/VF) following acute myocardial
infarction (AMI) are a major cause of in-hospital death, yet early
identification remains a clinical challenge. While traditional risk scores have
limited performance, end-to-end deep learning models often lack the
interpretability needed for clinical trust. This study aimed to develop a
hybrid predictive framework that integrates a large-scale electrocardiogram
(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to
improve both accuracy and interpretability. We analyzed 6,634 ECG recordings
from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder
model was used to extract 150-dimensional diagnostic probability features ,
which were then refined through feature selection to train the XGBoost
classifier. Model performance was evaluated using AUC and F1-score , and the
SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid
model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC
0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that
model-identified key features, such as "premature ventricular complexes" (risk
predictor) and "normal sinus rhythm" (protective factor), were highly
consistent with clinical knowledge. We conclude that this hybrid framework
provides a novel paradigm for VT/VF risk prediction by validating the use of
foundation model outputs as effective, automated feature engineering for
building trustworthy, explainable AI-based clinical decision support systems.

</details>


### [603] [Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users](https://arxiv.org/abs/2510.17173)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.AI

TL;DR: 研究了一个部署在网页上的工具增强LLM健康教练系统，通过真实用户测试发现，统一的工具密集型策略虽然提升平均价值，但对特定用户群体（特别是低健康素养/高自我效能用户）产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 探索工具增强LLM在健康教练应用中的个性化策略，特别是如何通过评估优先的方法避免对特定用户群体的伤害。

Method: 使用离线策略评估（OPE）分析因子化决策头（工具/风格），并通过轻量级模拟器验证添加早期信息增益奖励的效果。

Result: 统一的工具密集型策略会伤害特定用户群体；添加早期信息增益奖励可缩短特征识别时间，提高目标成功率和pass@3指标。

Conclusion: 提出了评估优先的个性化路径：冻结生成器，在类型化奖励上学习子群体感知的决策头，并始终报告每个原型指标以揭示被平均值掩盖的子群体伤害。

Abstract: We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

</details>


### [604] [Temporally Detailed Hypergraph Neural ODEs for Type 2 Diabetes Progression Modeling](https://arxiv.org/abs/2510.17211)
*Tingsong Xiao,Yao An Lee,Zelin Xu,Yupu Zhang,Zibo Liu,Yu Huang,Jiang Bian,Serena Jingchuan Guo,Zhe Jiang*

Main category: cs.AI

TL;DR: 提出了TD-HNODE模型，通过时间详细超图和神经ODE框架学习疾病进展的连续时间动态，在2型糖尿病和心血管疾病进展建模中表现优异


<details>
  <summary>Details</summary>
Motivation: 准确建模疾病进展（如2型糖尿病）可以改善患者亚型分型并指导及时干预，但现有方法难以处理不规则时间采样数据和患者异质性，且无法捕捉复杂的连续时间动态

Method: 使用时间详细超图表示临床认可的进展轨迹，通过神经ODE框架学习连续时间进展动态，包含可学习的TD-Hypergraph Laplacian来捕捉疾病并发症标记在进展轨迹内和轨迹间的相互依赖关系

Result: 在两个真实世界临床数据集上的实验表明，TD-HNODE在建模2型糖尿病和相关心血管疾病进展方面优于多个基线方法

Conclusion: TD-HNODE能够有效解决疾病进展建模中的挑战，为患者亚型分型和及时干预提供了更好的工具

Abstract: Disease progression modeling aims to characterize and predict how a patient's
disease complications worsen over time based on longitudinal electronic health
records (EHRs). Accurate modeling of disease progression, such as type 2
diabetes, can enhance patient sub-phenotyping and inform effective and timely
interventions. However, the problem is challenging due to the need to learn
continuous-time dynamics of progression patterns based on irregular-time event
samples and patient heterogeneity (\eg different progression rates and
pathways). Existing mechanistic and data-driven methods either lack
adaptability to learn from real-world data or fail to capture complex
continuous-time dynamics on progression trajectories. To address these
limitations, we propose Temporally Detailed Hypergraph Neural Ordinary
Differential Equation (TD-HNODE), which represents disease progression on
clinically recognized trajectories as a temporally detailed hypergraph and
learns the continuous-time progression dynamics via a neural ODE framework.
TD-HNODE contains a learnable TD-Hypergraph Laplacian that captures the
interdependency of disease complication markers within both intra- and
inter-progression trajectories. Experiments on two real-world clinical datasets
demonstrate that TD-HNODE outperforms multiple baselines in modeling the
progression of type 2 diabetes and related cardiovascular diseases.

</details>


### [605] [Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis](https://arxiv.org/abs/2510.17235)
*Chong Chen,Ze Liu,Lingfeng Bao,Yanlin Wang,Ting Chen,Daoyuan Wu,Jiachi Chen*

Main category: cs.AI

TL;DR: 提出Coinvisor，一个基于强化学习的多代理框架聊天机器人，为加密货币投资提供全面的分析支持，通过强化学习工具选择机制实现多步骤规划和实时数据集成。


<details>
  <summary>Details</summary>
Motivation: 解决加密货币投资中现有方法的局限性：手动分析耗时且易偏颇，数据聚合平台功能有限，基于静态预训练模型的LLM代理缺乏实时数据集成和多步骤推理能力。

Method: 采用多代理框架，集成多样化分析工具，核心创新是基于强化学习的工具选择机制，支持多步骤规划和动态内容的自适应分析。

Result: 在工具调用准确性上，相比基础模型召回率提升40.7%，F1分数提升26.6%；用户研究显示高满意度（4.64/5），参与者更偏好Coinvisor而非通用LLM和现有加密平台（4.62/5）。

Conclusion: Coinvisor通过强化学习驱动的多代理框架，有效解决了加密货币投资分析中的关键挑战，提供了准确且可操作的投资洞察。

Abstract: The cryptocurrency market offers significant investment opportunities but
faces challenges including high volatility and fragmented information. Data
integration and analysis are essential for informed investment decisions.
Currently, investors use three main approaches: (1) Manual analysis across
various sources, which depends heavily on individual experience and is
time-consuming and prone to bias; (2) Data aggregation platforms-limited in
functionality and depth of analysis; (3) Large language model agents-based on
static pretrained models, lacking real-time data integration and multi-step
reasoning capabilities. To address these limitations, we present Coinvisor, a
reinforcement learning-based chatbot that provides comprehensive analytical
support for cryptocurrency investment through a multi-agent framework.
Coinvisor integrates diverse analytical capabilities through specialized tools.
Its key innovation is a reinforcement learning-based tool selection mechanism
that enables multi-step planning and flexible integration of diverse data
sources. This design supports real-time interaction and adaptive analysis of
dynamic content, delivering accurate and actionable investment insights. We
evaluated Coinvisor through automated benchmarks on tool calling accuracy and
user studies with 20 cryptocurrency investors using our interface. Results show
that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base
model in tool orchestration. User studies show high satisfaction (4.64/5), with
participants preferring Coinvisor to both general LLMs and existing crypto
platforms (4.62/5).

</details>


### [606] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 提出了RubiSCoT AI框架，使用自然语言处理技术增强从提案到最终提交的论文评估过程，提供一致、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统论文评估方法耗时且受评估者主观性影响，需要更高效、一致的评估解决方案。

Method: 使用大型语言模型、检索增强生成和结构化思维链提示等先进NLP技术，包括初步评估、多维评估、内容提取、基于评分标准的评分和详细报告。

Result: 开发了RubiSCoT框架的设计和实现，展示了其在学术评估过程中的应用潜力。

Conclusion: RubiSCoT有潜力通过一致、可扩展和透明的评估来优化学术评估过程。

Abstract: The evaluation of academic theses is a cornerstone of higher education,
ensuring rigor and integrity. Traditional methods, though effective, are
time-consuming and subject to evaluator variability. This paper presents
RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from
proposal to final submission. Using advanced natural language processing
techniques, including large language models, retrieval-augmented generation,
and structured chain-of-thought prompting, RubiSCoT offers a consistent,
scalable solution. The framework includes preliminary assessments,
multidimensional assessments, content extraction, rubric-based scoring, and
detailed reporting. We present the design and implementation of RubiSCoT,
discussing its potential to optimize academic assessment processes through
consistent, scalable, and transparent evaluation.

</details>


### [607] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出LaGAT混合框架，将基于图注意力的神经网络策略MAGAT集成到搜索算法LaCAM中，用于解决密集多智能体路径规划问题，在密集场景中优于纯搜索和纯学习方法。


<details>
  <summary>Details</summary>
Motivation: 密集多智能体路径规划问题在实时求解中仍然具有挑战性，现有方法表现不佳，需要结合学习与搜索的优势。

Method: 开发LaGAT混合框架，集成改进的MAGAT神经网络架构与LaCAM搜索算法，采用预训练-微调策略和死锁检测机制。

Result: LaGAT在密集场景中超越了纯搜索和纯学习方法，证明了精心设计的混合搜索对复杂多智能体协调问题的有效性。

Conclusion: 经过精心设计的混合搜索为紧密耦合的挑战性多智能体协调问题提供了强大的解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [608] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 提出FBI_LTL，一种用于仿真规划问题的多样化规划器，使用线性时序逻辑定义语义多样性标准，生成语义多样化的规划方案。


<details>
  <summary>Details</summary>
Motivation: 传统基于仿真规划的单一规划器无法满足代理偏好，现有多样化规划方法可能产生语法不同但语义相同的解决方案。

Method: 利用线性时序逻辑定义语义多样性标准，并将这些基于LTL的多样性模型直接集成到搜索过程中。

Result: 在多个基准测试中，FBI_LTL相比基线方法能生成更多样化的规划方案。

Conclusion: 这项工作确立了在仿真环境中进行语义引导多样化规划的可行性，为在传统基于模型方法失效的现实非符号领域开辟了新途径。

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [609] [Active Inference for an Intelligent Agent in Autonomous Reconnaissance Missions](https://arxiv.org/abs/2510.17450)
*Johan Schubert,Farzad Kamrani,Tove Gustavi*

Main category: cs.AI

TL;DR: 提出了一种基于主动推理的路径规划方法，用于智能代理的自主控制，通过构建证据地图和计算变分自由能量来指导代理移动，平衡探索和利用。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够自主控制智能代理的方法，用于侦察地理区域以维持共同的作战态势图，解决探索与利用之间的平衡问题。

Method: 使用Dempster-Shafer理论和高斯传感器模型构建生成模型，采用贝叶斯方法更新后验概率分布，计算变分自由能量来指导代理向最小化自由能量的位置移动。

Result: 该方法能够有效指导代理在地理区域内的移动，平衡了广泛搜索区域和跟踪已识别目标对象的需求。

Conclusion: 基于主动推理的路径规划方法为智能代理的自主控制提供了一种有效解决方案，能够处理探索与利用的平衡问题，并在模拟中表现出良好的性能。

Abstract: We develop an active inference route-planning method for the autonomous
control of intelligent agents. The aim is to reconnoiter a geographical area to
maintain a common operational picture. To achieve this, we construct an
evidence map that reflects our current understanding of the situation,
incorporating both positive and "negative" sensor observations of possible
target objects collected over time, and diffusing the evidence across the map
as time progresses. The generative model of active inference uses
Dempster-Shafer theory and a Gaussian sensor model, which provides input to the
agent. The generative process employs a Bayesian approach to update a posterior
probability distribution. We calculate the variational free energy for all
positions within the area by assessing the divergence between a pignistic
probability distribution of the evidence map and a posterior probability
distribution of a target object based on the observations, including the level
of surprise associated with receiving new observations. Using the free energy,
we direct the agents' movements in a simulation by taking an incremental step
toward a position that minimizes the free energy. This approach addresses the
challenge of exploration and exploitation, allowing agents to balance searching
extensive areas of the geographical map while tracking identified target
objects.

</details>


### [610] [Label Indeterminacy in AI & Law](https://arxiv.org/abs/2510.17463)
*Cor Steging,Tadeusz Zbiegień*

Main category: cs.AI

TL;DR: 该论文指出法律机器学习中存在的标签不确定性（label indeterminacy）问题，即法律判决结果可能因人为干预而改变，这会影响模型训练。作者在欧洲人权法院案例分类中展示了标签构建方式对模型行为的显著影响。


<details>
  <summary>Details</summary>
Motivation: 法律机器学习通常将过去案例结果视为真实标签，但法律结果往往受到和解、上诉等人为干预的影响，导致标签不确定性。现有方法基于不可验证的假设来估算这些不确定标签。

Method: 在欧洲人权法院案例分类的背景下，研究不同标签构建方式如何影响模型行为，探讨标签不确定性对法律机器学习的影响。

Result: 研究表明，训练过程中标签的构建方式会显著影响模型行为，标签不确定性是AI与法律领域需要关注的重要问题。

Conclusion: 法律机器学习应用需要考虑标签不确定性，作者将这一问题定位为AI与法律领域的相关关切，并展示了它如何塑造模型行为。

Abstract: Machine learning is increasingly used in the legal domain, where it typically
operates retrospectively by treating past case outcomes as ground truth.
However, legal outcomes are often shaped by human interventions that are not
captured in most machine learning approaches. A final decision may result from
a settlement, an appeal, or other procedural actions. This creates label
indeterminacy: the outcome could have been different if the intervention had or
had not taken place. We argue that legal machine learning applications need to
account for label indeterminacy. Methods exist that can impute these
indeterminate labels, but they are all grounded in unverifiable assumptions. In
the context of classifying cases from the European Court of Human Rights, we
show that the way that labels are constructed during training can significantly
affect model behaviour. We therefore position label indeterminacy as a relevant
concern in AI & Law and demonstrate how it can shape model behaviour.

</details>


### [611] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个推理时的多模态虚假信息检测框架，通过分解验证过程为四个模块：视觉真实性评估、跨模态一致性分析、检索增强的事实核查和校准判断，无需领域特定训练即可在MMFakeBench上达到81.65% F1分数。


<details>
  <summary>Details</summary>
Motivation: 网络平台上每天有数十亿包含文本和图像的多模态帖子传播虚假信息，人工事实核查能力不足，而监督检测模型需要领域特定训练数据且难以泛化到不同的操纵策略。

Method: MIRAGE框架将多模态验证分解为四个顺序模块：检测AI生成图像、识别跨模态不一致、通过迭代问题生成进行检索增强的事实核查、以及整合所有信号的校准判断模块。

Result: 在MMFakeBench验证集上，MIRAGE与GPT-4o-mini配合达到81.65% F1和75.1%准确率，比最强的零样本基线高7.65 F1点，假阳性率34.3%远低于仅判断基线的97.3%。消融研究显示视觉验证贡献5.18 F1点，检索增强推理贡献2.97 F1点。

Conclusion: 分解的智能体推理与网络检索相结合，可以在没有领域特定训练的情况下匹配监督检测器的性能，为标记数据稀缺的多模态虚假信息检测提供了可行方案。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [612] [Reasoning Distillation and Structural Alignment for Improved Code Generation](https://arxiv.org/abs/2510.17598)
*Amir Jalilifard,Anderson de Rezende Rocha,Marcos Medeiros Raimundo*

Main category: cs.AI

TL;DR: 将大型语言模型的推理能力蒸馏到更小、更高效的模型中，通过结构感知损失优化方法提升代码生成质量，在多个基准测试中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 代码生成不仅需要准确预测token，更需要理解解决方案级别的结构关系。大型语言模型具备复杂推理能力，但部署成本高；小型模型缺乏这种推理能力。

Method: 通过结构感知损失优化方法，训练小型模型模拟大型模型的推理和问题解决能力，学习识别正确解决方案路径，建立问题定义与解决方案之间的结构对应关系。

Result: 经过廉价且易于实现的微调过程，模型在MBPP、MBPP Plus和HumanEval基准测试中，在pass@1、平均数据流和平均语法匹配指标上显著优于基线模型。

Conclusion: 通过知识蒸馏方法成功将大型语言模型的推理能力转移到小型模型中，实现了高效且高质量的代码生成，为实际部署提供了更经济可行的解决方案。

Abstract: Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

</details>


### [613] [LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena](https://arxiv.org/abs/2510.17638)
*Qingchuan Yang,Simon Mahns,Sida Li,Anri Gu,Jibang Wu,Haifeng Xu*

Main category: cs.AI

TL;DR: 本文系统评估了大语言模型在预测现实世界未来事件方面的能力，提出了"LLM-as-a-Prophet"新范式，发现LLMs已具备显著预测能力，但也存在关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着基于互联网规模数据训练的大语言模型快速发展，探索LLMs预测现实世界未来事件的能力具有重要意义，特别是在金融和经济等社会系统领域。

Method: 构建了Prophet Arena评估基准，持续收集实时预测任务并将每个任务分解为不同的流程阶段，以支持受控和大规模实验。

Result: 评估显示许多LLMs已表现出令人印象深刻的预测能力，如较小的校准误差、一致的预测置信度和有前景的市场回报。但也发现关键瓶颈，包括事件回忆不准确、数据源误解以及接近解决时信息聚合速度慢于市场。

Conclusion: LLMs在预测智能方面已具备显著潜力，但需要解决事件回忆准确性、数据源理解和信息聚合速度等关键瓶颈才能实现卓越的预测性能。

Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of
significant importance to societal systems such as finance and economics. With
the rapid advances of large language models (LLMs) trained on Internet-scale
data, it raises the promise of employing LLMs to forecast real-world future
events, an emerging paradigm we call "LLM-as-a-Prophet". This paper
systematically investigates such predictive intelligence of LLMs. To this end,
we build Prophet Arena, a general evaluation benchmark that continuously
collects live forecasting tasks and decomposes each task into distinct pipeline
stages, in order to support our controlled and large-scale experimentation. Our
comprehensive evaluation reveals that many LLMs already exhibit impressive
forecasting capabilities, reflected in, e.g., their small calibration errors,
consistent prediction confidence and promising market returns. However, we also
uncover key bottlenecks towards achieving superior predictive intelligence via
LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of
data sources and slower information aggregation compared to markets when
resolution nears.

</details>


### [614] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 该论文提出使用多智能体影响图(MAIDs)作为图形化框架来解决多智能体强化学习中的协调问题，设计了基于因果推理的目标干预范式，通过仅干预单个智能体来避免全局指导的复杂性。


<details>
  <summary>Details</summary>
Motivation: 在大规模多智能体强化学习中，对整个系统进行全局人工指导不切实际，而现有的协调机制设计主要依赖经验研究，缺乏易用的研究工具。

Method: 引入多智能体影响图(MAIDs)作为分析框架，设计了目标干预范式，并实现了一种称为预策略干预(PSI)的因果推理技术来执行目标干预。

Result: 实验证明了所提出的目标干预方法的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs提供了一个有效的图形化框架来分析和设计多智能体交互范式，目标干预方法能够通过仅干预单个智能体来实现期望结果，同时相关性图分析可以验证学习范式的可行性。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [615] [Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models](https://arxiv.org/abs/2510.17705)
*Dayan Pan,Zhaoyang Fu,Jingyuan Wang,Xiao Han,Yue Zhu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出Contextual Attention Modulation (CAM)机制和HyCAM框架，通过动态调节自注意力表示来增强任务特定特征并保留通用知识，在多任务适应中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多任务适应中知识保留与任务专业化之间的平衡问题，传统微调方法存在灾难性遗忘和资源消耗大的缺陷。

Method: 提出CAM机制动态调节自注意力模块表示，并构建HyCAM框架，结合共享的全参数CAM模块和多个轻量级专用CAM模块，采用动态路由策略进行自适应知识融合。

Result: 在问答、代码生成和逻辑推理等异构任务上的实验表明，该方法平均性能提升3.65%，显著优于现有方法。

Conclusion: CAM和HyCAM框架有效解决了LLM多任务适应中的挑战，实现了更好的知识保留与任务专业化平衡。

Abstract: Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

</details>


### [616] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 研究发现视觉语言模型在输出错误答案时仍能感知到正确视觉证据，这种现象称为"看见但不相信"。通过选择性注意力掩码的推理时干预，无需训练即可提升多个VLM家族的准确率。


<details>
  <summary>Details</summary>
Motivation: 系统研究视觉语言模型失败的原因：是未能感知视觉证据还是未能有效利用证据。发现模型在深层稀疏但可靠地关注局部证据区域，但经常在输出错误答案时仍能感知正确证据。

Method: 通过层间注意力动态分析，发现浅层主要关注文本，深层稀疏但可靠地关注局部证据区域。引入推理时干预方法，通过选择性注意力掩码突出深层证据区域。

Result: 干预方法无需训练，在LLaVA、Qwen、Gemma和InternVL等多个VLM家族中一致提升准确率，证明VLMs内部编码了可靠证据但未充分利用。

Conclusion: VLMs内部编码可靠证据但利用不足，使这些信号显式化可以弥合感知与推理之间的差距，推进对VLM的诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [617] [Call-Center Staff Scheduling Considering Performance Evolution under Emotional Stress](https://arxiv.org/abs/2510.16406)
*Yujun Zheng,Xinya Chen,Xueqin Lu,Weiguo Sheng,Shengyong Chen*

Main category: cs.NE

TL;DR: 提出了一种考虑情绪压力影响的呼叫中心员工排班方法，通过情感压力驱动模型估计员工工作表现，并使用混合优化算法解决排班问题。


<details>
  <summary>Details</summary>
Motivation: 现有员工排班方法通常忽视情绪压力对工作表现的显著影响，而现实中这种影响不可忽略。

Method: 建立情感压力驱动模型估计员工工作表现，提出结合全局变异和邻域搜索的混合优化算法，并利用深度强化学习辅助求解。

Result: 在银行呼叫中心真实案例上的实验结果表明，该方法在客户服务水平方面优于现有流行排班方法。

Conclusion: 通过明确建模和融入情绪压力，该方法在员工排班中体现了对人类行为的更现实理解和利用。

Abstract: Emotional stress often has a significant effect on the working performance of
staff, but this effect is commonly neglected in existing staff scheduling
methods. We study a call-center staff scheduling problem, which considers the
evolution of work performance of staff under emotional stress. First, we
present an emotional stress driven model that estimates the working performance
of call-center employees based on not only skill levels but also emotional
states. On the basis of the model, we formulate a combined short-term and
long-term call-center staff scheduling problem aiming at maximizing the
customer service level, which depends on the working performance of employees.
We then propose a memetic optimization algorithm combining global mutation and
neighborhood search assisted by deep reinforcement learning to efficiently
solve this problem. Experimental results on real-world problem instances of
bank call-center staff scheduling demonstrate the performance advantages of the
proposed method over selected popular staff scheduling methods. By explicitly
modeling and incorporating emotional stress, our method reflects a more
realistic understanding and utilization of human behavior in staff scheduling.

</details>


### [618] [Bombardier Beetle Optimizer: A Novel Bio-Inspired Algorithm for Global Optimization](https://arxiv.org/abs/2510.17005)
*Hisham A. Shehadeh,Mohd Yamani Idna Idris,Iqbal H. Jebril*

Main category: cs.NE

TL;DR: 提出了一种新型仿生优化算法——轰炸甲虫优化器(BBO)，灵感来自轰炸甲虫的防御和逃脱机制，在CEC 2017测试套件上表现优于多种知名优化算法。


<details>
  <summary>Details</summary>
Motivation: 受轰炸甲虫智能防御和逃脱捕食者行为的启发，开发一种新的仿生优化算法。轰炸甲虫能够感知威胁并触发有毒化学喷雾防御，同时计算与捕食者的距离并飞走逃脱。

Method: BBO算法模拟轰炸甲虫的两种机制：防御机制（触发有毒化学喷雾）和逃脱机制（计算距离并飞走）。算法在CEC 2017测试套件上进行验证。

Result: BBO在收敛速度和结果质量方面均优于Chernobyl Disaster Optimizer、Grey Wolf Optimizer、Particle Swarm Optimization、Bermuda Triangle Optimizer、Sperm Swarm Optimization和Gravitational Search Algorithm等算法。

Conclusion: 轰炸甲虫优化器(BBO)是一种有效的仿生优化算法，在测试中表现出优越的性能，证明了其在实际优化问题中的应用潜力。

Abstract: In this paper, a novel bio-inspired optimization algorithm is proposed,
called Bombardier Beetle Optimizer (BBO). This type of species is very
intelligent, which has an ability to defense and escape from predators. The
principles of the former one is inspired by the defense mechanism of Bombardier
Beetle against the predators, which the Bombardier Beetle triggers a toxic
chemical spray when it feels threatened. This reaction occurs in a specialized
reaction chamber inside its abdomen and includes a well regulated enzymatic
mechanism, which comprises hot water vapor, oxygen, and irritating substances
like p-benzoquinones. In addition, the proposed BBO simulates also the escape
mechanism of Bombardier Beetle from predator, which it has the ability to
calculate its distance from predator and it can fly away. The BBO is tested
with optimizing Congress on Evolutionary Computation (CEC 2017) test bed
suites. Moreover, it is compared against well-known metaheuristic optimization
algorithms includes Chernobyl Disaster Optimizer (CDO), Grey Wolf Optimizer
(GWO), Particle Swarm Optimization (PSO), Bermuda Triangle Optimizer (BTO),
Sperm Swarm Optimization (SSO) and Gravitational Search Algorithm (GSA). The
outcomes of this paper prove the BBO's efficiency in which outperforms the
other algorithms in terms of convergence rate and quality of results.

</details>


### [619] [ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine](https://arxiv.org/abs/2510.17392)
*Sonu Kumar,Arjun S. Nair,Bhawna Chaudhary,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.NE

TL;DR: 提出了一种基于CORDIC的Hodgkin Huxley神经元模型和皮层神经池架构，在FPGA上实现了资源效率高、速度快的SNN实现，适用于边缘AI应用。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的边缘AI应用开发生物准确、低资源的脉冲神经网络实现，解决现有设计在速度和资源效率方面的不足。

Method: 使用模块化和性能优化的CORDIC阶段构建RCHH神经元模型，采用延迟-面积权衡策略，并设计皮层神经池架构。

Result: 与最先进设计相比，RCHH神经元实现了24.5%的LUT减少和35.2%的速度提升，NRMSE改善70%；CNP架构在MNIST数据集上相比等效DNN引擎吞吐量提高2.85倍，准确率仅下降0.35%。

Conclusion: 该设计展示了生物准确、低资源的SNN实现，特别适合资源受限的边缘AI应用场景。

Abstract: We present a Cortical Neural Pool (CNP) architecture featuring a high-speed,
resource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike
shared CORDIC-based DNN approaches, the proposed neuron leverages modular and
performance-optimised CORDIC stages with a latency-area trade-off. The FPGA
implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved
speed, compared to SoTA designs, with 70% better normalised root mean square
error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69
GOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only
a 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The
overall results indicate that the design shows biologically accurate,
low-resource spiking neural network implementations for resource-constrained
edge AI applications.

</details>


### [620] [A Multi-Threading Kernel for Enabling Neuromorphic Edge Applications](https://arxiv.org/abs/2510.17745)
*Lars Niedermeier,Vyom Shah,Jeffrey L. Krichmar*

Main category: cs.NE

TL;DR: 提出了一种多线程内核，使神经形态应用能够在边缘设备上运行，相比单线程处理速度提升4倍，能效提升70%，支持多核负载均衡。


<details>
  <summary>Details</summary>
Motivation: 利用SNNs的稀疏事件驱动特性开发边缘神经形态应用，避免对云服务的依赖，实现低SWaP（尺寸、重量和功耗）的边缘计算。

Method: 设计多线程内核，在多核处理器（如ARM）上实现负载均衡，优化SNNs在边缘设备上的执行效率。

Result: 在中等规模SNNs上速度提升4倍，Synfire网络上提升1.7倍，能效比静态核心分配提高70%。

Conclusion: 该内核支持开发低SWaP的边缘应用，并为神经形态芯片的集成提供原型支持。

Abstract: Spiking Neural Networks (SNNs) have sparse, event driven processing that can
leverage neuromorphic applications. In this work, we introduce a
multi-threading kernel that enables neuromorphic applications running at the
edge, meaning they process sensory input directly and without any up-link to or
dependency on a cloud service. The kernel shows speed-up gains over single
thread processing by a factor of four on moderately sized SNNs and 1.7X on a
Synfire network. Furthermore, it load-balances all cores available on
multi-core processors, such as ARM, which run today's mobile devices and is up
to 70% more energy efficient compared to statical core assignment. The present
work can enable the development of edge applications that have low Size,
Weight, and Power (SWaP), and can prototype the integration of neuromorphic
chips.

</details>
