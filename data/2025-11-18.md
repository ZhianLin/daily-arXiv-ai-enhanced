<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 296]
- [cs.CL](#cs.CL) [Total: 88]
- [cs.LG](#cs.LG) [Total: 281]
- [cs.SE](#cs.SE) [Total: 24]
- [cs.AI](#cs.AI) [Total: 82]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.GT](#cs.GT) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Psychological stress during Examination and its estimation by handwriting in answer script](https://arxiv.org/abs/2511.11633)
*Abhijeet Kumar,Chetan Agarwal,Pronoy B. Neogi,Mayank Goswami*

Main category: cs.CV

TL;DR: 该研究结合笔迹学和人工智能，通过分析学生手写考试试卷来量化心理压力水平，开发了一个基于OCR和情感分析的创新学术取证框架。


<details>
  <summary>Details</summary>
Motivation: 传统评分系统无法深入了解学生在考试期间的认知和情绪状态，需要一种数据驱动的方法来量化心理压力，提供更全面的学生状态评估。

Method: 使用高分辨率图像处理、TrOCR和基于RoBERTa模型的情感熵融合来生成压力指数，通过五模型投票机制和无监督异常检测确保系统鲁棒性。

Result: 开发了一个能够通过分析手写考试试卷生成数值化压力指数的系统，在学术取证领域提供了创新框架。

Conclusion: 该研究成功地将笔迹学与人工智能技术结合，为评估学生考试期间的心理状态提供了一种客观、量化的方法，具有重要的学术和应用价值。

Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.

</details>


### [2] [Real-time pothole detection with onboard sensors and camera on vehicles](https://arxiv.org/abs/2511.11643)
*Aswath Muthuselvam,Jeevak Raj S,Mohanaprasad K*

Main category: cs.CV

TL;DR: 使用SVM分类器实时检测道路坑洞，基于车辆传感器数据实现98.1%的准确率


<details>
  <summary>Details</summary>
Motivation: 道路状况对日常通勤至关重要，随着车辆数量增加，需要频繁评估道路状况以确保交通顺畅，小裂缝可能因温度和车辆压力发展成大坑洞

Method: 利用车辆搭载的传感器收集数据，使用SVM（支持向量机）分类器来检测坑洞

Result: 在2公里长的本地道路上测试，该路段分布有26个坑洞，实现了98.1%的检测准确率

Conclusion: 该方法能够有效实时检测道路坑洞，为大规模坑洞管理和分析提供有用数据

Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes

</details>


### [3] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出CalMRL方法解决多模态表示学习中模态缺失问题，通过表示级插补和双步学习校准不完全对齐，缓解锚点偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要所有模态都存在才能进行跨模态对齐，但现实数据集中常存在模态缺失问题，导致局部锚点偏离最优对齐状态。

Method: 利用模态先验和内在联系在表示层面对缺失模态进行插补，采用双步学习方法结合后验分布的闭式解解决优化困境。

Result: 理论验证了锚点偏移的缓解和收敛性，实验证明CalMRL的优越性，为吸收缺失模态数据提供了新灵活性。

Conclusion: CalMRL有效解决了多模态表示学习中模态缺失导致的校准问题，扩展了现有先进方法的适用范围。

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [4] [A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model](https://arxiv.org/abs/2511.11659)
*Kesong Zheng,Zhi Song,Peizhou Li,Shuyi Yao,Zhenxing Bian*

Main category: cs.CV

TL;DR: 提出了一种基于自适应多层特征融合的动态加权特征融合网络（DWFF-Net），用于耕地生态系统栖息地的高精度分割，在自建数据集上取得了0.6979的mIoU和0.8049的F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决当前耕地生态系统缺乏标准栖息地分类系统、栖息地类型覆盖不完整，以及现有模型无法有效整合语义和纹理特征导致分割精度不足、多尺度栖息地边界模糊的问题。

Method: 构建包含15类耕地系统栖息地的超高分辨率遥感图像数据集；提出DWFF-Net模型，编码器使用冻结参数的DINOv3提取基础特征，引入数据级自适应动态加权策略进行特征融合；解码器包含动态权重计算网络实现多层特征充分融合，采用混合损失函数优化训练。

Result: 在构建的数据集上，模型达到0.6979的mIoU和0.8049的F1分数，分别比基线网络提升0.021和0.0161；消融实验证实多层特征融合的互补性，有效提高了田埂等微栖息地类别的IoU。

Conclusion: 建立了基于自适应多层特征融合的耕地系统栖息地识别框架，能够以低成本实现亚米级精度的栖息地制图，为耕地景观的细粒度栖息地监测提供有力技术支持。

Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.

</details>


### [5] [AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2511.11662)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: AGENet是一个用于医学图像分割的少样本学习框架，通过结合边缘感知的测地距离学习和自适应原型提取，在有限标注数据下实现精确的边界分割。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大量标注数据，这在临床应用中成为瓶颈。现有少样本分割方法在医学图像精确边界描绘方面表现欠佳，特别是当解剖结构相似且缺乏足够空间上下文时。

Method: AGENet包含三个主要组件：(1) 边缘感知测地距离学习模块，通过迭代快速行进细化来尊重解剖边界；(2) 自适应原型提取，通过空间加权聚合捕获全局结构和局部边界细节；(3) 自适应参数学习，自动适应不同器官特征。

Result: 在多个医学影像数据集上的广泛实验表明，该方法优于现有最先进方法，显著减少了边界误差，同时保持了计算效率。

Conclusion: AGENet通过轻量级几何建模，在有限标注数据下实现了精确的医学图像分割，非常适合需要精确分割的临床应用。

Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.

</details>


### [6] [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/abs/2511.13719)
*Zhongang Cai,Ruisi Wang,Chenyang Gu,Fanyi Pu,Junxiang Xu,Yubo Wang,Wanqi Yin,Zhitao Yang,Chen Wei,Qingping Sun,Tongxi Zhou,Jiaqi Li,Hui En Pang,Oscar Qian,Yukun Wei,Zhiqian Lin,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Xiangyu Fan,Hanming Deng,Lewei Lu,Liang Pan,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: SenseNova-SI系列多模态基础模型通过构建800万样本数据集，在空间智能任务上取得突破性表现，同时保持强大的通用多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型取得了显著进展，但在空间智能方面仍存在明显不足，需要专门提升模型的空间理解和推理能力。

Method: 基于现有多模态基础模型（Qwen3-VL、InternVL3、Bagel），系统构建包含800万多样化样本的SenseNova-SI-8M数据集，采用严格的空间能力分类法。

Result: 在多个空间智能基准测试中表现优异：VSI-Bench 68.7%、MMSI 43.3%、MindCube 85.6%、ViewSpatial 54.6%、SITE 50.1%，同时保持强大的通用多模态理解能力（MMBench-En 84.9%）。

Conclusion: SenseNova-SI项目展示了通过大规模多样化数据训练可以显著提升空间智能能力，并观察到新兴的泛化能力迹象。该项目将持续更新，所有模型将公开发布以促进相关研究。

Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.

</details>


### [7] [EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance](https://arxiv.org/abs/2511.11700)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: EPSegFZ是一个无需预训练的少样本和零样本3D点云语义分割网络，通过原型增强注意力、双相对位置编码和语言引导原型嵌入模块，在S3DIS和ScanNet基准上分别比现有最优方法提升5.68%和3.82%。


<details>
  <summary>Details</summary>
Motivation: 解决现有少样本3D点云分割方法过度依赖预训练、缺乏灵活性，以及未能充分利用支持集中文本信息的问题。

Method: 提出EPSegFZ网络，包含三个关键组件：原型增强寄存器注意力模块、基于双相对位置编码的交叉注意力机制、语言引导原型嵌入模块。

Result: 在S3DIS和ScanNet基准测试中，分别比现有最优方法提升5.68%和3.82%的性能。

Conclusion: EPSegFZ无需预训练即可实现优异的少样本和零样本3D点云分割性能，有效利用了视觉和文本信息。

Abstract: Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

</details>


### [8] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: TASA是一个几何优化的3D场景级可供性分割框架，通过任务感知的2D检测和3D几何细化，在效率和准确性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注对象级可供性或仅将2D预测提升到3D，忽略了点云中的丰富几何结构信息且计算成本高，需要开发能够同时利用2D语义线索和3D几何推理的方法。

Method: 提出TASA框架：1）任务感知的2D可供性检测模块识别可操作点；2）3D可供性细化模块整合2D语义先验与局部3D几何信息，采用从粗到细的方式。

Result: 在SceneFun3D数据集上的实验表明，TASA在场景级可供性分割的准确性和效率上都显著优于基线方法。

Conclusion: TASA通过联合利用2D语义线索和3D几何推理，有效解决了3D场景级可供性分割的挑战，为具身智能体在复杂环境中的交互提供了有力支持。

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [9] [LE-CapsNet: A Light and Enhanced Capsule Network](https://arxiv.org/abs/2511.11708)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: LE-CapsNet是一种轻量级、增强型的Capsule Network变体，比原始CapsNet参数更少、推理速度更快、准确率更高，在CIFAR-10上达到76.73%准确率且推理速度快4倍，在AffNIST上达到94.3%准确率。


<details>
  <summary>Details</summary>
Motivation: Capsule Network虽然在某些方面优于CNN，但存在速度慢、参数多、资源消耗大、准确率不如CNN等问题，需要改进。

Method: 提出LE-CapsNet作为CapsNet的轻量增强变体，使用更少的参数（380万权重）实现更好的性能。

Result: 在CIFAR-10数据集上达到76.73%准确率，推理速度比CapsNet快4倍；在AffNIST数据集上达到94.3%准确率，优于CapsNet的90.52%。

Conclusion: LE-CapsNet成功解决了原始CapsNet的速度和准确率问题，在保持对仿射变换鲁棒性的同时，实现了更高效的性能。

Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).

</details>


### [10] [Target-Balanced Score Distillation](https://arxiv.org/abs/2511.11710)
*Zhou Xu,Qi Wang,Yuxiao Yang,Luyuan Zhang,Zhang Liang,Yang Li*

Main category: cs.CV

TL;DR: 本文提出了Target-Balanced Score Distillation (TBSD)方法，通过多目标优化策略解决了SDS方法中纹理优化与形状失真之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统的Score Distillation Sampling (SDS)方法在3D资产生成中存在过饱和和过平滑问题，而引入负提示的变体方法面临纹理优化有限或纹理增益但形状失真的关键权衡。

Method: 提出Target-Balanced Score Distillation (TBSD)，将生成建模为多目标优化问题，并引入自适应策略来平衡纹理真实性和几何形状准确性。

Result: 大量实验表明，TBSD显著优于现有最先进方法，能够生成具有高保真纹理和几何精确形状的3D资产。

Conclusion: TBSD通过系统分析负提示的使用机制，有效解决了SDS方法中的纹理-形状权衡问题，为3D资产生成提供了更优的解决方案。

Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

</details>


### [11] [CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition](https://arxiv.org/abs/2511.11716)
*Sudhakar Sah,Nikhil Chabbra,Matthieu Durnerin*

Main category: cs.CV

TL;DR: CompressNAS是一个受MicroNAS启发的框架，通过全局搜索秩选择来压缩深度卷积神经网络，在保持精度的同时实现显著压缩。


<details>
  <summary>Details</summary>
Motivation: 深度卷积神经网络在微控制器和轻量级NPU上部署困难，现有低秩张量分解方法局部选择秩，忽略了压缩与精度之间的全局权衡。

Method: 将秩选择作为全局搜索问题，使用快速精度估计器评估候选分解，在内存和精度约束下进行高效而详尽的秩探索。

Result: 在ImageNet上，ResNet-18压缩8倍，精度下降小于4%；在COCO上，YOLOv5s压缩2倍无精度损失，YOLOv5n压缩2倍精度下降2.5%。

Conclusion: 提出了新的压缩模型家族STResNet，与其他高效模型相比具有竞争力。

Abstract: Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.

</details>


### [12] [AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks](https://arxiv.org/abs/2511.11720)
*Jiao Chen,Haoyi Wang,Jianhua Tang,Junyi Wang*

Main category: cs.CV

TL;DR: AdaptFly是一个无需权重更新的提示引导测试时适应框架，通过两种互补模式解决无人机在恶劣条件下的语义分割性能下降问题：轻量级令牌提示检索和梯度稀疏视觉提示优化。


<details>
  <summary>Details</summary>
Motivation: 低空无人机网络的语义分割模型在天气、光照和视角变化下性能急剧下降，资源受限的无人机无法运行基于梯度的测试时适应，而资源充足的无人机独立适应会浪费共享经验。

Method: 提出AdaptFly框架，包含两种适应模式：资源受限无人机使用轻量级令牌提示检索共享全局记忆；资源充足无人机使用协方差矩阵自适应进化策略进行梯度稀疏视觉提示优化。通过激活统计检测器触发适应，跨无人机知识池整合提示知识。

Result: 在UAVid和VDD基准测试以及真实无人机部署中，AdaptFly显著提高了分割精度和鲁棒性，优于静态模型和最先进的TTA基线方法。

Conclusion: AdaptFly为新兴低空经济中的弹性、通信高效感知提供了一条实用路径，通过跨无人机协作实现高效的知识共享和适应。

Abstract: Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.

</details>


### [13] [Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video](https://arxiv.org/abs/2511.11725)
*Zekai Shi,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: 提出一种基于生物视觉盲点的自监督视觉表征学习方法，用于从儿童视角数据中学习词汇-指称映射


<details>
  <summary>Details</summary>
Motivation: 儿童在6-9个月开始学习词汇，需要将语音与视觉指称关联。首次遇到新词时存在无数可能的解释，需要从生态有效的第一人称视角数据中学习

Method: 使用基于掩码自编码器的视觉骨干网络，结合人眼盲点知识设计新型掩码策略，模仿大脑填补视野空白的方式。预训练编码器用于对比学习视频-文本模型

Result: 提出的生物合理掩码策略在从跨情境和时间扩展片段中学习词汇-指称映射方面，至少与随机掩码同样有效

Conclusion: 生物启发的掩码策略为自监督学习提供了新视角，在保持性能的同时更具生物学合理性

Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

</details>


### [14] [GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion](https://arxiv.org/abs/2511.11730)
*Yongjun Xiao,Dian Meng,Xinlei Huang,Yanran Liu,Shiwei Ruan,Ziyue Qiao,Xubin Zheng*

Main category: cs.CV

TL;DR: GROVER是一个用于空间多组学数据自适应融合的新框架，通过图卷积网络和对比学习解决多模态异质性、分辨率不匹配和生物扰动等挑战。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学、蛋白质组学和表观基因组学缺乏病理形态学背景，需要与组织病理学图像整合以进行全面的疾病组织分析，但多模态异质性、分辨率不匹配和生物扰动给整合带来挑战。

Method: 使用基于Kolmogorov-Arnold网络的图卷积网络编码器捕获模态间非线性依赖关系，采用spot-feature-pair对比学习策略优化跨模态对应关系，设计动态专家路由机制自适应选择信息丰富的模态。

Result: 在真实世界空间组学数据集上的实验表明，GROVER优于最先进的基线方法。

Conclusion: GROVER为多模态整合提供了一个稳健可靠的解决方案。

Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.

</details>


### [15] [Exposing DeepFakes via Hyperspectral Domain Mapping](https://arxiv.org/abs/2511.11732)
*Aditya Mehta,Swarnim Chaudhary,Pratik Narang,Jagat Sesh Challa*

Main category: cs.CV

TL;DR: HSI-Detect是一个两阶段管道，通过从RGB图像重建31通道高光谱图像来进行Deepfake检测，在光谱域中放大操作伪影。


<details>
  <summary>Details</summary>
Motivation: 现代生成和扩散模型产生的图像高度逼真，可能误导人类感知和自动化检测系统。现有方法主要在RGB空间分析，仅使用三个光谱通道。

Method: 提出HSI-Detect两阶段管道：1) 从标准RGB输入重建31通道高光谱图像；2) 在高光谱域进行检测。通过扩展输入表示到更密集的光谱带，放大RGB域中微弱或不可见的操作伪影。

Result: 在FaceForensics++数据集上评估，相比仅使用RGB的基线方法显示出持续改进。

Conclusion: 光谱域映射在Deepfake检测中具有前景，高光谱分析能更好地揭示生成图像的伪影特征。

Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.

</details>


### [16] [Toward bilipshiz geometric models](https://arxiv.org/abs/2511.11735)
*Yonatan Sverdlov,Eitan Rosen,Nadav Dym*

Main category: cs.CV

TL;DR: 该论文研究了点云神经网络是否保持对称感知距离的bi-Lipschitz等价性，发现标准不变网络不满足此性质，并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 受等变学习文献启发，研究点云网络是否保持对称感知距离的bi-Lipschitz等价性，这对网络的理论保证很重要。

Method: 分析两种对称感知度量（Procrustes匹配和Hard Gromov Wasserstein距离）的bi-Lipschitz等价性，修改标准不变网络以获得bi-Lipschitz保证。

Result: 发现两种距离本身不是bi-Lipschitz等价的，标准不变网络也不满足此性质。改进后的模型在3D点云对应任务中表现更好。

Conclusion: 点云网络的bi-Lipschitz性质很重要，提出的改进方法能提供更好的理论保证和实际性能。

Abstract: Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.
  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.

</details>


### [17] [Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models](https://arxiv.org/abs/2511.11751)
*Sanchit Sinha,Guangzhi Xiong,Zhenghao He,Aidong Zhang*

Main category: cs.CV

TL;DR: Concept-RuleNet是一个多代理神经符号系统，通过挖掘视觉概念并构建可解释的一阶规则，在保持透明推理的同时增强视觉基础，提高预测准确性并减少幻觉符号。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽然预测准确率高，但缺乏决策解释性，容易产生幻觉事实，特别是在分布外数据上。神经符号框架虽然结合了黑盒感知和可解释符号推理，但现有方法仅从任务标签中提取符号，缺乏对底层视觉数据的充分基础。

Method: 1. 多模态概念生成器从训练图像子集中挖掘判别性视觉概念；2. 利用视觉概念条件化符号发现，将生成锚定在真实图像统计中；3. 大语言模型推理代理将符号组合成可执行的一阶规则；4. 推理时，视觉验证代理量化每个符号的存在程度，并与黑盒神经模型输出协同触发规则执行。

Result: 在五个基准测试（包括两个医学成像任务和三个代表性不足的自然图像数据集）上，该系统将最先进的神经符号基线平均提升了5%，同时将规则中的幻觉符号出现率降低了高达50%。

Conclusion: Concept-RuleNet通过恢复视觉基础同时保持透明推理，有效提高了神经符号系统的性能，并显著减少了幻觉问题，为构建更可靠和可解释的视觉语言模型提供了可行路径。

Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.

</details>


### [18] [Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition](https://arxiv.org/abs/2511.11754)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: 提出了一种新型隐式稀疏风格的Transformer变体架构——Batch Transformers，通过对重要维度（主成分）进行注意力机制，显著减小编码器-解码器ANN架构中的瓶颈大小。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer对序列或批次实体的整个维度进行注意力计算，而Batch Transformers旨在通过关注重要维度来减少模型瓶颈，提高效率。

Method: 采用隐式稀疏风格，对重要维度（主成分）实施注意力机制，而非对整个维度进行注意力计算。在编码器-解码器ANN架构中应用此方法。

Result: 在面部识别任务的合成图像生成中测试，特别是在化妆和遮挡数据集上，能够增加有限原始数据集的变异性。

Conclusion: Batch Transformers通过关注重要维度有效减小了模型瓶颈，在数据增强和合成图像生成任务中表现出良好性能。

Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.

</details>


### [19] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: Image-POSER是一个基于强化学习的框架，通过协调多个预训练的文生图和图生图专家模型，动态分解长提示词任务，并使用视觉语言模型进行结构化反馈监督，从而提升复杂提示词下的图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文生图模型在处理长而复杂的组合提示词时表现不稳定，无法满足创意工作流程的需求。需要开发能够可靠执行长格式提示词的端到端系统。

Method: 采用反射式强化学习框架，将图像合成和编辑建模为马尔可夫决策过程，动态协调预训练专家模型，通过视觉语言模型批评器提供结构化反馈监督每个步骤的对齐。

Result: 实验表明Image-POSER在行业标准和自定义基准测试中，在图像对齐度、保真度和美学质量方面均优于基线模型（包括前沿模型），并在人类评估中持续获得偏好。

Conclusion: 强化学习能够赋予AI系统自主分解、重新排序和组合视觉模型的能力，推动向通用视觉助手的发展。

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [20] [SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction](https://arxiv.org/abs/2511.11824)
*Zhongping Dong,Pengyang Yu,Shuangjian Li,Liming Chen,Mohand Tahar Kechadi*

Main category: cs.CV

TL;DR: SOTFormer是一个统一目标检测、跟踪和短期轨迹预测的端到端框架，使用恒定内存的时间transformer，在遮挡、尺度变化和快速运动场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决单目标跟踪和短期运动预测在遮挡、尺度变化和时间漂移下的挑战，这些因素破坏了实时感知所需的时间连贯性。

Method: 采用最小化恒定内存的时间transformer，通过真值引导记忆和烧录锚点损失稳定初始化，单个轻量级时间注意力层跨帧优化嵌入，实现实时推理和固定GPU内存。

Result: 在Mini-LaSOT基准测试上达到76.3 AUC和53.7 FPS（4.3 GB显存），在快速运动、尺度变化和遮挡情况下优于TrackFormer和MOTRv2等transformer基线模型。

Conclusion: SOTFormer通过统一的端到端框架成功解决了单目标跟踪中的时间连贯性问题，在保持实时性能的同时显著提升了跟踪精度。

Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

</details>


### [21] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: 提出了MP-GFormer，一种3D几何感知的动态图变换器，通过注意力机制将演化的3D几何表示集成到动态图学习中，以预测加工操作序列。


<details>
  <summary>Details</summary>
Motivation: 现有动态图学习方法在加工工艺规划中虽然能捕捉时空依赖关系，但未能融入零件的三维几何信息，缺乏领域感知能力。

Method: 利用StereoLithography表面网格表示每次加工操作后零件的3D几何，通过注意力机制将演化的3D几何表示集成到动态图学习中。

Result: 在合成数据集上评估，相比最先进方法，主要操作和子操作预测准确率分别提高了24%和36%。

Conclusion: MP-GFormer通过集成3D几何信息显著提升了加工操作序列预测的准确性，证明了3D几何感知在加工工艺规划中的重要性。

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [22] [Defending Unauthorized Model Merging via Dual-Stage Weight Protection](https://arxiv.org/abs/2511.11851)
*Wei-Jia Chen,Min-Yen Tsai,Cheng-Yi Lee,Chia-Mu Yu*

Main category: cs.CV

TL;DR: MergeGuard是一个双阶段权重保护框架，通过重新分配任务相关信息和注入结构化扰动来破坏模型合并兼容性，同时保持原始模型性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型和开放存储库的快速扩散使得模型合并成为一种方便但有风险的做法，未经授权的模型合并侵犯知识产权并破坏模型所有权和问责制。

Method: 第一阶段通过L2正则化优化重新分配任务相关信息，第二阶段注入结构化扰动以错位任务子空间，破坏损失景观中的曲率兼容性。

Result: 在视觉和语言模型上的实验表明，MergeGuard能将合并模型准确率降低高达90%，而保护模型的性能损失不到1.5%。

Conclusion: MergeGuard通过重塑模型参数几何形状，使合并模型崩溃为破坏性干扰，同时保护模型保持完全功能，有效防止未经授权的模型合并。

Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.

</details>


### [23] [FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision](https://arxiv.org/abs/2511.11864)
*Muzammal Shafique,Nasir Rahim,Jamil Ahmad,Mohammad Siadat,Khalid Malik,Ghaus Malik*

Main category: cs.CV

TL;DR: FocusSDF是一种基于符号距离函数的新型损失函数，通过自适应地为靠近边界的像素分配更高权重，使网络专注于边界区域，从而解决医学图像分割中的边界保持挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床实践中至关重要，但大多数分割模型没有显式编码边界信息，导致边界保持成为一个持续存在的挑战。

Method: 提出FocusSDF损失函数，基于符号距离函数，自适应地为靠近病变或器官边界的像素分配更高权重，使网络具有边界感知能力。

Result: 在涵盖脑动脉瘤、中风、肝脏和乳腺肿瘤分割任务的多个数据集上，与包括MedSAM在内的五种最先进模型和四种基于距离的损失函数进行比较，FocusSDF始终表现出优越性能。

Conclusion: FocusSDF在医学图像分割中优于现有的基于距离变换的损失函数，有效解决了边界保持问题。

Abstract: Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.

</details>


### [24] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Joëlle Taillon,Jérôme Théau*

Main category: cs.CV

TL;DR: 本研究探讨了使用合成图像补充有限训练数据以提高麝牛检测性能，在零样本和少样本设置中验证了合成图像的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统野生动物调查方法资源密集且受限于后勤挑战，深度学习目标检测模型在稀疏分布物种检测中因小数据集而受限，需要寻找替代方案。

Method: 比较了仅使用真实图像的基线模型与5个零样本和5个少样本模型，这些模型在训练集中逐步加入更多合成图像。

Result: 零样本模型中，添加合成图像提高了检测性能，但超过基线模型训练数据集100%时出现收益递减；少样本模型中，真实图像与合成图像结合可获得更好的召回率和略高的准确率。

Conclusion: 合成图像在数据稀缺时能有效训练准确的目标检测模型，为监测稀有或难以接近的物种提供了重要视角，可实现无真实数据启动模型并随时间获取真实图像进行优化。

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [25] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: 本文介绍了Harpia——一个基于CUDA的处理库，集成到Annotat3D中，用于支持大规模3D数据集的可扩展交互式分割工作流，特别适用于高性能计算和远程访问环境。


<details>
  <summary>Details</summary>
Motivation: 高分辨率体积成像技术生成的数据集规模越来越大，现有工具在处理效率、分割和交互探索方面面临挑战。

Method: 开发了Harpia库，具有严格内存控制、原生分块执行和GPU加速的过滤、注释和量化工具套件，能够在超过单个GPU内存容量的数据集上可靠运行。

Result: 实验结果显示，与NVIDIA cuCIM和scikit-image等广泛使用的框架相比，在处理速度、内存效率和可扩展性方面都有显著改进。

Conclusion: 该系统结合交互式人机界面和高效的GPU资源管理，特别适合共享HPC基础设施中的协作科学成像工作流。

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [26] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文提出使用DSPy框架对医学视觉语言模型进行自动提示优化，在5个医学影像任务上实现了53%的中位数相对性能提升，最大提升达300%-3400%，显著改善了医学AI系统的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言基础模型在医学任务上表现不佳，传统方法如微调需要大量数据和计算资源，手动提示工程难以泛化且对医疗机构不友好。需要开发能够利用模型嵌入知识、减少对人类设计提示依赖的自动化方法。

Method: 采用DSPy框架进行结构化自动提示优化，在放射学、胃肠病学和皮肤病学的5个医学影像任务上评估了10个开源VLM，使用了4种提示优化技术。

Result: 优化后的管道相比零样本提示基线实现了53%的中位数相对改进，在零样本性能较低的任务上最大提升达300%-3400%。

Conclusion: 自动提示优化在医学AI系统中具有巨大潜力，能够显著提升临床图像解释的准确性，减少对提示设计的依赖，让临床医生专注于患者护理和临床决策。

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [27] [PI-NAIM: Path-Integrated Neural Adaptive Imputation Model](https://arxiv.org/abs/2511.11908)
*Afifa Khaled,Ebrahim Hamid Sumiea*

Main category: cs.CV

TL;DR: PI-NAIM是一个双路径架构，根据缺失复杂度动态路由样本到优化的插补方法，结合统计插补和神经网络，在医学影像和多模态临床数据中实现最先进的缺失值处理性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像和多模态临床环境经常面临诊断流程中模态缺失的挑战，现有插补方法要么缺乏表示能力，要么计算成本高昂。

Method: 提出双路径架构：1)智能路径路由，将低缺失样本导向高效统计插补(MICE)，复杂模式导向神经网络(GAIN)；2)跨路径注意力融合，利用缺失感知嵌入智能结合两个分支；3)端到端联合优化插补精度和下游任务性能。

Result: 在MIMIC-III和多模态基准测试中实现最先进性能，RMSE为0.108（基线为0.119-0.152），死亡率预测AUROC达到0.812。

Conclusion: PI-NAIM的模块化设计能够无缝集成到处理不完整传感器测量、缺失模态或损坏输入的视觉流程中，为现实场景提供统一解决方案。

Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model

</details>


### [28] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON2.0是一个动态模态平衡的多模态表示学习框架，用于解决电商产品理解中的模态不平衡、内部对齐关系利用不足和数据噪声问题，在多个数据集上实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 解决电商多模态模型面临的三个挑战：(i) 模态混合训练导致的模态不平衡；(ii) 产品内视觉和文本信息内在对齐关系的利用不足；(iii) 电商多模态数据中噪声处理能力有限。

Method: 提出MOON2.0框架，包含：(1) 模态驱动的专家混合模块，根据模态组成自适应处理输入样本；(2) 双级对齐方法，更好利用产品内部语义对齐特性；(3) 基于MLLM的图像-文本协同增强策略，结合动态样本过滤提高训练数据质量。

Result: 在MBE2.0基准和多个公共数据集上实现了最先进的零样本性能，注意力热图可视化提供了改进多模态对齐的定性证据。

Conclusion: MOON2.0通过动态模态平衡、双级对齐和协同增强策略，有效解决了电商多模态产品理解的关键挑战，显著提升了模型性能。

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [29] [Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910)
*Siyou Li,Huanan Wu,Juexi Shao,Yinghao Ma,Yujian Gan,Yihao Luo,Yuwei Wang,Dong Nie,Lu Wang,Wengqing Wu,Le Zhang,Massimo Poesio,Juntao Yu*

Main category: cs.CV

TL;DR: QTSplus是一个轻量级视觉令牌选择模块，通过动态选择与文本查询最相关的视觉证据，将长视频的视觉令牌压缩高达89%，减少28%延迟，在保持准确性的同时显著提升长视频理解效率。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在长视频理解中的挑战，即视觉令牌数量随视频长度线性增长导致的注意力成本、内存和延迟爆炸问题。

Method: 开发Query-aware Token Selector (QTSplus)模块，通过交叉注意力评分视觉令牌、基于查询复杂度预测保留预算、使用可微分直通估计器选择Top-n令牌，并利用小型重新编码器保持时间顺序。

Result: 在Qwen2.5-VL中集成QTSplus，视觉流压缩达89%，端到端延迟减少28%，在八个长视频理解基准测试中保持接近原始模型的准确性，在TempCompass方向准确率提升20.5分、顺序准确率提升5.6分。

Conclusion: QTSplus是一种有效通用的机制，能够在保留任务相关证据的同时，将多模态大语言模型扩展到现实世界长视频场景。

Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.

</details>


### [30] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: 首次使用事件相机进行图像去雾，通过事件引导的扩散模型将事件的高动态范围信息传输到RGB图像中，解决传统方法在雾霾条件下动态范围有限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB的去雾方法由于动态范围有限（60dB），在雾霾条件下难以保留结构和光照细节。事件相机具有更高的动态范围（120dB）和微秒级延迟，更适合雾霾场景。

Method: 提出事件引导的扩散模型，设计事件引导模块将稀疏的高动态范围事件特征（如边缘、角点）映射到扩散潜空间，为生成过程提供精确的结构指导。

Result: 在两个基准数据集和自建的重雾霾无人机数据集（AQI=341）上实现了最先进的去雾效果，提高了视觉真实感并减少了语义漂移。

Conclusion: 事件相机为图像去雾提供了新的解决方案，通过有效传输高动态范围信息，显著改善了雾霾条件下的图像清晰度和结构保留能力。

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [31] [Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs](https://arxiv.org/abs/2511.11959)
*Leonardi Melo,Luís Gustavo,Dimmy Magalhães,Lucciani Vieira,Mauro Araújo*

Main category: cs.CV

TL;DR: 比较三种基于U-Net的架构在巴西岩画语义分割中的性能，其中Attention-Residual BEGL-UNet表现最佳，Dice Score达0.710。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同U-Net架构在考古遗产数字保护中的有效性，特别是对巴西岩画石刻的语义分割。

Method: 比较三种U-Net架构：(1) BEGL-UNet；(2) Attention-Residual BEGL-UNet（含残差块和门控注意力机制）；(3) Spatial Channel Attention BEGL-UNet（含空间通道注意力模块）。所有架构使用BEGL损失函数，在巴西Poço da Bebidinha考古遗址图像上进行5折交叉验证。

Result: Attention-Residual BEGL-UNet表现最佳：Dice Score 0.710，验证损失0.067，召回率0.854。Spatial Channel Attention BEGL-UNet性能相当：DSC 0.707，召回率0.857。基线BEGL-UNet的DSC为0.690。注意力机制相比基线提升2.5-2.9%的Dice Score。

Conclusion: 注意力机制在考古遗产数字保护中具有显著效果，Attention-Residual BEGL-UNet架构为岩画语义分割提供了最佳性能。

Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex, Piauí, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.

</details>


### [32] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: 提出了一种将Segment Anything 2 (SAM2)从自然图像预训练迁移到电子显微镜图像分割的框架，通过特征引导注意力模块和双亲和度解码器，在神经元分割任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜图像中的神经结构分割面临形态复杂、信噪比低和标注稀缺等挑战，现有方法在准确性和泛化性上受限。希望利用视觉基础模型在自然图像上学到的先验知识来解决这些问题。

Method: 使用SAM2提取通用特征，引入特征引导注意力模块利用SAM2的语义线索指导轻量级精细编码器关注困难区域，最后通过双亲和度解码器生成粗粒度和精炼的亲和度图。

Result: 实验表明，在SAM2权重冻结时性能与SOTA方法相当，在EM数据上微调后显著超越现有SOTA方法。

Conclusion: 验证了将自然图像预训练表示与针对性领域自适应指导相结合，可以有效解决神经元分割中的特定挑战。

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [33] [From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology](https://arxiv.org/abs/2511.11984)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Junchao Zhu,Haibo Wang,Daniel Reisenbüchler,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Steven Salvatoree,Surya Seshane,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 该研究评估了在数据受限的临床环境下，视觉语言模型在细粒度肾小球亚型分类中的表现，发现病理学专业化的视觉语言主干网络结合标准微调是最有效的起点。


<details>
  <summary>Details</summary>
Motivation: 解决肾活检中细粒度肾小球亚型分类的临床需求，但面临有价值标签稀缺和获取困难的挑战，需要评估视觉语言模型在数据约束下的适应能力。

Method: 将细粒度肾小球亚型分类建模为临床现实的少样本问题，系统评估病理学专业化和通用视觉语言模型，分析分类性能、特征对齐和亚型可分性。

Result: 病理学专业化视觉语言主干网络结合标准微调表现最佳，即使每个肾小球亚型只有4-8个标记样本，也能显著提升区分能力和校准度。

Conclusion: 监督水平和适应策略共同影响诊断性能和多模态结构，为模型选择、适应策略和标注投资提供了指导。

Abstract: Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.

</details>


### [34] [Region-Point Joint Representation for Effective Trajectory Similarity Learning](https://arxiv.org/abs/2511.13125)
*Hao Long,Silin Zhou,Lisi Chen,Shuo Shang*

Main category: cs.CV

TL;DR: RePo是一种新颖的轨迹相似性计算方法，通过联合编码区域级和点级特征来捕获空间上下文和细粒度移动模式，在各项评估指标上平均准确率比现有最优方法提升22.2%。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法虽然降低了传统轨迹相似性计算的计算复杂度，但未能充分利用轨迹信息的全面频谱进行相似性建模。

Method: 提出RePo方法：1）区域级表示：将GPS轨迹映射为网格序列，通过结构特征和视觉特征增强的语义上下文捕获空间上下文；2）点级表示：使用三个轻量级专家网络从密集GPS序列中提取局部、相关和连续移动模式；3）路由器网络自适应融合点级特征，再通过交叉注意力与区域级特征结合生成最终轨迹嵌入；4）使用包含困难负样本的对比损失进行训练。

Result: 实验结果显示，RePo在所有评估指标上比现有最优基线方法平均准确率提升22.2%。

Conclusion: RePo通过联合编码区域级和点级特征，有效捕获了轨迹的全面信息，显著提升了轨迹相似性计算的性能。

Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

</details>


### [35] [BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups](https://arxiv.org/abs/2511.11989)
*Songsong Zhang,Chuanqi Tang,Hongguang Zhang,Guijian Tang,Minglong Li,Xueqiong Li,Shaowu Yang,Yuanxi Peng,Wenjing Yang,Jing Zhao*

Main category: cs.CV

TL;DR: 提出了一种突破面部特写限制的IPPG方法，通过双线推理管道、身份自适应融合和身份聚合预置模块，实现身份保真度和场景语义创作的协同优化。


<details>
  <summary>Details</summary>
Motivation: 现有IPPG方法过度强调面部区域，导致输出被面部特写主导，存在视觉叙事性弱和复杂文本提示下语义一致性差的问题，核心限制在于身份特征嵌入削弱了生成模型的语义表达能力。

Method: 设计了身份语义分离的双线推理管道，提出身份自适应融合策略将ID-语义融合推迟到噪声预测阶段，引入身份聚合预置模块聚合ID信息并替换随机初始化。

Result: 实验验证该方法在超越面部特写的IPPG任务中实现稳定有效性能，无需手动遮罩或微调即可高效生成，作为即插即用组件可快速部署到现有IPPG框架。

Conclusion: 该方法解决了对面部特写的过度依赖，促进了电影级角色场景创作，为相关领域提供了更丰富的个性化生成能力。

Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.

</details>


### [36] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodríguez,Mario Almagro,Kunal Dahiya,David Jiménez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: ViXML框架通过整合视觉基础模型和少量亿级参数解码器，在极端多标签分类中实现了效率和性能的平衡，在四个公开数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 极端多标签分类(XMC)中，基础模型的潜力尚未充分挖掘，需要在效率和性能间取得平衡，同时有效利用更大的解码器模型和视觉信息。

Method: 提出ViXML框架，使用少量亿级参数解码器，并通过单图像嵌入池化高效整合视觉基础模型，保持计算效率的同时解锁多模态能力。

Result: 在四个公开文本数据集及其图像增强版本上验证，ViXML在最大数据集上P@1指标提升达+8.21%，小编码器版本在多数情况下优于纯文本解码器。

Conclusion: 视觉信息在XMC中具有重要作用，ViXML框架成功平衡了计算效率和性能，展示了图像信息可替代数十亿参数的效果。

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [37] [Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks](https://arxiv.org/abs/2511.11993)
*Jiaming Liang,Chi-Man Pun*

Main category: cs.CV

TL;DR: 本文提出了一种动态参数优化方法（DPO），通过同心衰减模型解释迁移攻击中参数强度的动态模式，显著提高了基于变换的攻击的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有基于变换的攻击存在参数优化盲区：仅考虑低迭代设置、对不同代理模型使用统一参数、传统网格搜索计算复杂度高，限制了攻击的迁移潜力。

Method: 提出动态参数优化（DPO）方法，基于上升后下降模式，利用同心衰减模型解释参数强度动态模式，将复杂度从O(mn)降低到O(nlogm)。

Result: 在不同代理模型、迭代次数和任务上的综合实验表明，DPO能显著提高基于变换攻击的迁移性。

Conclusion: DPO方法通过动态参数优化有效解决了现有攻击的参数优化局限性，显著提升了迁移攻击的效果。

Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.

</details>


### [38] [LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation](https://arxiv.org/abs/2511.12005)
*Xinyu He,Botong Zhao,Bingbing Li,Shujing Lyu,Jiwei Shen,Yue Lu*

Main category: cs.CV

TL;DR: 提出LithoSeg，一种用于光刻SEM图像分割的粗到精网络，通过人机交互引导SAM实现鲁棒性，并将2D分割转化为1D回归问题进行精确细化


<details>
  <summary>Details</summary>
Motivation: 准确分割和测量光刻SEM图像对于精确工艺控制、优化器件性能和提升半导体制造良率至关重要，但现有方法缺乏必要的精度和鲁棒性

Method: 粗阶段：引入人机交互引导的SAM实现鲁棒分割；精阶段：将2D分割转化为1D回归问题，使用粗掩模采样沟槽法向轮廓，通过轻量级MLP进行逐点细化

Result: LithoSeg在分割精度和计量精度方面均优于先前方法，同时需要更少的监督

Conclusion: 该方法为实际应用提供了有前景的解决方案

Abstract: Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.

</details>


### [39] [Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy](https://arxiv.org/abs/2511.12006)
*Kai-Wen K. Yang,Andrew Bai,Alexandra Bermudez,Yunqi Hong,Zoe Latham,Iris Sloan,Michael Liu,Vishrut Goyal,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.CV

TL;DR: 提出SIT-ADDA-Auto框架，仅调整早期卷积层而非整个网络，实现显微镜图像的无标签跨域适应，在多种域偏移场景下优于全编码器适应方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在显微镜应用中面临域偏移问题，传统对抗域适应方法会破坏已学习的语义表示，需要更高效的适应策略。

Method: SIT-ADDA-Auto框架：仅适应早期卷积层并冻结深层，结合浅层对抗对齐和预测不确定性自动选择适应深度，无需目标域标签。

Result: 在曝光、光照偏移、跨仪器迁移和多种染色场景下，SIT-ADDA在重建和下游分割任务上优于全编码器适应和非对抗基线方法，减少了语义特征漂移。

Conclusion: 为显微镜无标签域适应提供了设计原则，仅调整浅层网络即可实现可靠迁移，代码已开源。

Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.

</details>


### [40] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: 提出基于多摄像头计算机视觉的实时交通安全评估框架，通过后侵入时间计算来识别信号交叉口的高风险区域，实现亚秒级精度的实时危险可视化。


<details>
  <summary>Details</summary>
Motivation: 传统基于事故的交通安全分析受限于数据稀疏性和延迟问题，需要开发实时、高分辨率的评估方法来改善交叉口安全。

Method: 使用四个同步摄像头提供连续视觉覆盖，通过YOLOv11分割进行车辆检测，利用单应性矩阵将车辆多边形转换为统一的鸟瞰图，开发像素级PET算法进行车辆位置测量。

Result: 框架能够以2.68 FPS的平均速度生成800×800像素的对数热图，在边缘设备上实现实时处理，精确识别高风险区域。

Conclusion: 验证了去中心化视觉PET分析在智能交通系统中的可行性，为高分辨率、实时和可扩展的交叉口安全评估提供了可复制的方法论。

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [41] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: 提出了LIHE框架解决弱监督广义指代表达理解任务，通过两阶段方法处理零个或多个目标的情况，结合双曲和欧几里得几何解决语义表示崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督指代表达理解方法受限于一对一映射假设，无法处理现实场景中对应零个或多个目标的表达，需要更实用的广义指代表达理解范式。

Method: 提出LIHE框架，包含两个阶段：指称解耦阶段预测目标数量并分解复杂表达为子表达；指称定位阶段使用HEMix混合相似度模块结合欧几里得精确对齐和双曲几何层次建模能力。

Result: 在gRefCOCO和Ref-ZOM上建立了首个有效的弱监督WGREC基线，HEMix在标准REC基准上实现一致改进，IoU@0.5提升达2.5%。

Conclusion: LIHE框架成功解决了弱监督广义指代表达理解任务，证明了混合几何方法在防止语义崩溃和保持概念间细粒度区分方面的有效性。

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [42] [Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging](https://arxiv.org/abs/2511.12024)
*Jose Reinaldo Cunha Santos A V Silva Neto,Hodaka Kawachi,Yasushi Yagi,Tomoya Nakamura*

Main category: cs.CV

TL;DR: 提出NSDD方法，通过单次前向传播蒸馏DDNM+求解器的零空间组件，实现无需配对监督的快速、逼真无透镜成像重建。


<details>
  <summary>Details</summary>
Motivation: 现有无透镜相机重建方法依赖配对监督会产生域不匹配偏差，而通用扩散先验在噪声、高度复用和病态的无透镜反卷积场景中不稳定。

Method: 分离范围空间约束和零空间扩散先验更新，引入NSDD方法蒸馏迭代DDNM+求解器的零空间组件，以无透镜测量和范围空间锚点为条件。

Result: 在Lensless-FFHQ和PhlatCam数据集上，NSDD是第二快的方法（仅次于Wiener），接近教师模型的感知质量（第二优LPIPS，低于DDNM+），优于DPS和经典凸优化基线。

Conclusion: NSDD为快速、无需真实值、逼真的无透镜成像提供了一条实用路径，在保持测量一致性的同时实现逼真重建，显著降低运行时间和内存消耗。

Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.

</details>


### [43] [Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark](https://arxiv.org/abs/2511.12026)
*Rulin Zhou,Wenlong He,An Wang,Jianhang Zhang,Xuanhui Zeng,Xi Zhang,Chaowei Zhu,Haijun Hu,Hongliang Ren*

Main category: cs.CV

TL;DR: VL-SurgPT是首个大规模多模态手术点追踪数据集，结合视觉追踪与文本描述，包含908个体内视频片段，涵盖组织和器械追踪，并提出TG-SurgPT文本引导追踪方法提升在视觉挑战条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有手术追踪数据集缺乏语义上下文，难以理解追踪失败机制。复杂手术环境中的烟雾遮挡、镜面反射和组织变形等挑战使得准确点追踪仍然困难。

Method: 构建VL-SurgPT数据集（908个视频片段，754个组织追踪，154个器械追踪），建立8种最先进追踪方法的基准，并提出TG-SurgPT文本引导追踪方法，利用语义描述提升鲁棒性。

Result: 实验结果表明，结合点状态信息显著提高了追踪准确性和可靠性，特别是在传统仅视觉方法难以应对的不利视觉场景中。

Conclusion: 通过桥接视觉和语言模态，VL-SurgPT能够开发上下文感知的追踪系统，这对于推进计算机辅助手术应用至关重要，即使在具有挑战性的术中条件下也能保持性能。

Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.

</details>


### [44] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: GCAgent是一个用于长视频理解的全局上下文感知代理框架，通过结构化的事件记忆和感知-行动-反思循环，解决了MLLMs在长视频理解中的长期依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在长视频理解方面存在token限制和长期时间依赖捕获困难的问题，无法有效捕捉全局上下文和复杂事件关系。

Method: 提出了Schematic and Narrative Episodic Memory来结构化建模事件及其因果时间关系，采用多阶段的感知-行动-反思循环，通过Memory Manager检索相关情景上下文进行推理。

Result: 在Video-MME Long split上比强基线MLLM提升了23.5%的准确率，在7B规模MLLMs中达到最先进性能，Long split准确率为73.4%，整体平均准确率为71.9%。

Conclusion: 基于代理的推理范式和结构化记忆为认知启发的长视频理解提供了有效解决方案。

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [45] [VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation](https://arxiv.org/abs/2511.12030)
*Jun Zhou,Chi Xu,Kaifeng Tang,Yuting Ge,Tingrui Guo,Li Cheng*

Main category: cs.CV

TL;DR: 提出了一种联合视觉和物理线索的手-物体3D姿态估计框架，通过视觉-物理线索联合学习和候选姿态聚合，实现视觉一致且物理合理的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖视觉线索，常产生违反物理约束的结果，而现有的物理推理方法通常依赖后优化或不可微物理引擎，损害视觉一致性和端到端可训练性。

Method: 1) 视觉-物理线索联合学习：训练模型提取2D视觉线索和3D物理线索；2) 候选姿态聚合：通过扩散生成多个候选姿态，利用视觉和物理预测进行聚合。

Result: 在姿态精度和物理合理性方面显著优于现有最先进方法。

Conclusion: 提出的联合视觉和物理线索的框架能够有效提升手-物体姿态估计的准确性和物理合理性。

Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.

</details>


### [46] [Improved Masked Image Generation with Knowledge-Augmented Token Representations](https://arxiv.org/abs/2511.12032)
*Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Zihao Han,Yunming Ye*

Main category: cs.CV

TL;DR: KA-MIG是一个知识增强的掩码图像生成框架，通过引入三种token级语义依赖的先验知识图来提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有掩码图像生成方法仅依靠模型自身学习视觉token序列的语义依赖关系，但由于单个token缺乏明确语义且序列较长，直接学习这些依赖关系具有挑战性。

Method: 提出KA-MIG框架，构建三种先验知识图（共现图、语义相似图、位置-令牌不兼容图），设计图感知编码器学习token和位置感知表示，并通过轻量融合机制集成到现有MIG方法中。

Result: 实验结果表明，该方法在ImageNet上的类条件图像生成任务中优于现有MIG方法。

Conclusion: 通过引入先验知识图，KA-MIG有效增强了模型捕捉语义依赖关系的能力，提高了生成质量。

Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.

</details>


### [47] [SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2511.12040)
*Xinyuan Hu,Changyue Shi,Chuxiao Yang,Minghao Chen,Jiajun Ding,Tao Wei,Chen Wei,Zhou Yu,Min Tan*

Main category: cs.CV

TL;DR: SRSplat是一个前馈框架，通过结合外部高质量参考图像和内部纹理线索，从少量低分辨率视图重建高分辨率3D场景，解决了现有方法纹理细节恢复不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈3D重建方法从稀疏低分辨率图像重建时，由于LR输入缺乏高频信息，往往无法恢复精细纹理细节。

Method: 1) 使用MLLM和扩散模型为每个场景构建特定参考图库；2) 提出参考引导特征增强模块(RGFE)对齐融合LR输入和参考图像特征；3) 训练解码器预测高斯基元；4) 引入纹理感知密度控制(TADC)根据内部纹理丰富度自适应调整高斯密度。

Result: 在RealEstate10K、ACID和DTU等多个数据集上的广泛实验表明，SRSplat优于现有方法，并展现出强大的跨数据集和跨分辨率泛化能力。

Conclusion: SRSplat通过联合利用外部参考图像和内部纹理线索，有效解决了从稀疏低分辨率视图重建高分辨率3D场景时纹理细节恢复不足的问题。

Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.

</details>


### [48] [FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification](https://arxiv.org/abs/2511.12044)
*Cheng-Chang Tsai,Kai-Wen Cheng,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 提出FedSDA方法，通过扩散模型和染色分离技术对齐联邦学习中非IID病理图像的染色分布，缓解分布偏移问题


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中非IID病理图像数据带来的挑战，特别是特征分布偏移问题

Method: 使用扩散模型拟合数据分布，结合染色分离提取关键特征，在联邦学习框架下将各客户端的染色分布与目标分布对齐

Result: 实验结果表明FedSDA能有效改善基线方法，在缓解客户端间分布差异方面优于其他方法

Conclusion: FedSDA为计算病理学社区提供了有价值的实用见解

Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.

</details>


### [49] [DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging](https://arxiv.org/abs/2511.12047)
*Huimin Cheng,Xiaowei Yu,Shushan Wu,Luyang Fang,Chao Cao,Jing Zhang,Tianming Liu,Dajiang Zhu,Wenxuan Zhong,Ping Ma*

Main category: cs.CV

TL;DR: DCMM-Transformer是一种用于医学图像分析的ViT架构，通过引入度校正混合成员模型作为自注意力中的加性偏置，解决了现有方法无法建模复杂社区结构的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像存在潜在的解剖分组结构（如器官、组织和病理区域），但标准ViT无法利用这些结构。现有方法如SBM-Transformer存在不可微分、训练不稳定和无法建模复杂社区结构的问题。

Method: 提出DCMM-Transformer，将度校正混合成员模型作为自注意力中的加性偏置引入，以完全可微分和可解释的方式建模社区结构和度异质性。

Result: 在多种医学成像数据集（包括脑部、胸部、乳腺和眼部模态）上的综合实验表明，该方法具有优越的性能和泛化能力。

Conclusion: 所学的组结构和结构化注意力调制通过产生解剖学上有意义和语义连贯的注意力图，显著增强了可解释性。

Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.

</details>


### [50] [DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training](https://arxiv.org/abs/2511.12048)
*Saksham Kumar,Ashish Singh,Srinivasarao Thota,Sunil Kumar Singh,Chandan Kumar*

Main category: cs.CV

TL;DR: 提出DeiTFake方法，基于DeiT变换器和两阶段渐进训练策略，用于深度伪造检测，在OpenForensics数据集上达到99.22%准确率。


<details>
  <summary>Details</summary>
Motivation: 深度伪造对数字媒体完整性构成重大威胁，需要开发更有效的检测方法来应对这一挑战。

Method: 采用基于DeiT的变换器架构和两阶段渐进训练策略：第一阶段使用标准增强的迁移学习，第二阶段使用高级仿射和深度伪造特定增强进行微调。

Result: 在OpenForensics数据集（190,335张图像）上，第一阶段达到98.71%准确率，第二阶段达到99.22%准确率和0.9997 AUROC，优于最新基线方法。

Conclusion: DeiTFake方法通过知识蒸馏模型捕捉细微操作伪影，提高了检测模型的鲁棒性，为人脸深度伪造检测提供了实用基准。

Abstract: Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\% accuracy after stage one and 99.22\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.

</details>


### [51] [UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization](https://arxiv.org/abs/2511.12054)
*Cuiqun Chen,Qi Chen,Bin Yang,Xingyi Zhang*

Main category: cs.CV

TL;DR: UniABG是一个新颖的双阶段无监督跨视角地理定位框架，通过对抗视角桥接和图基对应校准来解决跨视角域差距问题，在无监督设置下达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 监督方法依赖大量配对标注限制了可扩展性，而无监督方法因跨视角域差距导致伪标签噪声问题。

Method: 采用双阶段框架：1）视角感知对抗桥接(VAAB)建模视角不变特征增强伪标签鲁棒性；2）异构图过滤校准(HGFC)构建双跨视角结构图实现可靠视角对应。

Result: 在University-1652数据集上卫星→无人机AP提升+10.63%，在SUES-200数据集上提升+16.73%，甚至超越监督基线。

Conclusion: UniABG有效解决了无监督跨视角地理定位中的伪标签噪声问题，实现了与监督方法相媲美的性能。

Abstract: Cross-view geo-localization (CVGL) matches query images ($\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\rightarrow$ Drone AP by +10.63\% on University-1652 and +16.73\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG

</details>


### [52] [PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling](https://arxiv.org/abs/2511.12056)
*Sijie Wang,Qiang Wang,Shaohuai Shi*

Main category: cs.CV

TL;DR: PipeDiT是一个用于加速视频生成的流水线框架，通过序列并行、模块解耦和注意力协同处理等创新技术，在OpenSoraPlan和HunyuanVideo上实现了1.06-4.02倍的加速效果。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiT)模型在视频生成方面表现出色，但实际部署受到推理速度慢和内存消耗高的限制，需要开发高效的加速方案。

Method: 提出PipeDiT流水线框架，包含三个核心技术：1) PipeSP序列并行流水线算法；2) DeDiVAE模块解耦技术；3) Aco注意力协同处理方法。

Result: 在8-GPU系统上的实验表明，PipeDiT在多种常见分辨率和时间步配置下，相比OpenSoraPlan和HunyuanVideo实现了1.06-4.02倍的加速。

Conclusion: PipeDiT通过创新的流水线设计有效解决了DiT模型在视频生成中的推理延迟和内存消耗问题，为实际部署提供了可行的解决方案。

Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.

</details>


### [53] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: 提出MovSemCL框架，通过运动语义对比学习解决轨迹相似度计算中的语义建模不足、计算成本高和物理不可行增强问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法存在三个关键局限：轨迹语义和层次建模不足、点级编码计算成本高、使用扭曲轨迹语义的物理不可行增强。

Method: 将原始GPS轨迹转换为运动语义特征并分块，使用块内和块间注意力编码局部和全局轨迹模式，采用曲率引导的增强策略保留信息段、掩码冗余段。

Result: 在真实数据集上优于最先进方法，相似度搜索任务平均排名接近理想值1，启发式近似提升达20.3%，推理延迟降低达43.4%。

Conclusion: MovSemCL通过层次表示和物理可行增强，有效解决了轨迹相似度计算的关键挑战，实现了更好的性能和效率。

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [54] [DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal](https://arxiv.org/abs/2511.12066)
*Jialang Lu,Shuning Sun,Pu Wang,Chen Wu,Feng Gao,Lina Gong,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: DCA-LUT是首个基于深度学习的紫边去除框架，通过色度感知坐标变换模块分离紫边到专用维度，并使用5D查找表进行高效色彩校正。


<details>
  <summary>Details</summary>
Motivation: 紫边是由相机镜头纵向色差引起的持久伪影，传统方法依赖昂贵硬件和手工特征提取，缺乏数据驱动方法。

Method: 提出色度感知坐标变换模块学习图像自适应色彩空间，将紫边分离到专用维度；然后使用5D查找表进行非线性色彩映射；构建了大规模合成紫边数据集PF-Synth。

Result: 在合成和真实数据集上的广泛实验表明，该方法在紫边去除方面达到了最先进的性能。

Conclusion: DCA-LUT框架通过深度学习有效解决了紫边问题，无需昂贵硬件，实现了精确的色彩恢复。

Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.

</details>


### [55] [Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound](https://arxiv.org/abs/2511.12077)
*Dengming Zhang,Weitao You,Jingxiong Li,Weishen Lin,Wenda Shi,Xue Zhao,Heda Zuo,Junxian Wu,Lingyun Sun*

Main category: cs.CV

TL;DR: VAEmotionLLM是一个两阶段框架，通过有限的音频预训练教会视觉语言模型跨模态理解情感，在艺术情感基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作多为人类中心或单模态，忽略了艺术作品有意表达的情感；当前音频-视觉语言模型需要大规模音频预训练，限制了可扩展性。

Method: 第一阶段VG-Align通过视觉引导音频对齐，将冻结的视觉通路蒸馏到新的音频通路；第二阶段EmoAdapter通过情感增强器和情感监督器注入情感敏感残差并应用情感监督。

Result: 在ArtEmoBenchmark上达到最先进结果，优于音频、视觉和音频-视觉基线；消融实验表明各组件互补。

Conclusion: 提出的框架能够以有限的音频预训练实现跨模态情感理解，在艺术情感理解任务上表现出色。

Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.

</details>


### [56] [Point Cloud Quantization through Multimodal Prompting for 3D Understanding](https://arxiv.org/abs/2511.12079)
*Hongxuan Li,Wencheng Zhu,Huiying Xu,Xinzhong Zhu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态提示的向量量化框架，用于点云分析，利用文本嵌入作为原型先验，通过双约束量化空间融合几何和语义信息。


<details>
  <summary>Details</summary>
Motivation: 当前基于可训练向量或聚类质心的原型方法在代表性和可解释性方面存在不足，而多模态对齐在视觉语言模型中显示出潜力，需要解决这些局限性。

Method: 使用预训练模型的文本嵌入作为原型先验，通过多模态提示自适应优化原型，引入双约束量化空间（紧凑性和分离性正则化），采用Gumbel-Softmax松弛实现可微分离散化。

Result: 在ModelNet40和ScanObjectNN数据集上的大量实验证明了该方法的优越有效性。

Conclusion: 提出的多模态提示驱动量化框架能够有效解决原型代表性和可解释性问题，在点云分析任务中表现出色。

Abstract: Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.

</details>


### [57] [Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning](https://arxiv.org/abs/2511.12082)
*Lokender Singh,Saksham Kumar,Chandan Kumar*

Main category: cs.CV

TL;DR: 提出了一种基于改进ResNet-101架构和概率推理的多标签图像分类方法，在COCO-2014数据集上取得了0.794 mAP的优异性能，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 多标签图像分类在计算机视觉应用中具有重要价值，但现有方法在处理标签依赖性和不确定性方面存在挑战。

Method: 使用改进的ResNet-101架构，通过概率推理模拟标签依赖性和不确定性来提升预测准确性。

Result: 在COCO-2014数据集上达到0.794 mAP，优于ResNet-SRN(0.771)和Vision Transformer基线(0.785)。

Conclusion: 将概率推理集成到深度学习模型中能有效解决多标签场景的挑战，取得了接近最先进水平的成果。

Abstract: Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.

</details>


### [58] [SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving](https://arxiv.org/abs/2511.12084)
*Ji-Ping Jin,Chen-Bin Feng,Rui Fan,Chi-Man Vong*

Main category: cs.CV

TL;DR: 提出了SemanticStitch框架，通过融入前景对象的语义先验来改善图像拼接质量，解决了传统方法忽略语义信息导致的前景连续性破坏问题。


<details>
  <summary>Details</summary>
Motivation: 图像拼接因拍摄角度、位置差异和物体移动等因素导致错位和视觉不一致。传统接缝雕刻方法忽略了语义信息，破坏了前景对象的连续性。

Method: 开发了基于深度学习的SemanticStitch框架，包含强调显著对象语义完整性的新型损失函数，并创建了两个专门的真实世界数据集进行评估。

Result: 实验结果表明，该方法相比传统技术有显著改进，为实际应用提供了有力支持。

Conclusion: SemanticStitch通过整合语义先验有效提升了图像拼接质量，特别是在保持前景对象完整性方面表现优异。

Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.

</details>


### [59] [Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning](https://arxiv.org/abs/2511.12090)
*Shengqin Jiang,Tianqi Kong,Yuankai Qi,Haokui Zhang,Lina Yao,Quan Z. Sheng,Qingshan Liu,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种分层分组提示调优方法，通过层分组共享提示和根提示生成子提示的机制，减少不必要更新，缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法在每个层独立添加任务特定提示，虽然灵活性高但容易导致某些层不必要更新，可能覆盖对先前任务重要的特征表示，增加灾难性遗忘风险。

Method: 分层分组提示调优：1) 同组层共享相似提示，通过位置编码调整，保持预训练模型内在特征关系；2) 使用单一任务特定根提示生成各层组的子提示，增强协同性。

Result: 在四个基准测试上的广泛实验表明，该方法相比多个最先进方法取得了优越性能。

Conclusion: 分层分组提示调优通过增强模型稳定性和减少层间独立性，有效缓解了灾难性遗忘问题，在持续学习中表现出色。

Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.

</details>


### [60] [Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio](https://arxiv.org/abs/2511.12095)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: PACE是首个面向SNN和事件视觉的数据集蒸馏框架，通过压缩大型训练数据集为紧凑合成数据集，显著降低SNN训练成本。


<details>
  <summary>Details</summary>
Motivation: 事件相机输出异步事件流与SNN生物启发动态特性匹配，但SNN因时序编码训练成本高，限制了实际部署。

Method: PACE包含两个核心模块：ST-DSM使用残差膜电位稠化脉冲特征并进行细粒度时空匹配，PEQ-N提供即插即用的概率整数量化器。

Result: 在多个数据集上超越现有方法，在N-MNIST上达到84.4%准确率（完整训练集性能的85%），训练时间减少50倍以上，存储成本降低6000倍。

Conclusion: PACE能够实现分钟级SNN训练和高效边缘部署，为事件视觉和SNN提供了实用的训练加速解决方案。

Abstract: Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \textbf{ST-DSM} and \textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \(84.4\%\) accuracy, about \(85\%\) of the full training set performance, while reducing training time by more than \(50\times\) and storage cost by \(6000\times\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.

</details>


### [61] [Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks](https://arxiv.org/abs/2511.12097)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Xudong Jiang,Dacheng Tao*

Main category: cs.CV

TL;DR: SpikeNM是首个面向SNN的半结构化N:M剪枝框架，通过线性化复杂度实现高效稀疏化，结合神经科学启发的蒸馏方法，在保持精度的同时生成硬件友好的稀疏模式。


<details>
  <summary>Details</summary>
Motivation: 现有SNN剪枝方法面临两难：非结构化剪枝难以硬件加速，结构化剪枝缺乏灵活性且精度损失严重。需要一种既能硬件友好又能保持精度的折中方案。

Method: 采用M维基-对数参数化和可微top-k采样器，将块复杂度从指数级降至线性；提出资格启发蒸馏(EID)，将时间累积信用转换为块级软目标，稳定高稀疏度下的搜索过程。

Result: 在2:4稀疏度下，SpikeNM在主流数据集上保持甚至提升了精度，同时生成硬件友好的稀疏模式，与固有脉冲稀疏性互补。

Conclusion: SpikeNM成功解决了SNN剪枝的硬件友好性与精度保持的平衡问题，为边缘部署提供了有效的解决方案。

Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.

</details>


### [62] [DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT](https://arxiv.org/abs/2511.12098)
*Xianhao Zhou,Jianghao Wu,Ku Zhao,Jinlong He,Huangxuan Zhao,Lei Chen,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: 提出DGCF框架，结合冻结的DINOv3 Transformer和可训练的CNN编码器-解码器，通过交叉融合模块整合全局表示和局部特征，并引入多级DINOv3感知损失，在SynthRAD2023盆腔数据集上实现最先进的CT合成性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型缺乏全局语义理解，而Transformer在小规模医学数据集上容易过拟合，需要平衡局部外观和上下文表示。

Method: DGCF框架：冻结的DINOv3 Transformer + 可训练CNN编码器-解码器 + 可学习交叉融合模块 + 多级DINOv3感知损失。

Result: 在SynthRAD2023盆腔数据集上，DGCF在MRI→CT和CBCT→CT转换任务中，在MS-SSIM、PSNR和分割指标方面达到最先进性能。

Conclusion: 这是首个将DINOv3表示用于医学图像转换的工作，展示了自监督Transformer指导在语义感知CT合成中的潜力。

Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.

</details>


### [63] [Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models](https://arxiv.org/abs/2511.12099)
*Tianle Cheng,Zeyan Zhang,Kaifeng Gao,Jun Xiao*

Main category: cs.CV

TL;DR: 提出Adaptive Begin-of-Video Tokens (ada-BOV)方法，用于自回归视频扩散模型，通过自适应层归一化调制吸收去噪的前帧，保持全局一致性并改善局部动态质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成长视频时面临挑战：分块扩展方法存在去噪延迟和误差累积问题，流去噪方法则存在一致性脆弱和运动动态差的问题。

Method: 1. 提出自适应BOV令牌，通过类似自适应层归一化的调制自适应吸收去噪前帧；2. 流去噪细化策略，解耦采样轨迹长度与注意力窗口大小约束；3. 扰动增强训练噪声调度，平衡收敛速度与模型鲁棒性。

Result: 在多个指标上取得了令人信服的定性和定量结果，证明了方法的有效性。

Conclusion: 提出的ada-BOV方法能够有效解决自回归视频扩散模型在生成长视频时的挑战，在保持全局一致性的同时改善了局部动态质量。

Abstract: Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.

</details>


### [64] [Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation](https://arxiv.org/abs/2511.12100)
*Yannan Chen,Ruoyu Chen,Bin Zeng,Wei Wang,Shiming Liu,Qunli Zhang,Zheng Hu,Laiyuan Wang,Yaowei Wang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出SS-CA方法，通过反事实增强训练来纠正模型的不完整因果学习，提高泛化能力和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型仅依赖有限充分原因进行预测，对分布变化敏感。模型的关键区域被遮蔽后模型会误分类，而人类仍能识别，表明模型学习到的依赖关系因果性不足

Method: 基于LIMA归因方法开发Counterfactual LIMA识别最小空间区域集，用自然背景替换这些区域进行数据增强，联合训练原始样本和增强样本

Result: 在多个ImageNet变体上的实验表明，SS-CA提高了ID测试数据的泛化能力，在OOD基准测试中表现更优，在噪声等扰动下也展现出增强的泛化能力

Conclusion: SS-CA有效利用可解释性洞察纠正模型缺陷，提升了性能和鲁棒性

Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.

</details>


### [65] [BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation](https://arxiv.org/abs/2511.12103)
*Sayad Ibna Azad,Md. Atiqur Rahman*

Main category: cs.CV

TL;DR: BdSL-SPOTER是一个基于姿态的transformer框架，用于准确高效地识别孟加拉手语，在BdSLW60基准测试中达到97.92%的Top-1验证准确率，比Bi-LSTM基线提升22.82%，同时保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为孟加拉手语开发一个准确且高效的识别框架，解决低资源区域手语识别问题，并为其他低资源区域手语提供可扩展模型。

Method: 扩展SPOTER范式，采用文化特定的预处理和紧凑的四层transformer编码器，优化可学习位置编码，并使用课程学习增强泛化能力和加速收敛。

Result: 在BdSLW60基准测试中达到97.92%的Top-1验证准确率，比Bi-LSTM基线提升22.82%，同时参数数量更少、FLOPs更低、FPS更高。

Conclusion: BdSL-SPOTER为现实世界无障碍应用提供了实用框架，并可作为其他低资源区域手语的可扩展模型。

Abstract: We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.

</details>


### [66] [TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery](https://arxiv.org/abs/2511.12104)
*Tammy Glazer,Gilles Q. Hacheme,Akram Zaytar,Luana Marotti,Amy Michaels,Girmaw Abebe Tadesse,Kevin White,Rahul Dodhia,Andrew Zolli,Inbal Becker-Reshef,Juan M. Lavista Ferres,Caleb Robinson*

Main category: cs.CV

TL;DR: TEMPO是一个基于高分辨率卫星影像和深度学习模型的全球时间序列建筑密度和高度数据集，提供2018年第一季度至2025年第二季度的季度更新数据。


<details>
  <summary>Details</summary>
Motivation: 需要大规模监测全球建筑发展模式和气候变化影响，为全球韧性和适应工作提供数据支持。

Method: 将现有建筑足迹和高度数据与季度PlanetScope卫星影像配对，训练多任务深度学习模型来预测37.6米分辨率的建筑密度和高度。

Result: 验证显示在不同人工标注子集上F1分数达到85%-88%，时间稳定性高（5年趋势一致性得分0.96），计算成本远低于可比方法。

Conclusion: TEMPO能够以较低计算成本捕捉建成区的季度变化，为大规模监测发展模式和气候影响提供了有效工具。

Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.

</details>


### [67] [Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection](https://arxiv.org/abs/2511.12107)
*Tianxiang Zhang,Peipeng Yu,Zhihua Xia,Longchen Dai,Xiaoyu Zhou,Hui Gao*

Main category: cs.CV

TL;DR: 提出了DFF-Adapter方法，通过轻量级多头LoRA模块适配DINOv2，同时处理真实性检测和细粒度伪造方法分类，仅需3.5M可训练参数即可达到或超越现有复杂方法的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于DINOv2的深度伪造检测方法将其视为通用二元分类，忽略了不同伪造方法产生的独特伪影特征，需要更精细的检测方法。

Method: 在DINOv2的每个transformer块中集成轻量级多头LoRA模块，同时处理真实性检测和细粒度伪造类型分类，通过共享分支将细粒度操作线索传播到真实性头部，实现多任务协同优化。

Result: 仅使用3.5M可训练参数，该方法在检测准确率上达到或超越了当前复杂的state-of-the-art方法。

Conclusion: DFF-Adapter通过参数高效的方式成功提升了深度伪造检测性能，证明了利用细粒度伪造方法知识可以显式增强真实性判别能力。

Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.

</details>


### [68] [MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images](https://arxiv.org/abs/2511.12110)
*Qinyue Tong,Ziqian Lu,Jun Liu,Rui Zuo,Zheming Lu*

Main category: cs.CV

TL;DR: 提出了MEMR-Seg任务，通过多轮实体级推理生成分割掩码，构建了MR-MedSeg数据集，并开发了MediRound基线模型，通过判断与校正机制解决多轮分割中的错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为任务特定且缺乏交互性，文本提示方法局限于单轮对话，无法进行多轮推理。

Method: 构建MR-MedSeg数据集(17.7万轮多轮医学分割对话)，提出MediRound模型，在推理阶段引入轻量级的判断与校正机制来缓解错误传播。

Result: 实验结果表明该方法有效解决了MEMR-Seg任务，性能优于传统的医学参考分割方法。

Conclusion: MEMR-Seg任务和MediRound模型为医学图像分割提供了多轮交互推理能力，显著提升了分割的准确性和用户交互体验。

Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

</details>


### [69] [RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving](https://arxiv.org/abs/2511.12117)
*Ruiqi Cheng,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: RadarMP是一种使用4D毫米波雷达低层回波信号进行精确3D场景运动感知的新方法，通过统一架构联合建模雷达目标检测和运动估计任务，在恶劣天气条件下实现可靠的运动感知。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达具有全天候工作能力，但稀疏和噪声的雷达点通常导致运动感知不精确，在光学传感器性能下降的恶劣天气条件下限制了自动驾驶车辆的感知能力。

Method: 提出RadarMP方法，使用两帧连续的低层雷达回波信号，在统一架构中联合建模雷达目标检测和运动估计，设计基于多普勒频移和回波强度的自监督损失函数来监督空间和运动一致性。

Result: 在公共数据集上的广泛实验表明，RadarMP在各种天气和光照条件下实现了可靠的运动感知，优于基于雷达的解耦运动感知流程。

Conclusion: RadarMP增强了全场景自动驾驶系统的感知能力，为恶劣天气条件下的可靠运动感知提供了有效解决方案。

Abstract: Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.

</details>


### [70] [OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description](https://arxiv.org/abs/2511.12131)
*Quanxing Xu,Ling Zhou,Feifei Zhang,Jinyu Tian,Rubing Huang*

Main category: cs.CV

TL;DR: 提出了OAD-Promoter方法，通过减轻语言偏见和提升领域迁移鲁棒性来增强基于LLM的视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: LLM在视觉问答中依赖大规模训练数据会继承语言偏见，导致预测不可靠且在分布外泛化方面表现不佳。

Method: 包含三个组件：对象集中示例生成模块生成全局描述和对象集中样本；记忆知识辅助模块从存储示例中检索相关知识；OAD提示整合前两个模块输出优化LLM推理。

Result: 实验表明OAD-Promoter在少样本或零样本设置下显著提升了LLM-based VQA方法的性能，达到了新的最先进结果。

Conclusion: OAD-Promoter通过减轻语言偏见和增强领域迁移能力，有效提升了LLM在视觉问答中的表现。

Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.

</details>


### [71] [Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware](https://arxiv.org/abs/2511.12136)
*Karol C. Jurzec,Tomasz Szydlo,Maciej Wielgosz*

Main category: cs.CV

TL;DR: 提出了一个轻量级的C语言运行时，用于在边缘设备上高效执行脉冲神经网络推理，通过优化减少延迟和内存使用，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络具有事件驱动特性，在时间处理和能效方面具有优势，但在资源受限的硬件上训练和部署仍然具有挑战性。

Method: 将SNNTorch训练好的模型转换为紧凑的C表示；使用静态、缓存友好的数据布局和预分配避免解释器和分配开销；利用稀疏脉冲活动修剪不活跃的神经元和突触。

Result: 在N-MNIST和ST-MNIST上的实验显示与Python基线功能相当，在桌面CPU上实现约10倍加速，通过修剪获得额外增益，内存大幅减少，可在微控制器上部署。

Conclusion: 结果表明，当与优化的运行时和脉冲驱动的模型压缩配对时，脉冲神经网络可以在传统嵌入式平台上高效执行。

Abstract: Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/

</details>


### [72] [MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering](https://arxiv.org/abs/2511.12142)
*Seokwon Song,Minsu Park,Gunhee Kim*

Main category: cs.CV

TL;DR: MAVIS是首个评估多模态源归属系统的基准，包含15.7万视觉问答实例，开发了信息量、基础性和流畅性的自动评估指标，发现多模态RAG能生成更信息丰富和流畅的答案，但在图像文档的基础性方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有源归属研究主要关注文本场景，忽视了多模态的作用，需要开发能够理解视觉问题意图、检索多模态证据并生成带引用长文本答案的系统。

Method: 构建包含15.7万视觉问答实例的数据集，每个答案都有事实级引用指向多模态文档，开发三维度自动评估指标（信息量、基础性、流畅性），比较不同提示方法的效果。

Result: 多模态RAG比单模态RAG生成更信息丰富和流畅的答案，但在图像文档的基础性方面较弱；不同提示方法在信息量和基础性之间存在权衡；缓解图像文档解释中的上下文偏差是未来关键方向。

Conclusion: MAVIS基准揭示了多模态源归属系统的关键挑战，特别是图像文档的基础性问题，为未来研究提供了重要方向和评估框架。

Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS

</details>


### [73] [Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain](https://arxiv.org/abs/2511.12150)
*Yuqi Xie,Shuhan Ye,Yi Yu,Chong Wang,Qixin Zhang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian,Guoqi Li*

Main category: cs.CV

TL;DR: TMKT是一个跨模态训练框架，通过时间步混合策略在RGB和DVS输入之间进行插值，结合轻量级模态感知目标，实现更平滑的知识迁移，提升脉冲神经网络在视觉任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机与脉冲神经网络结合具有能效优势，但事件数据稀缺和DVS输出稀疏性阻碍有效训练。现有RGB到DVS的知识迁移方法因模态间分布差异大而表现不佳。

Method: 提出时间步混合知识迁移框架，包含概率性时间步混合策略，在RGB和DVS输入的不同时间步进行插值，并引入模态感知指导和混合比感知两个轻量级目标来对齐时序特征。

Result: 在多个基准测试和不同SNN骨干网络上的广泛实验表明，该方法能够实现更平滑的知识迁移，缓解训练中的模态不匹配问题，在脉冲图像分类任务中取得优越性能。

Conclusion: TMKT通过时间步混合和模态感知目标有效解决了跨模态知识迁移中的分布差异问题，为事件相机与脉冲神经网络的结合提供了有效的训练框架。

Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.

</details>


### [74] [FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing](https://arxiv.org/abs/2511.12151)
*Kaixiang Yang,Boyang Shen,Xin Li,Yuchen Dai,Yuxuan Luo,Yueran Ma,Wei Fang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FIA-Edit是一个无需反演的图像编辑框架，通过频率交互注意力实现高保真编辑，在计算效率高的同时保持背景和语义一致性，并首次应用于医学图像增强。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的无反演方法虽然效率高，但缺乏有效的源信息整合，导致背景保留差、空间不一致和过度编辑问题。

Method: 提出频率交互注意力框架，包含频率表示交互模块（在自注意力中交换源和目标特征的频率分量）和特征注入模块（在交叉注意力中显式引入源侧查询、键、值和文本嵌入）。

Result: 在RTX 4090上每张512*512图像仅需约6秒，在视觉质量、背景保真度和可控性方面均优于现有方法，并成功应用于医学出血分类的数据增强。

Conclusion: FIA-Edit实现了高效高保真的图像编辑，为医学图像增强开辟了新途径，显著提升了下游分类任务的性能。

Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.

</details>


### [75] [Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function](https://arxiv.org/abs/2511.12162)
*Shuo Yin,Zhiyuan Yin,Yuqing Hou,Rui Liu,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: 提出Center-Reassigned Hashing (CRH)框架，通过动态重新分配预设码本中的哈希中心并联合优化哈希函数，避免了传统两阶段方法的复杂性和性能损失。


<details>
  <summary>Details</summary>
Motivation: 现有的基于哈希中心的方法存在随机初始化忽略类间语义关系的问题，而两阶段方法虽然能缓解这个问题，但引入了额外复杂性、计算开销和阶段间差异导致的次优性能。

Method: CRH采用端到端框架，从预设码本动态重新分配哈希中心，同时联合优化哈希函数，无需显式的中心优化阶段。还引入多头机制增强哈希中心的表示能力。

Result: 在三个基准数据集上的实验表明，CRH能够学习到具有语义意义的哈希中心，并在检索任务中优于现有的深度哈希方法。

Conclusion: CRH通过动态哈希中心重新分配和端到端优化，有效整合了语义关系，提升了哈希检索性能，避免了传统方法的局限性。

Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

</details>


### [76] [Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective](https://arxiv.org/abs/2511.12170)
*Wang Luo,Di Wu,Hengyuan Na,Yinlin Zhu,Miao Hu,Guocong Quan*

Main category: cs.CV

TL;DR: 提出了一种新的点云补全范式Completion-by-Correction，通过从预训练图像到3D模型生成拓扑完整的形状先验，并在特征空间进行校正，实现结构一致且对齐观测的重建。


<details>
  <summary>Details</summary>
Motivation: 传统Completion-by-Inpainting范式由于几何和语义约束有限，常导致结构不一致和拓扑伪影。需要更稳健的补全方法。

Method: 提出PGNet多阶段框架：进行双特征编码以锚定生成先验，合成结构对齐的粗粒度支架，通过分层校正逐步细化几何细节。

Result: 在ShapeNetViPC数据集上，PGNet在平均Chamfer Distance上提升23.5%，F-score提升7.1%，优于现有最优方法。

Conclusion: Completion-by-Correction范式将补全从无约束合成转变为引导细化，能够实现结构一致且观测对齐的重建。

Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).

</details>


### [77] [MixAR: Mixture Autoregressive Image Generation](https://arxiv.org/abs/2511.12181)
*Jinyuan Hu,Jiayou Zhang,Shaobo Cui,Kun Zhang,Guangyi Chen*

Main category: cs.CV

TL;DR: MixAR是一个新颖的自回归图像生成框架，通过混合离散和连续表示来提升生成质量。它利用离散token作为先验指导连续自回归建模，并提出了训练-推理混合策略来保持一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归方法将图像表示为离散token序列，但量化过程和有限码本大小会丢失细粒度信息，限制了生成保真度。虽然连续潜在空间建模能提供更高质量，但连续表示存在于广阔无结构空间中，给高效自回归建模带来挑战。

Method: MixAR采用因子化公式，利用离散token作为连续自回归预测的先验指导。研究了多种离散-连续混合策略：自注意力(DC-SA)、交叉注意力(DC-CA)和简单方法(DC-Mix)。还提出了训练-推理混合(TI-Mix)来弥合训练和推理分布之间的差距。

Result: 实验表明DC-Mix策略在计算效率和生成保真度之间取得了良好平衡，TI-Mix带来了持续改进。

Conclusion: MixAR通过混合离散和连续表示成功解决了连续空间自回归建模的挑战，在保持计算效率的同时显著提升了图像生成质量。

Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.

</details>


### [78] [MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis](https://arxiv.org/abs/2511.12193)
*Abdelrahman Elsayed,Ahmed Jaheen,Mohammad Yaqub*

Main category: cs.CV

TL;DR: MMRINet是一种轻量级脑肿瘤分割架构，使用线性复杂度的Mamba状态空间模型替代二次复杂度的注意力机制，在资源受限环境下实现高效体积上下文建模。


<details>
  <summary>Details</summary>
Motivation: 解决多参数MRI中自动脑肿瘤分割在资源受限环境下的挑战，特别是当深度3D网络计算成本过高时。

Method: 提出MMRINet架构，使用线性复杂度的Mamba状态空间模型；引入双路径特征精炼模块最大化特征多样性；采用渐进特征聚合实现有效的多尺度融合。

Result: 在BraTS-Lighthouse SSA 2025中，模型仅使用约250万参数，达到平均Dice分数0.752和平均HD95 12.23的强性能。

Conclusion: 该模型展示了在低资源临床环境中实现高效准确分割的潜力，适合资源受限的医疗环境部署。

Abstract: Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet.

</details>


### [79] [Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System](https://arxiv.org/abs/2511.12196)
*Aditi Bhalla,Christian Hellert,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 提出一个两阶段跨视角跨模态无监督域适应框架，用于驾驶员活动识别，解决视角变化和域偏移问题，在RGB视频数据上相比基线方法提升50%准确率。


<details>
  <summary>Details</summary>
Motivation: 驾驶员分心是交通事故主要原因，现有方法单独处理跨视角泛化或无监督域适应，缺乏在多样化车辆配置中稳健部署的解决方案。

Method: 两阶段框架：第一阶段使用对比学习在多视角数据中学习视角不变和动作区分特征；第二阶段使用信息瓶颈损失进行跨模态域适应，无需新域标注数据。

Result: 使用Video Swin和MViT视频变换器在Drive&Act数据集上评估，相比监督对比学习方法提升50%准确率，比仅无监督域适应方法提升5%。

Conclusion: 提出的联合框架能有效解决驾驶员活动识别中的跨视角和跨模态挑战，显著提升模型在真实部署中的性能。

Abstract: Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.

</details>


### [80] [Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation](https://arxiv.org/abs/2511.12200)
*Sujun Sun,Haowen Gu,Cheng Xie,Yanxu Ren,Mingwu Ren,Haofeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种分层语义学习框架来解决跨域小样本分割中的语义粒度差距问题，通过双重风格随机化和分层语义挖掘模块增强模型对不同粒度语义的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有跨域小样本分割方法主要关注源域和目标域之间的风格差距，而忽略了分割粒度差距，导致对目标域中新颖类别的语义区分能力不足。

Method: 提出了分层语义学习框架，包含双重风格随机化模块模拟目标域数据风格差异，分层语义挖掘模块利用多尺度超像素挖掘不同粒度下的类内一致性和类间区分性，以及原型置信度调制阈值模块缓解分割模糊问题。

Result: 在四个流行的目标域数据集上进行广泛实验，结果表明该方法达到了最先进的性能。

Conclusion: 所提出的分层语义学习框架有效解决了跨域小样本分割中的语义粒度差距问题，显著提升了模型性能。

Abstract: Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.

</details>


### [81] [OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs](https://arxiv.org/abs/2511.12201)
*Feng Chen,Yefei He,Shaoxuan He,Yuanyu He,Jing Liu,Lequan Lin,Akide Liu,Zhaoyang Li,Jiyuan Zhang,Zhenbang Sun,Bohan Zhuang,Qi Wu*

Main category: cs.CV

TL;DR: OmniSparse是一个训练感知的细粒度稀疏注意力框架，用于长视频多模态大语言模型，在训练和推理中都能实现动态token预算分配，达到与全注意力相当的性能，同时实现2.7倍预填充加速和2.4倍解码内存减少。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏注意力方法主要针对推理时加速，在预定义的稀疏模式下选择关键token，但无法弥合训练-推理差距，且缺乏在查询、键值对和注意力头等多个维度进行细粒度token选择的能力，导致性能次优和加速收益有限。

Method: OmniSparse包含三种自适应互补机制：(1)通过懒惰-活跃分类进行查询选择，保留捕捉广泛语义相似性的活跃查询，丢弃关注有限局部上下文且功能冗余的懒惰查询；(2)基于头级动态预算分配的KV选择，根据最平坦头确定共享预算并统一应用于所有头以确保注意力召回；(3)KV缓存瘦身，根据头级解码查询模式选择性获取视觉KV缓存以减少头级冗余。

Result: 实验结果表明，OmniSparse在保持与全注意力相当性能的同时，在预填充阶段实现了2.7倍加速，在解码阶段实现了2.4倍内存减少。

Conclusion: OmniSparse通过训练感知的细粒度稀疏注意力框架，有效弥合了训练-推理差距，在多维度实现动态token选择，为长视频MLLMs提供了高效的注意力机制解决方案。

Abstract: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

</details>


### [82] [LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image](https://arxiv.org/abs/2511.12202)
*Zhuojiang Cai,Yiheng Zhang,Meitong Guo,Mingdao Wang,Yuwang Wang*

Main category: cs.CV

TL;DR: 提出LSS3D方法，通过可学习空间偏移解决多视图不一致性和非正面输入问题，实现高质量图像到3D生成


<details>
  <summary>Details</summary>
Motivation: 现有基于多视图扩散的3D生成方法存在形状和纹理不对齐问题，导致几何细节不完整和纹理重影，且对倾斜视角输入鲁棒性差

Method: 为每个视图分配可学习空间偏移参数，通过重建网格引导调整各视图实现空间一致性，并加入输入视图作为额外约束增强鲁棒性

Result: 在几何和纹理评估指标上均取得领先结果，对更灵活的输入视角具有良好鲁棒性

Conclusion: LSS3D方法能有效处理多视图不一致性，生成具有完整几何细节和清晰纹理的高质量3D内容

Abstract: Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.

</details>


### [83] [GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction](https://arxiv.org/abs/2511.12204)
*Jiaqi Wu,Yaosen Chen,Shuyuan Zhu*

Main category: cs.CV

TL;DR: 提出Geometry-guided Multi-View Diffusion Model，通过几何信息提取和强度调节机制，解决多视图图像生成中的一致性和高分辨率问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于单图像扩展的多视图生成方法在保持跨视图一致性和生成高分辨率输出方面面临计算挑战，需要更有效的解决方案。

Method: 设计了多视图几何信息提取模块（深度图、法线图、前景分割掩码），解耦几何增强注意力机制，自适应学习策略，迭代细化过程和动态几何信息强度调节机制。

Result: 模型能够生成跨视图一致且细节丰富的图像，在保持结构一致性的同时提升整体图像质量和细节保留。

Conclusion: 该模型通过几何引导的方法有效解决了多视图图像生成中的一致性和质量问题，为3D重建、虚拟现实等应用提供了更可靠的解决方案。

Abstract: Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.

</details>


### [84] [A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR](https://arxiv.org/abs/2511.12206)
*Nishant Vasantkumar Hegde,Aditi Agarwal,Minal Moharir*

Main category: cs.CV

TL;DR: 开发基于AI的自动化交通违规检测系统，使用YOLOv8进行目标检测和EasyOCR进行车牌识别，能够检测头盔违规、摩托车后视镜缺失，并提取车牌号码，提高执法效率和道路安全。


<details>
  <summary>Details</summary>
Motivation: 手动执行头盔法和车辆安全标准检查资源密集且不一致，需要自动化解决方案来提高执法效率和道路安全。

Method: 使用YOLOv8进行目标检测，EasyOCR进行车牌识别，基于自定义标注图像数据集（经过数据增强），开发Streamlit界面进行实时监控和违规记录，采用高级图像预处理技术提升车牌识别效果。

Result: 模型整体精度0.9147，召回率0.886，平均精度(mAP@50)0.843，在更严格的IoU阈值下mAP@50-95为0.503，显示出强大的检测能力。

Conclusion: 该工作展示了一个实用有效的自动化交通规则执法解决方案，讨论了实际部署的考虑因素。

Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.

</details>


### [85] [Mixture of States: Routing Token-Level Dynamics for Multimodal Generation](https://arxiv.org/abs/2511.12207)
*Haozhe Liu,Ding Liu,Mingchen Zhuge,Zijian Zhou,Tian Xie,Sen He,Yukang Yang,Shuming Liu,Yuren Cong,Jiadong Guo,Hongyu Xu,Ke Xu,Kam-Woh Ng,Juan C. Pérez,Juan-Manuel~Pérez-Rúa,Tao Xiang,Wei Liu,Shikun Liu,Jürgen Schmidhuber*

Main category: cs.CV

TL;DR: MoS是一种新颖的多模态扩散模型融合范式，通过可学习的token-wise路由器在模态间创建状态交互，实现高效的多模态融合


<details>
  <summary>Details</summary>
Motivation: 现有方法在模态融合方面存在效率问题，需要更灵活、计算高效的多模态扩散模型融合方法

Method: 使用可学习的token-wise路由器稀疏选择top-k隐藏状态，采用ε-greedy训练策略，创建去噪时间步和输入相关的模态交互

Result: 在文本到图像生成和编辑任务中达到最先进水平，仅用3B-5B参数即可匹配或超越4倍大的模型

Conclusion: MoS为多模态扩散模型提供了一种灵活且计算高效的扩展范式

Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $ε$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.

</details>


### [86] [FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention](https://arxiv.org/abs/2511.12215)
*Peng Zhang,Zhihui Lai,Wenting Chen,Xu Wu,Heng Kong*

Main category: cs.CV

TL;DR: 提出了FaNe框架，通过语义感知的正样本挖掘、文本条件稀疏注意力池化和硬负样本感知对比损失，解决医学视觉语言预训练中的假阴性和细粒度对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言预训练方法存在由语义相似文本引起的假阴性问题，以及细粒度跨模态对齐不足的局限性。

Method: 1. 基于文本-文本相似度的语义感知正样本挖掘策略；2. 文本条件稀疏注意力池化模块实现细粒度图像-文本对齐；3. 硬负样本感知对比损失增强模态内区分度。

Result: 在五个下游医学影像基准测试中，FaNe在图像分类、目标检测和语义分割任务上均达到最先进性能。

Conclusion: FaNe框架有效解决了医学视觉语言预训练中的假阴性和细粒度对齐问题，验证了该方法的有效性。

Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.

</details>


### [87] [Suppressing VLM Hallucinations with Spectral Representation Filtering](https://arxiv.org/abs/2511.12220)
*Ameen Ali,Tamim Zoabi,Lior Wolf*

Main category: cs.CV

TL;DR: SRF是一种无需训练的后处理方法，通过分析特征协方差结构来抑制视觉语言模型中的幻觉现象，无需修改架构或增加推理开销。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型经常产生幻觉描述，过度依赖语言先验和跨模态对齐不精确是主要原因。

Method: 通过特征协方差矩阵的特征分解识别幻觉模式，使用软谱滤波器在深层投影权重中衰减这些模式，均衡特征方差同时保持语义保真度。

Result: 在LLaVA-1.5、MiniGPT-4和mPLUG-Owl2等模型上，SRF在MSCOCO、POPE-VQA等基准测试中持续降低幻觉率，达到最先进的忠实度且不降低描述质量。

Conclusion: SRF是一种轻量级、无需训练的有效方法，能够显著减少视觉语言模型的幻觉问题，同时保持零推理开销和无需架构修改的优势。

Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

</details>


### [88] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: 提出了DHMI，首个针对深度哈希的扩散模型反演框架，能够在黑盒设置下成功重建高分辨率、高质量的图像，揭示了深度哈希系统的严重隐私风险。


<details>
  <summary>Details</summary>
Motivation: 深度哈希虽然提高了检索效率，但引入了严重的隐私风险——能够从哈希码重建原始训练数据，可能导致生物特征伪造和隐私泄露。然而针对深度哈希的模型反演攻击尚未被探索。

Method: DHMI首先对辅助数据集进行聚类得到语义哈希中心作为替代锚点，然后引入替代引导的去噪优化方法，利用融合分类一致性和哈希接近度的新攻击指标动态选择候选样本，通过替代模型集群指导候选样本的细化。

Result: 在多个数据集上的实验表明，DHMI即使在最具挑战性的黑盒设置下（无法获取训练哈希码）也能成功重建高分辨率、高质量图像，在黑盒场景下优于现有最先进的模型反演攻击方法。

Conclusion: DHMI证实了深度哈希系统存在严重的隐私风险，提出的方法具有实际有效性，为深度哈希模型的安全性评估提供了重要参考。

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [89] [MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection](https://arxiv.org/abs/2412.15925)
*Andrea Moglia,Elia Clement Nastasio,Luca Mainardi,Pietro Cerveri*

Main category: cs.CV

TL;DR: 本文提出MiniGPT-Pancreas，一个多模态大语言模型，通过整合视觉和文本信息来支持临床医生进行胰腺癌诊断。该模型在胰腺检测、肿瘤分类和肿瘤检测任务上表现良好，但在胰腺肿瘤检测方面仍需改进。


<details>
  <summary>Details</summary>
Motivation: 胰腺放射成像具有挑战性，因为胰腺体积小、边界模糊，且在不同患者中形状和位置存在变异性。需要开发智能工具来支持临床医生的胰腺癌诊断工作。

Method: 对通用多模态大语言模型MiniGPT-v2进行级联微调，使用来自NIH和MSD数据集的CT扫描和多模态提示（结合问题和图像）进行胰腺检测、肿瘤分类和肿瘤检测。

Result: 在胰腺检测任务上，NIH和MSD数据集的IoU分别为0.595和0.550；胰腺癌分类任务的准确率、精确率和召回率分别为0.876、0.874和0.878；多器官检测中胰腺的IoU为0.497；胰腺肿瘤检测的IoU为0.168。

Conclusion: MiniGPT-Pancreas是支持临床医生进行胰腺肿瘤图像分类的有前景的解决方案，但在检测任务特别是胰腺肿瘤检测方面需要进一步改进。

Abstract: Problem: Pancreas radiological imaging is challenging due to the small size, blurred boundaries, and variability of shape and position of the organ among patients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large Language Model (MLLM), as an interactive chatbot to support clinicians in pancreas cancer diagnosis by integrating visual and textual information. Methods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way for pancreas detection, tumor classification, and tumor detection with multimodal prompts combining questions and computed tomography scans from the National Institute of Health (NIH), and Medical Segmentation Decathlon (MSD) datasets. The AbdomenCT-1k dataset was used to detect the liver, spleen, kidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over Union (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD datasets, respectively. For the pancreas cancer classification task on the MSD dataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878, respectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for multi-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney, 0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor detection task, the IoU score was 0.168 on the MSD dataset. Conclusions: MiniGPT-Pancreas represents a promising solution to support clinicians in the classification of pancreas images with pancreas tumors. Future research is needed to improve the score on the detection task, especially for pancreas tumors.

</details>


### [90] [Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets](https://arxiv.org/abs/2511.12255)
*Huy M. Le,Dat Tien Nguyen,Phuc Binh Nguyen,Gia-Bao Le-Tran,Phu Truong Thien,Cuong Dinh,Minh Nguyen,Nga Nguyen,Thuy T. N. Nguyen,Huy Gia Ngo,Tan Nhat Nguyen,Binh T. Nguyen,Monojit Choudhury*

Main category: cs.CV

TL;DR: Fusionista2.0是一个优化的视频检索系统，通过重新设计核心模块和用户界面，将检索时间减少75%，同时提高准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: Video Browser Showdown (VBS)挑战赛要求系统在严格时间限制下提供准确结果，需要开发快速高效的视频检索系统。

Method: 重新设计核心模块：使用ffmpeg快速提取关键帧，Vintern-1B-v3.5进行多语言OCR，faster-whisper实现实时语音识别，轻量级视觉语言模型进行问答。同时改进用户界面，提升响应性和可用性。

Result: 检索时间减少高达75%，准确性和用户满意度均得到提升，证明系统在大规模视频搜索中具有竞争力。

Conclusion: Fusionista2.0是一个高效、用户友好的视频检索系统，在速度和准确性方面表现出色，适合大规模视频搜索应用。

Abstract: The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.

</details>


### [91] [Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment](https://arxiv.org/abs/2511.12256)
*Tolga Demiroglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 基于MedSigLIP的提示条件框架，通过FiLM和多尺度池化注入文本先验，在LDCTIQA2023挑战赛上取得优异性能，超越已发表的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 开发能够根据临床意图进行条件化学习的数据高效方法，实现快速适应医学图像质量评估任务。

Method: 使用提示条件框架，通过Feature-wise Linear Modulation注入文本先验，结合全局、局部和纹理感知的多尺度池化，采用轻量级MLP融合不同回归头，使用成对排序损失进行训练。

Result: 在LDCTIQA2023数据集（1000张训练图像）上取得PLCC=0.9575、SROCC=0.9561、KROCC=0.8301的优异性能，超越已发表的最佳挑战赛提交结果。

Conclusion: 提出的提示引导方法在医学图像质量评估任务中表现出色，证明了文本先验和多尺度特征融合的有效性。

Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.

</details>


### [92] [A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation](https://arxiv.org/abs/2511.12259)
*Puzhen Wu,Hexin Dong,Yi Lin,Yihao Ding,Yifan Peng*

Main category: cs.CV

TL;DR: 提出了一种新颖的双阶段疾病感知框架用于胸部X光报告生成，通过疾病感知语义标记和视觉语言对齐，显著提高了临床准确性和语言质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在医学图像分析中缺乏足够的疾病感知能力和视觉语言对齐，导致忽略关键病理特征和生成临床不准确的报告。

Method: 双阶段疾病感知框架：第一阶段学习疾病感知语义标记并通过对比学习对齐视觉语言表示；第二阶段引入疾病-视觉注意力融合模块和双模态相似性检索机制。

Result: 在多个基准数据集上的实验表明，该框架在胸部X光报告生成方面达到了最先进的性能，临床准确性和语言质量显著提升。

Conclusion: 所提出的疾病感知框架有效解决了现有方法的局限性，为医学图像报告生成提供了更准确和可靠的解决方案。

Abstract: Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.

</details>


### [93] [CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2511.12263)
*Jingyao Li,Jingyun Wang,Molin Tan,Haochen Wang,Cilin Yan,Likun Shi,Jiayin Cai,Xiaolong Jiang,Yao Hu*

Main category: cs.CV

TL;DR: CrossVid是首个专门评估多模态大语言模型在跨视频推理能力上的基准测试，包含4个高层维度和10个具体任务，提供5,331个视频和9,015个问答对，实验显示当前MLLMs在跨视频推理方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准主要关注单视频分析，无法评估MLLMs同时推理多个视频的能力。虽然近期有评估多视角视频的基准，但其有限任务无法全面评估MLLMs在多样化真实跨视频场景中的表现。

Method: 构建CrossVid基准，包含广泛的分层任务（4个高层维度、10个具体任务），提供5,331个视频和9,015个挑战性问答对，涵盖单选、多选和开放式问题格式。

Result: 在各类开源和闭源MLLMs上的广泛实验表明，Gemini-2.5-Pro表现最佳，平均准确率为50.4%。深入案例研究显示大多数当前MLLMs在跨视频推理任务上表现挣扎，主要原因是无法整合或比较分布在多个视频中的证据进行推理。

Conclusion: CrossVid基准有潜力指导未来增强MLLMs跨视频推理能力的发展，当前MLLMs在跨视频推理方面仍有显著改进空间。

Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.

</details>


### [94] [ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks](https://arxiv.org/abs/2511.12267)
*Ruixun Liu,Bowen Fu,Jiayi Song,Kaiyu Li,Wanchen Li,Lanxuan Xue,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 提出ZoomEarth框架，通过主动感知范式在超高分辨率遥感图像处理中实现自适应裁剪缩放，在LRS-GRO基准和三个公共基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态分辨率和token剪枝方法受限于被动感知范式，在获取更精细视觉输入时会产生冗余。需要探索主动感知范式，使模型能够重新访问信息丰富区域。

Method: 提出ZoomEarth自适应裁剪缩放框架，使用区域引导奖励提供细粒度指导。通过监督微调和组相对策略优化进行训练。

Result: 在LRS-GRO基准和三个公共超高分辨率遥感基准的零样本设置中达到最先进性能。能够通过简单工具接口与下游模型无缝集成。

Conclusion: ZoomEarth展示了强大的多功能性和可扩展性，能够应用于云去除、去噪、分割和图像编辑等任务。

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.

</details>


### [95] [TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.12270)
*Yaxuan Jiao,Qing Xu,Yuxiang Luo,Xiangjian He,Zhen Chen,Wenting Duan*

Main category: cs.CV

TL;DR: TM-UNet是一个轻量级医学图像分割框架，通过多尺度token-memory机制实现高效全局推理，在显著降低计算成本的同时超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于transformer的方法在医学图像分割中取得了显著成果，但其高计算成本阻碍了临床部署，需要开发更高效的轻量级解决方案。

Method: 提出多尺度token-memory(MSTM)块，将2D空间特征转换为token序列，利用矩阵记忆单元选择性保留和传播判别性上下文信息，通过指数门控识别token有效性，并行池化操作实现多尺度上下文提取。

Result: 在多种医学分割任务中超越最先进方法，同时大幅降低计算成本。

Conclusion: TM-UNet通过创新的token-memory机制实现了高效的全局推理，为医学图像分割提供了一种计算效率高的解决方案。

Abstract: Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.

</details>


### [96] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: D³ToM是一种用于加速扩散多模态大语言模型推理的方法，通过动态合并冗余视觉令牌来减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 扩散多模态大语言模型虽然生成能力强，但推理速度慢，因为每个去噪步骤都需要对整个序列进行双向自注意力计算，计算复杂度为立方级，在处理数千个视觉令牌时变得不实用。

Method: 提出D³ToM方法，使用前一步生成的决策令牌构建重要性图，保留最显著的令牌，通过相似性聚合合并冗余令牌。这是一个即插即用模块，集成在单个Transformer层中，物理上缩短视觉令牌序列而不改变模型参数。

Result: 广泛实验表明D³ToM在保持竞争性能的同时显著加速推理。

Conclusion: D³ToM通过动态令牌合并有效解决了扩散多模态大语言模型的推理效率问题，实现了在同等计算预算下的优越性能。

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [97] [One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving](https://arxiv.org/abs/2511.12291)
*Andrea Bertogalli,Giacomo Boracchi,Luca Magri*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态外参标定框架，可同时估计事件相机、LiDAR和RGB相机之间的相对位姿，特别关注具有挑战性的事件相机标定问题。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等复杂视觉系统中，精确的多传感器对齐至关重要。现有方法通常依赖分离的成对标定，需要一种能够同时标定多种传感器的一体化解决方案。

Method: 设计并构建了新型3D标定靶标，包含平面特征、ChArUco图案和主动LED模式，分别针对LiDAR、RGB相机和事件相机的特性。实现一次性联合外参标定过程。

Result: 在定制数据集上的广泛实验评估验证了方法的准确性和鲁棒性，该数据集使用先进的自动驾驶传感器设置记录。

Conclusion: 该方法提供了一种准确、鲁棒的多传感器标定解决方案，特别适用于自动驾驶等需要精确多传感器对齐的应用场景。

Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.

</details>


### [98] [Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method](https://arxiv.org/abs/2511.12301)
*Chi Liu,Jincheng Liu,Congcong Zhu,Minghao Wang,Sheng Shen,Jia Gu,Tianqing Zhu,Wanlei Zhou*

Main category: cs.CV

TL;DR: 本文提出了频率重校准(FreRec)方法来解决医学图像生成数据增强中的频率失配问题，通过统计高频替换和重建高频映射来改善合成图像质量，提升下游分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学AI开发面临数据稀缺问题，生成数据增强(GDA)可以合成医学图像，但存在偏见风险，特别是频率失配可能导致不可靠的GDA并损害下游任务。

Method: 提出FreRec方法，包含：(1)统计高频替换(SHR)粗略对齐高频分量；(2)重建高频映射(RHM)提升图像质量并重建高频细节。这是一个与任何生成模型兼容的后处理步骤。

Result: 在脑部MRI、胸部X光、眼底图像等多种医学数据集上的实验表明，FreRec相比未校准的AI合成样本显著提高了下游医学图像分类性能。

Conclusion: FreRec是一种独立的频率重校准方法，能有效减少频率分布差异，改善生成数据增强的可靠性，可无缝集成到常见的医学GDA流程中。

Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.

</details>


### [99] [LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors](https://arxiv.org/abs/2511.12304)
*Qifeng Chen,Jiarun Liu,Rengan Xie,Tao Tang,Sicong Du,Yiru Zhao,Yuchi Huo,Sheng Yang*

Main category: cs.CV

TL;DR: LiDAR-GS++是一种基于高斯泼溅的LiDAR重建方法，通过扩散先验增强，在公共城市道路上实现实时高保真重模拟，解决了单次扫描重建不完整导致的插值视图伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于高斯泼溅的LiDAR渲染方法在插值新视图合成中会出现伪影，这主要是由于单次遍历扫描的重建不完整所致。

Method: 提出LiDAR-GS++方法，使用可控的LiDAR生成模型，以粗略插值渲染为条件生成额外的几何一致扫描，并采用有效的蒸馏机制进行扩展重建。

Result: 在多个公共数据集上的实验表明，LiDAR-GS++在插值和插值视点上都达到了最先进的性能，超越了现有的基于高斯泼溅和NeRF的方法。

Conclusion: 通过将重建扩展到欠拟合区域，该方法确保了插值新视图的全局几何一致性，同时保留了传感器捕获的详细场景表面。

Abstract: Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.

</details>


### [100] [Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 提出了一种简单有效的框架，为前馈分类器添加时间推理能力，无需修改模型架构或引入循环模块。通过SEQ学习范式构建时间连贯轨迹，学习类别特定的时间原型，并使用可微soft-DTW损失对齐预测序列。


<details>
  <summary>Details</summary>
Motivation: 现实世界视觉数据通常随时间逐渐演变，但传统分类器假设时间独立性，无法捕捉这种动态特性。需要在不改变模型架构的情况下为分类器添加时间推理能力。

Method: 采用支持-示例-查询(SEQ)学习范式，将训练数据组织成时间连贯轨迹。学习类别特定时间原型，使用可微soft-DTW损失对齐预测序列，并通过多目标函数促进语义一致性和时间平滑性。

Result: 该方法在静态和时序任务中都表现优异：提高了细粒度和超细粒度图像分类性能，在视频异常检测中提供精确且时间一致的预测。

Conclusion: 该框架通过损失设计引入强时间归纳偏置，以模块化和数据高效的方式桥接静态和时序学习，仅需在预提取特征上使用简单分类器。

Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.

</details>


### [101] [SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models](https://arxiv.org/abs/2511.12331)
*Sepehr Kazemi Ranjbar,Kumail Alhamoud,Marzyeh Ghassemi*

Main category: cs.CV

TL;DR: 提出了一种无需训练的框架，将否定建模为联合嵌入空间中的子空间而非单点，通过构建球形区域来同时接近肯定描述和远离否定描述，显著提升了视觉语言模型的否定理解能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理否定提示时表现不佳，现有方法通过微调大型否定数据集会损害模型在肯定提示上的零样本性能，需要一种能保持零样本性能的否定理解方法。

Method: 将否定建模为嵌入空间中的子空间，为肯定描述A和否定描述N构建两个球形区域，通过寻找既接近A又远离N的中心方向来匹配图像。

Result: 在检索、多项选择和文本到图像任务中，该方法将否定理解能力平均提升了约30%，缩小了肯定提示和否定提示之间的性能差距，同时保持了零样本性能。

Conclusion: 该方法无需训练即可有效提升视觉语言模型的否定理解能力，在保持零样本性能的同时解决了否定处理的问题，为视觉语言模型的理解能力提供了新的思路。

Abstract: Vision-Language Models (VLMs) struggle with negation. Given a prompt like "retrieve (or generate) a street scene without pedestrians," they often fail to respect the "not." Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as "A but not N," we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.

</details>


### [102] [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)
*Xiaoyu Lin,Aniket Ghorpade,Hansheng Zhu,Justin Qiu,Dea Rrozhani,Monica Lama,Mick Yang,Zixuan Bian,Ruohan Ren,Alan B. Hong,Jiatao Gu,Chris Callison-Burch*

Main category: cs.CV

TL;DR: DenseAnnotate是一个音频驱动的在线标注平台，能够高效创建图像和3D资产的密集细粒度标注，解决了传统文本标注在表达性、速度和视觉特征捕捉方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的训练数据主要依赖稀疏的互联网挖掘或手动输入标注，这些方法只能捕捉图像视觉内容的一小部分。密集标注更有价值但稀缺，传统文本标注流程在表达性、速度和视觉特征捕捉方面存在不足，特别是在多文化图像和3D资产标注等专业领域。

Method: 开发了DenseAnnotate平台，采用音频驱动的方法，标注者边观察边口头描述，同时将口语短语与图像区域或3D场景部分同步链接。平台整合了语音转文字转录和注意力区域标记功能。

Result: 通过超过1,000名标注者参与的案例研究，创建了包含3,531张图像、898个3D场景和7,460个3D对象的多模态数据集，包含20种语言的音频对齐密集标注，包括8,746个图像标题、2,000个场景标题和19,000个对象标题。基于该数据集训练的模型在多语言能力上提升5%，文化对齐能力提升47%，3D空间能力提升54%。

Conclusion: DenseAnnotate平台为未来视觉语言研究提供了可行的标注方法，能够应用于各种任务和多样化的数据类型，有效解决了密集标注稀缺的问题。

Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.

</details>


### [103] [Ground Plane Projection for Improved Traffic Analytics at Intersections](https://arxiv.org/abs/2511.12342)
*Sajjad Pakdamansavoji,Kumar Vaibhav Jha,Baher Abdulhai,James H Elder*

Main category: cs.CV

TL;DR: 该论文探索了将基础设施摄像头检测到的车辆反投影到地面平面进行3D坐标分析的优势，发现在单摄像头系统中反投影能提供更准确的轨迹分类和转向运动计数，多摄像头系统的弱融合能进一步提高准确性。


<details>
  <summary>Details</summary>
Motivation: 交叉口准确的转向运动计数对于信号控制、交通管理和城市规划至关重要。传统基于图像平面的计算机视觉系统存在局限性，需要探索在真实世界3D坐标中分析交通的优势。

Method: 将基础设施摄像头检测到的车辆反投影到地面平面进行3D坐标分析，比较单摄像头和多摄像头系统的弱融合方法。

Result: 单摄像头系统的反投影方法比图像平面分析提供更准确的轨迹分类和转向运动计数；多摄像头系统的弱融合能进一步提高准确性。

Conclusion: 交通分析应该在地面平面进行，而不是在图像平面进行。

Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane

</details>


### [104] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: 提出结合大语言模型和网格整数规划的自动化室内设计框架，联合优化房间布局和家具摆放。


<details>
  <summary>Details</summary>
Motivation: 解决现有两阶段设计流程在解决方案质量和计算效率方面的不足，实现更优化的室内设计自动化。

Method: 使用LLM提取结构化设计约束，编码到基于网格的统一表示中，采用粗到细优化策略，从低分辨率网格开始逐步细化。

Result: 在多样化场景中，联合优化方法显著优于现有两阶段设计流程，通过粗到细策略实现了显著的计算效率提升。

Conclusion: 该框架有效整合了LLM的语义理解能力和数学优化方法，为自动化室内设计提供了高质量且高效的解决方案。

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>


### [105] [CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification](https://arxiv.org/abs/2511.12346)
*Asmit Bandyopadhyay,Anindita Das Bhattacharjee,Rakesh Das*

Main category: cs.CV

TL;DR: 提出CLAReSNet混合架构，结合多尺度卷积和变换器注意力，通过自适应潜在瓶颈降低复杂度，在高光谱图像分类中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高光谱维度、复杂光谱-空间相关性和训练样本有限且类别不平衡等挑战，现有CNN和变换器方法存在二次复杂度和归纳偏差不足的问题。

Method: CLAReSNet集成多尺度卷积提取与变换器式注意力，使用多尺度卷积主干、增强卷积块注意力模块，结合双向RNN和多尺度光谱潜在注意力(MSLA)，通过自适应潜在令牌分配将复杂度从O(T²D)降至O(Tlog(T)D)。

Result: 在Indian Pines和Salinas数据集上分别达到99.71%和99.96%的总体准确率，显著超越HybridSN、SSRN和SpectralFormer等现有方法。

Conclusion: CLAReSNet在有限样本和严重类别不平衡条件下有效，学习到的嵌入表现出优异的类间可分离性和紧凑的类内聚类。

Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.

</details>


### [106] [Explainable AI-Generated Image Detection RewardBench](https://arxiv.org/abs/2511.12363)
*Michael Yang,Shijian Deng,William T. Doan,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CV

TL;DR: 提出了第一个评估多模态大语言模型判断AI生成图像检测解释质量的基准XAIGID-RewardBench，发现当前最佳模型与人类性能存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 传统基于分类的AI生成图像检测方法无法提供人类专家可理解的解释，降低了检测工具的可信度和说服力。使用MLLM作为评判者来评估其他MLLM生成的解释质量的方法尚未得到充分研究。

Method: 构建包含约3000个标注三元组的基准数据集，涵盖多种图像生成模型和MLLM作为检测器，评估MLLM作为奖励模型（评判者）的能力。

Result: 当前最佳奖励模型在该基准上得分88.76%，而人类标注者间一致性达到98.30%，表明当前MLLM与人类水平推理能力存在明显差距。

Conclusion: 需要进一步提升MLLM的推理能力以缩小与人类性能的差距，并分析了这些模型常见的错误类型。

Abstract: Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.

</details>


### [107] [Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.12365)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: DT-R1是一个强化学习框架，通过训练大语言模型构建视觉输入的数字孪生表示，并基于这些表示进行统一的可视推理，在多种任务和模态上超越特定任务模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法依赖特定任务的监督微调和架构设计，缺乏统一解决方案，限制了跨任务和跨模态的泛化能力。

Method: 使用GRPO强化学习训练大语言模型构建数字孪生表示，并通过验证结构完整性和输出准确性的新奖励函数进行训练。

Result: 在涵盖两种模态和四种任务类型的六个视觉推理基准测试中，DT-R1始终优于最先进的特定任务模型。

Conclusion: DT-R1开创了通过数字孪生表示的强化学习实现视觉推理的新方向。

Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.

</details>


### [108] [Fast Reasoning Segmentation for Images and Videos](https://arxiv.org/abs/2511.12368)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: FastReasonSeg是一种高效的推理分割方法，通过数字孪生表示将感知与推理解耦，使用监督微调和强化学习进行知识蒸馏，在保持高性能的同时大幅减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法需要数十亿参数的多模态大语言模型，超出了边缘设备的计算能力。现有蒸馏方法无法有效传递多步推理能力，需要新的蒸馏策略。

Method: 使用数字孪生表示将感知与推理解耦，先通过教师模型生成的推理链进行监督微调，然后使用联合奖励（分割准确性和推理质量对齐）进行强化微调。

Result: 在JiTBench、RVTBench、ReasonSeg和LLM-Seg40K四个基准测试中达到最先进性能，0.6B参数变体优于参数多20倍的模型，实现7.79 FPS吞吐量和仅2.1GB内存消耗。

Conclusion: FastReasonSeg实现了高效的实时推理分割，能够在资源受限环境中部署，为自主代理在真实环境中运行提供支持。

Abstract: Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.

</details>


### [109] [Changes in Real Time: Online Scene Change Detection with Multi-View Fusion](https://arxiv.org/abs/2511.12370)
*Chamuditha Jayanga Galappaththige,Jason Lai,Lloyd Windrim,Donald Dansereau,Niko Sünderhauf,Dimity Miller*

Main category: cs.CV

TL;DR: 提出首个姿态无关、无标签、保持多视角一致性的在线场景变化检测方法，在10+FPS下超越现有离线方法性能


<details>
  <summary>Details</summary>
Motivation: 现有在线场景变化检测方法精度远低于离线方法，需要开发能在无约束视角下实时检测相关变化的高性能在线方法

Method: 引入自监督融合损失从多线索推断变化、PnP快速姿态估计和基于变化指导的3D高斯泼溅场景表示快速更新策略

Result: 在复杂真实数据集上超越所有在线和离线基线方法，达到新的最先进性能

Conclusion: 该方法首次实现了高性能的在线场景变化检测，在保持实时性的同时超越了离线方法的精度

Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.

</details>


### [110] [Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models](https://arxiv.org/abs/2511.12371)
*Yiqing Shen,Chenxiao Fan,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: 提出了推理文本到视频检索的新范式，通过数字孪生表示和大型语言模型推理处理隐式查询，在基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频检索方法只能处理显式查询，无法应对需要推理的隐式查询，限制了实际应用能力。

Method: 采用两阶段框架：首先通过数字孪生表示进行组合对齐识别候选视频，然后使用大型语言模型进行推理并调用专业模型填补信息空白。

Result: 在ReasonT2VBench-135上达到81.2% R@1，比最强基线提升超过50个百分点；在三个传统基准测试中均取得最先进结果。

Conclusion: 通过数字孪生表示和大型语言模型推理，成功解决了隐式查询的文本到视频检索问题，显著提升了检索性能。

Abstract: The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).

</details>


### [111] [AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification](https://arxiv.org/abs/2511.12382)
*Ansh Makwe,Akansh Agrawal,Prateek Jain,Akshan Agrawal,Priyanka Bagade*

Main category: cs.CV

TL;DR: AGGRNet框架通过提取信息性和非信息性特征来区分医学图像中的细微类别，在复杂医学图像分析任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有注意力模型在医学图像分类中难以有效区分细微类别，因为它们难以捕捉类间相似性和类内变异性，导致错误诊断。

Method: 提出AGGRNet框架，提取信息性和非信息性特征以理解细粒度视觉模式，改进复杂医学图像分析的分类性能。

Result: 在多个医学影像数据集上实现最先进性能，在Kvasir数据集上比现有最佳模型提升高达5%。

Conclusion: AGGRNet能有效解决医学图像分析中复杂任务面临的挑战，显著提升分类准确性。

Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

</details>


### [112] [Leveraging Quantum-Based Architectures for Robust Diagnostics](https://arxiv.org/abs/2511.12386)
*Shabnam Sodagari,Tommy Long*

Main category: cs.CV

TL;DR: 本研究使用混合量子-经典框架，结合预训练的ResNet50编码器和量子卷积神经网络，通过CT图像诊断和区分肾结石、囊肿和肿瘤，在8量子比特和12量子比特配置下均达到99%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的优势提升医学图像诊断性能，特别是针对肾脏疾病的CT图像分类问题。

Method: 使用预处理的肾脏CT图像，通过ResNet50提取特征，然后用量子角编码将特征转换为量子比特，最后由量子卷积神经网络进行分类。

Result: 两种量子比特配置均快速收敛且性能稳定，12量子比特配置在囊肿检测中实现完美召回率，肿瘤F1分数达到0.9956，总体测试准确率为0.99。

Conclusion: 将经典预处理和深度特征提取与量子电路相结合，能够显著提升医学诊断性能。

Abstract: The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.

</details>


### [113] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: 提出了一个轻量级推理时间框架，通过解耦深度特征空间中的偶然不确定性和认知不确定性，实现无需采样、集成或额外前向传播的不确定性估计，显著减少计算成本并提高预测区间精度。


<details>
  <summary>Details</summary>
Motivation: 传统估计器将所有不确定性模式压缩为单一置信度分数，无法可靠地指导何时分配更多计算资源或调整推理过程，限制了实际视觉推理的自适应能力。

Method: 使用正则化全局密度模型估计偶然不确定性，认知不确定性由三个正交组件构成：局部支持不足、流形谱塌陷和跨层特征不一致性，无需采样、集成或额外前向传播。

Result: 在MOT17上减少约60%计算量且精度损失可忽略，分布自由共形校准产生显著更紧密的预测区间，消融实验显示正交不确定性分解在所有序列上均获得更高计算节省，比总不确定性基线提高13.6个百分点。

Conclusion: 该框架实现了实用的自调节视觉推理，通过解耦的不确定性指导自适应模型选择，显著提升计算效率同时保持性能。

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [114] [MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting](https://arxiv.org/abs/2511.12400)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: MSLoRA是一种与主干网络无关的参数高效适配器，通过重新加权特征响应而不是重新调优主干网络，统一了CNN和ViT的适应方法，仅需不到5%的主干参数即可提升分类、检测和分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩适应方法主要局限于视觉变换器(ViTs)，难以跨架构泛化。需要一种能够统一适应卷积神经网络(CNNs)和ViTs的通用方法。

Method: 结合低秩线性投影和多尺度非线性变换，通过点乘和残差连接融合这两个组件，联合调制空间和通道注意力，保持预训练权重冻结。

Result: 在分类、检测和分割任务上持续提升迁移性能，参数效率高（少于5%主干参数），优化稳定，收敛快速，跨架构泛化能力强。

Conclusion: MSLoRA通过重新加权而非重新调优，为冻结视觉主干的高效适应提供了一种简单通用的方法。

Abstract: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.

</details>


### [115] [VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving](https://arxiv.org/abs/2511.12405)
*Hyunki Seong,Seongwoo Moon,Hojin Ahn,Jehun Kang,David Hyunchul Shim*

Main category: cs.CV

TL;DR: VLA-R是一个开放世界端到端自动驾驶框架，通过整合开放世界感知和视觉-动作检索范式，在非结构化户外环境中实现强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶在非结构化户外环境中经常遇到训练时未见过的条件，需要强大的泛化能力来处理开放世界场景。

Method: 使用冻结的视觉语言模型进行开放世界检测和分割，通过Q-Former瓶颈聚合细粒度视觉表示与语言对齐的视觉特征，并引入视觉-动作对比学习方案来对齐视觉语言和动作嵌入。

Result: 在真实世界机器人平台上的实验表明，即使在数据有限的情况下，该方法在非结构化、未见过的环境中表现出强大的泛化和探索性能。

Conclusion: VLA-R框架成功地将开放世界感知与视觉-动作检索相结合，为端到端自动驾驶在开放世界场景中的泛化问题提供了有效解决方案。

Abstract: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.

</details>


### [116] [Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection](https://arxiv.org/abs/2511.12410)
*Xi Xiao,Zhuxuanzi Wang,Mingqiao Mo,Chen Liu,Chenrui Ma,Yanshu Li,Smita Krishnaswamy,Xiao Wang,Tianyang Wang*

Main category: cs.CV

TL;DR: PROBE是一个自监督框架，通过视觉探测目标域无需标签，解决了路面缺陷检测中的跨域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 自动化路面缺陷检测面临跨域泛化差的问题，监督检测器需要昂贵的重新标注，而标准自监督方法对域偏移仍然脆弱。

Method: 引入自监督提示增强模块(SPEM)从无标签目标数据中提取缺陷感知提示来指导冻结的ViT骨干网络，以及域感知提示对齐(DAPA)目标来对齐提示条件下的源域和目标域表示。

Result: 在四个挑战性基准测试中，PROBE始终优于强监督、自监督和适应基线方法，实现了鲁棒的零样本迁移、改进的域变化弹性以及少样本适应的高数据效率。

Conclusion: 自监督提示是构建可扩展和自适应视觉检测系统的实用方向。

Abstract: The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main

</details>


### [117] [Towards Rotation-only Imaging Geometry: Rotation Estimation](https://arxiv.org/abs/2511.12415)
*Xinrui Li,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: 提出了一种基于旋转流形的仅旋转优化框架，用于运动恢复结构任务，通过将平移表示为旋转的函数，在旋转流形上进行优化，实现了优于现有旋转估计方法的性能。


<details>
  <summary>Details</summary>
Motivation: 运动恢复结构是计算机视觉中的关键任务，近期仅姿态成像几何学通过姿态调整展示了更好的性能。本文延续仅姿态视角，探索场景结构、旋转和平移之间的关键关系。

Method: 将平移表示为旋转的函数，将成像几何表示压缩到旋转流形上，提出了基于重投影误差的仅旋转优化框架，适用于两视图和多视图场景。

Result: 实验结果表明，该方法在精度和鲁棒性方面优于当前最先进的旋转估计方法，甚至可与多次束调整迭代结果相媲美。

Conclusion: 这项工作有望为更准确、高效和可靠的3D视觉计算做出贡献。

Abstract: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.

</details>


### [118] [Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance](https://arxiv.org/abs/2511.12419)
*Wenjie Li,Jinglei Shi,Jin Han,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: DHGM是一个基于扩散模型的高频引导模型，用于同时去除雨纹和增强图像细节，解决传统方法中天气去除与超分辨率之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 真实世界图像常受恶劣天气影响而退化，现有的天气恢复方法可能会牺牲对分析小物体至关重要的高频细节。简单地级联天气去除和超分辨率方法存在内在冲突：去除旨在消除高频天气噪声，而超分辨率则要从现有细节中生成高频纹理。

Method: 提出DHGM模型，集成预训练的扩散先验与高通滤波器，同时去除雨纹伪影并增强结构细节。

Result: 大量实验表明，DHGM在性能上优于现有方法，且成本更低。

Conclusion: DHGM能够有效生成干净的高分辨率图像，解决了天气去除与超分辨率之间的冲突问题。

Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.

</details>


### [119] [MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation](https://arxiv.org/abs/2511.12422)
*Nuolin Sun,Linyuan Wang,Haonan Wei,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: MFI-ResNet通过压缩-扩展策略结合MeanFlow模块和ResNet，在减少参数的同时提高分类准确率，展示了生成式流场在判别式学习中的有效性。


<details>
  <summary>Details</summary>
Motivation: 受ResNet与ODE的关联以及MeanFlow一步生成模型的启发，希望利用生成式流场来改进ResNet的参数效率和判别性能。

Method: 采用压缩-扩展策略：压缩阶段将每个ResNet阶段简化为1-2个MeanFlow模块构建轻量元模型；扩展阶段在前三个阶段选择性孵化以匹配基线ResNet配置，最后一个阶段保持MeanFlow形式并进行微调。

Result: 在CIFAR-10和CIFAR-100上，相比ResNet-50分别减少46.28%和45.59%参数，同时准确率分别提高0.23%和0.17%。

Conclusion: 生成式流场能有效表征ResNet中的特征变换过程，为理解生成式建模与判别式学习的关系提供了新视角。

Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.

</details>


### [120] [RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning](https://arxiv.org/abs/2511.12428)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Souvik Kundu,Peter A. Beerel*

Main category: cs.CV

TL;DR: RedVTP是一种针对扩散视觉语言模型(DVLMs)的响应驱动视觉token剪枝策略，通过利用模型推理动态来提升推理效率，在不损失精度的情况下显著提高生成吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散视觉语言模型(DVLMs)虽然支持并行token解码，但大量视觉token仍然严重阻碍推理效率。虽然视觉token剪枝在自回归VLMs中已被广泛研究，但在DVLMs中仍未被充分探索。

Method: 提出RedVTP策略，利用掩码响应token的注意力来估计视觉token的重要性。基于重要性分数在推理步骤间保持一致的观察，RedVTP在第一个推理步骤后从掩码token中剪枝重要性较低的视觉token。

Result: 实验显示RedVTP将LLaDA-V和LaViDa的token生成吞吐量分别提升高达186%和28.05%，推理延迟分别降低64.97%和21.87%，且不损害精度，在某些情况下甚至提高了准确性。

Conclusion: RedVTP通过响应驱动的视觉token剪枝策略，有效解决了DVLMs的推理效率问题，为扩散视觉语言模型的高效推理提供了可行的解决方案。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.

</details>


### [121] [Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion](https://arxiv.org/abs/2511.12432)
*Xilai Li,Xiaosong Li,Weijun Jiang*

Main category: cs.CV

TL;DR: 提出UP-Fusion框架，通过通道扰动和预训练知识集成解决多模态图像融合中的梯度冲突问题，提升融合质量和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有统一模型在多模态图像融合中存在梯度冲突问题，而引入模态特定编码器的方法会降低跨不同融合任务的泛化能力

Method: 提出语义感知通道剪枝模块(SCPM)过滤和增强多模态特征通道，几何仿射调制模块(GAM)保持特征编码器的模态区分性，文本引导通道扰动模块(TCPM)在解码时重塑通道分布

Result: 在多种多模态图像融合和下游任务上优于现有方法

Conclusion: UP-Fusion框架通过创新的通道扰动和预训练知识集成策略，有效解决了多模态图像融合中的关键挑战

Abstract: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.

</details>


### [122] [Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning](https://arxiv.org/abs/2511.12438)
*ANK Zaman,Prosenjit Chatterjee,Rajat Sharma*

Main category: cs.CV

TL;DR: 开发基于深度卷积神经网络和OpenCV的实时驾驶员疲劳检测系统，通过面部特征识别疲劳状态并触发警报


<details>
  <summary>Details</summary>
Motivation: 长途驾驶容易导致驾驶员疲劳，疲劳驾驶对个人和其他道路使用者构成生命威胁，需要实时检测系统来预防事故

Method: 使用实时摄像头采集驾驶员面部图像，通过OpenCV检测面部特征点（眼睛开合度和打哈欠动作），利用预训练的DCNN模型分析疲劳状态

Result: 在NTHU-DDD和Yawn-Eye数据集上分别达到99.6%和97%的疲劳检测分类准确率

Conclusion: 该方法提供了一种非侵入式、低成本且有效的疲劳检测方案，能够实时预警并可能挽救生命

Abstract: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.

</details>


### [123] [CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training](https://arxiv.org/abs/2511.12446)
*Jiahe Qian,Yuhao Shen,Zhangtianyi Chen,Juexiao Zhou,Peisong Wang*

Main category: cs.CV

TL;DR: CoTBox-TTT是一种用于医学视觉问答的测试时训练方法，通过视觉思维链识别问题相关区域，并在保持主干网络冻结的情况下更新少量连续软提示，提高模型在领域偏移下的可靠性和证据基础。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉问答系统在领域偏移下可靠性不足，容易关注虚假区域，且在实际部署时重新训练或添加标签不现实。

Method: 采用测试时训练方法，仅更新少量连续软提示，通过视觉思维链识别问题相关区域，并在原始图像和局部裁剪图像上保持答案一致性。

Result: 在医学VQA实验中，将CoTBox-TTT添加到LLaVA模型后，在pathVQA上的闭式问答准确率提高了12.3%。

Conclusion: 该方法无需标签、即插即用，适用于各种主干网络，具有实际部署的可行性。

Abstract: Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.

</details>


### [124] [MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning](https://arxiv.org/abs/2511.12480)
*Jingshan Hong,Haigen Hu,Huihuang Zhang,Qianwei Zhou,Zhao Li*

Main category: cs.CV

TL;DR: 提出MaskAnyNet方法，将图像掩码区域视为辅助知识而非丢弃信息，通过重新学习机制同时利用可见和掩码信息来增强特征表示和保留细粒度细节。


<details>
  <summary>Details</summary>
Motivation: 传统图像掩码存在两个问题：丢弃像素未充分利用导致上下文信息丢失；掩码可能移除细粒度任务中的关键特征。而MIM表明掩码区域可以从部分输入重建，揭示了不完整数据与原始图像的强上下文一致性，掩码区域具有语义多样性潜力。

Method: 提出MaskAnyNet，结合掩码和重新学习机制，通过额外分支从重新组合的掩码区域联合学习，利用掩码区域的语义多样性来丰富特征并保留细粒度细节。该方法可轻松扩展到任何模型。

Result: 在CNN和Transformer骨干网络上的实验显示，在多个基准测试中均获得一致性能提升。进一步分析证实该方法通过重用掩码内容提高了语义多样性。

Conclusion: 将掩码内容作为辅助知识而非忽略信息的方法有效，能够利用掩码区域的语义多样性来增强模型特征表示能力，在多种架构上均能带来性能改进。

Abstract: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.

</details>


### [125] [Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion](https://arxiv.org/abs/2511.12498)
*Jongseong Bae,Junwoo Ha,Jinnyeong Heo,Yeongin Lee,Ha Young Kim*

Main category: cs.CV

TL;DR: 提出C3DFusion模块，通过显式对齐当前和历史帧的3D点特征来生成隐藏区域感知的3D特征几何，解决现有方法在重建ego-vehicle侧面关键区域方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于摄像头的3D语义场景补全方法主要关注增强帧内区域，但难以重建ego-vehicle侧面的关键帧外区域，尽管历史帧通常包含这些不可见区域的有价值上下文信息。

Method: 提出C3DFusion模块，通过历史上下文模糊化和以当前为中心的特征密集化两种互补技术进行增强的时间融合：抑制不准确扭曲历史点特征的噪声，并通过增加当前点特征的体积贡献来增强当前点特征。

Result: 在SemanticKITTI和SSCBench-KITTI-360数据集上显著优于最先进方法，并展现出强大的泛化能力，在其他基线模型上也能获得显著性能提升。

Conclusion: C3DFusion模块通过显式的时间融合有效解决了帧外区域重建问题，在标准SSC架构中简单集成即可实现强大效果和良好泛化能力。

Abstract: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.

</details>


### [126] [Visible Structure Retrieval for Lightweight Image-Based Relocalisation](https://arxiv.org/abs/2511.12503)
*Fereidoon Zangeneh,Leonard Bruns,Amit Dekel,Alessandro Pieropan,Patric Jensfelt*

Main category: cs.CV

TL;DR: 提出一种新的结构重定位范式，通过神经网络直接从图像观测中学习可见场景结构，避免传统图像检索或搜索启发式方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的方法在大场景中需要复杂的搜索启发式或图像检索，导致复杂流水线或存储需求随观测数量增长。

Method: 训练一个紧凑的神经网络，通过前向传播直接从查询图像中获取地图中可见的3D结构点子集，减少2D-3D对应搜索空间。

Result: 所提方法在定位精度上可与最先进方法相媲美，同时计算和存储开销更低。

Conclusion: 该工作展示了通过神经网络直接映射图像到可见结构的新范式，为大规模场景重定位提供了更高效的解决方案。

Abstract: Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.

</details>


### [127] [DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection](https://arxiv.org/abs/2511.12511)
*Jialiang Shen,Jiyang Zheng,Yunqi Xue,Huajie Chen,Yu Yao,Hui Kang,Ruiqi Liu,Helin Gong,Yang Yang,Dadong Wang,Tongliang Liu*

Main category: cs.CV

TL;DR: 提出基于师生知识蒸馏的模糊鲁棒AI生成图像检测框架，通过冻结教师模型将清晰图像的特征和逻辑响应蒸馏到处理模糊图像的学生模型，提升在运动模糊条件下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在真实世界退化（特别是运动模糊）下性能严重下降，而运动模糊在手持摄影、快速运动和压缩视频中频繁出现，扭曲精细纹理并抑制高频伪影。

Method: 使用DINOv3作为教师模型，在清晰图像上训练提供稳定语义表示；冻结教师模型保持泛化能力，将其特征和逻辑响应从清晰图像蒸馏到处理模糊图像的学生模型。

Result: 在运动模糊和清晰条件下均达到最先进性能，表现出改进的泛化能力和真实世界适用性。

Conclusion: 该方法通过知识蒸馏有效解决了AI生成图像检测在运动模糊条件下的性能退化问题，具有实际应用价值。

Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.

</details>


### [128] [MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics](https://arxiv.org/abs/2511.12525)
*Jing Li,Yifan Wang,Jiafeng Yan,Renlong Zhang,Bin Yang*

Main category: cs.CV

TL;DR: 提出MdaIF框架，通过大语言模型驱动的降解感知机制，解决恶劣天气条件下红外与可见光图像融合的性能下降问题，采用专家混合系统适应多种降解场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个问题：1) 未考虑恶劣天气条件下可见光图像的降解，影响融合性能；2) 依赖固定网络架构，难以适应多样化降解场景。

Method: 使用预训练视觉语言模型提取天气感知的降解知识和场景特征作为语义先验，提出降解感知通道注意力模块，采用专家混合系统进行自适应路由。

Result: 大量实验验证了MdaIF的有效性，在复杂降解场景下表现出优于现有最先进方法的性能。

Conclusion: MdaIF框架通过降解感知机制和专家混合系统，成功解决了多降解场景下的红外与可见光图像融合问题，展现出优越的适应性和鲁棒性。

Abstract: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.

</details>


### [129] [D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation](https://arxiv.org/abs/2511.12528)
*Zheyuan Zhang,Jiwei Zhang,Boyu Zhou,Linzhimeng Duan,Hong Chen*

Main category: cs.CV

TL;DR: 提出D²-VPR框架，通过知识蒸馏和可变形聚合器，在保持视觉基础模型强大特征提取能力的同时，显著减少模型参数和计算开销。


<details>
  <summary>Details</summary>
Motivation: DINOv2等视觉基础模型虽然显著提升了视觉位置识别性能，但模型复杂度和计算开销过大，难以在资源受限设备上部署。

Method: 采用两阶段训练策略（知识蒸馏+微调），引入蒸馏恢复模块对齐师生模型特征空间，设计基于自上而下注意力的可变形聚合器动态调整感兴趣区域。

Result: 在保持竞争力的性能同时，参数减少约64.2%，FLOPs减少约62.6%（相比CricaVPR）。

Conclusion: D²-VPR框架实现了更好的性能-效率权衡，为资源受限设备上的视觉位置识别应用提供了可行解决方案。

Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.

</details>


### [130] [ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding](https://arxiv.org/abs/2511.12530)
*Yuan Zhou,Litao Hua,Shilong Jin,Wentao Huang,Haoran Duan*

Main category: cs.CV

TL;DR: ReaSon是一个基于强化学习和因果信息瓶颈的关键帧选择框架，通过优化预测充分性和因果必要性来选择视频理解中的关键帧。


<details>
  <summary>Details</summary>
Motivation: 由于视觉语言模型的输入token有限以及视频帧中相关信息的时间稀疏性，关键帧选择对视频理解至关重要。需要选择既信息丰富又具有因果决定性的关键帧。

Method: 提出ReaSon框架，将关键帧选择建模为优化问题，使用因果信息瓶颈(CIB)定义关键帧应满足预测充分性和因果必要性。通过可学习的策略网络从候选帧中选择关键帧，并通过反事实干预评估因果必要性，使用复合奖励通过强化学习指导选择策略。

Result: 在NExT-QA、EgoSchema和Video-MME数据集上的广泛实验表明，ReaSon在有限帧设置下始终优于现有最先进方法。

Conclusion: ReaSon框架通过因果信息瓶颈原则有效解决了视频理解中的关键帧选择问题，验证了其有效性和泛化能力。

Abstract: Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.

</details>


### [131] [HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models](https://arxiv.org/abs/2511.12547)
*Zhiguang Lu,Qianqian Xu,Peisong Wen,Siran Da,Qingming Huang*

Main category: cs.CV

TL;DR: 提出了HiGFA方法，通过分层引导策略在扩散模型采样过程中实现细粒度数据增强，早期使用强文本和轮廓引导建立整体结构，后期激活细粒度分类器引导并动态调整强度。


<details>
  <summary>Details</summary>
Motivation: 解决生成扩散模型在细粒度任务中难以准确捕捉类别定义性细微特征的问题，避免标准方法可能生成误导性样本导致分类器性能下降。

Method: HiGFA方法：早期采样阶段使用固定强度的文本和轮廓引导建立场景、风格和结构；后期采样阶段激活细粒度分类器引导，并基于预测置信度动态调制所有引导信号的强度。

Result: 在多个细粒度视觉分类数据集上的实验证明了HiGFA的有效性。

Conclusion: 分层置信驱动的引导策略能够智能平衡全局结构形成与精确细节优化，生成多样化且忠实于原始类别的合成图像。

Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.

</details>


### [132] [EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis](https://arxiv.org/abs/2511.12554)
*Yijie Guo,Dexiang Hong,Weidong Chen,Zihan She,Cheng Ye,Xiaojun Chang,Zhendong Mao*

Main category: cs.CV

TL;DR: EmoVerse是一个大规模开源数据集，通过多层次的基于知识图谱的注释实现可解释的视觉情感分析，包含超过21.9万张图像，支持离散和连续情感表示。


<details>
  <summary>Details</summary>
Motivation: 现有视觉情感分析研究缺乏开源和可解释数据集，通常为整张图像分配单一离散情感标签，无法揭示视觉元素如何贡献情感。

Method: 将情感分解为背景-属性-主体三元组，将每个元素定位到视觉区域；采用多阶段标注流程确保高可靠性；提出可解释模型将视觉线索映射到维度情感空间。

Result: 构建了包含219k图像的大规模数据集，提供词级和主体级情感推理，支持分类情感状态和维度情感空间的双重标注。

Conclusion: EmoVerse数据集、标注流程和模型共同为推进可解释的高层次情感理解提供了全面基础。

Abstract: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.

</details>


### [133] [SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition](https://arxiv.org/abs/2511.12559)
*Qing Cai,Guihao Yan,Fan Zhang,Cheng Zhang,Zhi Liu*

Main category: cs.CV

TL;DR: 提出SEMC框架，结合结构感知特征融合与专家引导对比学习，解决超声标准平面识别中浅层结构信息利用不足和细粒度语义差异捕捉困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效利用浅层结构信息，且难以通过图像增强生成的对比样本捕捉细粒度语义差异，导致超声标准平面的结构和判别细节识别效果不佳。

Method: 提出SEMC框架：1）语义-结构融合模块(SSFM)利用多尺度结构信息，对齐浅层和深层特征；2）专家混合对比识别模块(MCRM)使用MoE机制在多级特征上进行分层对比学习和分类。

Result: 在内部数据集和两个公共数据集上的广泛实验表明，SEMC在各种指标上均优于现有最先进方法。

Conclusion: SEMC框架通过结构增强和专家混合对比学习，显著提升了超声标准平面识别的性能，并构建了大规模肝脏超声数据集以支持研究。

Abstract: Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.

</details>


### [134] [Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection](https://arxiv.org/abs/2511.12572)
*Mohamed Youssef,Lukas Brunner,Klaus Rundhammer,Gerald Czech,Oliver Bimber*

Main category: cs.CV

TL;DR: 提出了一种结合信号处理和机器学习的新方法，用于通过遮挡的森林植被重建地表温度，实现无人机自主野火监测和早期火灾检测。


<details>
  <summary>Details</summary>
Motivation: 目标是实现完全自动化的空中野火监测，使用自主无人机在烟雾或火焰可见之前早期检测地面火灾。合成孔径感知虽然减轻了树冠和阳光的遮挡，但引入了热模糊，掩盖了实际地表温度。

Method: 训练视觉状态空间模型从模糊数据中恢复部分遮挡的土壤和火灾热点的细微热信号。通过将潜在扩散模型集成到向量量化中，从真实野火记录生成大量逼真的地表温度模拟数据，并通过温度增强和程序化热森林模拟进一步扩展。

Result: 在模拟数据上，相比传统热成像和未校正的SA成像，方法将RMSE降低了2到2.5倍。在野外实验中，对高温热点的改进更为显著，相比传统热成像RMSE增益达12.8倍，相比未校正SA图像增益达2.6倍。

Conclusion: 该方法能够重建火灾和人类特征的完整形态，而传统成像因部分遮挡而失效。模型还展示了对其他热信号（如搜救中的人类特征）的泛化能力。

Abstract: We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.

</details>


### [135] [Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection](https://arxiv.org/abs/2511.12575)
*Jiayi Zhu,Yihao Huang,Yue Cao,Xiaojun Jia,Qing Guo,Felix Juefei-Xu,Geguang Pu,Bin Wang*

Main category: cs.CV

TL;DR: 提出一种基于文本扩展的语义感知排版攻击方法，通过在图像外部添加欺骗性文本来保护用户的地理位置隐私，有效降低大型视觉语言模型的地理位置推断准确率，同时保持图像视觉质量。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型能够从社交媒体图像中推断用户地理位置，造成严重的隐私泄露风险。现有的对抗性图像扰动方法需要强失真才能有效，这会显著降低图像视觉质量和分享价值。

Method: 采用两阶段语义感知排版攻击，在图像外部添加文本扩展，研究有效的文本语义来干扰地理位置推断，生成欺骗性文本保护用户隐私。

Result: 在三个数据集上的广泛实验表明，该方法显著降低了五个最先进商业LVLMs的地理位置预测准确率，实现了实用且视觉保持的保护策略。

Conclusion: 排版攻击是保护地理隐私的有前景方向，该方法为应对新兴地理隐私威胁提供了有效解决方案，在保护隐私的同时保持了图像的视觉质量。

Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.

</details>


### [136] [TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction](https://arxiv.org/abs/2511.12578)
*Yukuo Ma,Cong Liu,Junke Wang,Junqi Liu,Haibin Huang,Zuxuan Wu,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TempoMaster是一个新颖的长视频生成框架，将长视频生成建模为下一帧率预测任务，通过从低帧率到高帧率的渐进式生成实现高效且连贯的长视频合成。


<details>
  <summary>Details</summary>
Motivation: 解决长视频生成中的时间连贯性和计算效率问题，传统方法难以同时保证长距离时间一致性和生成效率。

Method: 首先生成低帧率视频作为粗粒度蓝图，然后逐步提高帧率来细化视觉细节和运动连续性；采用帧率级别内的双向注意力和跨帧率的自回归生成。

Result: 在长视频生成任务中达到了新的最先进水平，在视觉质量和时间连贯性方面都表现出色。

Conclusion: TempoMaster通过将长视频生成建模为帧率预测问题，成功实现了高效且连贯的长视频合成，为长视频生成提供了新的解决方案。

Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.

</details>


### [137] [Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting](https://arxiv.org/abs/2511.12588)
*Zuqi Huang,Mengxin Tian,Huan Liu,Wentao Li,Baobao Liang,Jie Wu,Fang Yan,Zhaoqing Tang,Zhongyu Li*

Main category: cs.CV

TL;DR: 提出CountIHC框架，通过排名感知的教师选择策略和多类细胞计数方法，解决IHC图像中细胞计数挑战，在12种生物标记物和5种组织类型上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: IHC图像中细胞计数面临染色体重叠、生物标记物染色变异和细胞形态多样性等挑战，现有回归方法很少支持端到端多类计数，且基础模型潜力未充分探索。

Method: 使用排名感知聚集框架，通过RATS策略评估教师模型计数能力并进行样本级选择；多类计数通过视觉-语言对齐，使用结构化文本提示的语义锚点指导类别特异性密度图回归。

Result: 在12种IHC生物标记物和5种组织类型上超越最先进方法，与病理学家评估高度一致，在H&E染色数据上也验证了方法的可扩展性。

Conclusion: CountIHC通过有效利用基础模型互补表示和创新的多类计数方法，显著提升了IHC图像细胞计数的准确性和鲁棒性。

Abstract: Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.

</details>


### [138] [Fine-Grained Representation for Lane Topology Reasoning](https://arxiv.org/abs/2511.12590)
*Guoqing Xu,Yiheng Li,Yang Yang*

Main category: cs.CV

TL;DR: TopoFG是一个细粒度的车道拓扑推理框架，通过分层先验提取、区域聚焦解码和鲁棒边界点拓扑推理，精确建模复杂车道结构，在OpenLane-V2基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常用单个查询表示每个车道，基于车道查询之间的相似性推断拓扑连接性，但难以准确建模复杂车道结构，导致拓扑预测不可靠。

Method: 将BEV特征到拓扑预测的过程分为三个阶段：分层先验提取器从BEV掩码提取全局空间先验和车道内关键点序列提取局部序列先验；区域聚焦解码器构建细粒度查询并采样ROI区域参考点；鲁棒边界点拓扑推理基于边界点查询特征建模车道连接性并应用拓扑去噪策略。

Result: 在OpenLane-V2基准测试中，TopoFG在subsetA上达到48.0%的OLS，在subsetB上达到45.4%的OLS，创造了新的最先进性能。

Conclusion: 通过将空间和序列先验集成到细粒度查询中，并对边界点拓扑推理应用去噪策略，该方法能够精确建模复杂车道结构并提供可信的拓扑预测。

Abstract: Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.

</details>


### [139] [Seg-VAR: Image Segmentation with Visual Autoregressive Modeling](https://arxiv.org/abs/2511.12594)
*Rongkun Zheng,Lu Qi,Xi Chen,Yi Wang,Kun Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Seg-VAR将分割任务重新定义为条件自回归掩码生成问题，通过多阶段训练策略在多个分割任务上超越之前的判别式和生成式方法。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归建模(VAR)策略在图像生成方面已取得进展，但其在需要精确空间感知的分割任务中的潜力尚未被探索。

Method: 提出Seg-VAR框架，包含三个核心组件：图像编码器生成潜在先验、空间感知的seglat编码器将分割掩码映射为离散潜在token、解码器从潜在重建掩码。采用多阶段训练策略。

Result: 实验表明Seg-VAR在各种分割任务和验证基准上优于之前的判别式和生成式方法。

Conclusion: 通过将分割构建为序列层次预测任务，Seg-VAR为将自回归推理集成到空间感知视觉系统中开辟了新途径。

Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.

</details>


### [140] [LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet](https://arxiv.org/abs/2511.12602)
*Ria Shekhawat,Sushrut Patwardhan,Raghavendra Ramachandra,Praveen Kumar Chandaliya,Kishor P. Upla*

Main category: cs.CV

TL;DR: 提出了一种基于师生框架的单图像形态攻击检测方法，使用CNN教师模型优化ViT学生模型，并集成LoRA进行微调以提高效率。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统对安全至关重要，但仍然容易受到形态攻击的威胁，这种攻击通过合成图像融合多个个体的生物特征。

Method: 采用师生框架，CNN教师模型优化ViT学生模型，集成低秩适应(LoRA)进行微调以降低计算成本。在由三个公开人脸数据集构建的形态数据集上进行实验，包含十种不同的形态生成算法。

Result: 与六种最先进的S-MAD技术相比，该方法展现出优越的检测性能和计算效率。

Conclusion: 所提出的师生框架结合LoRA微调的方法在形态攻击检测方面具有高性能和高效率。

Abstract: Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.

</details>


### [141] [Pixels or Positions? Benchmarking Modalities in Group Activity Recognition](https://arxiv.org/abs/2511.12606)
*Drishya Karki,Merey Ramazanova,Anthony Cioppa,Silvio Giancola,Bernard Ghanem*

Main category: cs.CV

TL;DR: 本文介绍了SoccerNet-GAR数据集，比较了视频和追踪两种模态在群体活动识别中的表现，发现基于追踪的方法在准确率、训练速度和模型大小方面均优于视频方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏标准化的基准来对齐视频和追踪数据以进行群体活动识别的公平比较，需要探索哪种模态更适合群体活动识别。

Method: 构建了SoccerNet-GAR多模态数据集，包含94,285个同步标注的群体活动；提出了基于图神经网络的追踪分类器，使用角色感知图架构编码战术结构。

Result: 追踪模型达到67.2%的平衡准确率，比最佳视频基线（58.1%）高出9.1个百分点，训练速度快4.25倍，参数少438倍（197K vs 86.3M）。

Conclusion: 追踪模态在群体活动识别中优于视频模态，强调了模态选择和角色感知建模的重要性。

Abstract: Group Activity Recognition (GAR) is well studied on the video modality for surveillance and indoor team sports (e.g., volleyball, basketball). Yet, other modalities such as agent positions and trajectories over time, i.e. tracking, remain comparatively under-explored despite being compact, agent-centric signals that explicitly encode spatial interactions. Understanding whether pixel (video) or position (tracking) modalities leads to better group activity recognition is therefore important to drive further research on the topic. However, no standardized benchmark currently exists that aligns broadcast video and tracking data for the same group activities, leading to a lack of apples-to-apples comparison between these modalities for GAR. In this work, we introduce SoccerNet-GAR, a multimodal dataset built from the $64$ matches of the football World Cup 2022. Specifically, the broadcast videos and player tracking modalities for $94{,}285$ group activities are synchronized and annotated with $10$ categories. Furthermore, we define a unified evaluation protocol to benchmark two strong unimodal approaches: (i) a competitive video-based classifiers and (ii) a tracking-based classifiers leveraging graph neural networks. In particular, our novel role-aware graph architecture for tracking-based GAR directly encodes tactical structure through positional edges and temporal attention. Our tracking model achieves $67.2\%$ balanced accuracy compared to $58.1\%$ for the best video baseline, while training $4.25 \times$ faster with $438 \times$ fewer parameters ($197K$ \vs $86.3M$). This study provides new insights into the relative strengths of pixels and positions for group activity recognition. Overall, it highlights the importance of modality choice and role-aware modeling for GAR.

</details>


### [142] [Open-World Test-Time Adaptation with Hierarchical Feature Aggregation and Attention Affine](https://arxiv.org/abs/2511.12607)
*Ziqiong Liu,Yushun Tang,Junyang Ji,Zhihai He*

Main category: cs.CV

TL;DR: 提出了一种分层阶梯网络和注意力仿射网络，通过提取OOD特征和自适应调整注意力机制来提升测试时适应性能，在存在域偏移和未见类别的情况下显著改善模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决测试时适应方法在面对未见类别样本时性能显著下降的问题，避免将OOD样本误分类为已知类别导致预测精度下降和适应过程受损。

Method: 使用分层阶梯网络从所有Transformer层聚合的类别标记中提取OOD特征，通过加权概率融合增强OOD检测；引入注意力仿射网络自适应调整自注意力机制以应对域偏移；采用加权熵机制动态抑制低置信度样本的影响。

Result: 在基准数据集上的实验结果表明，该方法在广泛使用的分类数据集上显著提升了性能。

Conclusion: 所提出的分层阶梯网络和注意力仿射网络能够有效提升模型在测试时适应过程中的鲁棒性，特别是在存在域偏移和未见类别的情况下。

Abstract: Test-time adaptation (TTA) refers to adjusting the model during the testing phase to cope with changes in sample distribution and enhance the model's adaptability to new environments. In real-world scenarios, models often encounter samples from unseen (out-of-distribution, OOD) categories. Misclassifying these as known (in-distribution, ID) classes not only degrades predictive accuracy but can also impair the adaptation process, leading to further errors on subsequent ID samples. Many existing TTA methods suffer substantial performance drops under such conditions. To address this challenge, we propose a Hierarchical Ladder Network that extracts OOD features from class tokens aggregated across all Transformer layers. OOD detection performance is enhanced by combining the original model prediction with the output of the Hierarchical Ladder Network (HLN) via weighted probability fusion. To improve robustness under domain shift, we further introduce an Attention Affine Network (AAN) that adaptively refines the self-attention mechanism conditioned on the token information to better adapt to domain drift, thereby improving the classification performance of the model on datasets with domain shift. Additionally, a weighted entropy mechanism is employed to dynamically suppress the influence of low-confidence samples during adaptation. Experimental results on benchmark datasets show that our method significantly improves the performance on the most widely used classification datasets.

</details>


### [143] [OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding](https://arxiv.org/abs/2511.12614)
*Artem Moroz,Vít Zeman,Martin Mikšík,Elizaveta Isianova,Miroslav David,Pavel Burget,Varun Burde*

Main category: cs.CV

TL;DR: 提出了一个统一的端到端框架，将物体检测和姿态估计与灵活的onboarding过程无缝集成。系统支持从3D CAD模型或通过多视角图像快速重建神经表示来生成物体表示，使用CNOS检测器定位目标物体，并通过基于transformer的OPFormer模块进行精确的6D姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中物体检测和姿态估计分离的问题，提供统一的端到端解决方案，支持模型存在和模型缺失两种场景，提高实际应用中的灵活性和实用性。

Method: 采用两阶段流程：onboarding阶段从3D CAD模型或多视角图像生成物体表示；推理阶段使用CNOS检测器定位物体，OPFormer模块（基于transformer架构）结合基础模型特征提取、多模板视图联合编码和NOCS几何先验，通过建立2D-3D对应关系估计最终姿态。

Result: 在BOP基准测试中表现出色，在准确性和效率之间取得了良好平衡，证明了在模型存在和模型缺失场景下的实际应用价值。

Conclusion: 该集成系统提供了一个实用且高效的解决方案，能够处理现实世界中物体检测和姿态估计的挑战，特别是在模型可用性不确定的情况下。

Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.

</details>


### [144] [C3Net: Context-Contrast Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12627)
*Baber Jan,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: 该论文提出了C3Net用于伪装目标检测，通过双路径解码器架构解决伪装目标的六大挑战，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 伪装目标检测面临传统分割方法和现代基础模型都无法有效处理的挑战，作者识别出六大根本性挑战需要综合的架构解决方案。

Method: 提出C3Net双路径解码器架构：边缘优化路径使用梯度初始化边缘增强模块恢复精确边界；上下文定位路径采用图像上下文引导机制实现内在显著性抑制；通过注意力融合模块协同整合两个路径。

Result: 在COD10K数据集上S-measure达到0.898，CAMO数据集0.904，NC4K数据集0.913，保持高效处理。

Conclusion: C3Net证明复杂多方面的检测挑战需要架构创新，通过专门组件协同工作实现超越孤立改进的全面覆盖。

Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.

</details>


### [145] [Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation](https://arxiv.org/abs/2511.12631)
*Yushe Cao,Dianxi Shi,Xing Fu,Xuechao Zou,Haikuo Peng,Xueqi Li,Chun Yu,Junliang Xing*

Main category: cs.CV

TL;DR: MDiTFace是一个基于扩散变换器的多模态人脸生成框架，通过统一的标记化策略处理语义掩码和文本输入，采用解耦注意力机制显著降低计算开销，在面部保真度和条件一致性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的特征融合方法在多模态人脸生成中难以实现有效的跨模态交互，导致生成效果不理想。需要解决异构模态表示之间的差异问题。

Method: 提出MDiTFace框架：1）统一标记化策略处理语义掩码和文本输入；2）堆叠多元变换器块实现同步多模态特征交互；3）设计解耦注意力机制，分离掩码标记和时间嵌入的隐式依赖，将内部计算分为动态和静态路径。

Result: 该方法将掩码条件引入的额外计算开销降低了94%以上，同时保持性能。在面部保真度和条件一致性方面显著优于其他竞争方法。

Conclusion: MDiTFace通过统一的标记化策略和解耦注意力机制，有效解决了多模态人脸生成中的跨模态交互问题，在保证性能的同时大幅降低了计算成本。

Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.

</details>


### [146] [Denoising Vision Transformer Autoencoder with Spectral Self-Regularization](https://arxiv.org/abs/2511.12633)
*Xunzhi Xiang,Xingye Tian,Guiyu Zhang,Yabo Chen,Shaofeng Zhang,Xuebo Wang,Xin Tao,Qi Fan*

Main category: cs.CV

TL;DR: 提出Denoising-VAE，通过频谱自正则化策略抑制高维潜在空间中的冗余高频噪声，解决了VAE在高维潜在空间中重建质量与生成性能的权衡问题，显著提升扩散模型的收敛速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统VAE在高维潜在空间中面临重建质量与生成性能的权衡：高维潜在空间能提升重建保真度，但会损害生成性能。现有方法依赖外部视觉基础模型进行正则化，但高维潜在空间如何影响生成模型优化仍不清楚。

Method: 提出频谱自正则化策略抑制冗余高频噪声，同时保持重建质量；开发基于ViT的Denoising-VAE，无需依赖外部视觉基础模型；引入频谱对齐策略优化基于Denoising-VAE的生成模型。

Result: 在ImageNet 256×256基准测试中，扩散模型收敛速度比SD-VAE快约2倍，达到最先进的重建质量（rFID=0.28，PSNR=27.26）和竞争性生成性能（gFID=1.82）。

Conclusion: 揭示了高维潜在空间中冗余高频分量阻碍扩散模型训练收敛的根本问题，提出的Denoising-VAE通过频谱正则化有效解决了这一优化困境，在重建和生成任务上均取得优异表现。

Abstract: Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.

</details>


### [147] [Medical Knowledge Intervention Prompt Tuning for Medical Image Classification](https://arxiv.org/abs/2511.12639)
*Ye Du,Nanxi Yu,Shujun Wang*

Main category: cs.CV

TL;DR: CILMP是一种结合大型语言模型和视觉语言模型的提示调优方法，通过LLMs提取疾病特定表示，在低秩线性子空间进行干预，生成疾病特定的自适应提示，在医学图像分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优方法无法精确区分不同医学概念，缺乏特定疾病相关特征。LLMs在医学知识方面表现出色，可以弥补这一缺陷。

Method: 提出CILMP方法：从LLMs提取疾病特定表示，在低秩线性子空间进行干预，结合条件机制生成实例自适应提示，将医学知识转移到VLM提示中。

Result: 在多个医学图像数据集上的实验表明，CILMP持续优于最先进的提示调优方法。

Conclusion: CILMP通过结合LLMs和VLMs，有效提升了医学图像分类任务的性能，证明了该方法在医学视觉任务中的有效性。

Abstract: Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.

</details>


### [148] [DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry](https://arxiv.org/abs/2511.12653)
*Cheng Liao*

Main category: cs.CV

TL;DR: DPVO-QAT++是一个层次化量化优化框架，通过可学习尺度参数化、视觉里程计前后端异构精度设计（前端FP16/FP32伪量化，后端全精度）和GPU原生核融合技术，在保持轨迹精度的同时显著降低内存占用并提升处理速度。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的视觉SLAM系统具有出色的几何推理能力，但其过高的计算开销严重限制了在资源受限的自主平台上的部署。

Method: 采用可学习尺度参数化、异构精度设计（前端伪量化，后端全精度）和GPU原生核融合技术，实现高效的量化感知训练。

Result: 在TartanAir数据集上实现平均FPS提升52.1%，中位延迟降低29.1%，峰值GPU内存占用减少64.9%；在EuRoC数据集上实现平均FPS提升30.1%，中位延迟降低23.1%，峰值GPU内存占用减少37.7%，同时保持与原始模型相当的轨迹精度。

Conclusion: DPVO-QAT++有效弥合了高精度深度视觉里程计与实际部署效率需求之间的差距，为在真实嵌入式平台上应用该技术提供了可行的工程范式。

Abstract: Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.
  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.

</details>


### [149] [Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis](https://arxiv.org/abs/2511.12658)
*Zeqin Yu,Haotao Xie,Jian Zhang,Jiangqun Ni,Wenkan Su,Jiwu Huang*

Main category: cs.CV

TL;DR: 提出了FSTS框架，通过分析真实篡改行为参数，使用傅里叶级数启发的分层建模方法合成多样化的训练数据，显著提升了文本图像伪造定位模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有T-IFL方法泛化能力差，主要原因是真实世界数据集规模有限，以及合成数据与真实篡改复杂性之间存在分布差距。

Method: FSTS框架：1) 从5种代表性篡改类型收集16,750个真实篡改实例；2) 通过结构化流水线记录人类编辑痕迹；3) 使用傅里叶级数启发的分层建模方法，在个体和群体层面分析行为模式；4) 从建模分布中采样合成多样化训练数据。

Result: 在四个评估协议上的广泛实验表明，使用FSTS数据训练的模型在真实世界数据集上实现了显著改进的泛化能力。

Conclusion: FSTS框架通过结构化建模真实篡改行为，能够生成更接近真实世界伪造痕迹的训练数据，有效解决了T-IFL方法的泛化问题。

Abstract: Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \href{https://github.com/ZeqinYu/FSTS}{Project Page}.

</details>


### [150] [Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/abs/2511.12662)
*Hongbin Huang,Junwei Li,Tianxin Xie,Zhuang Li,Cekai Weng,Yaodong Yang,Yue Luo,Li Liu,Jing Tang,Zhijing Shao,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出一个高保真实时对话数字人系统，结合逼真3D虚拟形象、个性化语音合成和知识驱动对话生成，通过异步执行管道实现低延迟多模态交互。


<details>
  <summary>Details</summary>
Motivation: 解决数字人类应用中视觉真实性与实时响应性难以兼顾的挑战，为沉浸式通信、教育和娱乐应用提供可信赖的数字人类交互体验。

Method: 采用异步执行管道协调多模态组件，结合检索增强方法（历史增强和意图路由）实现上下文感知响应生成，支持唤醒词检测和情感韵律表达。

Result: 构建了一个集成的数字人系统，能够实现响应迅速且可信的交互，具备自然流畅的对话能力和情感表达能力。

Conclusion: 该系统成功实现了高保真实时数字人类的构建，为沉浸式应用提供了可行的技术解决方案，展示了多模态组件协调和检索增强方法在数字人交互中的有效性。

Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.

</details>


### [151] [DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality](https://arxiv.org/abs/2511.12671)
*Tushar Anand,Advik Sinha,Abhijit Das*

Main category: cs.CV

TL;DR: 提出了一种基于非因果选择性状态空间的实时光学流和视差估计模型，通过融合成对输入图像实现密集感知任务。


<details>
  <summary>Details</summary>
Motivation: 解决实时应用中光学流和视差估计的高精度与低延迟需求之间的矛盾，为3D密集感知任务提供统一的实时解决方案。

Method: 使用非因果Mamba块构建模型，通过非因果选择性状态空间融合成对输入图像，优化推理时间和GPU使用效率。

Result: 模型在保持高精度的同时显著减少了推理时间，降低了GPU使用率，在真实场景中验证了其有效性。

Conclusion: 该模型能够用于统一的实时3D密集感知估计任务，在准确性和效率之间取得了良好平衡。

Abstract: In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD

</details>


### [152] [Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis](https://arxiv.org/abs/2511.12675)
*Saar Stern,Ido Sobol,Or Litany*

Main category: cs.CV

TL;DR: 提出了一个任务感知的评估框架PRISM，用于评估新视角合成(NVS)的质量，包含基于参考的D_PRISM和无参考的MMD_PRISM两个指标，能够可靠识别错误生成并与人类偏好一致。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标难以准确评估新视角合成图像是否既真实又忠实于源视图和视角变换，标准指标经常错误地将不正确结果排名靠前。

Method: 利用Zero123强NVS基础模型的特征，结合轻量级调优步骤增强判别能力，提出了基于参考的D_PRISM和无参考的MMD_PRISM两个评估指标。

Result: 在Toys4K、Google Scanned Objects和OmniObject3D三个基准测试中，MMD_PRISM产生了清晰稳定的模型排名，较低分数一致表示更强的模型。

Conclusion: 该框架为新视角合成评估提供了原则性和实用性的方法，为更可靠的NVS进展铺平了道路。

Abstract: The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\text{PRISM}}$, and a reference-free score, $\text{MMD}_{\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\text{MMD}_{\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.

</details>


### [153] [BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections](https://arxiv.org/abs/2511.12676)
*Subin Varghese,Joshua Gao,Asad Ur Rahman,Vedhus Hoskere*

Main category: cs.CV

TL;DR: 提出了BridgeEQA基准测试，用于评估具身智能体在真实桥梁检查场景中的问答能力，包含2200个开放词汇问答对，并提出新的图像引用相关性评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有具身问答基准在真实世界部署中的不足，特别是缺乏能够捕捉实际操作条件的基准，桥梁检查领域自然需要多尺度推理、长距离空间理解和复杂语义关系。

Method: 提出具身记忆视觉推理(EMVR)方法，将检查建模为基于图像场景图的顺序导航：图像作为节点，智能体通过马尔可夫决策过程遍历视图、比较证据和推理。

Result: 最先进的视觉语言模型在情景记忆EQA设置下表现出显著的性能差距，而EMVR方法在基准测试中显示出优于基线的强大性能。

Conclusion: 桥梁检查是开放词汇具身问答的引人注目领域，BridgeEQA基准和EMVR方法为解决真实世界部署挑战提供了重要进展。

Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.

</details>


### [154] [R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection](https://arxiv.org/abs/2511.12691)
*Shuaike Shen,Ke Liu,Jiaqing Xie,Shangde Gao,Chunhua Shen,Ge Liu,Mireia Crispin-Ortuzar,Shangqi Gao*

Main category: cs.CV

TL;DR: R²Seg是一个无需训练的两阶段框架，通过推理-拒绝机制提升医学图像分割在分布外肿瘤上的鲁棒性，显著减少假阳性


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割基础模型在分布外数据上表现不佳，容易产生碎片化的假阳性结果

Method: 两阶段方法：1) 推理阶段使用LLM引导的解剖学推理定位器官锚点并生成多尺度ROI；2) 拒绝阶段对基础模型生成的候选区域进行双样本统计检验，仅保留与正常组织显著不同的候选

Result: 在多中心多模态肿瘤分割基准测试中，R²Seg在Dice系数、特异性和敏感性方面显著优于基线方法和原始基础模型

Conclusion: R²Seg提供了一种无需参数更新的鲁棒OOD肿瘤分割方案，兼容零更新测试时增强，避免了灾难性遗忘

Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.

</details>


### [155] [HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models](https://arxiv.org/abs/2511.12693)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: HEDGE是一个统一的幻觉检测框架，通过视觉扰动、语义聚类和不确定性度量来检测视觉语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽然支持开放式视觉问答，但容易出现幻觉问题，需要系统化的检测方法。

Method: 结合受控视觉扰动、语义聚类（蕴含和嵌入两种方法）和鲁棒不确定性度量，构建可复现的检测流程。

Result: 在VQA-RAD和KvasirVQA-x1数据集上的评估显示，统一融合模型（如Qwen2.5-VL）的幻觉检测性最高，受限标记化架构（如Med-Gemma）最低。VASE度量与嵌入聚类组合效果最佳。

Conclusion: HEDGE将幻觉检测构建为几何鲁棒性问题，提供了评估多模态可靠性的原则性基础。

Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .

</details>


### [156] [X-VMamba: Explainable Vision Mamba](https://arxiv.org/abs/2511.12694)
*Mohamed A. Mabrok,Yalda Zafari*

Main category: cs.CV

TL;DR: 提出了一个基于可控性的可解释性框架，用于分析视觉状态空间模型（SSMs）如何处理空间信息，包含两种互补方法：适用于所有SSM架构的雅可比方法和针对对角SSM的格拉姆方法，均具有线性计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 尽管状态空间模型（特别是Mamba架构）在序列建模中表现出色，但由于缺乏类似注意力的透明机制，理解这些视觉SSM如何处理空间信息仍然具有挑战性。

Method: 提出了两种互补的可控性分析方法：1）雅可比方法：通过完整状态传播链测量影响，适用于任何SSM架构；2）格拉姆方法：针对对角SSM，通过闭式解析解实现更快的速度。两种方法均只需单次前向传播，无需架构修改或超参数调优。

Result: 在三种不同的医学成像模态上验证了该框架，发现SSM自然地实现了从早期层的扩散低层纹理到深层层的聚焦、临床有意义模式的分层特征细化。分析揭示了与诊断标准一致的领域特定可控性特征、跨网络层次的空间选择性进展，以及扫描策略对注意力模式的显著影响。

Conclusion: 该框架将可控性分析确立为跨所有领域的SSM的统一、基础性可解释性范式，在计算机视觉、自然语言处理和跨领域任务中具有广泛应用前景。

Abstract: State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication

</details>


### [157] [Counting Through Occlusion: Framework for Open World Amodal Counting](https://arxiv.org/abs/2511.12702)
*Safaeid Hossain Arib,Rabeya Akter,Abdul Monaf Chowdhury,Md Jubair Ahmed Sourov,Md Mehedi Hasan*

Main category: cs.CV

TL;DR: CountOCC是一个解决遮挡条件下物体计数的模态计数框架，通过多模态引导重建被遮挡物体特征，在多个数据集上显著提升了遮挡场景下的计数准确率。


<details>
  <summary>Details</summary>
Motivation: 现有物体计数方法在遮挡场景下表现不佳，因为主干网络会编码遮挡表面而非目标物体，导致特征表示被破坏。需要一种能够重建被遮挡物体特征的解决方案。

Method: 使用层次化多模态引导，通过整合可见片段的空间上下文和文本/视觉嵌入的语义先验，在多个金字塔层级上生成类别区分性特征。引入视觉等价目标确保遮挡和未遮挡视图产生空间对齐的注意力图。

Result: 在FSC 147数据集上验证集和测试集分别减少26.72%和20.80%的MAE；在CARPK数据集上减少49.89% MAE；在CAPTUREReal数据集上减少28.79% MAE，均达到SOTA性能。

Conclusion: CountOCC通过互补机制保留了遮挡条件下准确计数所需的区分性特征，在多个视觉领域验证了其鲁棒的模态计数能力。

Abstract: Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.

</details>


### [158] [FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling](https://arxiv.org/abs/2511.12708)
*Kaiser Hamid,Can Cui,Khandakar Ashrafi Akbar,Ziran Wang,Nade Liang*

Main category: cs.CV

TL;DR: FSDAM是一个少样本驾驶员注意力建模框架，仅需约100个标注样本即可联合预测注意力分布和生成解释性描述，比现有方法少两个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖大规模注视数据集来学习驾驶员注意力模式，但这些数据集收集成本高、标注耗时。需要开发在数据受限场景下仍能有效工作的驾驶员注意力系统。

Method: 采用双路径架构，分别处理空间预测和描述生成，通过跨模态对齐保持语义一致性，实现少样本学习。

Result: 在注意力预测方面达到竞争性性能，生成连贯且上下文感知的解释，并在多个驾驶基准测试中展示出强大的零样本泛化能力。

Conclusion: 证明在有限监督下可以实现有效的注意力条件生成，为在数据受限场景中部署可解释的驾驶员注意力系统开辟了新可能性。

Abstract: Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.

</details>


### [159] [Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning](https://arxiv.org/abs/2511.12735)
*Ankita Raj,Chetan Arora*

Main category: cs.CV

TL;DR: 本文首次研究开放词汇目标检测器(OVODs)的后门攻击，提出TrAP多模态后门注入策略，通过联合优化图像和文本模态的提示参数与视觉触发器，在不重新训练基础模型权重的情况下植入后门。


<details>
  <summary>Details</summary>
Motivation: 随着OVODs在机器人、自动驾驶等高风险应用中的普及，理解其安全风险变得至关重要。本文旨在揭示提示调优引入的新攻击面。

Method: 提出TrAP攻击策略，采用多模态后门注入方法，联合优化图像和文本模态的提示参数与视觉触发器，使用基于课程学习的训练策略逐步缩小触发器尺寸。

Result: 实验表明TrAP在多个数据集上实现了高攻击成功率，包括目标误分类和目标消失攻击，同时在下游数据集上相比零样本设置提高了干净图像性能。

Conclusion: TrAP展示了OVODs面临的新型安全威胁，通过轻量级可学习提示令牌即可植入后门，同时保持模型的泛化能力。

Abstract: Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.

</details>


### [160] [Direct Visual Grounding by Directing Attention of Visual Tokens](https://arxiv.org/abs/2511.12738)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出了一种新的注意力监督损失函数KLAL，通过直接监督视觉标记的注意力分布来改进视觉语言模型在视觉任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在LLM模块中，与查询最相关的视觉标记在最终层很少或没有受到答案标记的注意力，这可能导致视觉问答错误。标准的下一个标记预测损失对视觉标记注意力的监督不足。

Method: 提出KL注意力损失(KLAL)，通过KL散度将视觉标记的注意力分布与真实注意力图对齐。真实注意力图来自合成案例中的任务几何或真实图像中的标准标注，无需新标签。

Result: KLAL与NTP结合后，在几何任务、指向和指代表达理解等任务上均取得显著改进，在合成和真实数据上都表现出色。

Conclusion: 直接监督视觉标记的注意力分布可以有效提升VLMs在视觉任务上的性能，特别是在需要精确视觉定位的任务中。

Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.

</details>


### [161] [Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests](https://arxiv.org/abs/2511.12740)
*Amirhossein Hassanzadeh,Bartosz Krawczyk,Michael Saunders,Rob Wible,Keith Krause,Dimah Dera,Jan van Aardt*

Main category: cs.CV

TL;DR: 该研究探索从体素化LiDAR点云数据推断低层级体素内容信息（目标占用百分比），提出基于核点卷积的多目标回归方法，通过密度相关性和加权损失函数处理类别不平衡问题，并分析不同体素尺寸对森林场景建模的影响。


<details>
  <summary>Details</summary>
Motivation: 体素化虽然能降低LiDAR数据处理的计算成本，但会导致细尺度结构信息丢失。本研究旨在探索是否可以从高层级体素化数据中恢复低层级体素内容信息，特别是目标在体素内的占用百分比。

Method: 使用DIRSIG软件生成的LiDAR点云数据，提出基于核点卷积的多目标回归方法，采用密度相关性处理类别不平衡问题，结合加权均方误差、焦点回归和正则化来优化模型。

Result: 敏感性分析显示，较大体素尺寸（如2米）由于变异性降低而误差较小，而较小体素尺寸（如0.25或0.5米）误差较高，特别是在树冠区域。树皮和树叶目标在较小体素尺寸数据集上的误差显著高于较大体素尺寸。

Conclusion: 体素尺寸的选择取决于具体应用需求。该研究填补了深度不平衡学习模型在多目标回归和森林3D LiDAR点云模拟数据集方面的空白。

Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.

</details>


### [162] [SAGE: Saliency-Guided Contrastive Embeddings](https://arxiv.org/abs/2511.12744)
*Colton R. Crum,Adam Czajka*

Main category: cs.CV

TL;DR: SAGE是一种通过对比嵌入将人类显著性先验整合到神经网络训练中的方法，在图像空间之外使用潜在空间嵌入来引导训练，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于显著性的训练方法依赖内部模型机制可能不可靠，且主要局限于图像空间。需要将人类显著性指导从图像空间转移到潜在空间。

Method: 使用对比嵌入方法，对输入应用显著性保持和显著性退化信号增强，通过对比三元组损失引导模型关注显著特征并远离非显著特征，同时对logit分布进行完整性检查。

Result: 在开放集和封闭集场景下，相比最先进的基于显著性的方法，分类性能均有提升，且在不同骨干网络和任务上表现出良好的泛化能力。

Conclusion: SAGE通过将人类显著性指导从图像空间转移到潜在空间，有效提升了模型性能，并在多种任务中展现出良好的泛化能力。

Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.

</details>


### [163] [Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion](https://arxiv.org/abs/2511.12757)
*Nicholas Karris,Luke Durell,Javier Flores,Tegan Emerson*

Main category: cs.CV

TL;DR: 论文发现Stable Diffusion对CLIP嵌入矩阵具有置换不变性，因此可以将嵌入视为Wasserstein空间中的点云而非欧几里得空间中的矩阵。通过将嵌入插值问题重构为最优传输问题，计算嵌入之间的最短路径，从而在Stable Diffusion中生成更平滑和连贯的插值图像。


<details>
  <summary>Details</summary>
Motivation: 发现Stable Diffusion对CLIP嵌入矩阵的置换不变性，这启发了将嵌入解释为Wasserstein空间中的点云而非欧几里得空间中的矩阵的新视角，为理解嵌入空间的几何结构开辟了新可能性。

Method: 将嵌入插值问题重构为最优传输问题，通过求解该问题计算嵌入之间的最短路径（测地线），从而在嵌入空间中实现更自然和几何平滑的过渡。

Result: 实验表明，基于最优传输的方法相比其他标准插值方法能产生更平滑的图像插值，说明将嵌入视为点云（而非矩阵）能更好地反映和利用嵌入空间的几何特性。

Conclusion: 将CLIP嵌入视为Wasserstein空间中的点云而非欧几里得空间中的矩阵，通过最优传输方法进行插值，能够生成更平滑和连贯的中间图像，这更好地反映了嵌入空间的几何结构。

Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.

</details>


### [164] [RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.12767)
*Cătălin-Alexandru Rîpanu,Andrei-Theodor Hotnog,Giulia-Stefania Imbrea,Dumitru-Clementin Cercel*

Main category: cs.CV

TL;DR: 本文介绍了RoCoISLR数据集，这是首个针对罗马尼亚孤立手语识别的大规模标准化数据集，包含9000多个视频样本和近6000个标准化手势词汇，并评估了7种最先进的视频识别模型。


<details>
  <summary>Details</summary>
Motivation: 目前大多数手语识别数据集都专注于美国手语，而罗马尼亚孤立手语识别缺乏大规模标准化数据集，这限制了该领域的研究进展。

Method: 创建了RoCoISLR数据集，包含9000多个视频样本和近6000个标准化手势词汇。在一致的实验设置下评估了7种最先进的视频识别模型：I3D、SlowFast、Swin Transformer、TimeSformer、Uniformer、VideoMAE和PoseConv3D。

Result: 基于Transformer的架构优于基于卷积的基线模型，其中Swin Transformer获得了34.1%的Top-1准确率。研究还揭示了低资源手语中长尾类分布带来的挑战。

Conclusion: RoCoISLR为系统性的罗马尼亚孤立手语识别研究提供了初步基础，填补了该领域的数据集空白。

Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.

</details>


### [165] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: 提出一种轻量级的颜色协调方法MKL-Harmonizer，用于增强现实中的实时颜色协调，通过紧凑编码器预测Monge-Kantorovich传输映射。


<details>
  <summary>Details</summary>
Motivation: 解决增强现实中颜色协调问题，现有算法缺乏实时解决方案，无法集成到AR流程中。

Method: 利用经典最优传输理论，训练紧凑编码器预测Monge-Kantorovich传输映射，支持设备端推理。

Result: 在真实AR合成图像上，MKL-Harmonizer方法获得最佳综合评分，优于现有最先进方法。

Conclusion: 提出的轻量级方法成功解决了AR颜色协调的实时需求，并发布了专用AR数据集和工具包支持进一步研究。

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [166] [Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation](https://arxiv.org/abs/2511.12801)
*Andrew Zhou*

Main category: cs.CV

TL;DR: 提出了一种不确定性感知的脑肿瘤分割框架，在nnUNet基础上增加体素级不确定性通道，同时实现了肿瘤分割和健康脑结构分割，为临床手术决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在脑肿瘤分割中缺乏不确定性估计，无法提供分割错误的置信度评分，同时缺乏对肿瘤周围健康脑结构的分割，限制了临床手术应用。

Method: 在nnUNet架构中增加不确定性通道，使用BraTS2023数据集训练，能够单次推理同时输出肿瘤分割和不确定性图；通过整合正常和癌症数据集构建统一模型，实现全脑结构分割。

Result: 不确定性估计达到0.750的相关性和0.047的RMSD，不影响肿瘤分割精度；统一模型在脑结构分割上DSC达0.81，肿瘤分割DSC达0.86，关键区域表现稳健。

Conclusion: 该框架首次实现了在自然解剖背景下输出肿瘤分割结果并叠加不确定性图，为AI辅助外科决策提供关键洞察，能够评估预测质量并修正错误。

Abstract: Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.

</details>


### [167] [MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12810)
*Leena Alghamdi,Muhammad Usman,Hafeez Anwar,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: 提出了一种多尺度递归网络MSRNet，通过金字塔视觉Transformer提取多尺度特征，结合注意力机制和递归解码策略，在伪装目标检测任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 伪装目标检测面临低光照、部分遮挡、小目标、复杂背景模式和多目标等挑战，现有方法在复杂场景中仍难以精确检测伪装目标，特别是小目标和多目标情况。

Method: 使用金字塔视觉Transformer作为骨干网络提取多尺度特征，通过注意力机制进行特征融合，采用递归解码策略和细粒度融合单元优化特征表示。

Result: 在两个基准数据集上达到最先进性能，在另外两个数据集上排名第二，能够成功检测小目标和多伪装目标。

Conclusion: 多尺度学习和递归特征优化的联合使用能够有效提升伪装目标检测性能，特别是在处理小目标和多目标场景时表现出色。

Abstract: Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.

</details>


### [168] [SAGA: Source Attribution of Generative AI Videos](https://arxiv.org/abs/2511.12834)
*Rohit Kundu,Vishal Mohanty,Hao Xiong,Shan Jia,Athula Balachandran,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: SAGA是首个全面解决AI生成视频来源归属的框架，通过多粒度归属识别和高效预训练策略，仅需0.5%的源标记数据即可实现最先进的归属性能，并提供可解释的时间注意力签名。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的普及导致超逼真合成视频泛滥，传统二元真伪检测器已无法应对滥用风险，迫切需要大规模AI生成视频来源归属技术。

Method: 提出新颖的视频transformer架构，利用鲁棒视觉基础模型特征捕捉时空伪影；采用数据高效的预训练-归属策略；引入时间注意力签名(T-Sigs)可解释性方法。

Result: 在公共数据集上的广泛实验表明，SAGA在合成视频溯源方面设立了新基准，在跨域场景下表现优异，匹配全监督性能。

Conclusion: SAGA为法证和监管应用提供了关键且可解释的洞察，是首个实现多粒度AI生成视频来源归属的综合性框架。

Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.

</details>


### [169] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: 本文提出视觉思维链(vCoT)方法，通过生成帧间过渡事件描述来增强多模态大语言模型的视频理解能力，发现视频微调模型已隐式掌握时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视频理解上往往只是简单拼接帧标记，缺乏对帧间时序关系的显式建模。

Method: 提出视觉思维链(vCoT)，在连续帧之间生成过渡事件描述作为显式推理过程，并系统比较图像模型与视频微调模型的性能差异。

Result: vCoT显著提升图像模型在长视频问答任务上的表现，但对视频微调模型增益有限；视频模型能将时序推理能力迁移到静态视觉推理任务。

Conclusion: 视频微调使模型隐式学习帧间过渡关系，这种时序推理能力具有可迁移性，为多模态模型的时间理解提供了新视角。

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [170] [View-aware Cross-modal Distillation for Multi-view Action Recognition](https://arxiv.org/abs/2511.12870)
*Trung Thanh Nguyen,Yasutomo Kawanishi,Vijay John,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: 提出了ViCoKD框架，通过跨模态知识蒸馏解决部分重叠多视图动作识别问题，在模态和标注受限条件下提升性能


<details>
  <summary>Details</summary>
Motivation: 现实世界中多传感器系统通常只有部分重叠视图和有限输入模态，且缺乏密集帧级标注，现有方法对此研究不足

Method: 使用跨模态适配器和注意力机制，让模态受限的学生模型利用多模态相关性；提出视图感知一致性模块处理视图不对齐问题

Result: 在MultiSensor-Home数据集上，ViCoKD在不同骨干网络和环境设置下均优于竞争方法，在受限条件下甚至超越教师模型

Conclusion: ViCoKD能有效处理部分重叠多视图和模态受限的动作识别问题，为现实世界应用提供了实用解决方案

Abstract: The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.

</details>


### [171] [Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views](https://arxiv.org/abs/2511.12878)
*Junyi Ma,Wentao Bao,Jingyi Xu,Guanzhong Sun,Yu Zheng,Erhang Zhang,Xieyuanli Chen,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出EgoLoc方法，用于在自我中心视频中零样本定位手与物体接触和分离的时间点，解决现有方法依赖类别标注和物体掩码的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注交互行为模式（如何交互），但更精细的手与目标物体接触/分离时刻定位（何时交互）问题尚未充分探索，这对混合现实和机器人运动规划至关重要。

Method: 提出EgoLoc方法：引入手部动态引导采样生成高质量视觉提示，利用视觉语言模型识别接触/分离属性、定位特定时间戳，并提供闭环反馈进行进一步优化。

Result: 在公共数据集和新基准测试上的综合实验表明，EgoLoc在自我中心视频中实现了可信的时间交互定位，并在多个下游应用中有效促进自我中心视觉和机器人操作任务。

Conclusion: EgoLoc消除了对物体掩码和动词-名词分类法的需求，实现了可推广的零样本实现，为混合现实和机器人应用提供了有效的时间交互定位解决方案。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [172] [Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings](https://arxiv.org/abs/2511.12880)
*Zihao Lin,Zhenshan Shi,Sasa Zhao,Hanwei Zhu,Lingyu Zhu,Baoliang Chen,Lei Mo*

Main category: cs.CV

TL;DR: 提出一个基于数据和认知理论的可解释性绘画创造力自动评估框架，通过分析绘画内容和风格两个维度来预测创造力分数。


<details>
  <summary>Details</summary>
Motivation: 当前绘画创造力评估主要依赖专家主观评分，既费时又主观。受认知科学启发，创造力可以从绘画内容（画什么）和风格（怎么画）两个互补维度来理解。

Method: 首先扩展现有创造力标注数据集，增加内容类别标注。然后提出多模态多任务学习框架，同时预测创造力分数、分类内容类型和提取风格特征。引入条件学习机制，根据绘画的风格和语义线索动态调整视觉特征提取。

Result: 实验结果显示，该模型相比现有基于回归的方法达到了最先进的性能，并能提供与人类判断一致的可解释可视化结果。

Conclusion: 该研究为自动、可解释的创造力评估提供了有效解决方案，代码和标注将公开发布。

Abstract: Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025

</details>


### [173] [ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation](https://arxiv.org/abs/2511.12893)
*Kaixin Zhang,Ruiqing Yang,Yuan Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: ActVAR通过动态激活框架在VAR模型中引入权重和token序列的双重稀疏性，在保持模型容量的同时提升效率，实现了21.2%的FLOPs减少且性能损失最小。


<details>
  <summary>Details</summary>
Motivation: 解决VAR模型随着序列长度增长计算成本急剧上升的问题，同时避免现有静态剪枝方法永久移除权重或token导致的性能下降和预训练依赖关系破坏。

Method: 将前馈网络分解为轻量级专家子网络，使用可学习路由器动态选择token特定的专家子集；同时通过门控token选择器识别高更新潜力token进行计算，重构未选中token以保持全局上下文和序列对齐；采用两阶段知识蒸馏策略训练。

Result: 在ImageNet 256×256基准测试中，ActVAR实现了高达21.2%的FLOPs减少，同时保持最小性能下降。

Conclusion: ActVAR提供了一种有效的动态激活框架，能够在保持VAR模型性能的同时显著提升计算效率，为大规模视觉自回归模型的高效部署提供了可行方案。

Abstract: Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.

</details>


### [174] [Reconstructing 3D Scenes in Native High Dynamic Range](https://arxiv.org/abs/2511.12895)
*Kaixuan Zhang,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: 提出了首个直接从原生HDR数据重建3D场景的方法NH-3DGS，通过新颖的亮度-色度分解技术，在整个重建流程中保持完整动态范围。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重建主要基于LDR数据，限制了在专业工作流程中的应用。虽然已有方法可从LDR重建HDR场景，但依赖多曝光融合或逆色调映射，增加了捕获复杂性且需要合成监督。

Method: 提出NH-3DGS方法，采用新颖的亮度-色度分解颜色表示，直接从原生HDR相机数据进行优化，在3D高斯溅射框架中保持完整动态范围。

Result: 在合成和真实多视角HDR数据集上，NH-3DGS在重建质量和动态范围保持方面显著优于现有方法，能够直接从原生HDR捕获实现专业级3D重建。

Conclusion: 该方法为专业数字媒体创作提供了直接从原生HDR数据重建3D场景的有效解决方案，代码和数据集将公开。

Abstract: High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.

</details>


### [175] [FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI](https://arxiv.org/abs/2511.12899)
*Hao Li,Zhenfeng Zhuang,Jingyu Lin,Yu Liu,Yifei Chen,Qiong Peng,Lequan Yu,Liansheng Wang*

Main category: cs.CV

TL;DR: 提出了频率分解预处理（FDP）框架，通过频域分析提升脑MRI无监督异常检测性能，无需人工异常模拟即可实现病理抑制和解剖结构保持。


<details>
  <summary>Details</summary>
Motivation: 脑MRI解剖结构多样性和标注数据稀缺使得监督异常检测困难，现有无监督方法使用人工噪声模拟异常缺乏生物物理真实性和形态复杂性。

Method: 通过系统频域分析发现异常具有独特频率模式，提出FDP框架利用频域重建同时实现病理抑制和解剖保持，可与现有异常模拟技术无缝集成。

Result: FDP显著提升异常检测性能，与LDM结合时DICE分数提高17.63%，在多个基准方法上均保持稳健改进。

Conclusion: FDP是首个利用频域重建的无监督异常检测方法，通过频率分解有效区分正常解剖和病理特征，为脑MRI异常检测提供了新思路。

Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.

</details>


### [176] [DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2511.12908)
*Junbo Zou,Haotian Xia,Zhen Ye,Shengjie Zhang,Christopher Lai,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: DeepSport是首个端到端训练的多模态大语言模型框架，专为多任务、多运动的视频理解设计，通过主动推理和专门帧提取工具实现"用视频思考"。


<details>
  <summary>Details</summary>
Motivation: 现有体育视频理解方法存在局限：要么专注于单一运动，要么局限于特定任务，或者依赖无训练范式缺乏稳健的学习推理过程。需要解决这一研究空白。

Method: 提出数据蒸馏管道从10个不同数据源合成高质量思维链轨迹，创建78k训练数据集；采用两阶段训练策略：监督微调后接强化学习，使用新颖的门控工具使用奖励优化推理过程。

Result: 在6.7k问题测试基准上的广泛实验表明，DeepSport达到最先进性能，显著优于专有模型和开源模型基线。

Conclusion: 这项工作为领域特定的视频推理建立了新基础，以应对多样化体育的复杂性。

Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.

</details>


### [177] [CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection](https://arxiv.org/abs/2511.12909)
*Yaohua Zha,Xue Yuerong,Chunlin Fan,Yuansong Wang,Tao Dai,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出了一个基于曲率增强的自监督学习框架（CASL），用于3D异常检测，该框架通过多尺度曲率提示指导解码器预测点云坐标，在不依赖特定异常检测机制的情况下实现了领先的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D异常检测方法通常针对特定任务设计，缺乏通用性；而经典的自监督点云模型在统一微调范式下对异常检测效果不佳，因此需要开发一个更通用的3D模型。

Method: 基于U-Net架构，引入多尺度曲率提示来指导解码器预测每个点的空间坐标，通过重建范式进行自监督学习，然后通过简单的异常分类微调实现异常检测。

Result: 仅使用点曲率作为异常评分已优于多个经典自监督和专用异常检测模型；CASL框架在不依赖专用异常检测机制的情况下实现了领先的检测性能，且学到的表示能很好地泛化到标准3D理解任务。

Conclusion: 曲率在3D异常检测中起着关键作用；CASL框架提供了一个通用的3D模型解决方案，既能有效检测异常，又能适应其他3D理解任务。

Abstract: Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.

</details>


### [178] [Explore How to Inject Beneficial Noise in MLLMs](https://arxiv.org/abs/2511.12917)
*Ruishu Zhu,Sida Huang,Ziheng Jiao,Hongyuan Zhang*

Main category: cs.CV

TL;DR: 提出了一种通过注入有益随机噪声的新型微调策略MuNG，仅需调整1-2%的额外参数，就能超越全参数微调和其他现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法往往忽略跨模态异质性，限制了多模态大语言模型的潜力。

Method: 从变分推理角度重新制定MLLMs的推理过程，设计多模态噪声生成器动态分析图像-文本对的跨模态关系，生成任务自适应的有益噪声。

Result: 在QwenVL和LLaVA上的实验表明，该方法超越了全参数微调和其他现有微调方法。

Conclusion: 注入有益噪声能有效抑制不相关的语义成分，显著改善跨模态表示对齐，提升下游任务性能。

Abstract: Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\sim2\%$ additional parameters. The relevant code is uploaded in the supplementary.

</details>


### [179] [CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation](https://arxiv.org/abs/2511.12919)
*Dexin Zuo,Ang Li,Wei Wang,Wenxian Yu,Danping Zou*

Main category: cs.CV

TL;DR: CoordAR是一个用于未见物体6D姿态估计的自回归框架，仅需单张参考视图而非完整3D模型，通过离散化坐标映射和概率预测解决对称性和遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于单参考视图的6D姿态估计方法依赖实值坐标回归，受限于卷积架构的局部性质导致全局一致性不足，且在对称或遮挡场景中缺乏不确定性建模。

Method: 1) 坐标映射标记化实现离散3D空间的概率预测；2) 模态解耦编码策略分别处理RGB外观和坐标线索；3) 基于位置对齐查询特征和部分生成标记序列的自回归Transformer解码器。

Result: 在多个基准测试中显著优于现有方法，在真实世界测试中对对称性、遮挡等挑战表现出强鲁棒性。

Conclusion: CoordAR通过自回归概率建模有效解决了单参考6D姿态估计中的全局一致性和不确定性挑战，为未见物体的姿态估计提供了可靠解决方案。

Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

</details>


### [180] [Generative Photographic Control for Scene-Consistent Video Cinematic Editing](https://arxiv.org/abs/2511.12921)
*Huiqiang Sun,Liao Shen,Zhan Peng,Kun Wang,Size Wu,Yuhang Zang,Tianqi Liu,Zihao Huang,Xingyu Zeng,Zhiguo Cao,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: CineCtrl是首个提供专业相机参数精细控制的视频电影编辑框架，通过解耦交叉注意力机制分离相机运动和摄影输入，实现独立控制而不影响场景一致性。


<details>
  <summary>Details</summary>
Motivation: 生成视频模型中控制摄影效果（如景深、曝光）极具挑战，现有方法大多仅限于相机运动控制，无法实现专业摄影参数的精细编辑。

Method: 提出解耦交叉注意力机制分离相机运动和摄影输入；开发综合数据生成策略，结合模拟摄影效果和真实世界采集管道构建大规模数据集。

Result: 模型能够生成具有精确控制、用户指定摄影相机效果的高保真视频。

Conclusion: CineCtrl框架成功实现了对专业相机参数的精细控制，为视频电影编辑提供了新的可能性。

Abstract: Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.

</details>


### [181] [Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes](https://arxiv.org/abs/2511.12932)
*Feng Lv,Haoxuan Feng,Zilu Zhang,Chunlong Xia,Yanfeng Li*

Main category: cs.CV

TL;DR: 提出了一个统一的文本驱动框架，用于交通场景的图像生成和编辑，通过可控掩码机制和多视角数据增强几何多样性，采用两阶段训练策略提升文本-图像对齐和细节质量。


<details>
  <summary>Details</summary>
Motivation: 解决智能交通系统中文本驱动图像生成和编辑面临的挑战：交通元素语义丰富度不足、相机视角有限、合成图像视觉保真度低、文本描述与生成内容对齐差。

Method: 使用可控掩码机制统一图像生成和编辑任务；结合车端和路端多视角数据增强几何多样性；采用两阶段训练（大规模粗粒度数据概念学习+细粒度描述数据微调）；引入掩码区域加权损失动态强调关键小区域。

Result: 在交通场景的文本驱动图像生成和编辑任务中取得了领先性能。

Conclusion: 所提出的框架有效解决了交通场景图像生成和编辑中的关键问题，显著提升了生成质量和文本-图像对齐效果。

Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.

</details>


### [182] [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/abs/2511.12935)
*Dianbing Xi,Guoyuan An,Jingsen Zhu,Zhijian Liu,Yuan Liu,Ruiyuan Zhang,Jiayuan Lu,Rui Wang,Yuchi Huo*

Main category: cs.CV

TL;DR: PFAvatar是一种从日常穿搭照片重建高质量3D虚拟形象的新方法，通过两阶段流程：首先微调姿势感知扩散模型，然后蒸馏为NeRF表示的3D虚拟形象，在5分钟内完成个性化，比之前方法快48倍。


<details>
  <summary>Details</summary>
Motivation: 解决从日常穿搭照片重建3D虚拟形象的挑战，这些照片通常包含多样姿势、遮挡和复杂背景，传统方法分解图像为服装配饰进行3D组装容易导致不一致。

Method: 两阶段方法：1) 使用ControlNet进行姿势估计和条件先验保持损失微调扩散模型；2) 通过规范SMPL-X空间采样和多分辨率3D-SDS优化NeRF表示，避免网格表示的离散化问题。

Result: PFAvatar在重建保真度、细节保留和对遮挡/截断的鲁棒性方面优于最先进方法，能够正确处理遮挡并保留高频纹理如头发细节。

Conclusion: 该方法推进了从真实世界日常穿搭相册生成实用3D虚拟形象的能力，支持虚拟试穿、动画和视频重演等下游应用。

Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

</details>


### [183] [ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios](https://arxiv.org/abs/2511.12938)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: ProtoAnomalyNCD是一个基于原型学习的框架，用于发现和分类多种未见过的工业异常类型，通过结合Grounded SAM定位目标区域和异常图引导注意力机制来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法主要判断是否存在异常，但实际应用需要发现和分类多种异常类型。由于工业异常语义细微且现有方法未能充分利用图像先验，直接聚类方法效果不佳。

Method: 1) 使用Grounded SAM和文本提示定位目标区域作为先验；2) 引入异常图引导注意力块，设计区域引导因子区分背景、目标区域和异常区域；3) 在统一原型学习框架下发现未见异常类并进行多类型异常分类。

Result: 在MVTec AD、MTD和Real-IAD数据集上优于最先进方法，并能扩展到检测未见异常值，实现任务级统一。

Conclusion: ProtoAnomalyNCD通过结合目标区域定位和异常图引导注意力，有效提升了多类型工业异常发现和分类的性能。

Abstract: Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.

</details>


### [184] [Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking](https://arxiv.org/abs/2511.12939)
*Wei Jiang,Jiahao Cui,Yizheng Wu,Zhan Peng,Zhiyu Pan,Zhiguo Cao*

Main category: cs.CV

TL;DR: 提出了一种基于半监督学习的高动态范围图像重建方法，通过教师模型生成伪HDR标签，并使用不确定性掩码机制过滤不可靠区域，仅需6.7%的HDR真实标签即可达到全监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于学习的方法需要LDR-HDR图像对，但这些配对数据难以获取，因此研究如何在有限HDR真实标签下实现可比较性能的标注高效HDR图像重建方法。

Method: 采用半监督学习框架，教师模型为无标签的LDR样本生成伪HDR标签，提出基于不确定性的像素级和块级掩码过程来过滤伪标签中的不可靠区域，学生模型仅从可信区域学习。

Result: 该方法不仅优于之前的标注高效算法，而且仅使用6.7%的HDR真实标签即可达到最新全监督方法的可比性能。

Conclusion: 提出的不确定性掩码机制有效缓解了确认偏差问题，使得半监督HDR重建方法在有限标注数据下也能取得优异性能。

Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.

</details>


### [185] [Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention](https://arxiv.org/abs/2511.12940)
*Taiye Chen,Zihan Ding,Anjian Li,Christina Zhang,Zeqi Xiao,Yisen Wang,Chi Jin*

Main category: cs.CV

TL;DR: 本文提出了一种名为RAD的循环自回归扩散框架，通过在扩散变换器中引入LSTM来解决长视频生成中的记忆遗忘和时空不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型在生成长视频时缺乏有效的记忆压缩和检索机制，导致超出窗口大小后出现遗忘和时空不一致问题。

Method: 在扩散变换器框架中引入循环神经网络（RNN），特别是带有注意力的LSTM，并提出RAD框架实现训练和推理时一致的帧级自回归记忆更新与检索。

Result: 在Memory Maze和Minecraft数据集上的实验表明，RAD在长视频生成方面表现优越，证明了LSTM在序列建模中的有效性。

Conclusion: RAD框架通过结合LSTM和扩散模型，有效解决了长视频生成中的记忆保持问题，为视频世界模型的发展提供了新思路。

Abstract: Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.

</details>


### [186] [T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving](https://arxiv.org/abs/2511.12956)
*Chen Ma,Ningfei Wang,Junhao Zheng,Qing Guo,Qian Wang,Qi Alfred Chen,Chao Shen*

Main category: cs.CV

TL;DR: DiffSign是一个基于文本到图像生成的新型交通标志识别对抗攻击框架，通过CLIP损失和掩码提示提高攻击效果，能够生成物理鲁棒、高效、可迁移且隐蔽的外观攻击。


<details>
  <summary>Details</summary>
Motivation: 现有交通标志识别系统的对抗攻击方法存在局限性：像素级扰动方法缺乏隐蔽性且过拟合特定模型，扩散模型方法效果有限且泛化能力差。

Method: 提出DiffSign框架，集成CLIP损失和掩码提示提高攻击聚焦性和可控性，采用两种新颖的风格定制方法指导视觉外观，提升跨域交通标志攻击泛化能力和隐蔽性。

Result: 在多种真实世界条件下（不同距离、角度、光照条件和标志类别）评估，平均物理世界攻击成功率达到83.3%，具有高攻击可迁移性。

Conclusion: DiffSign能够生成物理鲁棒、高效、可迁移、实用且隐蔽的外观攻击，克服了现有方法的局限性。

Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.
  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.

</details>


### [187] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: EndoSight AI是一个深度学习架构，用于在胃肠内窥镜检查中实时检测和分割息肉，在GPU上达到35+ FPS的实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 在内窥镜检查过程中精确实时检测胃肠道息肉对于结直肠癌的早期诊断和预防至关重要。

Method: 利用公开的Hyper-Kvasir数据集，采用深度学习架构，结合临床相关性能指标和新型热感知程序来确保模型鲁棒性和效率。

Result: 系统在息肉检测方面达到88.3%的平均精度(mAP)，分割方面Dice系数最高达69%，在GPU硬件上实现超过35帧/秒的实时推理速度。

Conclusion: 这种集成的AI解决方案旨在无缝部署到内窥镜工作流程中，有望提高胃肠道医疗的诊断准确性和临床决策能力。

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [188] [CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models](https://arxiv.org/abs/2511.12964)
*Mehrab Mustafy Rahman,Jayanth Mohan,Tiberiu Sosea,Cornelia Caragea*

Main category: cs.CV

TL;DR: CalibrateMix是一种针对半监督学习模型的校准方法，通过识别易学和难学样本进行目标性mixup，在保持或提高分类准确率的同时改善模型校准效果。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法存在校准问题，模型会产生过度自信的预测。虽然mixup在监督学习中能改善校准，但在半监督学习中由于伪标签的不可靠性，随机mixup面临挑战。

Method: 利用标记和未标记样本的训练动态识别易学和难学样本，然后对易学和难学样本进行目标性mixup。

Result: 在多个基准图像数据集上的实验结果表明，该方法相比现有半监督学习方法实现了更低的预期校准误差和更高的准确率。

Conclusion: CalibrateMix能够有效改善半监督学习模型的校准性能，同时维持或提升分类准确率。

Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

</details>


### [189] [GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.12968)
*Ning Han,Zhenyu Ge,Feng Han,Yuhua Sun,Chengqing Li,Jingjing Chen*

Main category: cs.CV

TL;DR: 提出GrOCE框架，无需训练即可通过图语义推理实现精确自适应的概念擦除，在概念相似度和图像质量指标上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法要么依赖昂贵微调，要么语义分离粗糙，会损害无关概念且难以适应动态概念集

Method: 构建动态语义图建模概念关系，包含动态拓扑图构建、自适应聚类识别和选择性边切断三个组件

Result: 在概念相似度和FID指标上达到最优性能，实现高效准确稳定的概念擦除

Conclusion: GrOCE提供了一种无需重新训练的训练无关框架，能够精确自适应地移除有害内容同时保留非目标语义

Abstract: Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fréchet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.

</details>


### [190] [HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology](https://arxiv.org/abs/2511.12969)
*Ziqiao Weng,Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee AD Cooper,Weidong Cai,Bo Zhou*

Main category: cs.CV

TL;DR: HiFusion是一个深度学习框架，通过层次化建模和跨尺度融合，从H&E染色全玻片图像预测空间转录组基因表达，在多个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 空间转录组技术面临临床采用障碍，现有计算方法难以捕捉斑点内的生物异质性，且在整合周围组织上下文信息时易受形态学噪声影响。

Method: 提出HiFusion框架，包含层次化斑点内建模模块（多分辨率子补丁分解）和上下文感知跨尺度融合模块（交叉注意力机制），综合建模细胞级特征和组织微环境线索。

Result: 在两个基准ST数据集上的实验表明，HiFusion在2D切片交叉验证和更具挑战性的3D样本特定场景中均达到最先进性能。

Conclusion: HiFusion作为从常规组织病理学进行ST推断的稳健、准确且可扩展解决方案具有巨大潜力。

Abstract: Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.

</details>


### [191] [MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning](https://arxiv.org/abs/2511.12976)
*Yoonjae Seo,Ermal Elbasani,Jaehong Lee*

Main category: cs.CV

TL;DR: MCAQ-YOLO是一种基于形态复杂度的自适应量化框架，通过五种形态学指标动态调整空间位精度，在目标检测任务中实现比均匀量化更高的准确率和收敛效率。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络量化方法在空间区域上采用统一的位精度，忽略了视觉数据的异构结构和纹理复杂性。

Method: 使用五种形态学指标（分形维度、纹理熵、梯度方差、边缘密度和轮廓复杂度）来表征局部视觉形态，并基于这些指标与量化敏感性的相关性进行空间自适应位分配；采用课程式量化感知训练方案逐步增加量化难度。

Result: 在安全设备数据集上达到85.6% mAP@0.5，平均4.2位精度和7.6倍压缩比，比均匀4位量化高3.5个百分点mAP，每张图像仅增加1.8ms运行时间；在COCO和Pascal VOC数据集上验证了性能一致性。

Conclusion: 形态驱动的空间量化可以提升计算受限、安全关键视觉识别任务的效率和鲁棒性。

Abstract: Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.

</details>


### [192] [ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes](https://arxiv.org/abs/2511.12977)
*Yixuan Yang,Luyang Xie,Zhen Luo,Zixiang Zhao,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: ArtiWorld是一个从文本场景描述中自动识别可关节化对象并重建可执行URDF模型的场景感知流水线，核心是Arti4URDF，利用3D点云、大语言模型先验知识和URDF导向提示设计，将刚性对象转换为交互式关节对象。


<details>
  <summary>Details</summary>
Motivation: 构建交互式模拟器和可扩展机器人学习环境需要大量关节化资产，但现有3D资产大多是刚性的，手动转换成本极高，因此需要自动识别和转换可关节化对象的方法。

Method: 使用3D点云、大语言模型先验知识和URDF导向提示设计，通过Arti4URDF组件将刚性对象快速转换为交互式URDF关节对象，同时保持原始3D形状。

Result: 在3D模拟对象、完整3D模拟场景和真实世界扫描场景三个层面评估，方法始终优于现有方法并达到最先进性能，保持对象几何形状并正确捕捉对象交互性。

Conclusion: 为直接从现有3D资产构建交互式、机器人就绪的模拟环境提供了实用路径。

Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.

</details>


### [193] [Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach](https://arxiv.org/abs/2511.12978)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: 提出了一种基于聚类的概念重要性方法CCI，用于分析CLIP等视觉语言模型中的虚假相关性，特别是背景过度依赖问题。该方法通过聚类补丁嵌入、掩码和评估预测变化来提升模型解释性，并在COVAR新基准上评估了18个CLIP变体。


<details>
  <summary>Details</summary>
Motivation: 对比视觉语言模型如CLIP在零样本识别方面表现强劲，但容易受到虚假相关性的影响，特别是过度依赖背景信息。现有基准如CounterAnimals仅依赖准确性，无法区分不同类型的错误来源。

Method: 提出CCI方法：使用CLIP自身的补丁嵌入将空间补丁分组为语义连贯的聚类，掩码这些聚类，并评估模型预测的相对变化。结合GroundedSAM自动将预测分类为前景驱动或背景驱动。

Result: CCI在忠实性基准测试中达到新的最先进水平，在MS COCO检索的删除AUC指标上实现了两倍以上的改进。通过COVAR基准系统性地评估了18个CLIP变体，揭示了视角变化、尺度偏移和细粒度对象混淆等错误来源。

Conclusion: CCI方法提供了关键的诊断能力，结合COVAR基准能够全面评估视觉语言模型的鲁棒性，为开发更稳健的视觉语言模型指明了方向。

Abstract: Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.

</details>


### [194] [UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective](https://arxiv.org/abs/2511.12988)
*Furui Xu,Shaobo Wang,Jiajun Zhang,Chenghao Sun,Haixiang Tang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出UNSEEN框架，从泛化视角进行数据集剪枝，通过使用未见过样本的模型进行评分，解决传统方法中评分密集分布问题，并在多步场景中优化核心集质量。


<details>
  <summary>Details</summary>
Motivation: 传统数据集剪枝方法基于训练阶段模型性能评分，导致样本评分密集分布在狭窄数值范围内，降低了样本区分度，影响选择效果。

Method: 提出UNSEEN框架：1）从泛化视角评分，使用未见过样本的模型；2）扩展到多步场景，通过在不同核心集上训练的模型进行增量选择；3）动态优化核心集质量。

Result: 在CIFAR-10、CIFAR-100和ImageNet-1K上显著优于现有SOTA方法，在ImageNet-1K上减少30%训练数据仍能实现无损性能。

Conclusion: 从泛化视角进行数据集剪枝能有效提高样本区分度，UNSEEN框架在多个数据集上表现出色，为大规模深度学习提供了高效的数据集压缩方案。

Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

</details>


### [195] [Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection](https://arxiv.org/abs/2511.12992)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 提出WSAE-Net方法，通过加权语义图和自适应候选编辑序列，优化非生成式视觉反事实解释的语义相关性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统非生成式视觉反事实解释方法在替换图像区域时忽略语义相关性，损害模型可解释性并阻碍编辑流程。

Method: WSAE-Net包含两个关键创新：1）加权语义图生成，减少非语义特征单元计算；2）自适应候选编辑序列，确定最优计算顺序。

Result: 通过全面实验验证，该方法展现出优越性能，有助于更清晰深入地理解视觉反事实解释。

Conclusion: WSAE-Net通过语义相关性和计算效率的优化，显著提升了非生成式视觉反事实解释的质量和实用性。

Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.

</details>


### [196] [PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching](https://arxiv.org/abs/2511.12998)
*Zewei Chang,Zheng-Peng Duan,Jianxing Zhang,Chun-Le Guo,Siyu Liu,Hyungju Chun,Hyunhee Park,Zikun Liu,Chongyi Li*

Main category: cs.CV

TL;DR: PerTouch是一个基于扩散模型的图像润色框架，支持语义级图像润色，通过参数映射实现细粒度控制，并利用VLM驱动的智能体处理用户指令，实现个性化图像增强。


<details>
  <summary>Details</summary>
Motivation: 解决图像润色中可控性与主观性平衡的挑战，满足用户个性化审美偏好，同时保持全局美学效果。

Method: 使用包含语义区域属性值的参数映射作为输入，构建显式的参数到图像映射；引入语义替换和参数扰动机制提升语义边界感知；开发VLM驱动的智能体处理用户指令，具备反馈驱动反思和场景感知记忆机制。

Result: 广泛实验验证了各组件有效性，PerTouch在个性化图像润色方面表现出优越性能。

Conclusion: PerTouch通过统一的扩散框架成功实现了语义级图像润色，能够更好地对齐用户意图并捕捉长期偏好。

Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.

</details>


### [197] [Medal S: Spatio-Textual Prompt Model for Medical Segmentation](https://arxiv.org/abs/2511.13001)
*Pengcheng Shi,Jiawei Chen,Jiaqi Liu,Xinglin Zhang,Tao Chen,Lei Li*

Main category: cs.CV

TL;DR: Medal S是一个医学分割基础模型，支持原生分辨率空间和文本提示的端到端训练框架，在5种医学影像模态上实现高效的多类别分割，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本提示方法缺乏空间感知能力的问题，以及分辨率不匹配导致的精度损失，实现空间精度与语义文本指导的协调。

Method: 采用通道级对齐的体素提示和文本嵌入，轻量级3D卷积模块进行体素空间优化，支持并行空间提示处理，提出动态重采样、两阶段推理等优化策略。

Result: 在BiomedSegFM数据集验证集上，5种模态平均DSC达75.44（vs SAT 69.83），NSD达77.34（vs 71.06），并行提示使推理时间减少90%以上。

Conclusion: Medal S通过协调空间精度与语义文本指导，在多类别医学分割任务中展现出卓越的效率和准确性，优于基于顺序提示的方法。

Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.

</details>


### [198] [Infinite-Story: A Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2511.13002)
*Jihun Park,Kyoungmin Lee,Jongmin Gim,Hyeonseo Jo,Minseok Oh,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Minwoo Choi,Sunghoon Im*

Main category: cs.CV

TL;DR: Infinite-Story是一个无需训练、基于尺度自回归模型的文本到图像生成框架，专门解决多提示词故事生成中的身份和风格不一致问题，提供6倍加速推理。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中多提示词场景下的身份不一致和风格不一致问题，现有方法要么需要微调要么推理速度慢。

Method: 提出三种互补技术：身份提示替换缓解文本编码器上下文偏差；统一注意力引导机制包括自适应风格注入和同步引导适配，共同确保全局风格和身份外观一致性。

Result: 在多样化提示词下实现高身份和风格一致性，推理速度达每张图像1.72秒，比现有最快模型快6倍以上。

Conclusion: Infinite-Story在无需训练的情况下实现了最先进的生成性能，为实际视觉故事讲述提供了高效实用的解决方案。

Abstract: We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.

</details>


### [199] [SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias](https://arxiv.org/abs/2511.13005)
*Wenqian Ye,Di Wang,Guangtao Zheng,Bohan Liu,Aidong Zhang*

Main category: cs.CV

TL;DR: 提出SAGE方法，通过引导式提示选择来缓解CLIP模型中的多模态伪偏差，无需训练或微调即可提升零样本分类的鲁棒性


<details>
  <summary>Details</summary>
Motivation: CLIP模型存在多模态伪偏差问题，即依赖虚假特征进行推理（如根据背景推断物体类型），这严重影响了模型在分布外数据上的鲁棒性。现有方法需要微调或先验知识，破坏了CLIP的开箱即用性

Method: 提出SAGE方法，通过理论分析多模态伪偏差的影响，在提示模板空间中探索并选择能诱导最大类别语义分离的提示，无需训练、微调或外部标注

Result: 在4个真实世界基准数据集和5个主流骨干模型上的实验表明，SAGE一致提升了零样本性能和泛化能力，优于现有零样本方法

Conclusion: SAGE是一种简单有效的零样本方法，能够缓解多模态伪偏差，提高模型鲁棒性，同时保持CLIP的开箱即用特性

Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

</details>


### [200] [Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis](https://arxiv.org/abs/2511.13011)
*Qingsen Ma,Chen Zou,Dianyun Wang,Jia Wang,Liuyu Xiang,Zhaofeng He*

Main category: cs.CV

TL;DR: DTGS是一个统一框架，将Retinex启发的光照分解与热引导的3D高斯泼溅相结合，用于极低光照条件下的新视角合成，解决了标准3DGS在欠曝光输入下的几何和颜色一致性问题。


<details>
  <summary>Details</summary>
Motivation: 在极低光照条件下，新视角合成面临几何、颜色一致性和辐射稳定性的严重退化。标准的3D高斯泼溅管道直接应用于欠曝光输入时会失败，因为跨视图的独立增强会导致光照不一致和几何失真。

Method: DTGS通过循环增强-重建机制，在增强、几何和热监督之间进行联合优化。包含热监督分支动态平衡增强、结构和热损失，以及嵌入3DGS循环的Retinex分解模块提供物理可解释的反射-光照分离。

Result: 在构建的RGBT-LOW多视图低光热数据集上的广泛实验表明，DTGS显著优于现有的低光增强和3D重建基线，在极端光照条件下实现了优异的辐射一致性、几何保真度和颜色稳定性。

Conclusion: DTGS通过紧密耦合光照分解和热引导的3D高斯泼溅，成功解决了极低光照条件下的新视角合成问题，为光照不变重建提供了有效解决方案。

Abstract: Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.

</details>


### [201] [You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.13013)
*Guoyi Zhang,Guangsheng Xu,Siyang Chen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: 提出BP-FPN，一种从反向传播角度设计的特征金字塔架构，用于解决红外小目标检测中的特征表示瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要关注时空特征聚合，但收益有限，发现根本瓶颈在于模糊的逐帧特征表示而非时空建模本身。

Method: 引入梯度隔离低层捷径(GILS)有效整合细粒度目标细节而不引发捷径学习，以及方向梯度正则化(DGR)在反向传播中强制层次特征一致性。

Result: 在多个公共数据集上的广泛实验表明，BP-FPN始终达到新的最先进性能。

Conclusion: 这是首个完全从反向传播角度为该任务设计的FPN架构，具有理论依据、计算开销可忽略，并能无缝集成到现有框架中。

Abstract: Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.

</details>


### [202] [Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues](https://arxiv.org/abs/2511.13015)
*King-Man Tam,Satoshi Ikehata,Yuta Asano,Zhaoyi An,Rei Kawakami*

Main category: cs.CV

TL;DR: GeoUniPS是一个通用光度立体网络，通过整合合成监督和大规模3D重建模型的高层几何先验，解决了复杂野外场景中多光照线索不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 传统通用光度立体方法在复杂野外场景中，当多光照线索不可靠时（如偏置光照、阴影或自遮挡区域）表现不佳，需要引入更强的几何先验。

Method: 设计了光-几何双分支编码器，从冻结的3D重建模型中提取多光照线索和几何先验；创建了PS-Perp数据集以解决传统正交投影假设的局限性。

Result: 在多个数据集上的广泛实验表明，GeoUniPS在定量和定性评估中都达到了最先进的性能，特别是在复杂野外场景中。

Conclusion: GeoUniPS通过整合3D重建模型的几何先验，显著提升了通用光度立体在复杂野外场景中的性能，证明了视觉-几何基础模型的有效性。

Abstract: Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.

</details>


### [203] [MeanFlow Transformers with Representation Autoencoders](https://arxiv.org/abs/2511.13019)
*Zheyuan Hu,Chieh-Hsin Lai,Ge Wu,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: 提出了一种在表示自编码器潜在空间中训练MeanFlow的高效方法，通过一致性中间训练和两阶段方案解决梯度爆炸问题，显著降低了训练和推理成本，并在ImageNet上取得了优异的单步生成效果。


<details>
  <summary>Details</summary>
Motivation: MeanFlow在潜在空间中训练存在计算量大、不稳定、推理时解码器成本高、依赖复杂引导参数等问题，需要开发更高效的训练和采样方案。

Method: 在表示自编码器潜在空间中训练MeanFlow，采用一致性中间训练进行轨迹感知初始化，使用两阶段方案：从预训练流匹配教师蒸馏加速收敛，然后可选的自举阶段进一步减少与理想平均流的偏差。

Result: 在ImageNet 256上实现了单步FID 2.03，优于vanilla MF的3.43，同时采样GFLOPS减少38%，总训练成本降低83%。在ImageNet 512上达到单步FID 3.23，在所有基线中GFLOPS最低。

Conclusion: 该方法消除了对引导的需求，简化了训练配置，显著降低了训练和采样计算成本，在保持高质量生成的同时实现了高效的单步生成。

Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

</details>


### [204] [SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction](https://arxiv.org/abs/2511.13020)
*Yufei Wen,Yuting Zhang,Jingdan Kang,Hao Ren,Weibin Cheng,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出SpectralAdapt半监督域适应框架，通过谱密度掩码和谱端元表示对齐技术，解决医疗HSI重建中的领域差异、谱退化及数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决医疗应用中人类HSI数据稀缺问题，弥合通用领域与人类中心HSI数据集之间的领域差距。

Method: 采用半监督域适应框架，引入谱密度掩码自适应掩蔽RGB通道，以及谱端元表示对齐利用物理可解释端元作为领域不变锚点。

Result: 在基准数据集上实验显示，在谱保真度、跨领域泛化能力和训练稳定性方面均有持续改进。

Conclusion: SSDA是医疗高光谱成像的高效解决方案，有望推动医疗HSI应用发展。

Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.

</details>


### [205] [REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding](https://arxiv.org/abs/2511.13026)
*Jiaze Li,Hao Yin,Wenhui Tan,Jingyang Chen,Boshen Xu,Yuxun Qu,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 提出了REVISOR框架，通过跨模态反思机制增强多模态大语言模型的长视频理解能力，无需额外监督微调或外部模型。


<details>
  <summary>Details</summary>
Motivation: 纯文本反思机制在长视频理解中存在局限：1) 仅反思文本信息不足，需要针对视觉信息的反思过程；2) 缺乏跨模态交互能力，无法在反思中充分整合视觉信息。

Method: REVISOR框架支持文本和视觉模态的协作反思过程，设计了双归因解耦奖励机制(DADR)来确保模型在强化学习中准确回顾与问题相关的视频片段。

Result: 在VideoMME、LongVideoBench、MLVU和LVBench四个基准测试中取得了显著成果，显著提升了MLLMs的长视频理解能力。

Conclusion: REVISOR框架通过跨模态反思机制有效解决了长视频理解中的视觉信息整合问题，为多模态大语言模型的长视频理解提供了新思路。

Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.

</details>


### [206] [Towards 3D Object-Centric Feature Learning for Semantic Scene Completion](https://arxiv.org/abs/2511.13031)
*Weihua Wang,Yubo Cui,Xiangru Lin,Zhiheng Li,Zheng Fang*

Main category: cs.CV

TL;DR: Ocean是一个面向自动驾驶的3D语义场景补全框架，采用对象中心的方法，通过实例分割、3D语义组注意力、全局相似性引导注意力和实例感知局部扩散模块，在复杂环境中实现更精确的语义占用预测。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉的3D语义场景补全方法通常采用以自我为中心的范式，在复杂环境中容易忽略细粒度的对象级细节，导致语义和几何模糊。

Method: 1) 使用MobileSAM从输入图像提取实例掩码；2) 3D语义组注意力模块利用线性注意力在3D空间中聚合对象中心特征；3) 全局相似性引导注意力模块处理分割错误和缺失实例；4) 实例感知局部扩散模块通过生成过程改进实例特征并在BEV空间中优化场景表示。

Result: 在SemanticKITTI和SSCBench-KITTI360基准测试中达到最先进性能，mIoU分数分别为17.40和20.28。

Conclusion: Ocean通过对象中心的方法有效解决了复杂环境中语义场景补全的挑战，显著提升了预测精度。

Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.

</details>


### [207] [Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts](https://arxiv.org/abs/2511.13032)
*Sheng Liu,Yuanzhi Liang,Jiepeng Wang,Sidan Du,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Uni-Inter是一个统一的人类动作生成框架，支持人-人、人-物、人-场景等多种交互场景，通过统一的交互体积表示实现异构实体的空间编码和关系推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定设计且泛化能力有限，需要开发一个任务无关的统一架构来处理多种交互场景。

Method: 提出统一交互体积(UIV)作为体积表示，将异构交互实体编码到共享空间场中，通过关节级概率预测来建模空间依赖关系。

Result: 在三个代表性交互任务上的实验表明，Uni-Inter实现了竞争性性能，并能很好地泛化到新的实体组合。

Conclusion: 统一建模复合交互为复杂环境中的可扩展动作合成提供了有前景的方向。

Abstract: We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.

</details>


### [208] [uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data](https://arxiv.org/abs/2511.13036)
*Dahyun Chung,Donghyun Shin,Yujin Sung,Seunggi Moon,Jinwoo Jeon,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 提出了一种轻量级、数据高效的多语言视觉-语言对齐框架，无需图像-文本对或文本-文本对，仅训练1.7M参数的投影模块，通过英语表示作为语义锚点实现多语言对齐。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP模型在低资源语言上扩展受限的问题，特别是捷克语、芬兰语、克罗地亚语、匈牙利语和罗马尼亚语等代表性不足语言在XM3600基准测试中检索性能低下的问题。

Method: 冻结预训练的图像编码器和多语言文本编码器，仅训练紧凑的投影模块，使用对比损失以英语表示为语义锚点进行多语言对齐。

Result: 在多个多语言检索基准测试中表现出色，在五个代表性不足语言上取得显著性能提升，现有模型在这些语言上通常表现不佳。

Conclusion: 基于枢纽的参数高效对齐策略对于包容性多模态学习具有有效性，证明了轻量级方法在多语言视觉-语言对齐中的可行性。

Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.

</details>


### [209] [MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization](https://arxiv.org/abs/2511.13039)
*Zhenying Fang,Richang Hong*

Main category: cs.CV

TL;DR: 提出了MGCA-Net网络，通过多粒度类别感知机制解决开放词汇时序动作定位问题，在THUMOS'14和ActivityNet-1.3基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇时序动作定位方法大多在单一粒度上识别动作类别，这会降低基类和新类动作的识别准确率。

Method: 设计了包含定位器、动作存在预测器、传统分类器和粗到细分类器的多粒度类别感知网络，通过不同粒度的分类器分别处理基类和新类动作。

Result: 在THUMOS'14和ActivityNet-1.3基准测试中达到最先进性能，同时在零样本时序动作定位设置下也取得最佳结果。

Conclusion: 多粒度类别感知机制能有效提升定位性能，通过粗到细的类别感知处理新类动作，结合传统分类器处理基类动作，实现了更好的开放词汇动作定位效果。

Abstract: Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.

</details>


### [210] [DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation](https://arxiv.org/abs/2511.13047)
*Yan Gong,Jianli Lu,Yongsheng Gao,Jie Zhao,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.CV

TL;DR: DiffPixelFormer是一个用于RGB-D室内场景语义分割的差分像素感知Transformer，通过改进模态内和模态间特征建模，实现了更精确的特征对齐和判别性表示。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB-D融合方法依赖计算密集的跨注意力机制，对模态内和模态间特征关系建模不足，导致特征对齐不精确和表示能力有限。

Method: 提出Intra-Inter Modal Interaction Block (IIMIB)，通过自注意力捕获模态内长程依赖，使用Differential-Shared Inter-Modal (DSIM)模块建模模态间交互，分离模态特定和共享线索，实现像素级跨模态对齐。采用动态融合策略平衡模态贡献。

Result: 在SUN RGB-D和NYUDv2基准测试中，DiffPixelFormer-L分别达到54.28%和59.95%的mIoU，比DFormer-L分别提升1.78%和2.75%。

Conclusion: DiffPixelFormer通过增强模态内表示和建模模态间交互，在RGB-D室内语义分割任务中取得了优越性能。

Abstract: Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.

</details>


### [211] [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054)
*Bo Fang,Yuxin Song,Qiangqiang Wu,Haoyuan Sun,Wenhao Wu,Antoni B. Chan*

Main category: cs.CV

TL;DR: 提出了一种新的自监督强化学习算法Pretext-GRPO和ViSS-R1框架，通过处理变换后的视觉输入和前置任务来增强多模态大语言模型在视频推理中的视觉中心理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于R1的方法在视频任务中往往未能充分利用丰富的视觉信息，容易导致捷径学习和幻觉问题，需要更稳健的视觉中心视频理解方法。

Method: 首先提出Pretext-GRPO自监督强化学习算法，通过正确解决变换视觉输入的前置任务来分配正奖励；然后构建ViSS-R1框架，将前置任务自监督学习直接集成到MLLM的R1后训练范式中。

Result: 在六个广泛使用的视频推理和理解基准测试中进行了全面评估，证明了Pretext-GRPO和ViSS-R1在复杂视频推理中的有效性和优越性。

Conclusion: 提出的方法能够强制模型对变换后的视觉输入进行推理，同时处理前置任务问题和真实用户查询，从而制定准确答案，显著提升了视频推理性能。

Abstract: Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.

</details>


### [212] [Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries](https://arxiv.org/abs/2511.13055)
*Ruixin Liu,Zejian Yuan*

Main category: cs.CV

TL;DR: MonoUnc是一个基于BEV-free的单目3D车道线检测方法，通过显式建模局部车道结构的不确定性来应对观测噪声带来的不确定性挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的单目3D车道线检测方法依赖于简化的几何假设，无法捕捉真实场景中的结构变化和随机不确定性，特别是在处理观测噪声方面存在局限。

Method: 将3D车道线投影到前视图空间并用参数曲线逼近，基于曲线预测动态生成曲线点查询嵌入，将相邻点形成的线段建模为3D高斯分布，并设计新的3D高斯匹配损失函数。

Result: 在ONCE-3DLanes和OpenLane数据集上的实验表明，MonoUnc在更严格的评估标准下优于之前的最先进方法，并提出了两个新的综合评估指标。

Conclusion: MonoUnc通过显式建模局部结构不确定性，有效提升了单目3D车道线检测的精度和鲁棒性，为不确定性建模提供了新的思路。

Abstract: Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.

</details>


### [213] [RobustGait: Robustness Analysis for Appearance Based Gait Recognition](https://arxiv.org/abs/2511.13065)
*Reeshoon Sayera,Akash Kumar,Sirshapan Mitra,Prudvi Kamtam,Yogesh S Rawat*

Main category: cs.CV

TL;DR: 提出了RobustGait框架，用于评估外观步态识别系统在真实世界干扰和轮廓变化下的鲁棒性，涵盖四种扰动类型、轮廓提取方法、模型架构能力和部署场景。


<details>
  <summary>Details</summary>
Motivation: 现有外观步态识别在受控数据集上表现良好，但缺乏对其在真实世界干扰和轮廓变化下鲁棒性的系统性评估。

Method: 开发RobustGait基准，包含15种干扰类型、5个严重级别，在CASIA-B、CCPG和SUSTech1K数据集上评估，并在MEVID上进行野外验证。评估了六种最先进的步态系统。

Result: 发现RGB级噪声能更好反映真实世界退化；步态精度对轮廓提取器偏差高度敏感；鲁棒性取决于扰动类型和架构设计；噪声感知训练和知识蒸馏能提升性能。

Conclusion: RobustGait框架揭示了步态识别系统的关键鲁棒性挑战，并展示了提升系统部署准备度的有效策略。

Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.

</details>


### [214] [Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.13079)
*Jiacheng Tang,Mingyue Feng,Jiachao Liu,Yaonong Wang,Jian Pu*

Main category: cs.CV

TL;DR: 提出AdaptiveAD架构，通过双分支结构解耦场景感知与自车状态，解决现有端到端自动驾驶系统过度依赖自车状态的问题，提升泛化能力和场景理解鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模块化自动驾驶架构过度依赖自车状态，导致泛化能力受限和场景理解不鲁棒。根本原因在于架构设计中自车状态过早融合到BEV编码器中，形成了信息捷径。

Method: 采用双分支结构：一个分支基于多任务学习进行场景驱动推理（BEV编码器中故意省略自车状态），另一个分支基于规划任务进行自车驱动推理。通过场景感知融合模块自适应整合两个分支的决策。引入路径注意力机制、BEV单向蒸馏和自回归在线映射两个辅助任务。

Result: 在nuScenes数据集上的广泛评估表明，AdaptiveAD实现了最先进的开环规划性能，显著减轻了对自车状态的过度依赖，并在多样化场景中展现出令人印象深刻的泛化能力。

Conclusion: AdaptiveAD通过架构层面的多上下文融合策略有效解决了端到端自动驾驶系统过度依赖自车状态的问题，提升了系统的泛化性和鲁棒性。

Abstract: Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.

</details>


### [215] [Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations](https://arxiv.org/abs/2511.13081)
*Yehonatan Elisha,Seffi Cohen,Oren Barkan,Noam Koenigstein*

Main category: cs.CV

TL;DR: 提出了RFxG分类法来组织显著性解释，包含参考框架（点式vs对比式）和粒度（细粒度vs粗粒度）两个维度，并开发了新的评估指标来系统评估解释质量。


<details>
  <summary>Details</summary>
Motivation: 显著性图在深度学习视觉解释中广泛使用，但缺乏对其目的和用户查询对齐的共识，这阻碍了评估方法和实际效用的有效性。

Method: 引入RFxG分类法框架，提出四个新的忠实性评估指标，在十个最先进的显著性方法、四个模型架构和三个数据集上进行综合评估。

Result: 发现现有评估指标主要关注点式忠实性，而忽略了对比推理和语义粒度，证明了RFxG框架在评估解释质量方面的有效性。

Conclusion: 通过转向用户意图驱动的评估，为开发既忠实于模型行为又与人类理解复杂性有意义的视觉解释提供了概念基础和实践工具。

Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.

</details>


### [216] [MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images](https://arxiv.org/abs/2511.13099)
*Doanh C. Bui,Ba Hung Ngo,Hoai Luan Pham,Khang Nguyen,Maï K. Nguyen,Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: MergeSlide是一个用于全切片图像终身学习的框架，将终身学习视为模型合并问题，通过正交持续合并策略和任务到类别提示对齐推理来缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 全切片图像体积巨大，终身学习可以减少数据传输和处理所需的资源和努力，但面临灾难性遗忘的挑战。

Method: 使用视觉语言病理学基础模型，通过类别感知提示定义新任务，用无MLP的主干进行少量轮次微调，采用正交持续合并策略合并模型，并通过任务到类别提示对齐推理进行类别增量学习。

Result: 在六个TCGA数据集上的实验表明，MergeSlide优于基于排练的持续学习和视觉语言零样本基线方法。

Conclusion: MergeSlide提供了一个简单有效的终身学习框架，能够在不忘记先前任务的情况下持续学习新任务，在病理图像分析中表现出色。

Abstract: Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.

</details>


### [217] [CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation](https://arxiv.org/abs/2511.13102)
*Yu Zhu,Dan Zeng,Shuiwang Li,Qijun Zhao,Qiaomu Shen,Bo Tang*

Main category: cs.CV

TL;DR: 提出CapeNext框架，通过层次化跨模态交互和双流特征细化解决静态关节嵌入的歧义性和区分性不足问题，在MP-100数据集上显著超越现有CAPE方法。


<details>
  <summary>Details</summary>
Motivation: 现有类别无关姿态估计方法使用固定文本关键点描述作为语义先验，但存在两个固有局限：(1) 多义词导致的跨类别歧义（如'腿'在人类和家具中的视觉表现差异）(2) 对细粒度类别内变化区分性不足（如不同姿态和毛色的猫）

Method: 提出新框架，创新性地整合层次化跨模态交互与双流特征细化，通过文本描述和特定图像中的类别级和实例特定线索增强关节嵌入

Result: 在MP-100数据集上的实验表明，无论使用何种网络骨干，CapeNext都大幅超越最先进的CAPE方法

Conclusion: 该框架有效解决了静态关节嵌入的局限性，为类别无关姿态估计提供了更鲁棒和灵活的解决方案

Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.

</details>


### [218] [PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking](https://arxiv.org/abs/2511.13105)
*Seungjae Kim,SeungJoon Lee,MyeongAh Cho*

Main category: cs.CV

TL;DR: PlugTrack是一个新颖的多目标跟踪框架，通过多感知运动理解自适应融合卡尔曼滤波器和数据驱动的运动预测器，在保持计算效率的同时提升非线性运动模式的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界跟踪场景同时包含线性和非线性运动模式，但卡尔曼滤波器无法处理非线性运动，而数据驱动预测器存在领域泛化性差和计算开销大的问题。研究发现即使在非线性运动主导的数据集中，卡尔曼滤波器在34%的情况下仍优于数据驱动方法。

Method: 提出PlugTrack框架，通过多感知运动分析生成自适应融合因子，在不修改现有运动预测器的情况下，自适应融合卡尔曼滤波器和数据驱动运动预测器的优势。

Result: 在MOT17/MOT20上取得显著性能提升，在DanceTrack上达到最先进水平。

Conclusion: PlugTrack是首个通过自适应融合桥接经典和现代运动预测范式的多目标跟踪框架，有效利用了不同预测器的互补性。

Abstract: Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.

</details>


### [219] [Low-Level Dataset Distillation for Medical Image Enhancement](https://arxiv.org/abs/2511.13106)
*Fengzhi Xu,Ziyuan Yang,Mengyu Sun,Joey Tianyi Zhou,Yi Zhang*

Main category: cs.CV

TL;DR: 提出了首个用于医学图像增强的低级数据集蒸馏方法，通过共享解剖先验和个性化生成模块解决像素级保真度问题，同时保护患者隐私。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像增强方法需要大规模数据集学习复杂像素级映射，但训练和存储成本高。数据集蒸馏可减轻负担，但现有方法主要针对高级任务，低级任务的像素级保真度要求使得蒸馏成为欠定问题。

Method: 利用患者间解剖相似性构建共享解剖先验作为初始化，通过结构保持个性化生成模块整合患者特定解剖信息，构建任务特定的高低质量训练对，并通过梯度对齐注入患者特定知识。

Result: 提出的方法能够生成包含抽象训练信息的蒸馏数据集，无需访问原始患者数据，在保护隐私的同时实现有效的医学图像增强。

Conclusion: 该方法成功解决了低级数据集蒸馏的像素级保真度挑战，为医学图像增强提供了一种高效且隐私保护的解决方案。

Abstract: Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.

</details>


### [220] [DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection](https://arxiv.org/abs/2511.13108)
*Jiazhen Yan,Ziqiang Li,Fan Wang,Boyu Wang,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出了DGS-Net框架，通过梯度空间分解和蒸馏引导来防止预训练先验的灾难性遗忘，在50个生成模型上实现了优于现有方法的检测性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如GANs和扩散模型）的快速发展导致AI生成图像泛滥，引发了对错误信息、隐私侵犯和数字媒体信任侵蚀的担忧。虽然多模态模型如CLIP提供了强大的可迁移表示来检测合成内容，但微调往往会导致灾难性遗忘，从而降低预训练先验并限制跨域泛化能力。

Method: 提出了蒸馏引导梯度手术网络（DGS-Net），通过梯度空间分解在优化过程中分离有害和有益的下降方向。将任务梯度投影到有害方向的正交补空间，并与从冻结CLIP编码器蒸馏的有益方向对齐，实现先验保持和不相关抑制的统一优化。

Result: 在50个生成模型上的广泛实验表明，该方法平均优于最先进方法6.6个百分点，实现了卓越的检测性能和跨不同生成技术的泛化能力。

Conclusion: DGS-Net通过梯度手术和蒸馏引导有效解决了预训练模型微调中的灾难性遗忘问题，在AI生成图像检测任务中表现出色，具有强大的跨域泛化能力。

Abstract: The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.

</details>


### [221] [Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing](https://arxiv.org/abs/2511.13110)
*Shuaibin Fan,Senming Zhong,Wenchao Yan,Minglong Xue*

Main category: cs.CV

TL;DR: 提出了一种基于隐式神经退化表示的无监督去雾方法，通过结合通道独立和通道依赖机制来增强非线性依赖学习能力，并设计隐式神经表示将雾霾退化建模为连续函数。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂场景时难以平衡不均匀雾霾分布的细粒度特征表示与全局一致性建模，且需要更好地学习雾霾在空间变化中的共同退化表示。

Method: 1. 基于Kolmogorov-Arnold表示定理，结合通道独立和通道依赖机制；2. 设计隐式神经表示将雾霾退化建模为连续函数；3. 设计密集残差增强模块消除冗余信息。

Result: 在多个公共和真实世界数据集上实现了具有竞争力的去雾性能，在复杂场景中获得了良好的视觉感知效果。

Conclusion: 该方法通过隐式神经退化表示实现了高质量图像恢复，无需依赖显式特征提取和物理模型，代码将在GitHub上开源。

Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.

</details>


### [222] [Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining](https://arxiv.org/abs/2511.13113)
*Zhaocheng Yu,Kui Jiang,Junjun Jiang,Xianming Liu,Guanglu Sun,Yi Xiao*

Main category: cs.CV

TL;DR: 提出MPHM网络用于图像去雨，结合CLIP文本先验和DINOv2视觉先验，通过渐进式融合注入和分层Mamba模块实现语义和结构细节的保真恢复。


<details>
  <summary>Details</summary>
Motivation: 现有去雨方法在语义和空间细节保真度方面存在不足，影响计算机视觉系统在自动驾驶等应用中的性能。

Method: 集成宏观语义文本先验(CLIP)和微观结构视觉先验(DINOv2)，设计渐进式先验融合注入(PFI)策略，并构建分层Mamba模块(HMM)进行特征表示。

Result: 在Rain200H数据集上获得0.57 dB PSNR增益，在真实雨天场景中表现出优越的泛化能力。

Conclusion: MPHM网络通过多先验融合和分层Mamba设计，实现了最先进的图像去雨性能，有效提升了语义和结构细节的恢复质量。

Abstract: Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.

</details>


### [223] [A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features](https://arxiv.org/abs/2511.13115)
*Hanzhe Liang,Jie Zhou,Can Gao,Bingyang Guo,Jinbao Wang,Linlin Shen*

Main category: cs.CV

TL;DR: 提出了一种旋转不变特征框架用于3D异常检测，通过坐标映射和轻量级卷积网络解决点云方向位置变化导致的特征不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测方法在处理方向和位置变化的点云时面临挑战，因为特征会显著变化，需要开发旋转不变的特征表示。

Method: 1) 点坐标映射技术将点映射到旋转不变空间；2) 轻量级卷积变换特征网络提取旋转不变特征；3) 使用迁移学习和3D数据增强预训练特征提取器。

Result: 在Anomaly-ShapeNet数据集上平均P-AUROC提升17.7%，在Real3D-AD数据集上提升1.6%，表现出强泛化能力。

Conclusion: RIF框架通过旋转不变特征有效解决了3D异常检测中的方向敏感问题，具有工业应用潜力。

Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.

</details>


### [224] [CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model](https://arxiv.org/abs/2511.13121)
*Yuqi Zhang,Guanying Chen,Jiaxing Chen,Chuanyu Fu,Chuan Huang,Shuguang Cui*

Main category: cs.CV

TL;DR: CloseUpShot：基于扩散的框架，通过点条件视频扩散从稀疏输入合成特写新视图，解决了特写场景中像素扭曲条件稀疏性和背景泄漏的问题。


<details>
  <summary>Details</summary>
Motivation: 从稀疏输入视图重建3D场景和合成新视图具有挑战性。现有方法主要针对适度视角变化设计，在特写场景中由于输入信息严重受限而难以捕捉细粒度细节。

Method: 提出分层扭曲和遮挡感知噪声抑制来增强条件图像质量；引入全局结构指导，利用密集融合点云为扩散过程提供一致几何上下文。

Result: 在多个数据集上的广泛实验表明，该方法优于现有方法，特别是在特写新视图合成方面。

Conclusion: CloseUpShot框架在特写新视图合成中表现出色，验证了所提设计的有效性。

Abstract: Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.

</details>


### [225] [VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127)
*Zonghao Ying,Moyang Chen,Nizhang Li,Zhiqiang Wang,Wenxin Zhang,Quanchen Zou,Zonglei Jing,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: 提出VEIL框架，通过包含中性场景锚点、潜在听觉触发器和风格调制器的模块化提示设计，利用T2V模型的跨模态关联模式，实现隐式诱导生成违反安全策略的视频内容。


<details>
  <summary>Details</summary>
Motivation: 现有针对文本到视频模型的越狱攻击通常通过添加明显不安全的对抗性扰动来实现，容易被检测和防御。本文旨在探索看似良性的提示如何能够诱导模型生成语义不安全但保留原始被阻止意图的视频。

Method: VEIL框架采用模块化提示设计：中性场景锚点提供表面场景描述；潜在听觉触发器利用音频-视觉共现先验；风格调制器通过电影化指令增强触发效果。通过约束优化和引导搜索平衡隐蔽性和有效性。

Result: 在7个T2V模型上的广泛实验表明，该攻击方法在商业模型中的平均攻击成功率提高了23%。

Conclusion: 研究表明，包含丰富隐式线索的良性提示能够有效诱导T2V模型生成违反安全策略的视频，揭示了现有安全防护的关键盲点。

Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

</details>


### [226] [Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack](https://arxiv.org/abs/2511.13132)
*Chenyang Li,Wenbing Tang,Yihao Huang,Sinong Simon Zhan,Ming Hu,Xiaojun Jia,Yang Liu*

Main category: cs.CV

TL;DR: 提出了基于室内照明的对抗攻击框架ILA，通过操纵全局光照来破坏视觉语言导航(VLN)代理的鲁棒性，包括静态和动态两种攻击模式。


<details>
  <summary>Details</summary>
Motivation: 现有对抗评估常依赖不现实的纹理扰动，缺乏实用性。室内照明作为内在场景属性对导航有重要影响，但被忽视。

Method: ILA框架包含两种攻击模式：SILA(静态光照攻击，光照强度恒定)和DILA(动态光照攻击，在关键时刻开关灯造成光照突变)。

Result: 在两个最先进的VLN模型和三个导航任务上测试，ILA显著提高了失败率并降低了轨迹效率。

Conclusion: VLN代理对现实室内光照变化存在先前未被认识的脆弱性，ILA揭示了这一重要问题。

Abstract: Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.

</details>


### [227] [MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation](https://arxiv.org/abs/2511.13135)
*Junjie Yang,Yuhao Yan,Gang Wu,Yuxuan Wang,Ruoyu Liang,Xinjie Jiang,Xiang Wan,Fenglei Fan,Yongquan Zhang,Feiwei Qin,Changmiao Wan*

Main category: cs.CV

TL;DR: 提出了MedGEN-Bench基准测试，用于评估医疗AI系统的多模态生成能力，包含6422个专家验证的图像-文本对，涵盖6种成像模态和16个临床任务。


<details>
  <summary>Details</summary>
Motivation: 现有医疗视觉基准存在局限性：查询模糊、简化诊断推理为封闭式捷径、忽视图像生成能力评估。需要更全面的多模态评估框架。

Method: 构建包含6422个专家验证图像-文本对的MedGEN-Bench数据集，涵盖视觉问答、图像编辑和上下文多模态生成三种格式，采用三层评估框架（像素级指标、语义文本分析、临床相关性评分）。

Result: 系统评估了10个组合框架、3个统一模型和5个视觉语言模型，展示了不同方法在多模态医疗任务上的表现。

Conclusion: MedGEN-Bench为医疗AI研究提供了全面的多模态评估基准，强调上下文交织指令和开放式生成输出，推动超越多项选择题限制的医疗AI发展。

Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.

</details>


### [228] [WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection](https://arxiv.org/abs/2511.13138)
*Longhui Zheng,Qiming Xia,Xiaolu Chen,Zhaoliang Liu,Chenglu Wen*

Main category: cs.CV

TL;DR: WinMamba是一种基于Mamba的3D特征编码骨干网络，通过窗口尺度自适应模块和窗口移位策略，在保持计算效率的同时有效捕获长距离空间依赖关系，在KITTI和Waymo数据集上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决3D目标检测中同时最大化计算效率和捕获长距离空间依赖关系的根本挑战。现有Mamba方法在固定窗口内进行轴对齐扫描会丢弃空间信息。

Method: 提出WinMamba块组成的骨干网络，包含窗口尺度自适应模块来补偿不同分辨率下的体素特征，以及带可学习位置编码和窗口移位策略的WinMamba层来获取丰富上下文线索。

Result: 在KITTI和Waymo数据集上的大量实验表明，WinMamba显著优于基线方法。消融研究验证了WSF和AWF模块对提升检测精度的独立贡献。

Conclusion: WinMamba通过创新的窗口尺度自适应和窗口移位策略，成功平衡了3D目标检测的计算效率和长距离依赖捕获能力，代码将公开提供。

Abstract: 3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.

</details>


### [229] [Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks](https://arxiv.org/abs/2511.13145)
*Cesar Portocarrero Rodriguez,Laura Vandeweyen,Yosuke Yamamoto*

Main category: cs.CV

TL;DR: 该研究探索使用计算机视觉技术进行道路损坏分割，评估GAN生成合成数据的效果，并比较CNN和MaskFormer模型性能，发现MaskFormer在mAP50和IoU指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 美国道路基础设施状况不佳（D级），传统检测方法效率低下且成本高昂。随着自动驾驶车辆实时视觉数据的增加，有机会应用计算机视觉技术进行先进的道路监测。

Method: 1) 使用生成对抗网络(GANs)生成合成数据评估训练效果；2) 应用卷积神经网络(CNNs)进行道路损坏分割；3) 研究基于transformer的MaskFormer模型。

Result: GAN生成的数据能提升模型性能；MaskFormer在mAP50和IoU两个指标上优于CNN模型。

Conclusion: 计算机视觉技术特别是MaskFormer模型在道路损坏分割方面具有良好应用前景，GAN生成数据可有效辅助模型训练。

Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.

</details>


### [230] [Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification](https://arxiv.org/abs/2511.13150)
*Rifen Lin,Alex Jinpeng Wang,Jiawei Mo,Min Li*

Main category: cs.CV

TL;DR: 提出了CSIP-ReID，首个基于骨架的预训练框架，用于视频行人重识别，通过对比学习对齐骨架和视觉特征，并引入原型融合更新器和骨架引导时序建模模块，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的多模态预训练方法存在两个根本限制：(1)缺乏真正的多模态预训练，(2)文本难以捕捉细粒度时序运动信息，而这对视频中区分身份至关重要。

Method: 两阶段方法：第一阶段使用对比学习在序列级别对齐骨架和视觉特征；第二阶段引入动态原型融合更新器(PFU)来优化多模态身份原型，融合运动和外观线索；并提出骨架引导时序建模(SGTM)模块从骨架数据中提取时序线索并整合到视觉特征中。

Result: 在标准视频ReID基准测试(MARS, LS-VID, iLIDS-VID)中达到新的最先进结果；在仅骨架ReID任务(BIWI, IAS)上表现出强大的泛化能力，显著优于先前方法。

Conclusion: CSIP-ReID开创了无标注和运动感知的ReID预训练范式，为多模态表示学习开辟了新前沿。

Abstract: Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.

</details>


### [231] [SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration](https://arxiv.org/abs/2511.13168)
*Haodong Wang,Tao Zhuo,Xiuwei Zhang,Hanlin Yin,Wencong Wu,Yanning Zhang*

Main category: cs.CV

TL;DR: SOMA是一个SAR与光学图像密集配准框架，通过集成结构梯度先验和混合匹配策略，显著提升了配准精度。


<details>
  <summary>Details</summary>
Motivation: SAR与光学图像由于成像机制和视觉特性不同，像素级配准具有挑战性。传统梯度信息在手工描述符中很重要，但在深度学习框架中未被有效利用。

Method: 提出特征梯度增强器(FGE)将多尺度多方向梯度滤波器嵌入特征空间，并设计全局-局部仿射流匹配器(GLAM)结合仿射变换和基于流的细化。

Result: 在SEN1-2数据集上CMR@1px提升12.29%，在GFGE_SO数据集上提升18.50%，表现出强鲁棒性和良好泛化能力。

Conclusion: SOMA通过有效利用梯度信息和混合匹配策略，显著改善了SAR-光学图像配准性能。

Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.

</details>


### [232] [THIR: Topological Histopathological Image Retrieval](https://arxiv.org/abs/2511.13170)
*Zahra Tabatabaei,Jon Sporring*

Main category: cs.CV

TL;DR: THIR是一个基于拓扑数据分析的无监督医学图像检索框架，使用Betti数和持久同调来表征组织病理学图像的结构模式，无需训练即可实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性主要死因之一，早期诊断和准确临床决策至关重要。传统深度学习方法需要大量标注数据和GPU资源，限制了临床应用。

Method: 使用立方体持久性从RGB组织病理学图像中提取拓扑指纹，将环的演化编码为紧凑可解释的特征向量，通过计算拓扑描述符之间的距离进行相似性检索。

Result: 在BreaKHis数据集上的实验表明，THIR优于现有监督和无监督方法，在标准CPU上20分钟内处理完整数据集。

Conclusion: THIR提供了一个快速、可扩展且无需训练的临床图像检索解决方案，特别适合资源受限的医疗环境。

Abstract: According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.
  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.

</details>


### [233] [HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution](https://arxiv.org/abs/2511.13175)
*Chao Yang,Boqian Zhang,Jinghao Xu,Guang Jiang*

Main category: cs.CV

TL;DR: 提出基于小波分解的高频引导扩散网络HDW-SR，通过在残差图上进行扩散并利用小波变换实现多尺度频率分解，显著提升单图像超分辨率中的高频细节恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的单图像超分辨率方法在高频域缺乏有效引导，导致恢复的细节模糊。需要更有效的高频信息恢复机制。

Method: 使用小波分解替代传统CNN下采样，实现多尺度频率分解；在残差图上进行扩散；引入稀疏交叉注意力机制连接高频子带和低频子带；设计动态阈值块优化高频选择；利用小波变换的可逆性实现低损失特征重建。

Result: 在合成和真实世界数据集上的实验表明，HDW-SR在超分辨率性能上具有竞争力，特别是在恢复细粒度图像细节方面表现优异。

Conclusion: HDW-SR通过小波分解和残差扩散的协同设计，有效解决了高频细节恢复问题，为单图像超分辨率提供了新的解决方案。

Abstract: Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.

</details>


### [234] [GenTract: Generative Global Tractography](https://arxiv.org/abs/2511.13183)
*Alec Sargood,Lemuel Puglisi,Elinor Thompson,Mirco Musolesi,Daniel C. Alexander*

Main category: cs.CV

TL;DR: GenTract是首个用于全局纤维束追踪的生成模型，将纤维束追踪构建为生成任务，直接从dMRI映射到完整的解剖学合理纤维束。相比现有方法，在精度上表现显著更优，特别是在低分辨率和噪声数据上。


<details>
  <summary>Details</summary>
Motivation: 解决传统局部纤维束追踪方法的误差累积和高假阳性率问题，以及全局方法计算成本高的问题。

Method: 提出GenTract生成模型，采用扩散模型和流匹配两种范式，直接从dMRI数据生成完整的纤维束轨迹。

Result: GenTract的精度比次优方法TractOracle高2.1倍，在低分辨率和噪声设置下优势更加明显，比最接近的竞争对手高一个数量级。

Conclusion: GenTract在保持研究级数据高精度的同时，在不完美的低分辨率数据上也保持可靠性，是全局纤维束追踪的有前景解决方案。

Abstract: Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.

</details>


### [235] [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)
*Haoran Tang,Meng Cao,Ruyang Liu,Xiaoxi Liang,Linglong Li,Ge Li,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出了Object-Centric 3D Rollout (OCR)方法，通过引入结构化扰动来增强多模态大语言模型在视频空间推理中的表现，解决了现有模型存在的查询锁定推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视频空间推理（理解物体位置、方向和物体间关系）方面存在局限，模型往往只关注提示中明确提到的物体而忽略关键上下文线索。

Method: 提出OCR策略，在训练中对选定物体的3D几何结构引入结构化扰动，通过降级物体特定视觉线索并将改变的几何结构投影到2D空间，迫使模型对整个场景进行整体推理。设计了基于rollout的训练流程，联合利用原始视频和区域噪声视频来优化空间推理轨迹。

Result: 实验显示最先进性能：3B参数模型在VSI-Bench上达到47.5%准确率，优于多个7B基线模型。消融实验确认OCR优于先前的rollout策略。

Conclusion: OCR方法有效提升了多模态大语言模型在视频空间推理任务中的表现，通过结构化扰动训练策略解决了查询锁定推理问题。

Abstract: Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).

</details>


### [236] [Birth of a Painting: Differentiable Brushstroke Reconstruction](https://arxiv.org/abs/2511.13191)
*Ying Jiang,Jiayin Lu,Yunuo Chen,Yumeng He,Kui Wu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: 提出了一个可微分笔触重建框架，统一了绘画、风格化纹理和涂抹操作，能够真实再现人类绘画-涂抹循环过程


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要关注最终图像生成或基于补丁的过程模拟，缺乏明确的笔触结构，无法产生平滑逼真的阴影效果

Method: 使用可微分笔触重建框架，通过并行可微分绘制渲染器优化单色和双色贝塞尔笔触，结合风格生成模块合成几何条件纹理，并引入可微分涂抹算子实现自然色彩混合

Result: 在油画、水彩、水墨和数字绘画上的广泛实验表明，该方法能产生逼真且富有表现力的笔触重建、平滑的色调过渡和丰富的风格化外观

Conclusion: 该方法为表达性数字绘画创作提供了一个统一模型，能够真实再现绘画过程中的笔触结构和色彩混合效果

Abstract: Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.

</details>


### [237] [Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection](https://arxiv.org/abs/2511.13195)
*Soyul Lee,Seungmin Baek,Dongbo Min*

Main category: cs.CV

TL;DR: MonoDLGD是一个基于难度感知标签引导去噪的框架，通过自适应扰动和重建真实标签来改进单目3D目标检测，在KITTI基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测由于深度线索的固有模糊性而存在根本性问题，现有方法在深度估计不准确且忽略了实例级检测难度（如遮挡、距离、截断），导致检测性能不理想。

Method: 提出难度感知标签引导去噪框架，根据检测不确定性自适应扰动和重建真实标签：对简单实例应用更强扰动，对困难实例应用更弱扰动，然后重建标签以提供显式几何监督。

Result: 在KITTI基准测试上的大量实验表明，MonoDLGD在所有难度级别上都实现了最先进的性能。

Conclusion: 通过联合优化标签重建和3D目标检测，MonoDLGD促进了几何感知表示学习，并提高了对不同目标复杂度的鲁棒性。

Abstract: Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.

</details>


### [238] [Self-Supervised Ultrasound Screen Detection](https://arxiv.org/abs/2511.13197)
*Alberto Gomez,Jorge Oliveira,Ramon Casero,Agis Chartsias*

Main category: cs.CV

TL;DR: 提出一种自监督流水线，从超声监视器照片中提取超声图像，绕过DICOM传输瓶颈，实现快速算法测试和原型开发。


<details>
  <summary>Details</summary>
Motivation: 超声设备内置显示器显示图像，但常规传输到医院系统依赖DICOM格式，存在传输瓶颈。需要一种方法能够快速获取超声图像用于算法开发和测试。

Method: 开发自监督流水线，通过拍摄超声监视器照片来提取和校正超声图像，无需依赖DICOM传输流程。

Result: 在概念验证研究中，校正后的图像保持了足够的视觉保真度，与原始DICOM图像相比，心脏视图分类的平衡准确率达到0.79。

Conclusion: 该方法成功绕过了DICOM传输瓶颈，能够从监视器照片中提取可用的超声图像，为快速算法测试和原型开发提供了可行方案。

Abstract: Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.

</details>


### [239] [RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204)
*Junhee Lee,ChaeBeen Bang,MyoungChul Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: RefineVAD是一个弱监督视频异常检测框架，通过联合建模时间运动模式和语义结构来检测多种异常类型，包含运动感知时间注意力重校准和类别导向细化两个核心模块。


<details>
  <summary>Details</summary>
Motivation: 现有方法将异常事件视为单一类别，忽略了真实世界中异常事件的多样语义和时间特性，需要模拟人类感知异常的双重过程推理。

Method: 提出RefineVAD框架：1）MoTAR模块通过移位注意力和Transformer建模估计运动显著性并动态调整时间关注；2）CORE模块通过跨注意力将片段级特征与可学习类别原型对齐，注入软异常类别先验。

Result: 在WVAD基准测试上的广泛实验验证了RefineVAD的有效性，并强调了整合语义上下文来引导特征细化朝向异常相关模式的重要性。

Conclusion: 通过联合利用时间动态和语义结构，明确建模运动如何演化以及它类似于什么语义类别，能够有效提升弱监督视频异常检测性能。

Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.

</details>


### [240] [End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer](https://arxiv.org/abs/2511.13208)
*Yonghui Yu,Jiahang Cai,Xun Wang,Wenwu Yang*

Main category: cs.CV

TL;DR: PAVE-Net是一个端到端的多人视频姿态估计框架，消除了传统两阶段方法中的启发式操作，通过姿态感知注意力机制实现跨帧个体关联。


<details>
  <summary>Details</summary>
Motivation: 现有的多人视频姿态估计方法依赖检测、RoI裁剪和NMS等启发式操作，限制了准确性和效率。本文旨在开发完全端到端的框架来消除这些限制。

Method: 提出PAVE-Net，包含空间编码器和时空姿态解码器，采用姿态感知注意力机制使每个姿态查询能够选择性地聚合跨连续帧的相同个体特征，并显式建模姿态关键点间的时空依赖关系。

Result: 在PoseTrack2017上比基于图像的端到端方法提升6.0 mAP，与最先进的基于视频的两阶段方法精度相当，同时效率显著提升。

Conclusion: PAVE-Net是首个用于多帧2D人体姿态估计的端到端方法，在准确性和效率方面均表现出色，证明了端到端框架在视频姿态估计中的有效性。

Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet

</details>


### [241] [3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale](https://arxiv.org/abs/2511.13211)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 3DAlign-DAER是一个通过动态注意力策略和高效检索策略实现文本与3D几何对齐的统一框架，在大规模3D数据库中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将细粒度文本语义与详细几何结构对齐，且在大规模3D数据库上性能显著下降。

Method: 提出动态注意力策略（DAP）使用分层注意力融合模块表示细粒度token到点的注意力，并通过蒙特卡洛树搜索动态校准注意力权重；在推理时使用高效检索策略（ERS）进行分层搜索。

Result: 在多个基准测试中表现出优越性能，构建了包含200万文本-3D对的大规模数据集Align3D-2M。

Conclusion: 3DAlign-DAER通过动态注意力策略和高效检索策略有效解决了文本与3D几何的细粒度对齐问题，在大规模场景下表现优异。

Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.

</details>


### [242] [Hybrid-Domain Adaptative Representation Learning for Gaze Estimation](https://arxiv.org/abs/2511.13222)
*Qida Tan,Hongyu Yang,Wenchao Du*

Main category: cs.CV

TL;DR: 提出HARL框架，通过混合域自适应表示学习，从低质量面部图像中分离出与注视相关的表示，并结合头部姿态的几何约束，实现跨域鲁棒的注视估计。


<details>
  <summary>Details</summary>
Motivation: 解决基于外观的注视估计方法在跨域评估中因表情、佩戴物和图像质量等无关因素干扰导致的性能显著下降问题。

Method: 采用无监督域自适应方式对齐高质量近眼图像特征，设计稀疏图融合模块探索注视方向与头部姿态之间的几何约束。

Result: 在EyeDiap、MPIIFaceGaze和Gaze360数据集上分别达到5.02°、3.36°和9.26°的最先进精度，并在跨数据集评估中表现出竞争力。

Conclusion: HARL框架能够有效学习鲁棒的注视表示，在跨域场景下显著提升注视估计性能。

Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

</details>


### [243] [MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI](https://arxiv.org/abs/2511.13232)
*Malek Al Abed,Sebiha Demir,Anne Groteklaes,Elodie Germani,Shahrooz Faghihroohi,Hemmen Sabir,Shadi Albarqouni*

Main category: cs.CV

TL;DR: MRIQT是一个3D条件扩散框架，用于将便携式超低场MRI图像质量转换为高场MRI质量，显著提升新生儿脑部成像的诊断价值。


<details>
  <summary>Details</summary>
Motivation: 便携式超低场MRI在新生儿护理中具有可及性优势，但信噪比低、诊断质量差，需要提升图像质量以达到高场MRI的诊断水平。

Method: 结合物理一致的K空间降级模拟、v预测与分类器自由引导的稳定图像生成、以及SNR加权的3D感知损失，使用体积注意力-UNet架构进行结构保持的转换。

Result: 在新生儿队列上训练，MRIQT在PSNR指标上比现有最佳方法提升1.78%，医生评价85%的输出图像质量良好且病理清晰可见。

Conclusion: MRIQT实现了基于扩散模型的高保真便携式超低场MRI增强，为可靠的新生儿脑部评估提供了解决方案。

Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.

</details>


### [244] [MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.13242)
*Junjie Wu,Guohong Fu*

Main category: cs.CV

TL;DR: 提出了MMD-Thinker框架，通过自适应多维度思考解决多模态虚假信息检测中推理不足和推理偏差的问题，在领域内外数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息在社交媒体泛滥，AI生成内容时代下虚假信息创建成本低、欺骗性强，对社会构成重大威胁。现有通用多模态大语言模型在检测中存在推理不足和推理偏差两个关键限制。

Method: 提出两阶段框架：1）为多模态虚假信息检测设计定制化思考模式；2）通过任务特定指令调优将定制思考模式注入通用MLLMs；3）使用混合优势函数的强化学习策略激励推理能力；同时构建包含8K+图像文本对的多模态虚假信息推理数据集。

Result: 实验结果表明，MMD-Thinker在领域内和领域外基准数据集上都实现了最先进的性能，同时保持灵活的推理和token使用。

Conclusion: MMD-Thinker通过自适应多维度思考有效解决了多模态虚假信息检测中的关键挑战，为这一领域提供了有效的解决方案。

Abstract: Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.

</details>


### [245] [Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention](https://arxiv.org/abs/2511.13249)
*Yu Wen,Shuyong Gao,Shuping Zhang,Miao Huang,Lili Tao,Han Yang,Haozhe Xing,Lihe Zhang,Boxue Hou*

Main category: cs.CV

TL;DR: 提出RFMNet模型，通过多阶段编码特征交互融合和重叠窗口交叉注意力机制，提升基于参考图像的伪装目标检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将参考图像转化为一维提示信息，但未能充分利用参考图像中丰富的显著目标特征与伪装目标特征的多上下文融合

Method: 1) 多阶段编码特征交互融合；2) 重叠窗口交叉注意力机制关注局部信息匹配；3) 参考特征聚合模块进行渐进式解码分割

Result: 在Ref-COD基准测试中达到最先进的性能

Conclusion: 通过充分利用参考图像的多阶段特征和局部信息匹配，能够有效提升伪装目标检测的准确性

Abstract: Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.

</details>


### [246] [GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models](https://arxiv.org/abs/2511.13259)
*Yushuo Zheng,Jiangyong Ying,Huiyu Duan,Chunyi Li,Zicheng Zhang,Jing Liu,Xiaohong Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: GeoX-Bench是一个用于评估大型多模态模型在跨视角地理定位和姿态估计任务中能力的综合基准，包含10,859个全景-卫星图像对和755,976个问答对。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型在多种任务中表现出色，但其在跨视角地理定位和姿态估计领域的能力尚未被探索，这些能力对导航、自动驾驶、户外机器人等应用有重要价值。

Method: 构建包含128个城市、49个国家的10,859个全景-卫星图像对和755,976个问答对的GeoX-Bench基准，并评估25个最先进的大型多模态模型在这些任务上的表现。

Result: 当前大型多模态模型在地理定位任务中表现优异，但在更复杂的姿态估计任务上效果显著下降，通过指令调优可以显著提升跨视角地理感知能力。

Conclusion: GeoX-Bench揭示了大型多模态模型在跨视角地理任务中的能力差距，特别是姿态估计任务需要进一步改进，指令调优是提升这些能力的有效方法。

Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.

</details>


### [247] [Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges](https://arxiv.org/abs/2511.13261)
*Junlong Li,Huaiyuan Xu,Sijie Cheng,Kejun Wu,Kim-Hui Yap,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了以自我为中心的流程AI助手(EgoProceAssist)概念，旨在通过第一人称视角为日常流程任务提供逐步支持，并定义了三个核心任务：错误检测、流程学习和问答。


<details>
  <summary>Details</summary>
Motivation: 受视觉语言模型和自我中心感知研究的进展推动，需要开发专门针对第一人称视角日常流程任务的AI助手。

Method: 提出了新的分类法，包含三个核心任务，并对现有技术、相关数据集和评估指标进行全面综述，同时通过实验评估代表性VLM方法。

Result: 识别了现有VLM助手与EgoProceAssist之间的差距，提供了全面的评估结果。

Conclusion: 讨论了未来挑战和研究方向，并建立了持续更新的公开资源库。

Abstract: Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant

</details>


### [248] [SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2511.13264)
*Keshav Gupta,Akshat Sanghvi,Shreyas Reddy Palley,Astitva Srivastava,Charu Sharma,Avinash Sharma*

Main category: cs.CV

TL;DR: SymGS是一个基于对称感知的3D高斯泼溅压缩框架，通过引入可学习镜像来消除局部和全局的反射冗余，在保持渲染质量的同时实现108倍压缩。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术在渲染速度和真实感方面表现出色，但其内存占用随场景复杂度快速增长。现有压缩方法主要利用基元级冗余进行压缩，但未能充分利用场景中的对称性。

Method: 提出SymGS框架，引入可学习镜像到场景中，通过检测和利用镜像对称性来消除冗余基元，可作为现有压缩方法（如HAC）的即插即用增强模块。

Result: 相比HAC方法，在基准数据集上实现1.66倍压缩（大规模场景可达3倍），平均实现108倍的3DGS场景压缩，同时保持渲染质量。

Conclusion: SymGS通过利用对称性感知技术显著提升了3D高斯泼溅的压缩效率，为大规模场景应用提供了可行的解决方案。

Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}

</details>


### [249] [Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation](https://arxiv.org/abs/2511.13269)
*Lingfeng Zhang,Yuchen Zhang,Hongsheng Li,Haoxiang Fu,Yingbo Tang,Hangjun Ye,Long Chen,Xiaojun Liang,Xiaoshuai Hao,Wenbo Ding*

Main category: cs.CV

TL;DR: 提出了SpatialSky-Bench基准测试和Sky-VLM模型，专门用于评估和提升视觉语言模型在无人机导航中的空间智能能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在无人机场景中的空间智能能力尚未充分探索，存在导航和动态环境理解方面的局限性。

Method: 开发了包含13个子类别的SpatialSky-Bench基准测试，并构建了包含100万样本的SpatialSky-Dataset数据集，基于此训练了专门用于无人机空间推理的Sky-VLM模型。

Result: 主流视觉语言模型在复杂无人机导航场景中表现不佳，而Sky-VLM在所有基准测试任务中达到了最先进的性能。

Conclusion: Sky-VLM为开发适用于无人机场景的视觉语言模型开辟了新途径，显著提升了空间推理能力。

Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.

</details>


### [250] [Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models](https://arxiv.org/abs/2511.13276)
*Noam Tsfaty,Avishai Weizman,Liav Cohen,Moshe Tshuva,Yehudit Aperstein*

Main category: cs.CV

TL;DR: 提出基于视频级监督的双主干框架，结合卷积和transformer表示，通过top-k池化检测监控视频中的罕见多样化异常


<details>
  <summary>Details</summary>
Motivation: 解决在仅有视频级监督的情况下检测监控视频中罕见且多样化异常的挑战

Method: 双主干框架结合卷积和transformer表示，采用top-k池化技术

Result: 在UCF-Crime数据集上达到90.7%的AUC

Conclusion: 该框架能有效检测监控视频中的罕见多样化异常，仅需视频级监督

Abstract: We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.

</details>


### [251] [SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.13278)
*Zihan Li,Tengfei Wang,Wentian Gan,Hao Zhan,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: SF-Recon是一种直接从多视角图像重建轻量级建筑表面的方法，无需后处理网格简化，通过3D高斯溅射和法线梯度引导优化实现高效重建。


<details>
  <summary>Details</summary>
Motivation: 传统多视角几何流程依赖密集重建、网格化和后续简化，过程繁琐且质量敏感，需要一种直接重建轻量级建筑表面的方法。

Method: 首先训练3D高斯溅射场获得视图一致表示，然后通过法线梯度引导的高斯优化选择与屋顶和墙壁边界对齐的基元，最后通过多视角深度约束的Delaunay三角剖分转换为轻量级网格。

Result: 在提出的SF数据集上，SF-Recon能够直接从多视角图像重建轻量级建筑模型，显著减少面和顶点数量，同时保持计算效率。

Conclusion: SF-Recon提供了一种直接从多视角图像重建轻量级建筑表面的有效方法，避免了传统流程的复杂后处理步骤。

Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/

</details>


### [252] [Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space](https://arxiv.org/abs/2511.13282)
*Kaiwen Wang,Kaili Zheng,Yiming Shi,Chenyi Guo,Ji Wu*

Main category: cs.CV

TL;DR: 提出DTO方法解决多人人体网格恢复中的场景一致性缺失问题，创建DTO-Humans数据集，并开发Metric-Aware HMR网络实现度量尺度的人体网格恢复。


<details>
  <summary>Details</summary>
Motivation: 现有的野外人体网格伪真值生成流程以单人为中心，缺乏场景级一致性，导致同一图像中个体深度和尺度冲突。

Method: 引入深度条件平移优化(DTO)方法，利用人体高度先验和单目深度估计线索，在MAP框架下联合优化所有个体的相机空间平移；提出Metric-Aware HMR端到端网络，通过相机分支和相对度量损失直接估计度量尺度的人体网格和相机参数。

Result: 构建了DTO-Humans数据集（56万张高质量场景一致性多人图像，平均每图4.8人）；在相对深度推理和人体网格恢复方面达到最先进性能。

Conclusion: DTO方法有效解决了多人场景中的深度和尺度一致性问题，Metric-Aware HMR网络实现了度量尺度的人体网格恢复，为多人人体网格恢复提供了新的解决方案。

Abstract: Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.

</details>


### [253] [TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing](https://arxiv.org/abs/2511.13283)
*Jongha Kim,Minseong Bae,Sanghyeok Lee,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: TabFlash是一个高效的表格图像理解MLLM，通过渐进式问题条件化、剪枝策略和token聚焦来生成信息丰富且紧凑的视觉特征，在性能提升的同时显著降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 表格图像存在冗余背景区域和需要问题特定关注的特点，现有MLLM方法忽略了这些特性，导致视觉表示信息不足且冗余。

Method: 1. 渐进式问题条件化：将问题信息以逐渐增加的频率注入ViT层；2. 剪枝策略：丢弃背景token提高效率；3. Token聚焦：训练策略鼓励模型在保留token中集中关键信息。

Result: TabFlash实现了最先进的性能，优于开源和专有MLLM，同时相比第二好的MLLM减少了27%的FLOPs和30%的内存使用。

Conclusion: TabFlash通过结合渐进式问题条件化、剪枝和token聚焦，为表格理解提供了一个高效且有效的MLLM解决方案。

Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

</details>


### [254] [SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design](https://arxiv.org/abs/2511.13285)
*Yunjie Yu,Jingchen Wu,Junchen Zhu,Chunze Lin,Guibin Chen*

Main category: cs.CV

TL;DR: SkyReels-Text是一个无需字体标签或微调的字体可控框架，能够同时编辑多个文本区域并保持非编辑区域的视觉外观，在文本保真度和视觉真实感方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决专业海报设计中需要快速精确修改文本内容，同时保持视觉和谐和排版意图的问题，特别是在不同字体样式下的细粒度、字体感知文本编辑需求。

Method: 提出一种字体可控的框架，用户只需提供所需字体的裁剪字形补丁，无需字体标签或推理时的微调，即可同时编辑多个文本区域并保持非编辑区域的视觉外观。

Result: 在多个数据集（包括手写文本基准）上的广泛实验表明，SkyReels-Text在文本保真度和视觉真实感方面达到了最先进的性能，提供了对字体家族和风格细微差别的空前控制。

Conclusion: 这项工作弥合了通用图像编辑和专业级排版设计之间的差距，为专业设计工作流程提供了强大的文本编辑工具。

Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.

</details>


### [255] [CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving](https://arxiv.org/abs/2511.13297)
*Enhui Ma,Lijun Zhou,Tao Tang,Jiahuan Zhang,Junpeng Jiang,Zhan Zhang,Dong Han,Kun Zhan,Xueyang Zhang,XianPeng Lang,Haiyang Sun,Xia Zhou,Di Lin,Kaicheng Yu*

Main category: cs.CV

TL;DR: 提出了一个名为CorrectAD的自校正代理系统，通过扩散视频生成模型和3D布局来自动纠正自动驾驶规划器的故障案例，显著降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 解决端到端自动驾驶规划方法因长尾问题导致的鲁棒性不足，特别是罕见但安全关键的故障案例。

Method: 引入PM-Agent制定数据需求，提出DriveSora生成与3D布局对齐的时空一致视频，构建完整的自校正代理系统CorrectAD。

Result: 在nuScenes和内部数据集上，CorrectAD分别纠正了62.5%和49.8%的故障案例，碰撞率分别降低了39%和27%。

Conclusion: CorrectAD是一个端到端模型无关的自动化管道，能够有效提升任何端到端规划器的性能。

Abstract: End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.

</details>


### [256] [DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving](https://arxiv.org/abs/2511.13309)
*Kaiwen Cai,Xinze Liu,Xia Zhou,Hengtong Hu,Jie Xiang,Luyao Zhang,Xueyang Zhang,Kun Zhan,Yifei Zhan,Xianpeng Lang*

Main category: cs.CV

TL;DR: DriveLiDAR4D是一个新颖的LiDAR生成流水线，包含多模态条件和序列噪声预测模型LiDAR4DNet，能够生成时间一致的LiDAR场景，具有高度可控的前景对象和逼真的背景。


<details>
  <summary>Details</summary>
Motivation: 现有3D LiDAR点云生成方法存在局限性，包括缺乏序列生成能力、无法产生精确定位的前景对象和逼真背景，这阻碍了其实际应用。

Method: 提出DriveLiDAR4D流水线，包含多模态条件和LiDAR4DNet序列噪声预测模型，以端到端方式实现具有完整场景操作能力的LiDAR场景序列生成。

Result: 在nuScenes和KITTI数据集上评估，在nuScenes数据集上获得FRD 743.13和FVD 16.96，相比当前SOTA方法UniScene，FRD提升37.2%，FVD提升24.1%。

Conclusion: 这是首个以端到端方式解决具有完整场景操作能力的LiDAR场景序列生成的工作，显著超越了现有技术水平。

Abstract: The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.

</details>


### [257] [Computer Vision based group activity detection and action spotting](https://arxiv.org/abs/2511.13315)
*Narthana Sivalingam,Santhirarajah Sivasthigan,Thamayanthi Mahendranathan,G. M. R. I. Godaliyadda,M. P. B. Ekanayake,H. M. V. R. Herath*

Main category: cs.CV

TL;DR: 提出一种结合深度学习和图关系推理的群体活动检测框架，通过Mask R-CNN进行精确定位，融合掩码信息优化特征表示，构建演员关系图建模交互，使用图卷积网络预测个体动作和群体活动。


<details>
  <summary>Details</summary>
Motivation: 多人场景中的群体活动检测面临复杂人际交互、遮挡和时间外观变化的挑战，需要有效建模个体间关系和群体动态。

Method: 使用Mask R-CNN进行演员定位，多种骨干网络提取特征，RoIAlign保持空间对齐，融合掩码信息优化特征，构建演员关系图编码外观相似性和位置关系，图卷积网络进行关系推理。

Result: 在Collective Activity数据集上的实验表明，该方法在拥挤和非拥挤场景下都能提高识别性能。

Conclusion: 该方法展示了结合分割、特征提取和图关系推理在复杂视频理解任务中的潜力。

Abstract: Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.

</details>


### [258] [YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection](https://arxiv.org/abs/2511.13344)
*Ori Meiraz,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv9-T的混合专家目标检测框架，通过自适应路由实现动态特征专业化，相比单一模型提升了mAP和AR指标


<details>
  <summary>Details</summary>
Motivation: 为了克服单一目标检测模型在特征表示方面的局限性，通过专家混合框架实现更灵活和专门化的特征处理

Method: 采用混合专家框架，集成多个YOLOv9-T专家模型，并设计自适应路由机制进行动态特征分配

Result: 相比单一YOLOv9-T模型，该框架在平均精度(mAP)和平均召回率(AR)指标上均有提升

Conclusion: 混合专家框架能够有效提升目标检测性能，通过动态特征专业化实现更优的检测效果

Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

</details>


### [259] [Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images](https://arxiv.org/abs/2511.13353)
*Lucas Gabriel Telesco,Danila Nejamkin,Estefanía Mata,Francisco Filizzola,Kevin Wignall,Lucía Franco Troilo,María de los Angeles Cenoz,Melissa Thompson,Mercedes Leguía,Ignacio Larrabide,José Ignacio Orlando*

Main category: cs.CV

TL;DR: 提出一种混合半监督学习方法，通过结合整体质量的人工标签和质量细节的伪标签，在多任务框架下改进视网膜图像质量评估，无需大量手动标注即可获得更可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜图像质量评估工具大多仅分类整体图像质量，无法指示采集缺陷以指导重新拍摄，这主要由于详细标注的高成本。

Method: 使用在小型数据集上训练的教师模型生成伪标签，然后在多任务设置中微调预训练模型，采用ResNet-18骨干网络。

Result: 多任务模型在EyeQ数据集上F1得分为0.875（vs. 0.863），在DeepDRiD数据集上为0.778（vs. 0.763），匹配或超越现有方法。在大多数细节预测任务中与教师模型性能统计相当（p > 0.05）。

Conclusion: 该半监督方法不仅改进了整体质量评估，还提供了关于采集条件（光照、清晰度、对比度）的可解释反馈，增强了可解释性且无需额外手动标注成本，为临床图像重拍提供可操作输出。

Abstract: Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.

</details>


### [260] [Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model](https://arxiv.org/abs/2511.13387)
*Fei Kong*

Main category: cs.CV

TL;DR: 本文提出了广义去噪扩散压缩模型(gDDCM)，将DDCM扩展到主流的扩散模型及其变体，包括DDPM、基于分数的模型、一致性模型和整流流，并在CIFAR-10和LSUN Bedroom数据集上验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: DDCM只能应用于DDPM方法，无法扩展到其他扩散模型，限制了其应用范围。

Method: 提出gDDCM框架，将DDCM的核心思想（在反向过程中使用特定集合的噪声而非随机噪声）推广到多种扩散模型架构。

Result: 在CIFAR-10和LSUN Bedroom数据集上的实验表明，gDDCM成功将DDCM扩展到多种扩散模型，并实现了性能提升。

Conclusion: gDDCM有效扩展了DDCM的应用范围，为多种扩散模型的图像压缩应用提供了统一框架。

Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.

</details>


### [261] [Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)](https://arxiv.org/abs/2511.13397)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: DTPQA是一个专门用于评估自动驾驶中视觉语言模型感知能力的基准数据集，包含合成和真实世界两部分，具有距离标注功能，可用于分析模型性能随目标距离增加而下降的情况。


<details>
  <summary>Details</summary>
Motivation: 由于自动驾驶是安全关键领域，需要视觉语言模型具备强大的感知能力，特别是在复杂交通场景和远距离目标识别方面。现有评估方法往往混合了感知、推理和世界知识，难以单独评估纯感知能力。

Method: 创建了DTPQA基准数据集，包含合成基准（使用模拟器生成）和真实世界基准（基于真实交通场景图像）。每个样本包含图像、问题、真实答案和目标距离标注，专注于评估纯感知能力。

Result: 提供了完整的DTPQA数据集和生成脚本，能够系统评估VLMs在交通场景中的感知性能，特别是分析模型在不同距离下的表现退化情况。

Conclusion: DTPQA是一个专门设计的基准，能够有效隔离和评估视觉语言模型的纯感知能力，为自动驾驶领域的模型可信度评估提供了重要工具。

Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

</details>


### [262] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: TripleFDS是一个用于场景文本编辑的新框架，通过解耦文本样式、文本内容和背景三个属性，实现了更灵活和视觉一致的文本编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可编辑属性解耦方面不完整，通常只处理文本内容编辑，限制了可控性和视觉一致性。

Method: 提出TripleFDS框架和SCB Synthesis数据集，使用SCB Group作为基本训练单元，通过组间对比正则化和组内多特征正交性实现三特征解耦，在合成阶段进行特征重映射以防止重建中的"捷径"现象。

Result: 在主流STE基准测试中达到最先进的图像保真度（SSIM为44.54）和文本准确率（ACC为93.58%），支持样式替换和背景迁移等新操作。

Conclusion: TripleFDS通过三特征解耦实现了更灵活的场景文本编辑，在保持视觉一致性的同时显著提升了编辑质量。

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [263] [What Color Is It? A Text-Interference Multimodal Hallucination Benchmark](https://arxiv.org/abs/2511.13400)
*Jinkun Zhao,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: 本文发现多模态大模型在视觉感知中存在颜色感知干扰问题，导致幻觉风险，为此构建了"What Color Is It"数据集来验证并研究解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型的快速发展，这些模型在视觉感知中容易受到信息干扰，特别是在颜色感知方面，这会增加幻觉风险。

Method: 通过构建"What Color Is It"数据集，使用简单方法触发多模态大模型中的单模态视觉幻觉，并基于此研究幻觉的根本原因。

Result: 验证了多模态大模型在视觉模态中确实存在幻觉问题，特别是在颜色感知方面。

Conclusion: 提出了增强多模态大模型鲁棒性的潜在解决方案，以解决视觉模态中的幻觉问题。

Abstract: With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the "What Color Is It" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.

</details>


### [264] [Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source](https://arxiv.org/abs/2511.13417)
*Mykola Lavreniuk,Nataliia Kussul,Andrii Shelestov,Yevhenii Salii,Volodymyr Kuzin,Sergii Skakun,Zoltan Szantoi*

Main category: cs.CV

TL;DR: DelAnyFlow是一个分辨率无关的大规模农田边界制图方法，结合DelAny实例分割模型和结构化后处理流程，能够快速生成拓扑一致的矢量边界，在乌克兰应用中6小时内完成了60.3万平方公里的完整农田边界制图。


<details>
  <summary>Details</summary>
Motivation: 现有农田边界提取方法存在边界不完整、相邻农田合并以及难以扩展的问题，需要开发能够大规模应用的准确农田边界制图方法。

Method: 使用基于YOLOv11骨干网络的DelAny实例分割模型，在FBIS 22M数据集（包含67.3万个多分辨率图像块和2290万个验证农田实例）上训练，结合结构化后处理、合并和矢量化序列。

Result: DelAny模型比SAM2准确率提高100%以上，推理速度快400倍；在乌克兰应用中，DelAnyFlow在5米分辨率下绘制了375万个农田，在2.5米分辨率下绘制了515万个农田，显著优于Sinergise Solutions和NASA Harvest的现有产品。

Conclusion: 该工作为缺乏数字地籍数据的地区提供了一种可扩展、成本效益高的农田边界制图方法。

Abstract: Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.

</details>


### [265] [VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task](https://arxiv.org/abs/2511.13420)
*Xingming Long,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 提出了VOPE方法，用于评估大型视觉语言模型在自愿想象任务中的幻觉问题，发现现有模型在此类任务中普遍存在严重幻觉，且现有缓解方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注禁止生成图像中不存在内容的描述任务中的幻觉问题，而忽视了在故事写作等自愿想象任务中的幻觉评估，这些任务需要模型生成超出图像的新内容。

Method: 提出了VOPE方法，通过提出重新检查式问题来评估LVLM如何解释其响应中想象对象的在场情况，根据模型解释与图像中对象在场的一致性来判断是否产生幻觉。

Result: 应用VOPE评估主流LVLM和幻觉缓解方法发现：(1)大多数LVLM在自愿想象中严重幻觉，对想象对象的在场评估表现差；(2)现有幻觉缓解方法在自愿想象任务中效果有限。

Conclusion: 自愿想象任务中的幻觉问题是一个重要的研究方向，现有方法对此类任务效果不佳，需要进一步研究。

Abstract: Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.

</details>


### [266] [FUSE: A Flow-based Mapping Between Shapes](https://arxiv.org/abs/2511.13431)
*Lorenzo Olearo,Giulio Viganò,Daniele Baieri,Filippo Maggioli,Simone Melzi*

Main category: cs.CV

TL;DR: 提出基于流匹配模型的3D形状间映射新表示方法，支持跨表示形状匹配，无需大规模训练或数据驱动过程


<details>
  <summary>Details</summary>
Motivation: 开发计算高效且支持跨表示（点云、网格、SDF、体积数据）的形状匹配方法，避免传统方法对大规模训练数据的依赖

Method: 将3D形状表示为从固定锚分布通过连续可逆流映射诱导的概率分布，通过逆流（源到锚）与正向流（锚到目标）的组合实现形状间连续映射

Result: 在多样化基准测试和挑战性设置中一致实现高覆盖率和准确性，在人体原始点云扫描的UV映射和配准等任务中也表现良好

Conclusion: 该方法提供了一种可逆且模态无关的形状映射表示，在形状匹配及相关任务中展现出优越性能

Abstract: We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.

</details>


### [267] [Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline](https://arxiv.org/abs/2511.13442)
*Rui Zuo,Qinyue Tong,Zhe-Ming Lu,Ziqian Lu*

Main category: cs.CV

TL;DR: Foresee是一个无需训练的基于多模态大语言模型的图像伪造分析流程，通过类型先验驱动策略和灵活特征检测器模块，在保持轻量推理的同时，在篡改定位准确性和文本解释丰富性方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测和定位方法在跨数据集泛化能力有限且可解释性不足，而多模态大语言模型在视觉语言任务中展现出强大泛化潜力，但现有方法需要大规模训练且未能充分利用原始模型的潜力。

Method: 提出训练免费的Foresee流程，采用类型先验驱动策略，使用灵活特征检测器模块专门处理复制-移动篡改，无需额外训练即可释放原始多模态大语言模型在取证领域的潜力。

Result: 实验表明该方法在定位准确性上表现优异，提供更全面的文本解释，并在复制-移动、拼接、移除、局部增强、深度伪造和AIGC编辑等多种篡改类型上展现出更强的泛化能力。

Conclusion: Foresee无需训练即可实现优越的篡改定位性能，为图像伪造分析提供了一种高效且可解释的解决方案。

Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.

</details>


### [268] [Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling](https://arxiv.org/abs/2511.13478)
*Adam Hazimeh,Ke Wang,Mark Collier,Gilles Baechler,Efi Kokiopoulou,Pascal Frossard*

Main category: cs.CV

TL;DR: SliDer是一个使用视觉语言模型将幻灯片图像转换为可编辑SVG格式的语义文档反渲染框架，能够保持高层次结构并区分图像和文本元素。


<details>
  <summary>Details</summary>
Motivation: 多媒体文档通常以静态栅格格式分发，限制了编辑和定制。现有几何栅格-矢量转换方法无法保持高层次语义结构，导致文本和图像元素的语义区分丢失。

Method: SliDer使用视觉语言模型检测和提取栅格输入中的图像和文本元素属性，并将其组织成连贯的SVG格式。关键是在推理过程中迭代优化预测，模拟人类设计过程。

Result: SliDer实现了0.069的重建LPIPS，在82.9%的情况下被人类评估者认为优于最强的零样本VLM基线。

Conclusion: SliDer能够有效地将幻灯片图像转换为可编辑的SVG表示，保持语义结构并支持迭代优化，为文档反渲染提供了新方法。

Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.

</details>


### [269] [InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE](https://arxiv.org/abs/2511.13488)
*Lipeng Wang,Hongxing Fan,Haohua Chen,Zehuan Huang,Lu Sheng*

Main category: cs.CV

TL;DR: InterMoE是一个基于动态时间选择性专家混合的新框架，用于生成高质量的人类交互动作，通过路由机制结合文本语义和运动上下文，在保持个体特征的同时确保高语义保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成人类交互时难以保持独特的个体特征或完全遵循文本描述，这限制了在虚拟现实和机器人等应用中的价值。

Method: 基于动态时间选择性专家混合框架，使用路由机制协同利用高级文本语义和低级运动上下文，将时间运动特征分配给专业专家，专家动态确定选择容量并关注关键时间特征。

Result: 在InterHuman数据集上FID分数降低9%，在InterX数据集上降低22%，在个体特定高保真3D人类交互生成方面达到最先进性能。

Conclusion: InterMoE框架通过动态专家选择和特征路由机制，有效解决了保持个体特征和语义保真度的挑战，在人类交互生成任务中表现出色。

Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.

</details>


### [270] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: LGIP基准测试评估视觉语言模型对语义保持改写和语义改变翻转的鲁棒性，发现EVA02-CLIP和大型OpenCLIP变体在不变性和敏感性方面表现最佳，而SigLIP模型存在较大问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在零样本任务中表现良好，但其对受控语言扰动的响应可靠性尚不清楚，需要评估模型对语义保持改写的不变性和对语义改变翻转的敏感性。

Method: 使用40k MS COCO图像和人工标注，自动生成语义保持的改写和基于规则的语义翻转（改变对象类别、颜色或数量），通过不变性误差、语义敏感性差距和正率统计来总结模型行为。

Result: EVA02-CLIP和大型OpenCLIP变体在不变性-敏感性边界上表现最佳，结合了低改写引起的方差和原始描述相对于翻转对应物的更高分数。SigLIP和SigLIP2显示出更大的不变性误差，并且经常偏好翻转描述而非人工描述。

Conclusion: LGIP为视觉语言模型的语言鲁棒性提供了模型无关的诊断工具，超越了传统检索指标，揭示了标准指标无法发现的模型失败情况。

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [271] [Mapping the Vanishing and Transformation of Urban Villages in China](https://arxiv.org/abs/2511.13507)
*Wenyu Zhang,Yao Tong,Yiqiu Liu,Rui Cao*

Main category: cs.CV

TL;DR: 本研究开发了一个基于深度学习的框架来监测中国城中村的时空变化，通过多时相遥感影像语义分割和土地用途分类，揭示了城中村改造的复杂性和非线性特征。


<details>
  <summary>Details</summary>
Motivation: 中国城中村经历了大规模拆迁改造，但缺乏对拆迁后土地利用效率的系统评估，需要评估当前改造实践的成效和可持续性。

Method: 使用多时相遥感影像语义分割绘制城中村边界变化，将拆迁后土地利用分为六类：不完全拆迁、闲置土地、建筑工地、建筑物、绿地和其他，选取广州、郑州、西安和哈尔滨四个代表性城市作为研究区域。

Result: 城中村改造过程经常被拖延；改造主要发生在城市外围区域，而城市核心区相对稳定；揭示了三种时空转化路径：同步改造、延迟改造和渐进优化。

Conclusion: 城中村改造具有碎片化、复杂性和非线性特征，需要制定分层和因地制宜的规划策略，研究结果为更包容、高效和可持续的城市更新提供了实证依据。

Abstract: Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the "remained-demolished-redeveloped" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.

</details>


### [272] [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](https://arxiv.org/abs/2511.13533)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: 提出了一种渐近极小极大方法用于多目标共形预测，在确保联合边际覆盖的同时提供紧密的预测区间，并应用于多指标盲图像质量评估、多任务不确定性量化和多轮测量采集。


<details>
  <summary>Details</summary>
Motivation: 在不适定成像逆问题中，不确定性量化是一个基本挑战。现有方法仅处理标量估计目标，而实际应用常涉及多个目标，需要多目标不确定性量化方法。

Method: 提出渐近极小极大多目标共形预测方法，在保证联合边际覆盖的前提下优化预测区间的紧密度。

Result: 数值实验表明，与现有多目标共形预测方法相比，该方法在合成数据和磁共振成像数据上表现更优。

Conclusion: 该方法为多目标不确定性量化提供了有效的解决方案，在多个应用场景中具有实用价值。

Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.

</details>


### [273] [Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew](https://arxiv.org/abs/2511.13535)
*Farhin Farhad Riya,Shahinul Hoque,Jinyuan Stella Sun,Olivera Kotevska*

Main category: cs.CV

TL;DR: 本文提出了一种新的联邦学习攻击方法，通过微小颜色扰动在不影响模型准确性的情况下破坏模型解释性，使显著性图偏离语义相关区域。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在安全关键领域的部署，模型解释性变得至关重要。本文揭示了一个新的攻击面：攻击者可以在不影响预测准确性的情况下破坏模型的可解释性，这对模型审计提出了挑战。

Method: 提出了色度扰动模块，在联邦学习环境中通过改变前景和背景之间的颜色对比度来系统性地制作对抗样本，这些扰动在训练轮次中累积，以隐蔽且持久的方式毒化全局模型的内部特征归因。

Result: 攻击将Grad-CAM解释中的峰值激活重叠降低了35%，同时在所有评估数据集上保持96%以上的分类准确率。标准训练流程无法检测或缓解这种解释性退化。

Conclusion: 研究挑战了模型审计中"正确预测意味着忠实解释"的常见假设，证明可解释性本身可以成为攻击面，特别是在联邦学习环境中，细微的颜色扰动更难被发现。

Abstract: As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.

</details>


### [274] [BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse](https://arxiv.org/abs/2511.13539)
*Yuanchao Wang,Tian Qin,Eduardo Valle,Bruno Abrahao*

Main category: cs.CV

TL;DR: BootOOD是一个完全自监督的OOD检测框架，通过从ID数据中合成伪OOD特征，并利用神经崩溃现象，使用基于特征范数的轻量级辅助头进行OOD检测。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测器在处理与ID类别语义相似的OOD样本时表现不佳，需要一种能够专门处理语义挑战性OOD样本的检测方法。

Method: 通过ID表示变换合成伪OOD特征，利用神经崩溃现象，引入基于特征范数的轻量级辅助头进行半径分类，将OOD检测与主分类器解耦。

Result: 在CIFAR-10、CIFAR-100和ImageNet-200上的实验表明，BootOOD优于现有后处理方法，超越无异常暴露的训练方法，并与最先进的异常暴露方法竞争，同时保持或提高ID准确率。

Conclusion: BootOOD提供了一种有效的自监督OOD检测方法，特别适用于处理语义相似的OOD样本，且不损害ID分类性能。

Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.

</details>


### [275] [Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks](https://arxiv.org/abs/2511.13545)
*Md. Iqbal Hossain,Afia Sajeeda,Neeresh Kumar Perla,Ming Shao*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

</details>


### [276] [TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images](https://arxiv.org/abs/2511.13552)
*Sining Chen,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了TSE-Net，一种用于半监督单目高度估计的自训练框架，通过教师-学生-考试网络结构利用未标记数据提升模型性能


<details>
  <summary>Details</summary>
Motivation: 单目高度估计在遥感3D感知中很重要，但现有方法受限于标注数据稀缺且获取成本高的问题，需要利用大量未标记数据来提升模型泛化能力

Method: 采用教师-学生-考试网络的自训练框架：教师网络生成伪标签，学生网络在未标记数据上训练，考试网络作为学生网络的时序集成以稳定性能。教师网络结合回归和分类分支，使用分层双切割策略处理高度值的长尾分布，并用Plackett-Luce模型校准类别概率

Result: 在三个不同分辨率和成像模式的数据集上进行了评估，证明了方法的有效性

Conclusion: TSE-Net通过半监督学习框架有效利用未标记数据，解决了单目高度估计中标注数据稀缺的问题，提高了模型性能

Abstract: Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.

</details>


### [277] [Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation](https://arxiv.org/abs/2511.13571)
*Ziyang Huang,Jiagang Chen,Jin Liu,Shunping Ji*

Main category: cs.CV

TL;DR: Opt3DGS是一个增强3D高斯泼溅（3DGS）优化的两阶段框架，通过自适应探索和曲率引导开发来解决局部最优和收敛质量不足的问题。


<details>
  <summary>Details</summary>
Motivation: 3DGS在新视角合成中表现出色，但其核心优化问题未被充分探索，存在陷入局部最优和收敛质量不足两个关键问题。

Method: 采用两阶段优化：探索阶段使用自适应加权随机梯度朗之万动力学增强全局搜索；开发阶段使用局部拟牛顿方向引导的Adam优化器利用曲率信息进行精确收敛。

Result: 在多个基准数据集上的广泛实验表明，Opt3DGS在不改变3DGS底层表示的情况下，通过优化过程实现了最先进的渲染质量。

Conclusion: Opt3DGS通过改进3DGS的优化过程，有效解决了其优化挑战，显著提升了渲染质量。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.

</details>


### [278] [Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification](https://arxiv.org/abs/2511.13575)
*Linhan Zhou,Shuang Li,Neng Dong,Yonghang Tai,Yafei Zhang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了分层提示学习（HPL）框架，通过任务感知提示建模联合优化图像到图像（I2I）和文本到图像（T2I）的行人重识别任务。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常将I2I和T2I任务分开处理，这可能导致表示纠缠和性能不佳。I2I强调判别性身份学习，而T2I需要准确的跨模态语义对齐。

Method: 使用任务路由Transformer，在共享视觉编码器中引入双分类令牌来路由I2I和T2I分支的特征。开发分层提示生成方案，集成身份级可学习令牌与实例级伪文本令牌。通过跨模态提示正则化策略在提示令牌空间中强制语义对齐。

Result: 在多个ReID基准测试上的广泛实验验证了该方法的有效性，在I2I和T2I任务上都达到了最先进的性能。

Conclusion: HPL框架能够有效统一处理I2I和T2I行人重识别任务，通过分层提示学习和跨模态正则化实现了优异的性能。

Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.

</details>


### [279] [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](https://arxiv.org/abs/2511.13586)
*Yinuo Xu,Yan Cui,Mingyao Li,Zhi Huang*

Main category: cs.CV

TL;DR: NuClass是一个病理学家工作流启发的多尺度细胞分类框架，通过整合核形态和微环境上下文来识别细胞类型和亚型，解决了现有方法缺乏组织上下文和细粒度标注的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像块的方法能捕捉详细的核形态但缺乏组织上下文，且人类标注通常粗糙且分布不均，难以获得细粒度的亚型级监督。

Method: NuClass包含两个主要组件：Path local（224x224像素裁剪关注核形态）和Path global（1024x1024像素邻域建模周围环境），通过可学习门控模块自适应平衡局部细节和上下文线索，并采用不确定性引导目标促进互补学习。

Result: 在三个完全保留的队列上评估，NuClass最佳类别F1得分达96%，优于强基线方法。

Conclusion: 多尺度、不确定性感知融合可以弥合幻灯片级病理基础模型与可靠细胞级表型预测之间的差距。

Abstract: Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.
  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.
  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.

</details>


### [280] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 提出VVS框架，通过部分验证跳过来加速视觉自回归模型的推理，将目标模型前向传递次数减少2.8倍，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归模型的next-token预测范式引入显著推理延迟，传统推测解码的"一步草稿、一步验证"范式无法直接减少前向传递次数，限制了加速潜力。

Method: 基于视觉令牌可互换性，提出VVS框架，包含三个模块：动态截断的验证自由令牌选择器、令牌级特征缓存与重用、细粒度跳过步调度。

Result: VVS将目标模型前向传递次数减少2.8倍，相比传统推测解码框架提供更优的速度-质量权衡。

Conclusion: VVS通过部分验证跳过有效加速视觉自回归生成，揭示了重塑推测解码范式的强大潜力。

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


### [281] [ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement](https://arxiv.org/abs/2511.13607)
*Xin Xu,Hao Liu,Wei Liu,Wei Wang,Jiayi Wu,Kui Jiang*

Main category: cs.CV

TL;DR: 提出了ICLR框架，通过双流交互增强模块和协方差校正损失，解决低光图像增强中色度与亮度分支交互的分布差异和梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像增强中色度与亮度分支交互的两个关键问题：自然图像中两分支的分布差异限制了互补特征提取，以及大均匀色区域中色度分支间弱相关性导致的传统像素级损失梯度冲突。

Method: 提出ICLR框架，包含双流交互增强模块(DIEM)从融合和增强两个维度改进互补信息提取，以及协方差校正损失(CCL)利用亮度残差统计惩罚色度误差并约束色度分支协方差来平衡梯度冲突。

Result: 在多个数据集上的实验结果表明，提出的ICLR框架优于最先进的方法。

Conclusion: ICLR框架通过改进色度与亮度交互机制，有效提升了低光图像增强的性能，在对比度增强和细节恢复方面表现出色。

Abstract: Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.

</details>


### [282] [AtlasMorph: Learning conditional deformable templates for brain MRI](https://arxiv.org/abs/2511.13609)
*Marianne Rakic,Andrew Hoopes,S. Mazdak Abulnaga,Mert R. Sabuncu,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: 提出了一种基于卷积配准神经网络的机器学习框架，能够根据特定属性（如年龄、性别）高效生成条件化模板，并利用分割信息生成解剖标签图，同时可用于图像配准。


<details>
  <summary>Details</summary>
Motivation: 传统可变形模板开发计算成本高，可用模板数量有限，导致分析常使用不具代表性的模板，特别是在群体变异较大时。

Method: 使用卷积配准神经网络学习一个函数，该函数根据特定属性输出条件化模板，并利用分割信息生成相应的解剖分割图。

Result: 在3D脑MRI数据集上验证，该方法能学习到高质量的代表性模板，带标签的条件模板比无标签模板配准效果更好，优于其他模板构建方法。

Conclusion: 该方法能够高效生成具有代表性的条件化模板，并改善医学图像配准性能。

Abstract: Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.

</details>


### [283] [Tissue Aware Nuclei Detection and Classification Model for Histopathology Images](https://arxiv.org/abs/2511.13615)
*Kesi Xu,Eleni Chiou,Ali Varamesh,Laura Acqualagna,Nasir Rajpoot*

Main category: cs.CV

TL;DR: TAND是一种新型组织感知细胞核检测框架，通过结合组织掩码条件化，在点级监督下实现细胞核检测和分类的联合学习，显著降低标注负担并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有细胞核检测方法依赖详细专家标注且未能充分利用组织上下文信息，限制了在计算病理学中的实际应用。

Method: 提出TAND框架，将ConvNeXt编码器-解码器与冻结的Virchow-2组织分割分支耦合，通过新颖的多尺度空间特征线性调制技术，利用语义组织概率选择性地调节分类流。

Result: 在PUMA基准测试中达到最先进性能，超越组织无关基线和掩码监督方法，特别在组织依赖性细胞类型（如上皮细胞、内皮细胞和基质细胞）上表现显著提升。

Conclusion: 这是首个基于学习组织掩码调节单细胞分类的方法，为减少标注负担提供了实用途径。

Abstract: Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.

</details>


### [284] [A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio](https://arxiv.org/abs/2511.13618)
*Ashlesha G. Sawant,Shreyash S. Kamble,Raj S. Kanade,Raunak N. Kanugo,Tanishq A. Kapse,Karan A. Bhapse*

Main category: cs.CV

TL;DR: 开发基于面部特征和眼动分析的驾驶员疲劳检测系统，使用Eye Aspect Ratio方法和MediaPipe Face Mesh实时监测驾驶员状态，通过声音警报预防疲劳驾驶事故。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是导致道路事故的主要原因之一，每年造成数千人死亡和受伤，需要开发有效的实时监测系统来提高道路安全。

Method: 使用标准网络摄像头，结合OpenCV图像处理和MediaPipe Face Mesh框架实时追踪面部特征，重点通过Eye Aspect Ratio方法分析眼动状态，检测长时间闭眼和低眨眼频率等疲劳特征。

Result: 实验分析表明系统具有高准确性和快速响应能力，能够有效检测驾驶员疲劳状态并及时发出警报。

Conclusion: 该系统提供了一种高性能、低成本的驾驶员监控解决方案，可作为先进驾驶辅助系统的组成部分，有效预防疲劳驾驶相关事故。

Abstract: One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).

</details>


### [285] [Alpha Divergence Losses for Biometric Verification](https://arxiv.org/abs/2511.13621)
*Dimitrios Koutsianos,Ladislav Mosner,Yannis Panagakis,Themos Stafylakis*

Main category: cs.CV

TL;DR: 本文提出了两种基于α-散度的边界损失函数：Q-Margin和A3M，通过在参考度量或对数中引入角度边界，显著提升了人脸和说话人验证任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的α-散度损失函数虽然能诱导稀疏解，但难以直接集成角度边界这一对验证任务至关重要的组件。

Method: 探索了两种集成角度边界的途径：通过参考度量（Q-Margin）和通过对数（A3M），并针对A3M的训练不稳定性提出了原型重新初始化策略。

Result: 在IJB-B、IJB-C人脸验证基准和VoxCeleb说话人验证任务上取得了显著性能提升，特别是在低误接受率下表现优异。

Conclusion: 提出的方法在高安全性应用中具有重要价值，能够有效减少误认证，特别适用于银行认证等场景。

Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.

</details>


### [286] [CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2511.13644)
*Shrenik Patel,Daivik Patel*

Main category: cs.CV

TL;DR: CacheFlow是一种无需训练的视频问答解决方案，通过动态令牌丢弃和压缩长期记忆机制，显著减少处理令牌数量（最多87%），同时保持长范围推理能力，特别适用于实时流媒体VQA。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理长视频问答时面临注意力机制和KV缓存随运行时间增长的挑战，导致推理成本高昂或只能使用短视的滑动窗口方法。

Method: 结合动态令牌丢弃（基于帧间余弦相似度在线修剪补丁令牌）和压缩长期记忆（使用小型循环编码器总结关键信息，检索时重新加载相关KV对），采用共识检索机制选择最相关的Top-K块进行注意力计算。

Result: 在离线和流媒体VQA基准测试中优于现有强基线方法，同时处理令牌数量减少高达87%。

Conclusion: CacheFlow实现了视觉语言模型在长视频理解中的高效和上下文感知，为实际应用铺平了道路，且无需微调即可直接应用。

Abstract: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

</details>


### [287] [Part-X-MLLM: Part-aware 3D Multimodal Large Language Model](https://arxiv.org/abs/2511.13647)
*Chunshi Wang,Junliang Ye,Yunhan Yang,Yang Li,Zizhuo Lin,Jun Zhu,Zhuo Chen,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: Part-X-MLLM是一个原生3D多模态大语言模型，通过结构化可执行语法将多样3D任务统一为程序，实现基于部件的生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 统一多样3D任务，通过语言原生前端控制几何引擎，解耦符号规划与几何合成。

Method: 使用双编码器架构预训练分离结构与语义，在大规模部件中心数据集上进行指令调优，自回归生成包含部件边界框、语义描述和编辑命令的连贯标记序列。

Result: 模型在生成高质量结构化规划方面表现出色，在接地问答、组合生成和局部化编辑任务中实现最先进性能。

Conclusion: 通过单一统一接口实现了3D任务的符号规划与几何合成的有效解耦，为多样化3D应用提供了灵活的控制前端。

Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/

</details>


### [288] [PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image](https://arxiv.org/abs/2511.13648)
*Ziang Cao,Fangzhou Hong,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: PhysX-Anything是首个仿真就绪的物理3D生成框架，能从单张图像生成具有明确几何、关节和物理属性的高质量仿真就绪3D资产，显著提升在具身AI中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法大多忽略了关键的物理和关节属性，限制了其在具身AI中的应用。需要将3D建模从静态视觉表示转向可直接用于仿真和交互的物理化、关节化资产。

Method: 提出首个基于VLM的物理3D生成模型，采用新的3D表示方法将几何体高效token化，token数量减少193倍。构建了PhysX-Mobility数据集，包含2000+常见真实世界物体，物理标注丰富。

Result: 在PhysX-Mobility数据集和真实世界图像上的实验表明，PhysX-Anything具有强大的生成性能和鲁棒泛化能力。仿真实验验证了生成的仿真就绪资产可直接用于接触密集的机器人策略学习。

Conclusion: PhysX-Anything能够显著赋能下游应用，特别是在具身AI和基于物理的仿真领域，为3D生成开辟了新的物理感知方向。

Abstract: 3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.

</details>


### [289] [Distribution Matching Distillation Meets Reinforcement Learning](https://arxiv.org/abs/2511.13649)
*Dengyang Jiang,Dongyang Liu,Zanyi Wang,Qilong Wu,Xin Jin,David Liu,Zhen Li,Mengmeng Wang,Peng Gao,Harry Yang*

Main category: cs.CV

TL;DR: DMDR是一个结合强化学习的扩散模型蒸馏框架，通过同时进行蒸馏和强化学习来提升少步生成器的性能，甚至超越多步教师模型。


<details>
  <summary>Details</summary>
Motivation: 传统的分布匹配蒸馏方法中，少步生成器的性能往往受限于多步教师模型。为了突破这一限制，需要开发能够解锁少步生成器潜力的新方法。

Method: 将强化学习技术融入蒸馏过程，使用DMD损失作为正则化项，设计动态分布引导和动态重噪声采样训练策略来改进初始蒸馏过程。

Result: 实验表明DMDR在少步方法中实现了领先的视觉质量和提示一致性，甚至超越了多步教师模型的性能。

Conclusion: DMDR框架通过结合蒸馏和强化学习，成功解锁了少步生成器的能力，为高效扩散模型生成提供了新的解决方案。

Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

</details>


### [290] [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)
*Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema*

Main category: cs.CV

TL;DR: OlmoEarth是一个多模态时空基础模型，专门为地球观测数据设计，在多个基准测试和实际任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据具有空间性（如图像）、序列性（如视频或文本）和多模态特性，需要专门设计的模型来处理这些复杂特征。

Method: 采用新颖的自监督学习公式、掩码策略和损失函数，专门针对地球观测领域设计。

Result: 在24个任务中的15个任务上获得最佳嵌入性能，在29个任务中的19个任务上通过全微调获得最佳性能，优于其他12个基础模型。

Conclusion: OlmoEarth作为端到端平台的核心，为非营利组织和NGO提供前沿基础模型和强大数据管理工具，用于解决全球重大问题。

Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.

</details>


### [291] [Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting](https://arxiv.org/abs/2511.13684)
*Jiangnan Ye,Jiedong Zhuang,Lianrui Mu,Wenjie Zheng,Jiaqi Hu,Xingze Zou,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: GS-Light是一个基于高斯泼溅的文本引导3D场景重光照系统，通过训练免费的扩散模型扩展和多视图输入处理，实现高效的重光照效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本引导的3D场景重光照方面存在效率低、光照方向控制不准确等问题，需要一种能够准确理解用户光照意图并保持多视图一致性的解决方案。

Method: 使用大视觉语言模型解析光照先验，结合几何和语义估计器生成光照图，通过精心设计的初始潜码指导扩散模型生成重光照结果，最后微调3D高斯泼溅场景。

Result: 在室内外场景评估中，GS-Light在多个定量指标（多视图一致性、图像质量、美学评分、语义相似度等）和用户研究中均优于现有基线方法。

Conclusion: GS-Light提供了一种高效、准确的文本引导3D场景重光照方法，能够更好地满足用户的光照期望，特别是在光照方向控制方面表现突出。

Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.

</details>


### [292] [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704)
*Harold Haodong Chen,Disen Lan,Wen-Jie Shu,Qingyang Liu,Zihan Wang,Sirui Chen,Wenkai Cheng,Kanghao Chen,Hongfei Zhang,Zixin Zhang,Rongjin Guo,Yu Cheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出了TiViBench基准来评估图像到视频生成模型的推理能力，包含四个维度的24个任务场景，并开发了VideoTPO测试时优化策略来提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型评估主要关注视觉保真度和时间一致性，缺乏对高阶推理能力的评估，需要专门基准来填补这一空白。

Method: 设计分层基准TiViBench，系统评估结构推理、空间视觉模式推理、符号逻辑推理、行动规划四个维度；提出VideoTPO测试时策略，通过LLM自分析生成候选来识别优劣。

Result: 商业模型（如Sora 2、Veo 3.1）展现更强推理潜力，开源模型因训练规模和数据多样性受限而潜力未充分挖掘；VideoTPO无需额外训练即可显著提升推理性能。

Conclusion: TiViBench和VideoTPO为评估和推进视频生成模型的推理能力奠定了基础，为该新兴领域的未来研究设定了框架。

Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.

</details>


### [293] [Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine](https://arxiv.org/abs/2511.13713)
*Xincheng Shuai,Zhenyuan Qin,Henghui Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: FFSE是一个3D感知的自回归框架，能够在真实图像上实现直观、物理一致的对象编辑，通过建模为3D变换序列来支持平移、缩放、旋转等操作，同时保持背景效果和场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在语义图像编辑方面取得了进展，但大多数方法无法进行3D感知的对象操作，要么在图像空间操作，要么需要缓慢且容易出错的3D重建。

Method: 提出FFSE框架，将编辑建模为学习的3D变换序列，并引入3DObjectEditor混合数据集，从多样化的对象和场景中构建模拟编辑序列，支持多轮和动态条件下的有效训练。

Result: 大量实验表明，FFSE在单轮和多轮3D感知编辑场景中显著优于现有方法。

Conclusion: FFSE通过3D感知的自回归框架和专门构建的数据集，成功实现了对真实图像的直观、物理一致的多轮对象编辑。

Abstract: Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.

</details>


### [294] [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)
*Junwei Yu,Trevor Darrell,XuDong Wang*

Main category: cs.CV

TL;DR: UnSAMv2是一个无需人工标注就能实现任意粒度分割的模型，通过发现丰富的掩码-粒度对和引入粒度控制嵌入，显著提升了SAM-2的分割能力。


<details>
  <summary>Details</summary>
Motivation: SAM系列模型在控制分割粒度方面存在局限，用户需要手动细化结果才能达到所需细节水平，这个过程既模糊又耗时，且收集所有粒度的密集标注成本过高。

Method: 扩展了UnSAM的分治策略，发现丰富的掩码-粒度对，并引入新颖的粒度控制嵌入，实现对分割尺度的精确连续控制。

Result: 仅使用6K未标注图像和0.02%额外参数，UnSAMv2显著提升了SAM-2，在交互式、全图像和视频分割任务中实现任意粒度分割。在11个基准测试中，NoC90从5.69提升到4.75，1-IoU从58.0提升到73.1，AR1000从49.6提升到68.3。

Conclusion: 少量未标注数据结合粒度感知的自监督学习方法可以释放视觉基础模型的潜力。

Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

</details>


### [295] [Segment Anything Across Shots: A Method and Benchmark](https://arxiv.org/abs/2511.13715)
*Hengrui Hu,Kaining Ying,Henghui Ding*

Main category: cs.CV

TL;DR: 提出了SAAS模型和TMA数据增强策略，用于解决多镜头半监督视频对象分割问题，通过模拟镜头转换来提升跨镜头分割性能，并在新的Cut-VOS基准上取得先进结果。


<details>
  <summary>Details</summary>
Motivation: 现有VOS方法主要针对单镜头视频，难以处理镜头间的不连续性，限制了实际应用。需要解决多镜头视频对象分割问题。

Method: 提出TMA数据增强策略，使用单镜头数据模拟跨镜头泛化；开发SAAS模型，能够有效检测和理解镜头转换；构建Cut-VOS基准数据集。

Result: 在YouMVOS和Cut-VOS数据集上的大量实验表明，SAAS模型通过有效模拟、理解和跨复杂转换分割，实现了最先进的性能。

Conclusion: SAAS模型能够有效处理多镜头视频对象分割问题，通过模拟镜头转换提升跨镜头分割能力，为MVOS领域提供了新的解决方案和评估基准。

Abstract: This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.

</details>


### [296] [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720)
*Tianhong Li,Kaiming He*

Main category: cs.CV

TL;DR: 本文提出JiT方法，直接预测干净图像而非噪声，利用流形假设使低容量网络在高维空间中有效工作，使用大块Transformer在像素级别实现竞争性生成结果。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型预测噪声而非干净图像，但根据流形假设，自然数据位于低维流形而噪声数据不在，直接预测干净数据能更有效地利用网络能力。

Method: 使用简单的大块Transformer直接在像素级别操作，无需分词器、预训练或额外损失，通过16和32的大块尺寸在ImageNet上训练。

Result: 在ImageNet 256×256和512×512分辨率上获得竞争性结果，而预测高维噪声的方法在这些情况下会失败。

Conclusion: 直接预测干净数据使Transformer能够在原始自然数据上进行扩散建模，回归到基于流形的基本原理，提供了一种自包含的生成范式。

Abstract: Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [297] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 提出了TimeStampEval基准测试，用于从长文本记录中检索非逐字引用的精确时间戳。通过两阶段方法显著提高检索准确性，同时降低90%以上的推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊匹配在处理语义相同但句法不同的引用时的失败问题，特别是在对齐官方书面记录与语音转文字记录的场景中。

Method: 采用两阶段方法：首先使用RapidFuzz进行预过滤，然后使用LLM在短片段上进行验证。通过优化提示设计（将查询放在文本前并使用紧凑格式）来提高性能。

Result: 在2,800句（120k标记）的转录本上评估了6个现代LLM，结果显示：提示设计比模型选择更重要；适度的推理预算可将准确率从37%提升到77%以上；该方法将模糊匹配准确率提高了50个百分点，同时将延迟减半，每个正确结果的成本降低了96%。

Conclusion: 该方法对转录本长度、词汇漂移和领域变化具有鲁棒性，在10个转录本（50k-900k标记，1989-2025年）的扩展测试中，对不存在目标的拒绝准确率保持在95-100%。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [298] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0是一个开源研究代理，通过交互扩展作为第三个性能改进维度，在保持256K上下文窗口下能执行600次工具调用，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索模型层面的交互扩展，作为模型规模和上下文长度之外的第三个性能改进维度，解决传统LLM在长推理链中性能下降的问题。

Method: 通过强化学习实现交互扩展，利用环境反馈和外部信息获取来纠正错误和优化轨迹，支持深度多轮推理和复杂研究工作流。

Result: 72B变体在GAIA、HLE、BrowseComp和BrowseComp-ZH基准测试中分别达到81.9%、37.7%、47.1%和55.6%的准确率，超越先前开源代理并接近商业模型。

Conclusion: 交互扩展展现出与模型规模和上下文长度类似的扩展行为，成为构建下一代开源研究代理的第三个关键维度。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [299] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 该论文质疑语言模型是否真正具备推理能力，认为LM只是实现隐式有限阶马尔可夫核的统计模式匹配器，而非真正的推理者。


<details>
  <summary>Details</summary>
Motivation: 评估NLP领域中关于语言模型推理能力的定义是否合理，澄清LM的计算本质与人类推理概念之间的不匹配。

Method: 通过分析推理的定义和关键论文的用法，假设基于transformer的LM实现隐式有限阶马尔可夫核，将上下文映射到条件标记分布。

Result: 发现LM产生的类似推理输出对应于学习核中的统计规律性和近似统计不变性，而非显式逻辑机制的实施。

Conclusion: LM是统计模式匹配器而非真正推理者，这一区分对于评估LM中的认知不确定性至关重要，需要更准确地描述NLP系统的计算过程。

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [300] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 评估7个开源大语言模型(0.6B-70B参数)在水电许可文档信息提取中的性能-计算资源权衡，发现14B参数是性能跃迁的关键阈值。


<details>
  <summary>Details</summary>
Motivation: 解决监管文档信息提取中性能与计算资源之间的关键权衡问题，为实际部署提供实证指导。

Method: 在水电许可文档上评估7个不同参数规模(0.6B-70B)的开源模型，分析验证方法的有效性。

Result: 识别出14B参数阈值，验证方法从无效(F1<0.15)转变为可行(F1=0.64)；消费级模型可达64% F1，小模型停滞在51%，大规模模型接近77%但需企业基础设施。

Conclusion: 建立了首个监管背景下开源信息提取的资源-性能映射，为基于证据的模型选择提供指导，这些发现对水电合规有直接价值，且参数缩放效应的见解可推广到其他信息提取任务。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [301] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 探索使用基于LLM的自动形式化方法来验证LLM生成的输出，通过两个实验展示了该方法在一致性检查和逻辑验证方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在从自然语言生成结构化输出方面表现出潜力，但缺乏正式方法来验证这些输出的准确性。本文旨在填补这一空白。

Method: 使用简单的基于LLM的自动形式化器，对LLM生成的输出与自然语言需求进行验证，通过两个实验测试其有效性。

Result: 实验一：自动形式化器成功识别出两个不同表述的自然语言需求在逻辑上是等价的；实验二：识别出给定自然语言需求与LLM生成输出之间存在逻辑不一致。

Conclusion: 自动形式化在确保LLM生成输出的保真度和逻辑一致性方面具有显著潜力，为未来更广泛的研究奠定了基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [302] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 提出一个分析电影剧本情感弧线的框架，使用基于NRC-VAD数据集的定制词典进行情感分析，并通过层次聚类技术对相似情感模式进行分组。


<details>
  <summary>Details</summary>
Motivation: 故事理解和分析是自然语言理解中的挑战领域，需要深度计算语义表示和句法处理。大量叙事数据需要自动化语义分析而非手动方法。

Method: 使用基于NRC-VAD数据集Valence、Arousal和Dominance分数的定制词典进行词典式情感分析，应用LabMTsimple storylab模块，并通过Wards层次聚类技术对相似情感情节进行聚类。

Result: 在电影数据集上的实验评估表明，该分析结果有助于消费者和读者在选择叙事或故事时做出决策。

Conclusion: 该框架能够提取叙事中传达的高层和低层概念，为电影剧本的情感分析提供了有效工具。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [303] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 该研究引入了一个包含6,393份放射学报告的标注语料库，用于系统比较传统机器学习分类器与生成式大语言模型在随访依从性检测任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门用于评估大语言模型在放射学任务性能的领域特定数据集，需要开发可靠的随访依从性检测系统。

Method: 使用标注语料库比较了逻辑回归、支持向量机、Longformer、微调Llama3-8B-Instruct与GPT-4o、GPT-OSS-20B等模型，后者在基础设置和任务优化设置下测试，并改进了提示策略。

Result: GPT-4o（高级设置）表现最佳（F1=0.832），GPT-OSS-20B（高级设置）紧随其后（F1=0.828），逻辑回归和支持向量机也表现良好（F1=0.776和0.775）。标注者间一致性高（F1=0.846）。

Conclusion: 虽然大语言模型通过提示优化接近人类水平的一致性，但可解释且资源效率高的传统模型仍然是重要的基准。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [304] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: MedPT是首个针对巴西葡萄牙语的大规模真实医患问答数据集，包含384,095对问答，通过多阶段筛选和LLM标注增强，用于开发更公平、准确的医疗技术。


<details>
  <summary>Details</summary>
Motivation: 现有LLM开发主要关注高资源语言，而简单翻译无法捕捉特定临床和文化细微差别（如地方性疾病），需要为葡萄牙语等语言构建专门的医疗数据集。

Method: 构建包含384,095对真实医患问答的语料库，采用混合定量-定性分析进行多阶段筛选，使用LLM驱动标注将问题分为7种语义类型，并分析其主题广度和语言特性。

Result: 在医疗专科路由任务中，微调1.7B参数模型在20类设置下达到94%的F1分数，错误分析显示误分类反映了真实的临床模糊性，证明了数据集的语义丰富性。

Conclusion: MedPT数据集将促进葡萄牙语世界更公平、准确和文化敏感的医疗技术发展，已公开发布。

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [305] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: ClinStructor使用大语言模型将临床自由文本转换为结构化问答对，以解决临床笔记中的偏见、泛化性和可解释性问题，在ICU死亡率预测任务中性能仅轻微下降2-3% AUC。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含丰富信息但格式非结构化，存在偏见、跨系统泛化性差和可解释性不足等问题，需要改进临床机器学习模型的可靠性和可解释性。

Method: 利用大语言模型将临床自由文本转换为任务特定的结构化问答对，作为预测建模的前处理步骤。

Result: 与直接微调相比，在ICU死亡率预测任务中仅导致AUC轻微下降2-3%，但显著提高了透明度和可控性。

Conclusion: ClinStructor为在临床环境中构建可靠、可解释和可泛化的机器学习模型奠定了坚实基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [306] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 通过监督微调和强化学习技术增强GPT-2的心理治疗对话生成能力，在多个评估指标上取得显著提升，特别是情感准确率达到99.34%。


<details>
  <summary>Details</summary>
Motivation: COVID-19加剧了心理健康服务的可及性挑战，需要远程心理健康支持。虽然大语言模型提供24/7可用性和无偏见交互，但预训练模型缺乏必要的上下文和情感意识来提供适当的治疗响应。

Method: 重构输入格式以同时处理上下文信息和情感状态，采用包含专业治疗师响应和标注情感的多组件奖励函数，应用监督微调和强化学习技术。

Result: 强化学习在多个评估指标上优于基线GPT-2：BLEU(0.0111)、ROUGE-1(0.1397)、ROUGE-2(0.0213)、ROUGE-L(0.1317)、METEOR(0.0581)。情感准确率达到99.34%，而基线GPT-2为66.96%。LLM评估确认高上下文相关性和专业性。

Conclusion: 强化学习在开发治疗对话系统方面非常有效，可以作为治疗师的有价值辅助工具，同时保持必要的人类临床监督。

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [307] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: CALM是一个可解释的临床文本分类框架，通过将预测分解为各个语义组件的贡献总和，提供透明的风险解释。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在临床文本分类中预测不透明的问题，为研究人员和医生提供可理解的决策依据。

Method: 使用加性模型结构，将输入分解为语义组件（如病历章节、问答字段），预测结果为各组件贡献的总和。

Result: CALM在保持与传统LLM分类器相当性能的同时，提高了模型的可信度，支持质量检查，并揭示了临床有意义的模式。

Conclusion: CALM为半结构化临床文本提供了一种可解释的分类方法，有助于模型开发和审计，促进临床实践中的可信采用。

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [308] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 提出了InData数据集来评估LLM在多步骤工具推理方面的能力，发现当前LLM在复杂数据分析任务中表现不佳


<details>
  <summary>Details</summary>
Motivation: 解决LLM直接生成和执行代码访问敏感数据的安全风险，通过预定义安全工具来限制LLM的数据访问

Method: 引入Indirect Data Engagement (InData)数据集，包含三个难度级别的数据分析问题，评估15个开源LLM的多步骤工具推理能力

Result: 大型模型在简单任务上准确率达97.3%，但在困难任务上降至69.6%，显示当前LLM缺乏稳健的多步骤工具推理能力

Conclusion: InData为开发和评估具有更强多步骤工具使用能力的LLM迈出了重要一步，将公开发布数据集和代码

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [309] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 提出了LLM-KAT评估方法来衡量知识图谱对话生成中LLMs对知识图谱的依赖程度，并提出实体匿名化技术来增强LLMs对外部知识的利用。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在各种NLP任务中表现出色，但在知识图谱对话生成任务中，它们往往过度依赖内部知识而忽视提供的外部知识图谱，导致生成回复与知识图谱脱节。

Method: 引入LLM-KAT评估程序来测量生成回复中的知识附着度；提出简单的实体匿名化技术，通过匿名化知识图谱中的实体来强制LLMs依赖外部知识。

Result: 在OpenDialKG数据集上的实验表明，该方法有效提升了LLMs对外部知识图谱的依赖程度。

Conclusion: 实体匿名化是一种简单而有效的技术，能够显著改善LLMs在知识图谱对话生成任务中对外部知识的利用能力。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [310] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 本文研究了语言模型的熵校准问题，发现模型存在校准错误，且这种错误随规模扩大改善缓慢。理论分析表明校准错误的缩放指数接近0，实证研究验证了这一点。作者证明了在理论上可以通过预测未来熵来减少熵同时保持对数损失。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型熵校准问题，探索模型规模扩大是否能改善校准错误，以及是否存在无需权衡的校准方法。

Method: 首先在简化理论设置中分析校准错误的缩放行为，然后在0.5B到70B参数的语言模型中进行实证测量，最后从理论上证明减少熵同时保持对数损失的可能性。

Result: 发现校准错误的缩放指数接近0，意味着大模型与小模型以相似速率积累错误；理论上证明了通过预测未来熵可以减少熵同时保持对数损失。

Conclusion: 语言模型的校准错误随规模扩大改善缓慢，截断分布不是理想解决方案；理论上存在无需权衡的校准方法，但需要能够预测未来熵的黑盒模型。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [311] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 提出了一个名为ReasoningNER的推理框架，将NER从隐式模式匹配转变为显式推理，通过CoT生成、CoT调优和推理增强三个阶段，在零样本场景下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 生成式LLMs在NER任务中通常通过指令调优提升性能，但缺乏显式可验证的推理机制，导致在零样本和低资源场景下性能不佳和泛化能力脆弱。

Method: 三阶段框架：1) 生成带有NER导向CoT注释的数据集；2) 使用CoT调优NER模型，在得出最终答案前生成连贯的推理链；3) 推理增强阶段使用综合奖励信号优化推理过程。

Result: 在零样本设置下实现了SOTA性能，F1分数比GPT-4高出12.3个百分点，展示了在NER任务中令人印象深刻的认知能力。

Conclusion: ReasoningNER框架通过显式推理机制显著提升了NER性能，特别是在零样本场景下，为面向推理的信息提取研究提供了重要潜力。

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [312] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: 研究发现CoT解释在道德场景中具有双重作用：既能增强透明度，也能通过确认偏见误导用户，特别是当解释采用自信语气时，会抑制错误检测并维持用户依赖。


<details>
  <summary>Details</summary>
Motivation: 探讨CoT解释在透明度与确认偏见之间的平衡问题，研究解释如何影响用户对AI推理的信任和错误检测能力。

Method: 在多模态道德场景中系统性地扰动推理链并操纵解释语气，分析视觉语言模型中的推理错误及其对用户信任和错误检测的影响。

Result: 发现两个关键效应：用户常将信任等同于结果一致性，即使推理有缺陷仍保持依赖；自信语气会抑制错误检测但维持依赖，表明表达风格可凌驾于正确性之上。

Conclusion: CoT解释既能澄清也能误导，NLP系统需要提供鼓励审查和批判性思维而非盲目信任的解释。

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [313] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 本文提出了一个评估大语言模型文化能力的新基准，通过真实情境上下文来测试文化推理能力，并引入四个补充指标来全面评估响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文化能力评估方法局限于去情境化的正确性或强制选择判断，忽视了文化理解和推理的需求，无法准确评估模型在多元文化环境中的表现。

Method: 引入基于真实情境上下文的评估基准，除了标准精确匹配指标外，还提出了覆盖度、特异性、内涵和连贯性四个补充指标，采用厚评估方法分析模型的文化推理深度。

Result: 实证分析显示，传统薄评估方法会系统性高估文化能力且评估结果不稳定、方差大；而厚评估方法能暴露推理深度差异，减少方差，提供更稳定、可解释的文化理解信号。

Conclusion: 厚评估方法比传统薄评估更能准确衡量大语言模型的文化能力，为模型在多元文化环境中的部署提供了更可靠的评估框架。

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [314] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: 结合回译和微调在小规模日语语料上显著提升英日神经机器翻译质量，COMET分数从0.460提升至0.597。


<details>
  <summary>Details</summary>
Motivation: 研究在有限训练数据情况下，如何通过回译和微调的协同作用提升低资源语言对的翻译质量。

Method: 首先使用回译生成合成数据，然后在真实小规模平行数据集上进行微调，最后结合两种方法：先用回译增强数据集，再进行微调。

Result: 基线模型COMET=0.460，仅回译提升至0.468，仅微调提升至0.589，结合两种方法达到0.597。

Conclusion: 回译和针对性微调的协同使用能显著提升翻译质量，为低资源语言对提供了轻量级但强大的改进策略。

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [315] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMLagBench是一个用于评估LLM训练数据时间边界的基准测试，通过检测模型对近期事件的了解来识别知识截止时间。


<details>
  <summary>Details</summary>
Motivation: LLM在特定时间点前的文本数据上预训练，这形成了严格的知识边界。当这个限制未知或被忽视时，模型可能在推理任务中无意混合过时的时效性信息与通用知识，影响回答准确性。

Method: 引入LLMLagBench基准测试，系统性地通过评估LLM对近期事件的知识来识别其训练数据的最早可能时间边界。评估了大量LLM，包括有明确声明和未声明训练截止时间的模型。

Result: 通过人工验证和与公开的LLM预训练信息比较，评估了基准测试的可靠性。

Conclusion: LLMLagBench为识别LLM知识边界提供了系统方法，有助于理解模型的时间知识局限性。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [316] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出了U-MStance数据集和PRISM模型，解决多模态对话立场检测中的伪多模态和用户同质性问题，通过用户画像和多模态对齐提升立场检测性能


<details>
  <summary>Details</summary>
Motivation: 现有研究存在伪多模态（源帖有视觉内容但评论仅文本）和用户同质化（忽略个人特质）问题，无法反映真实多模态交互

Method: PRISM模型：1）从历史数据提取纵向用户画像；2）通过思维链对齐文本和视觉线索；3）使用相互任务强化机制联合优化立场检测和立场感知响应生成

Result: 在U-MStance数据集上，PRISM相比强基线模型取得显著性能提升

Conclusion: 用户中心化和上下文基础的多模态推理对于现实立场理解具有有效性

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [317] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: AI-Salesman是一个用于目标驱动说服对话的双阶段框架，通过贝叶斯监督强化学习训练销售策略，并在推理阶段使用动态大纲引导代理，在真实销售场景中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 目标驱动的说服对话（如电话营销）需要复杂的多轮规划和严格的事实准确性，这对现有LLM构成重大挑战。缺乏任务特定数据和直接应用LLM存在策略脆弱性和事实幻觉问题。

Method: 构建TeleSalesCorpus数据集，提出AI-Salesman框架：训练阶段使用贝叶斯监督强化学习从噪声对话中学习稳健销售策略；推理阶段采用动态大纲引导代理（DOGA），利用预建脚本库提供动态策略指导。

Result: 实验结果表明，AI-Salesman在自动指标和全面人工评估中均显著优于基线模型，在复杂说服场景中表现出色。

Conclusion: AI-Salesman框架通过双阶段架构和动态大纲引导，有效解决了目标驱动说服对话中的策略规划和事实准确性问题，为类似应用提供了可行解决方案。

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [318] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: VBackChecker是一个无需参考的幻觉检测框架，通过像素级Grounding LLM验证MLLM生成响应与视觉输入的一致性，在R²-HalBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在严重幻觉问题，需要准确检测以确保实际应用中的可靠性。

Method: 基于"眼见为实"原则，使用具有推理和分割能力的像素级Grounding LLM，设计了R-Instruct指令调优数据生成流程，包含丰富上下文描述、定位掩码和困难负样本。

Result: 在R²-HalBench基准上超越先前复杂框架，性能媲美GPT-4o，在像素级定位任务中比先前方法提升超过10%。

Conclusion: VBackChecker为MLLM幻觉检测提供了有效的无参考框架，具有处理丰富上下文场景的能力和可解释性。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [319] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: CriticSearch是一个细粒度信用分配框架，通过回顾性批评机制为搜索代理提供密集的回合级反馈，解决了强化学习中稀疏奖励导致的训练不稳定和低效探索问题。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理管道依赖强化学习优化，但面临稀疏结果奖励的问题，导致训练效率低下和不稳定。

Method: 使用冻结的非对称批评LLM，基于完整轨迹和黄金答案的优先信息回顾性评估每个回合，将评估转化为稳定的密集奖励来指导策略改进。

Result: 在多个多跳推理基准测试中，CriticSearch持续优于现有基线，实现了更快的收敛、更好的训练稳定性和更高的性能。

Conclusion: CriticSearch通过密集的回合级反馈机制有效提升了搜索代理的训练效率和性能。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [320] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: MME-RAG是一个多管理器-专家检索增强生成框架，通过将实体识别分解为类型级判断和跨度级提取两个协调阶段，解决了LLMs在领域适应和检索可控性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在面向任务的对话中，在领域适应和检索可控性方面仍面临挑战，特别是在细粒度实体识别任务上。

Method: 提出MME-RAG框架，包含轻量级管理器进行类型级判断和专门专家进行跨度级提取，每个专家由KeyInfo检索器支持，在推理时注入语义对齐的少样本示例。

Result: 在CrossNER、MIT-Movie、MIT-Restaurant和新构建的多领域客服数据集上的实验表明，MME-RAG在大多数领域表现优于现有基线方法。

Conclusion: 层次分解和KeyInfo引导的检索是鲁棒性和跨领域泛化的关键驱动因素，MME-RAG为自适应对话理解提供了可扩展和可解释的解决方案。

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [321] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: CONFACTCHECK是一种高效的幻觉检测方法，通过检查生成文本中事实探针回答的一致性来检测幻觉，无需外部知识库且资源消耗少。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成文本时经常产生事实错误的幻觉，这在医疗、金融等领域存在严重风险。现有方法在模型访问受限时需要多次API调用，增加了延迟和成本。

Method: 基于直觉：生成文本中对事实探针的回答应该在单个LLM内部和不同LLM之间保持一致。该方法不依赖外部知识库，通过检查回答一致性来检测幻觉。

Result: 在多个数据集上的实证评估显示，CONFACTCHECK能以更少资源高效检测幻觉事实，在相似条件下比现有基线方法获得更高的准确率。

Conclusion: CONFACTCHECK提供了一种资源高效的幻觉检测方法，特别适用于模型访问受限的场景，在保持高准确率的同时减少了API调用和资源消耗。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [322] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: 提出了ViConBERT框架，结合对比学习和基于词义注释的蒸馏方法，用于学习越南语上下文嵌入，并创建了首个大规模越南语语义理解评估数据集ViConWSD。


<details>
  <summary>Details</summary>
Motivation: 越南语缺乏强大的语义理解模型和评估资源，而现有的上下文词嵌入进展主要集中在英语等高资源语言上。

Method: 使用对比学习(SimCLR)和基于词义注释的蒸馏方法，构建ViConBERT框架学习越南语上下文嵌入。

Result: ViConBERT在WSD任务上F1得分0.87，在ViCon数据集上AP得分0.88，在ViSim-400数据集上Spearman相关系数0.60，表现优于基线模型。

Conclusion: ViConBERT框架有效提升了越南语语义理解能力，在词义消歧和上下文相似度任务上均取得良好性能。

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [323] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出使用小型LLM压缩大型LLM输入的新范式，开发了Cmprsr模型，在保持语义信息和控制压缩率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 降低使用黑盒大型语言模型的高成本，通过压缩输入来减少API调用费用。

Method: 使用Textgrad优化压缩元提示，对Qwen3-4B进行监督微调和GRPO训练，实现压缩率控制和下游任务性能最大化。

Result: Cmprsr在MeetingBank、LongBench和GSM8k数据集上优于提取式和普通抽象压缩方法，能精确控制压缩率。

Conclusion: Cmprsr模型在成本-质量权衡方面提供了精细控制，具有良好的泛化能力。

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [324] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 本文提出了一种利用现有抽象摘要自动生成对应抽取式摘要的轻量级方法，旨在为法律案例摘要研究社区创建包含两种摘要类型的增强数据集。


<details>
  <summary>Details</summary>
Motivation: 法律判决摘要对法律从业者构成沉重认知负担，而深度学习方法生成的抽象摘要容易误判法律术语或忽略关键细节，因此抽取式摘要器使用趋势上升。但人工标注抽取式摘要成本高昂。

Method: 设计轻量透明的流程，利用现有抽象摘要自动生成对应的抽取式摘要版本，确保专家意见从原始抽象摘要传递到转换后的抽取式摘要中。

Result: 增强了7个现有案例摘要数据集，包含对应抽取式摘要，并通过结构、词汇和语义维度的广泛比较评估确保增强摘要质量。

Conclusion: 承诺公开发布增强数据集，相信该资源将推动法律文档自动摘要领域的发展。

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [325] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: 研究比较了LLMs和人类在抢答式问答中的表现差异，发现LLMs在维基百科未覆盖的问题和需要数值答案的问题上表现较差


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在许多NLP任务中超越了人类表现，但尚不清楚对人类困难的问题是否对LLMs同样困难，因此研究在抢答设置下LLMs与人类在问答难度上的差异

Method: 收集包含问题、答案和人类正确率的日语问答数据，在多种设置下让LLMs回答这些问题，并从两个分析角度比较LLMs与人类的正确率

Result: 实验结果显示，与人类相比，LLMs在正确答案未被维基百科条目覆盖的问题上表现更差，且在需要数值答案的问题上也有困难

Conclusion: LLMs与人类在问题难度上存在差异，特别是在知识覆盖范围和数值推理方面表现出不同的能力特征

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [326] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型中的讽刺反弹现象——否定指令反而会增加被禁止概念的可及性。通过两个实验发现，否定后立即出现反弹，语义干扰会加剧反弹，而重复有助于抑制。极性分离程度与反弹持续性相关。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在否定指令下是否会出现与人类相似的讽刺反弹现象，即试图抑制某个概念反而会激活它。

Method: 进行两个实验：(1) 负载与内容实验：在否定指令后使用不同类型的干扰文本（语义、句法、重复）测量反弹强度；(2) 极性分离实验：测试模型是否能区分同一概念的中性和负面表述，以及这种区分是否预测反弹持续性。还进行了电路追踪分析。

Result: 否定后立即出现反弹，语义干扰会加剧反弹，而重复有助于抑制。更强的极性分离与更持久的反弹相关。电路分析发现中间层注意力头会放大被禁止的token，而早期层则进行抑制。

Conclusion: 该研究将认知科学中的讽刺反弹预测与长上下文干扰的机制性理解联系起来，并发布了ReboundBench数据集来支持未来研究。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [327] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 通过系统算法审计发现，Google搜索的AI概览和精选摘要功能在婴儿护理和孕产相关查询中存在信息不一致、医疗安全保障缺失等问题，需要加强AI系统的质量控制。


<details>
  <summary>Details</summary>
Motivation: 评估Google搜索中AI生成内容（AI概览和精选摘要）在健康信息领域的质量和一致性，因为这些功能用户依赖度高但缺乏控制权。

Method: 对1,508个真实婴儿护理和孕产相关查询进行系统算法审计，使用评估框架分析答案一致性、相关性、医疗安全保障、来源类别和情感一致性等多个质量维度。

Result: AI概览和精选摘要在同一搜索结果页面上存在33%的信息不一致；医疗安全保障严重缺失（AI概览仅11%，精选摘要仅7%）；健康网站是主要来源，但精选摘要也常链接商业来源。

Conclusion: AI介导的健康信息存在重大质量缺陷，需要更强的质量控制，特别是在直接影响用户福祉的高风险领域。

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [328] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: ILAKKANAM是首个泰米尔语专用语言评估基准，包含820个斯里兰卡学校考试问题，评估显示LLMs在低年级表现良好但随语言复杂度增加而下降，性能可能源于暴露而非真正理解。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准依赖英语翻译数据集，无法捕捉泰米尔语等低资源、形态丰富语言的语言和文化细微差别，需要专门评估LLMs在这些语言中的语言能力。

Method: 使用820个斯里兰卡学校泰米尔语考试问题构建ILAKKANAM基准，由训练有素的语言学家在五个语言类别和一个事实知识类别下标注，涵盖1-13年级，采用标准化评估框架评估闭源和开源LLMs。

Result: Gemini 2.5表现最佳，开源模型落后；所有模型在低年级问题表现良好但随语言复杂度增加明显下降；模型整体性能与识别语言类别能力无强相关性。

Conclusion: LLMs在泰米尔语中的表现可能更多源于训练数据暴露而非真正的语言理解，需要专门基准来评估低资源语言的真实语言能力。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [329] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 提出了MRMBench基准测试和推理时探测方法，用于评估奖励模型在多维度偏好上的表现，发现该方法与LLM对齐性能强相关，揭示了奖励模型在多维度偏好捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估方法通常在固定成对排名测试集上进行，但无法提供各偏好维度的性能信息，需要更细粒度的评估方法。

Method: 构建MRMBench基准测试（包含6个不同偏好维度的探测任务），并提出推理时探测方法分析奖励预测时使用的维度，增强可解释性。

Result: MRMBench与LLM对齐性能强相关；奖励模型在多维度偏好捕捉上存在困难；推理时探测方法能可靠评估奖励预测置信度。

Conclusion: MRMBench是开发先进奖励模型的可靠参考，多目标优化在奖励建模中具有潜力，推理时探测方法能提升LLM对齐效果。

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [330] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 提出了SerenQA框架来评估LLM在科学知识图谱问答中发现意外见解的能力，重点关注药物重定位任务中的相关性、新颖性和惊喜度。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA系统通常返回高度相关但可预测的答案，缺乏发现意外和新颖（"serendipitious"）答案的能力。

Method: 正式定义了serendipity-aware KGQA任务，提出了基于相关性、新颖性和惊喜度的严格serendipity指标，构建了来自临床知识图谱的专家标注基准，并设计了包含知识检索、子图推理和serendipity探索三个子任务的结构化评估流程。

Result: 实验表明，最先进的LLM在检索方面表现良好，但在识别真正令人惊喜和有价值的发现方面仍有困难。

Conclusion: LLM在发现意外见解方面仍有显著改进空间，SerenQA框架为未来研究提供了资源和基准。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [331] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1是一个轻量级的大型语言模型安全护栏系统，包含ContentFilter和JailbreakFilter两个专门模型，用于检测有害内容和筛选对抗性提示，基于2B参数的Granite模型支持12种语言。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM在人类-AI对话场景中的安全风险，包括有害内容检测和对抗性提示攻击防护，同时保持轻量级部署。

Method: 构建两个专门组件：ContentFilter基于MLCommons危害分类学识别安全风险，JailbreakFilter通过精心设计的课程学习覆盖60种主要攻击类型；使用约140万训练实例进行指令调优。

Result: 在公共和专有安全基准测试中达到最先进的安全性能，同时保持轻量级，减少部署开销，并提供多类安全预测和置信度分数以提高可解释性。

Conclusion: SGuard-v1提供了一个有效的轻量级安全解决方案，在Apache-2.0许可下发布，支持进一步研究和实际部署。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [332] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: QA-Noun是一个基于问答的框架，专门用于捕捉名词为中心的语义关系，通过9个问题模板覆盖名词的显式句法和隐式上下文角色，与QA-SRL结合提供更细粒度的句子语义分解。


<details>
  <summary>Details</summary>
Motivation: 现有的QA语义方法主要关注谓词-论元关系，但对名词为中心的语义关系处理不足，需要开发专门框架来补充这一空白。

Method: 定义9个问题模板覆盖名词的句法和上下文角色，创建超过2000个标注名词的数据集，训练模型并与QA-SRL集成。

Result: QA-Noun几乎完全覆盖了AMR的名词论元，同时捕捉了更多上下文隐含关系，与QA-SRL结合比FactScore和DecompScore等方法的粒度提高了130%以上。

Conclusion: QA-Noun完善了基于QA的语义框架，为跨文本对齐提供了全面且可扩展的细粒度语义分解方法。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [333] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: TAdaRAG是一个新颖的检索增强生成框架，通过动态构建任务自适应知识图谱来解决传统RAG中信息截断和无关细节的问题，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法存在两个主要问题：1）由于输入上下文窗口限制，检索到的外部知识被截断成小块，导致信息丢失，引发响应幻觉和推理链断裂；2）检索非结构化知识会引入无关细节，阻碍准确推理。

Method: 提出TAdaRAG框架，包含：1）意图驱动路由机制到领域特定提取模板；2）监督微调；3）基于强化学习的隐式提取机制，确保知识整合的简洁性、连贯性和非冗余性。

Result: 在6个公共基准测试和1个真实业务基准（NowNewsQA）上，使用3个骨干模型进行评估，TAdaRAG在多个领域和长文本任务中均优于现有方法。

Conclusion: TAdaRAG展示了强大的泛化能力和实际有效性，能够通过动态构建任务自适应知识图谱显著提升RAG性能。

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [334] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 提出了一个因果框架来分析和缓解RLHF奖励模型中的长度偏差问题，通过反事实数据增强方法训练奖励模型，使其能够独立于冗长度评估内容质量。


<details>
  <summary>Details</summary>
Motivation: RLHF训练的奖励模型存在长度偏差，系统性地倾向于偏爱更长的回答，将冗长度与质量混为一谈。

Method: 使用反事实数据增强方法，构建长度差异但内容相似的对以及内容差异但长度相似的对，用于训练奖励模型。

Result: 实证评估表明，该方法减少了奖励分配中的长度偏差，并导致策略模型产生更简洁、内容聚焦的输出。

Conclusion: 所提出的方法有效减少了长度偏差，提高了RLHF管道中奖励建模的鲁棒性和内容敏感性。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [335] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: MMWOZ是一个新的多模态对话数据集，扩展自MultiWOZ 2.3，包含GUI前端界面和操作指令，旨在弥合传统任务导向对话系统与现实应用之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统任务导向对话系统依赖定制后端API，而现实场景中广泛存在前端GUI界面且缺乏定制API，这造成了实际应用中的显著差距。

Method: 开发web风格GUI作为前端，设计自动化脚本将对话状态和系统动作转换为GUI操作指令，收集网页快照和对应操作指令，并提出了MATE多模态模型作为基线。

Result: 创建了MMWOZ数据集，提出了MATE模型作为基线，并进行了全面的实验分析来研究实用多模态任务导向对话代理的构建。

Conclusion: MMWOZ数据集和MATE模型为构建更实用的多模态任务导向对话系统提供了基础，有助于弥合传统系统与现实应用之间的差距。

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [336] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: GAPO是一种基于GRPO的扩展方法，通过计算群体层面的奖励来解决LLM的模式崩溃问题，提高生成响应的多样性而不损害准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现模式崩溃问题，即使存在多个有效答案，也会重复生成相同的少数完成结果，这限制了在各种任务中的多样性。

Method: GAPO是GRPO的简单扩展，计算群体整体奖励，能够从群体级属性（如多样性和覆盖度）中学习。使用频率感知奖励函数鼓励在有效LLM完成结果中均匀采样。

Result: GAPO训练的模型产生有效且更多样化的响应，在开放提示下也能泛化，并在标准LLM基准测试（GSM8K、MATH、HumanEval、MMLU-Pro）中提高响应多样性而不损害准确性。

Conclusion: GAPO通过群体层面的奖励计算有效解决了LLM的模式崩溃问题，显著提高了生成响应的多样性，同时保持了模型的准确性。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [337] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0是一个完全开源的通用多模态大模型，基于Qwen2.5-7B架构构建，通过动态容量MoE设计、渐进式训练策略和多模态数据匹配技术，在语言中心的多模态理解、推理和生成方面取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 推进Lychee Uni-MoE系列在语言中心多模态理解、推理和生成方面的能力，构建一个能够处理10种跨模态输入的通用多模态模型。

Method: 采用动态容量MoE框架（共享、路由和空专家），Omni-Modality 3D RoPE确保跨模态对齐，渐进式监督微调策略激活模态特定专家，结合平衡数据组合和迭代GSPO-DPO方法稳定强化学习训练。

Result: 在85个基准测试中达到SOTA或高度竞争性能，在76个基准中超过50个优于Qwen2.5-Omni（使用1.2T token训练），视频理解提升7%，通用多模态理解提升7%，视听推理提升4%，长语音处理WER降低4.2%，在低级图像处理和可控生成方面领先。

Conclusion: Uni-MoE 2.0展示了在计算效率和能力之间的良好平衡，通过创新的MoE架构和训练策略，在多个多模态任务上实现了卓越性能，为通用多模态模型的发展提供了重要贡献。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [338] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 提出了NOTAM语义解析任务，构建了Knots数据集，通过多智能体协作框架提升航空文本理解能力，实现了显著改进。


<details>
  <summary>Details</summary>
Motivation: NOTAMs作为关键飞行安全信息渠道，其复杂语言结构和隐式推理给自动解析带来挑战，现有研究缺乏深度语义理解。

Method: 提出NOTAM语义解析任务，构建包含12,347条专家标注NOTAMs的Knots数据集，采用多智能体协作框架进行领域发现，系统评估提示工程策略和模型适配技术。

Result: 在航空文本理解和处理方面取得显著改进，实验证明了所提方法的有效性。

Conclusion: 该方法为自动NOTAM分析系统提供了有价值的见解，代码已开源。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [339] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: Reason-KE++是一个SFT+RL框架，通过过程级忠实度对齐解决LLM在多跳推理任务中的事实幻觉问题，在MQUAKE-CF-3k上达到95.48%的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法存在"忠实度差距"，LLM强大的参数先验会覆盖上下文事实，导致关键事实幻觉，如从"NASA"错误推理出"休斯顿"。

Method: 提出Reason-KE++框架，包含阶段感知奖励机制，为中间推理步骤（如分解、子答案正确性）提供密集监督，避免仅基于结果的RL陷阱。

Result: 在MQUAKE-CF-3k上达到95.48%的准确率，比之前提升5.28%，而仅基于结果的RL方法会破坏推理完整性（跳跃准确率仅19.00%）。

Conclusion: 对于复杂任务，对齐推理过程对于构建可信赖的LLM至关重要，过程感知框架是实现忠实推理的关键。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [340] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 本文提出了一种用于波斯语到英语语音翻译的直接语音到语音翻译系统，通过自监督预训练、离散语音单元和合成平行数据来解决低资源语言的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 直接语音到语音翻译系统需要大量的平行语音数据，但对于波斯语等低资源语言来说，这类数据非常稀缺，因此需要开发有效的方法来缓解数据不足的问题。

Method: 系统包含三个组件：基于conformer的编码器、因果transformer解码器和基于单元的神经声码器。通过使用大语言模型翻译波斯语语音转录并合成英语语音，构建了新的波斯语-英语平行语音语料库。

Result: 在CVSS语料库的波斯语-英语部分，使用合成数据的模型比直接基线提高了4.6 ASR BLEU分数，可用平行语音数据量增加了约六倍。

Conclusion: 结合自监督预训练、离散语音单元和合成平行数据的方法对于改善波斯语-英语等低资源语言对的直接语音到语音翻译是有效的。

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [341] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: EvoSynth是一个自主框架，通过进化合成而非优化提示来生成全新的越狱方法，实现了85.5%的攻击成功率，显著超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队框架的越狱逻辑局限于选择、组合或优化已有攻击策略，无法自主发明全新的攻击机制，限制了其创造力。

Method: 采用多智能体系统自主设计、进化和执行基于代码的新型攻击算法，包含代码级自校正循环，能够在失败时迭代重写自身攻击逻辑。

Result: 在Claude-Sonnet-4.5等高度鲁棒模型上达到85.5%的攻击成功率，创下新记录，且生成的攻击比现有方法更加多样化。

Conclusion: EvoSynth通过将范式从攻击规划转向进化合成，为越狱方法的自主创新开辟了新方向，推动了该领域的发展。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [342] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: AFM是一种动态上下文管理器，通过语义相似度、半衰期权重和重要性分类为历史消息分配三种保真度级别，在严格token预算下显著减少推理成本同时保持安全性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在多轮对话中固定上下文窗口和简单内存策略的瓶颈问题，避免完整对话重放的高成本或静态摘要导致关键安全细节丢失。

Method: AFM根据当前查询的语义相似度、半衰期权重和重要性分类，将历史消息动态分配为FULL、COMPRESSED或PLACEHOLDER三种保真度级别，在token预算内按时间顺序打包消息。

Result: 在涉及花生过敏用户规划泰国旅行的安全基准测试中，AFM在短中长度对话中保持过敏信息，安全性能与完整重放相当，平均token使用量比基线减少66%。

Conclusion: AFM提供了一种模块化解决方案，可在不牺牲安全性或事实连续性的前提下显著降低推理成本，适用于OpenAI兼容API和离线操作。

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [343] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: LLMs在复杂推理任务中表现优异，但在简单的集合成员查询任务中却频繁失败，显示出其推理能力的脆弱性和不可预测性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在复杂任务中表现超人类，却在简单问题上失败的矛盾现象，以揭示其可靠性和可解释性的问题。

Method: 通过大规模系统实验，使用集合成员查询作为基础推理任务，考察提示措辞、语义结构、元素排序和模型选择等多个维度。

Result: LLMs在这一基础任务上的表现持续脆弱且不可预测，表明模型对集合概念的理解是碎片化和复杂的。

Conclusion: 简单问题的大规模实验方法能够全面映射和分析LLMs的失败模式，为LLM评估提供了有价值的方法论。

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [344] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 研究发现语言模型训练中存在相变重组现象，即使在小型模型中也能观察到，可在线性训练空间中直接检测，且发生在训练早期阶段。


<details>
  <summary>Details</summary>
Motivation: 探究相变现象是否仅限于大型语言模型，能否在线性训练空间中直接检测，以及是否在训练早期就出现。

Method: 训练小型GPT风格transformer模型，分析词汇使用演变，跟踪平均词长、正确/错误词数量、词汇多样性变化，应用泊松和亚泊松统计量化词汇连接和重组。

Result: 在训练过程中发现明显的相变点，这些转变在标准损失或验证曲线中不明显，但通过词汇和统计探针变得可见。

Conclusion: 相变重组是语言模型训练的普遍特征，可在小型模型中观察到，在线性训练空间中可检测，且发生在训练早期，为理解语言模型训练的非线性动力学提供新视角。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [345] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 提出通过中断机制来增强大语言模型的对齐性，通过在用户输入中定期插入控制语句来防止越狱行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐研究主要关注通过训练和提示来提高模型对抗攻击的鲁棒性，但缺乏随着用户输入长度增加而扩展的对齐方法。研究发现LLM越狱概率随用户输入或对话长度增加而上升。

Method: 提出中断机制，即在用户输入中每隔x个token插入控制语句，这种方法可以推广到思维链过程中以防止策略性行为。

Result: 论文提出了中断作为解决对齐性随输入长度扩展问题的可能方案。

Conclusion: 中断机制是增强大语言模型对齐性的一个有前景的方法，特别是在处理长输入时。

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [346] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: 评估大型语言模型在自动形式化任务中对语义相似但表达不同的自然语言输入的鲁棒性，发现即使是细微的语义变化也会显著影响模型输出质量。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在自动形式化方面表现出色，但最近研究表明它们对自然语言输入的改写很敏感。本文旨在验证这种敏感性是否存在于自动形式化领域。

Method: 使用MiniF2F和Lean 4版本的ProofNet基准，生成语义相似的自然语言改写语句，在两个现代LLMs上进行交叉评估，测量语义有效性和编译有效性。

Result: 结果显示不同改写输入之间存在性能差异，表明自然语言语句的微小变化会显著影响模型输出。

Conclusion: LLMs在自动形式化任务中对自然语言输入的细微变化敏感，这影响了其输出的可靠性和可验证性。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [347] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: BioMedJImpact是一个大规模生物医学期刊影响数据集，整合了文献计量指标、合作特征和LLM提取的AI参与度指标，揭示了合作强度和AI参与度共同影响科学影响力的趋势。


<details>
  <summary>Details</summary>
Motivation: 现有开放资源很少捕捉合作结构和AI研究如何共同塑造生物医学期刊声望，需要开发一个能分析科学影响和AI参与度的期刊级数据集。

Method: 从174万篇PubMed Central文章构建数据集，整合文献计量指标、合作特征，并通过可复现的三阶段LLM流程提取AI参与度特征。

Result: 发现两个一致趋势：合作强度更高的期刊（特别是拥有更大更多样化作者团队的）获得更高引用影响；AI参与度日益成为期刊声望的强相关因素，特别是在四分位排名中。

Conclusion: BioMedJImpact既是捕捉生物医学与AI交叉的全面数据集，也是经过验证的方法框架，支持可扩展、内容感知的科学计量分析。

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [348] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 通过激活工程方法，在LLaMA 3.1-8B模型中实现更人性化的情感表达，无需大量微调即可增强对话的情感特性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对话流畅性方面不断提升，但赋予其细腻、人性化的情感表达仍是一个重大挑战。现有的对齐技术通常只处理表层输出或需要大量微调。

Method: 使用归因修补识别因果影响组件，通过对比文本对（正面vs负面情感示例）生成的情感表达向量，在对话提示中应用这些向量。

Result: 引导后的响应显示出增强的积极情感（如喜悦、信任）和更频繁的第一人称代词使用，表明更高的个人参与度。

Conclusion: 研究提供了一个精确且可解释的框架，为对话AI研究开辟了新方向。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [349] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: 本文提出了一种新的稳定性度量方法，通过利用LDA的生成特性创建带有真实主题的语料库，评估LDA主题模型的一致性和准确性。研究发现LDA能够正确识别文档中的主题数量，且多次运行结果具有内部一致性，但这些主题并非真实主题。


<details>
  <summary>Details</summary>
Motivation: 概率主题模型由于其随机性，在重新运行时会产生不同的结果，导致潜在主题的不一致性。这种不稳定性影响了模型的可重复性、可靠性和解释性，引发了对主题模型是否真正捕捉到有意义主题还是仅仅捕获噪声的担忧。

Method: 定义了一种新的稳定性度量方法，结合准确性和一致性，利用LDA的生成特性生成带有真实主题的新语料库。将这些生成的语料库通过LDA运行50次，以确定输出的变异性。

Result: 研究表明LDA能够正确确定文档中的基础主题数量。同时发现LDA具有更高的内部一致性，多次重新运行返回相似的主题，但这些主题并非真实主题。

Conclusion: 虽然LDA在识别主题数量和内部一致性方面表现良好，但其生成的主题与真实主题存在差异，表明需要进一步改进主题模型的稳定性和准确性。

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [350] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: NeuroLex是一个专门针对脑电图报告的轻量级领域自适应语言模型，相比通用模型在EEG报告处理方面表现更好


<details>
  <summary>Details</summary>
Motivation: 通用语言模型无法捕捉临床脑电图报告中的领域特定语言惯例，需要专门的领域自适应模型

Method: 使用哈佛脑电图数据库的纯文本报告进行训练，采用span-corruption预训练和指令式微调，专注于报告润色、段落总结和术语问答

Result: 相比同规模通用模型，NeuroLex实现了更低的困惑度、更高的提取和总结准确率、更好的标签效率，以及对否定和事实幻觉更强的鲁棒性

Conclusion: NeuroLex通过EEG感知的语言骨干网络，连接了生物医学文本建模和脑机接口应用，为可解释和语言驱动的神经解码提供了基础

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [351] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文系统综述了多模态思维链(MCoT)技术，分析了其背景动机、主流方法、评估基准、应用场景，并讨论了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在感知任务中的成功，提升其复杂推理能力成为关键研究方向。现有模型存在推理路径不透明和泛化能力不足等问题，而思维链推理在语言模型中已证明能增强推理透明度和输出可解释性，有望在扩展至多模态领域后提升模型推理能力。

Method: 从三个方面介绍主流MCoT方法：思维链范式、后训练阶段和推理阶段，并分析其底层机制。

Result: 总结了现有的评估基准和指标，讨论了MCoT的应用场景。

Conclusion: 分析了MCoT当前面临的挑战，并对其未来研究方向进行了展望。

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [352] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 本文比较了三种transformer架构(BERT、GPT-2、DeBERTa)在文本希望表达分类任务上的性能，发现BERT在准确率和计算效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 开发计算框架来分析文本中的希望表达，应用于心理健康和社交媒体分析领域。

Method: 使用BERT、GPT-2和DeBERTa三种transformer架构进行二元分类(希望vs非希望)和多类别分类(五个希望相关类别)。

Result: BERT表现最佳(二元分类84.49%，多类别72.03%)且计算效率最高(443秒训练时间)，GPT-2准确率最低但擅长检测讽刺表达(92.46%召回率)，DeBERTa表现中等但计算成本最高(947秒)。

Conclusion: 对于专业化的情感检测任务，架构的适用性可能比模型规模更重要，BERT在希望表达分类任务中提供了最佳的性能与效率平衡。

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [353] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 本文提出了Visual Room 2.0基准，用于评估多模态大语言模型(MLLMs)的感知-认知对齐，发现MLLMs虽然能准确描述视觉细节但缺乏对情感和意图的理解能力。


<details>
  <summary>Details</summary>
Motivation: 基于Searle的中文房间思想扩展到多模态领域，探讨MLLMs是否真正理解所见内容，即"看到不等于理解"的问题。

Method: 构建包含3个层次(低、中、高)和17个代表性任务的分层基准，涵盖从属性识别到场景理解的感知组件，以及从文本蕴含到因果和社会推理的认知组件，共350个多模态样本和2100个渐进问题。

Result: 评估10个SOTA MLLMs发现：(1)感知能力优于认知能力(8.0%↑)；(2)认知不因果依赖于基于感知的推理；(3)认知随模型规模扩展，但感知能力不随模型变大而一致提升。

Conclusion: 将"看到≠理解"操作化为可测试假设，为MLLMs从感知处理到认知推理提供了新范式。

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [354] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: HCNR方法通过识别和恢复关键表达控制神经元来修复SFT后LLM的诚实性，相比基线方法实现33.25%的诚实性恢复和至少2.23倍加速


<details>
  <summary>Details</summary>
Motivation: 监督微调严重损害了LLM的诚实性，这对高风险领域的安全部署至关重要。现有恢复方法假设SFT深度破坏了模型识别知识边界的能力，但研究发现模型仍保留此能力，受损的是忠实表达这种意识的能力

Method: 提出Honesty-Critical Neurons Restoration (HCNR)方法：1）识别并恢复关键表达控制神经元到预训练状态；2）通过Hessian引导的补偿机制协调任务导向神经元

Result: 在4个QA任务和5个LLM家族上的实验表明，HCNR有效恢复了33.25%的受损诚实性，相比基线方法实现至少2.23倍加速和超过10倍的数据减少

Conclusion: HCNR为可信赖LLM部署提供了实用解决方案，通过外科手术式修复被压制的表达能力，而非全局参数调整

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [355] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: AA-Omniscience是一个评估语言模型事实回忆和知识校准的基准，包含6000个问题，涵盖42个经济相关主题。结果显示前沿模型在事实性和校准方面存在持续弱点，Claude 4.1 Opus得分最高(4.8)。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要衡量通用能力，但在各领域可靠使用需要事实准确性和识别知识差距的能力。

Method: 从权威学术和行业来源提取问题，涵盖6个领域的42个经济相关主题，使用全知指数(-100到100)评估事实回忆，同时惩罚幻觉和奖励不确定时的弃权。

Result: Claude 4.1 Opus得分最高(4.8)，只有三个模型得分超过零。不同研究实验室的模型在六个领域表现各异。

Conclusion: 模型在事实性和校准方面存在持续弱点，应根据具体用例需求而非通用性能来选择模型。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [356] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本文探讨了多语言嵌入模型与对齐单语言模型在双语词典归纳任务中的表现差异，分析了BLI作为对齐度评估指标的局限性，并提出基于词干的新BLI方法和词汇剪枝技术来改进评估效果。


<details>
  <summary>Details</summary>
Motivation: 多语言嵌入已成为主流选择，但需要验证其是否在所有方面都优于对齐的单语言模型，以及高昂计算成本是否总是合理。本文旨在探索BLI作为嵌入空间对齐度评估指标的优缺点。

Method: 使用双语词典归纳任务评估不同嵌入对齐技术，包括传统对齐方法、新型多语言模型和组合对齐技术，在高资源和低资源语言环境下进行测试，并考虑语言家族的影响。

Result: 发现BLI在某些情况下不能准确衡量对齐度，组合嵌入对齐技术通常表现更好，但在低资源语言情况下多语言嵌入表现更优。

Conclusion: BLI作为对齐度评估指标存在局限性，提出的基于词干的BLI方法和词汇剪枝技术能更准确地评估嵌入空间对齐程度，多语言嵌入和对齐单语言模型各有优势场景。

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [357] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: Spark-Prover-X1是一个7B参数的定理证明模型，通过三阶段训练框架提升轻量级LLM的形式推理能力，在多个基准测试中达到同类开源模型的最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在自动定理证明中因缺乏多样化和高质量形式语言数据而受限的问题，探索如何提升中等规模LLM的推理潜力。

Method: 采用三阶段训练框架：1）在广泛数学语料上进行持续预训练，引入"思维链增强状态预测"任务；2）在专家迭代循环中进行监督微调；3）使用组相对策略优化针对最具挑战性问题进行优化。

Result: Spark-Prover-X1-7B在类似规模开源模型中达到最先进性能，平均通过率37.0%（pass@32），在PutnamBench上解决27个问题，在CombiBench上达到24.0%。

Conclusion: 多样化的训练数据和渐进精炼的训练流程为增强轻量级LLM的形式推理能力提供了有效路径。

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [358] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: BeDiscovER是一个评估现代LLM语篇理解能力的综合基准套件，包含5个语篇任务和52个数据集，覆盖词汇、句子和文档层面。评估显示前沿模型在时间推理方面表现良好，但在完整文档推理和某些语义语篇现象上仍有困难。


<details>
  <summary>Details</summary>
Motivation: 随着推理语言模型的发展，需要建立一个全面评估语篇层面知识的基准，涵盖传统任务和新挑战，以系统评估现代LLM的语篇理解能力。

Method: 整合5个公开可用的语篇任务（包括语篇词汇、多句和文档层面），共52个数据集，评估开源LLM（Qwen3系列、DeepSeek-R1）和前沿模型（GPT-5-mini）在BeDiscovER上的表现。

Result: 最先进的模型在时间推理的算术方面表现强劲，但在完整文档推理和某些细微语义语篇现象（如修辞关系识别）上表现不佳。

Conclusion: BeDiscovER为评估LLM语篇理解能力提供了全面基准，揭示了当前模型在文档级推理和复杂语篇现象处理方面的局限性。

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [359] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: 本研究评估了当代大语言模型在零样本设置下识别随机对照试验对CONSORT 2010声明依从性的准确性，发现模型整体表现一般，能较好识别合规项目但难以检测不合规和不适用的项目。


<details>
  <summary>Details</summary>
Motivation: 手动验证CONSORT依从性是一个耗时费力的过程，构成了同行评审和证据合成的重要瓶颈。本研究旨在系统评估当代LLMs在零样本设置下识别已发表RCTs对CONSORT 2010声明依从性的准确性和可靠性。

Method: 构建了包含150篇已发表RCTs的金标准数据集，涵盖不同医学专业。主要结果是三类分类任务的宏平均F1分数，辅以项目级性能指标和定性错误分析。

Result: 整体模型表现一般。表现最佳的模型Gemini-2.5-Flash和DeepSeek-R1分别获得0.634和0.282的宏F1分数和Cohen's Kappa系数，仅与专家共识达成一般一致性。模型在识别合规项目时表现良好（F1分数>0.850），但在识别不合规和不适用项目时表现较差（F1分数很少超过0.400）。

Conclusion: LLMs作为CONSORT检查的初步筛选助手具有潜力，能够有效识别报告良好的项目。然而，它们目前无法可靠地检测报告遗漏或方法学缺陷，因此不适合在试验质量的关键评估中替代人类专业知识。

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [360] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: 提出了Agent-Event-Coder (AEC)框架，将事件抽取视为代码生成过程，通过多智能体协作解决零样本事件抽取的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在零样本事件抽取中存在的输出不完整、结构无效、模式违反等问题，需要复杂推理和领域特定理解。

Method: 采用多智能体框架，将事件抽取分解为检索、规划、编码和验证四个专门子任务，每个任务由专用LLM智能体处理，事件模式表示为可执行类定义。

Result: 在五个不同领域和六个LLM上的实验表明，AEC始终优于先前的零样本基线方法。

Conclusion: 将事件抽取视为代码生成的方法能够产生精确、完整且模式一致的抽取结果，展示了编程启发式方法的有效性。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [361] [A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.13126)
*Nigar Alishzade,Gulchin Abdullayeva*

Main category: cs.CL

TL;DR: 比较循环神经网络和注意力机制在孤立手语识别中的表现，发现基于注意力的Vanilla Transformer在准确率上优于ConvLSTM，但ConvLSTM计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 系统比较循环神经网络和注意力机制在手语识别中的性能差异，为架构选择提供指导。

Method: 在Azerbaijani Sign Language Dataset和Word-Level American Sign Language数据集上实现并评估ConvLSTM和Vanilla Transformer模型。

Result: Vanilla Transformer在两个数据集上都优于ConvLSTM，在AzSLD上Top-1准确率达76.8%，在WLASL上达88.3%。ConvLSTM计算效率更高但准确率较低。

Conclusion: Transformer在准确率和手语者独立性方面表现更好，ConvLSTM在计算效率和时序建模方面有优势，应根据应用需求和资源约束选择架构。

Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.

</details>


### [362] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: 提出了一个零样本语法能力评估框架，利用未标记数据和大型语言模型生成伪标签，通过噪声标签处理训练基于transformer的模型，实现高精度的语法能力评分。


<details>
  <summary>Details</summary>
Motivation: 口语语法评估面临自发、非结构化和不流畅的挑战，且需要大量专家标注，大规模数据创建不切实际。

Method: 使用基于语法能力量表的提示词让LLM在未标记数据上生成预测作为伪标签，通过专门设计的训练框架训练transformer模型以有效处理标签噪声。

Result: 实验结果表明该方法能以高精度估计语法能力分数，LLM选择和干净-噪声样本比例对模型性能和稳定性有重要影响。

Conclusion: 该方法为可扩展、低资源的语法评估系统铺平了道路，具有鲁棒性和可解释性。

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [363] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: 提出了首个孟加拉语ASR转录本中重复性不流畅和形态学重复的区分数据集，通过LLM和微调方法建立了基准，BanglaBERT模型达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语ASR转录本中词重复的歧义问题：区分无意的重复不流畅和故意的形态学重复，避免标准不流畅校正方法误删有效语言信息。

Method: 创建了20,000条手动标注的孟加拉语语料库，使用两种范式：多语言LLM的少样本提示和任务特定的编码器模型微调。

Result: LLM在少样本提示下达到82.68%准确率，但微调方法更优，BanglaBERT模型达到84.78%准确率和0.677 F1分数。

Conclusion: 为孟加拉语建立了强大的语言学基准，为开发语义保留的文本规范化系统提供了重要数据。

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [364] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: TCM-5CEval是一个针对中医领域的更细粒度综合基准，评估LLMs在五个关键维度：核心知识、经典文献、临床决策、中药学和临床非药物治疗。研究发现LLMs在基础知识回忆方面表现良好，但在经典文本解释和推理稳定性方面存在显著弱点。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在通用领域表现出色，但在中医等高度专业化和文化丰富的领域需要更严谨和细致的评估。基于前期TCM-3CEval工作，发现系统知识差距和文化语境对齐的重要性。

Method: 引入TCM-5CEval基准，在五个维度评估15个主流LLMs：核心知识、经典文献、临床决策、中药学和临床非药物治疗。使用基于排列的一致性测试来评估推理稳定性。

Result: 发现显著的性能差异，deepseek_r1和gemini_2_5_pro表现最佳。模型在基础知识回忆方面熟练，但在经典文本解释方面困难。排列测试显示所有模型都存在推理脆弱性，对选项顺序敏感。

Conclusion: TCM-5CEval不仅为中医领域的LLM能力提供了更详细的诊断工具，还暴露了其推理稳定性的根本弱点。基准已上传至Medbench平台以促进进一步研究。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [365] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: 本文提出了一种量化方法来估计翻译熵，通过分析翻译器在保持翻译不变的情况下替换特定标记的概率，从而评估翻译器的性能。


<details>
  <summary>Details</summary>
Motivation: 在信息时代，机器翻译的需求日益增长，但目前缺乏客观的量化方法来评估翻译器的性能，主要是因为单种语言的熵仍然未知。

Method: 通过给定翻译器，构建多个仅在一个选定标记上不同的句子，分析这些句子产生相同翻译的统计规律，计算替换特定标记而保持翻译不变的概率，从而得到翻译熵。

Result: 该方法能够量化排名多个公开可用的翻译器，揭示互译熵是否对称，并发现替换两个标记时翻译退化度与两个标记退化度的乘积成正比。

Conclusion: 翻译熵是一个可测量的属性，为人工翻译器提供了客观的基准测试方法，研究基于MarianMT、T5-Base和NLLB-200翻译器进行验证。

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [366] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: 评估多种大语言模型在罗马尼亚语变音符号恢复任务中的表现，发现GPT-4o等模型表现优异，而Llama系列模型表现波动较大。


<details>
  <summary>Details</summary>
Motivation: 自动变音符号恢复对于像罗马尼亚语这样具有丰富变音符号的语言的文本处理至关重要。

Method: 使用包含OpenAI GPT系列、Google Gemini、Meta Llama系列、MistralAI Mixtral等多个大语言模型，在从零样本到复杂多样本指令的多种提示模板下进行测试。

Result: GPT-4o等模型达到高准确率，始终超过中性回显基线，而Meta的Llama系列模型表现出更大的变异性。

Conclusion: 模型架构、训练数据和提示设计对变音符号恢复性能有重要影响，为改进变音符号丰富语言的NLP工具指明了有前景的方向。

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [367] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文评估了视觉语言模型在语音识别任务中的表现，发现即使经过微调，模型也难以正确解读语音频谱图和波形图，表明需要特定的参数知识而不仅仅是配对样本。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和视觉语言模型的发展，研究者希望了解这些模型在融合视觉和语言模态任务中的能力，特别是能否像专业语音学家一样解读语音的频谱图和波形图。

Method: 创建了一个包含4000多个英语单词的新数据集，包含孤立的语音样本及其对应的频谱图和波形图。通过多项选择任务测试模型，要求模型从4个音位或字位转录中选择正确的答案，其中3个干扰项基于与真实值的音位编辑距离选择。

Result: 无论是零样本还是微调后的模型，其表现都很少超过随机猜测水平，表明模型缺乏解读此类图形的特定参数知识。

Conclusion: 视觉语言模型需要特定的参数知识来正确解读语音的频谱图和波形图，仅仅依靠配对样本是不够的。

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [368] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: 提出SoCE方法，通过识别不同类别的专家模型并使用非均匀加权平均来优化模型融合，在多领域提升性能


<details>
  <summary>Details</summary>
Motivation: 传统模型融合方法使用均匀权重平均，忽略了不同基准类别间模型性能的低相关性，无法充分利用各模型的专长

Method: 基于基准类别识别弱相关的类别簇，为每个簇选择专家模型，使用优化的非均匀加权平均而非均匀权重进行模型融合

Result: 在多种领域（多语言能力、工具调用、数学）中提升性能和鲁棒性，在伯克利函数调用排行榜上达到最先进结果

Conclusion: SoCE方法通过利用基准类别间的低相关性进行专家模型选择和优化加权，为模型融合提供了更有效的策略

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [369] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: RegionMarker是一种区域触发语义水印框架，通过在低维空间中定义触发区域并向相关文本嵌入注入水印，为EaaS提供全面的版权保护。


<details>
  <summary>Details</summary>
Motivation: 现有的EaaS水印方法只能抵抗部分攻击，无法提供全面保护，存在版权泄露风险。

Method: 使用秘密降维矩阵投影到子空间，随机选择触发区域，在整个触发区域嵌入水印，并将文本嵌入本身作为水印。

Result: 在多个数据集上的实验表明，RegionMarker能有效抵抗不同攻击方法。

Conclusion: RegionMarker框架能够全面保护EaaS的版权，抵抗各种水印移除攻击。

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [370] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 阿拉伯酒店业需要阿拉伯方言情感分析工具，为此创建了一个包含沙特和摩洛哥方言的多方言酒店评论数据集，用于情感检测共享任务。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯世界酒店业越来越依赖客户反馈来改进服务，这推动了对先进阿拉伯语情感分析工具的需求。

Method: 使用从现代标准阿拉伯语翻译成沙特和摩洛哥方言的多方言人工策划数据集，包含538条情感平衡的酒店评论，翻译由母语者验证以确保方言准确性和情感保留。

Result: 超过40个团队注册参与共享任务，12个团队提交系统，最佳系统F1得分达到0.81。

Conclusion: 该资源支持开发面向客户体验分析的方言感知NLP系统，证明了跨阿拉伯方言情感分析的可行性和持续挑战。

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [371] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 本文通过PEFT/LoRA方法研究LLMs在任务和语言间的迁移效应，发现任务内跨语言迁移稳定正向，而跨任务迁移常导致性能下降，揭示了任务和语言间的稳定捐赠-接收结构。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在一个任务或语言上的改进如何影响其他任务和语言及其组合，目前仍缺乏系统研究。

Method: 采用PEFT/LoRA方法在多个开源LLM家族和尺寸上进行控制研究，将任务和语言视为迁移轴，在每个模型上对单一任务-语言源进行微调，并测量在所有其他任务-语言目标对上的迁移效果。

Result: 发现两个一致模式：1) 任务内跨语言迁移可靠正向，而跨任务迁移常导致附带性能下降；2) 跨语言和任务存在稳定的捐赠-接收结构（枢纽捐赠者vs脆弱接收者）。

Conclusion: 研究结果为风险感知微调和模型专业化提供了重要启示，强调了任务内跨语言迁移的稳定性与跨任务迁移的风险。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [372] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: PEDIASBench评估框架显示，当前大型语言模型在儿科医疗中表现出色基础知识能力，但在复杂推理、动态诊疗决策和人文关怀方面存在局限，尚不能独立承担儿科医生职责，但有潜力作为决策支持和教育工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医学领域的快速发展，需要评估它们是否能在真实临床环境中胜任儿科医生角色，了解当前模型的优势和局限。

Method: 开发PEDIASBench系统评估框架，从基础知识应用、动态诊疗能力、医疗安全和伦理三个维度，评估12个代表性模型在19个儿科亚专业211种典型疾病上的表现。

Result: 先进模型在基础知识上表现良好（Qwen3-235B-A22B在执照级问题上准确率超90%），但任务复杂度增加时性能下降约15%；动态诊疗场景中DeepSeek-R1得分最高（平均0.58），但多数模型难以适应实时患者变化；医疗伦理安全任务中Qwen2.5-72B最佳（准确率92.05%），但人文敏感性有限。

Conclusion: 当前儿科大语言模型受限于动态决策能力不足和人文关怀发展不成熟，未来应关注多模态整合和临床反馈-模型迭代循环，增强安全性、可解释性和人机协作。虽然不能独立承担儿科诊疗，但在决策支持、医学教育和患者沟通方面具有潜力。

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [373] [Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction](https://arxiv.org/abs/2511.13410)
*Zhaopei Huang,Qifeng Dai,Guozheng Wu,Xiaopeng Wu,Kehan Chen,Chuan Yu,Xubin Li,Tiezheng Ge,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 提出了PAL-Bench基准测试和H²Memory记忆框架，用于评估和改进面向服务的个性化对话助手在长期交互中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着智能个人设备的普及，面向服务的人机交互日益普遍，需要能够理解用户特定特征并定制响应的个性化对话助手。现有方法往往忽视长期交互的复杂性，未能捕捉用户的主观特征。

Method: 1) 开发了多步骤基于LLM的合成流水线，创建了PAL-Set中文数据集；2) 提出了H²Memory层次异构记忆框架，结合检索增强生成来改进个性化响应生成。

Result: 在PAL-Bench和外部数据集上的综合实验证明了所提出记忆框架的有效性。

Conclusion: PAL-Bench为评估服务导向助手的个性化能力提供了新基准，H²Memory框架显著提升了长期用户-代理交互中的个性化服务表现。

Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

</details>


### [374] [Non-Linear Scoring Model for Translation Quality Evaluation](https://arxiv.org/abs/2511.13467)
*Serge Gladkoff,Lifeng Han,Katerina Gasova*

Main category: cs.CL

TL;DR: 提出了一种基于对数函数的非线性翻译质量评估模型，解决了传统线性评分对不同长度文本样本的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于MQM的线性误差-惩罚评分在不同长度样本上存在偏差，短文本被过度惩罚，长文本被低估惩罚，与专家直觉不符。

Method: 构建两参数对数模型E(x) = a * ln(1 + b * x)，基于心理物理学和认知负荷理论，通过参考容忍度校准参数。

Result: 实证数据显示可接受错误数量随样本大小呈对数增长而非线性增长，模型提高了评分可解释性、公平性和评分者间信度。

Conclusion: 该非线性评分模型为翻译质量评估提供了更准确、可扩展的评估范式，为AI驱动的文档级评估奠定了更好基础。

Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.

</details>


### [375] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: 使用基于方面的情感分析(ABSA)解码泰语财务年报中的模糊情感，开发标注指南并标注100多份报告，通过事件研究验证对股价的实际影响。


<details>
  <summary>Details</summary>
Motivation: 财务文件常使用模糊语言来呈现积极或中性前景，即使实际情况可能不利，理解这些情感对洞察市场行为至关重要。

Method: 开发模糊情感标注指南，标注100多份泰语财务年报，使用多种文本分类模型进行基准测试，并进行事件研究评估对股价的影响。

Result: 在情感分类任务中表现出色，市场反应受到报告中特定方面的选择性影响。

Conclusion: 财务文本的情感分析具有复杂性，解决模糊语言问题对于准确评估市场情绪至关重要。

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [376] [Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505)
*Elinor Poole-Dayan,Daniel T Kessler,Hannah Chiou,Margaret Hughes,Emily S Lin,Marshall Ganz,Deb Roy*

Main category: cs.CL

TL;DR: 提出基于大语言模型的公共叙事自动标注框架，在8个叙事和14个代码上达到平均F1分数0.80，接近专家水平，并扩展到22个故事和政客演讲分析。


<details>
  <summary>Details</summary>
Motivation: 公共叙事是领导力发展和公民动员的重要工具，但由于主观解释和专家标注成本高，系统分析面临挑战。

Method: 开发与领域专家共同制定的代码本，利用大语言模型自动进行公共叙事的定性标注，并与专家标注进行性能比较。

Result: LLM在8个叙事和14个代码上实现平均F1分数0.80，接近人类专家水平；成功将分析扩展到22个故事和政客演讲数据集。

Conclusion: 展示了LLM辅助标注在可扩展叙事分析中的潜力，为计算公民叙事研究提供了新视角，同时指出了关键局限性和未来研究方向。

Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

</details>


### [377] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: 为解决匈牙利语语音识别数据不足问题，研究者构建了两个新数据集BEA-Large和BEA-Dialogue，分别包含255小时自发语音和85小时对话语料，并建立了可复现的基线模型。


<details>
  <summary>Details</summary>
Motivation: 高资源语言的自动语音识别(ASR)发展迅速，但匈牙利语等语言因缺乏自发和对话语料库而代表性不足，需要填补这一空白。

Method: 从匈牙利语语音语料库BEA中构建两个新数据集：BEA-Large(255小时自发语音，433名说话人)和BEA-Dialogue(85小时自发对话)。使用公开可用的ASR模型建立可复现基线，包括微调Fast Conformer模型。

Result: 微调后的Fast Conformer模型在自发语音上词错误率为14.18%，在重复语音上为4.8%。说话人日志实验的错误率在13.05%到18.26%之间。

Conclusion: 对话ASR仍然具有挑战性，特别是由于不流利、重叠和非正式语音模式。通过发布这些数据集和基线，旨在推动匈牙利语语音技术发展，并为其他语言开发自发和对话基准提供方法框架。

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [378] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 提出了基于核心意图、语句类型、语法结构和关键动作的新文本到SQL分类法，并基于此构建了SQL-Synth数据集，该数据集比现有基准具有更好的多样性和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL数据集覆盖范围有限，无法捕捉真实应用的多样性，需要更全面的分类和数据集来训练和评估模型。

Method: 提出新的文本到SQL分类法，结合大语言模型构建了taxonomy-guided数据集合成管道，生成了SQL-Synth数据集。

Result: SQL-Synth在多样性和覆盖范围上优于现有基准，现有LLM在SQL-Synth上表现有限，但微调能显著提升性能。

Conclusion: 该分类法具有重要潜力，不仅能全面分析数据集和LLM性能，还能指导LLM训练数据的构建。

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [379] [Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents](https://arxiv.org/abs/2511.13593)
*Piaohong Wang,Motong Tian,Jiaxian Li,Yuan Liang,Yuqing Wang,Qianben Chen,Tiannan Wang,Zhicong Lu,Jiawei Ma,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: O-Mem是一个基于主动用户画像的新型记忆框架，通过动态提取和更新用户特征与事件记录，支持分层检索，在个性化响应方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在复杂环境中维持长期交互时面临上下文一致性和动态个性化挑战，传统基于语义分组的记忆系统会忽略语义无关但关键的用户信息并引入检索噪声。

Method: 提出O-Mem框架，基于主动用户画像动态提取和更新用户特征与事件记录，支持人物属性和主题相关上下文的分层检索。

Result: 在LoCoMo基准测试上达到51.76%，比之前的SOTA LangMem提升近3%；在PERSONAMEM上达到62.99%，比A-Mem提升3.5%；同时在token和交互响应时间效率上也有提升。

Conclusion: 该工作为开发高效且类人的个性化AI助手开辟了有前景的方向。

Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

</details>


### [380] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: 使用大型语言模型将机器学习检测虚假评论的词汇线索转化为人类可理解的语言现象，帮助人们在没有检测分类器的情况下评估评论可信度


<details>
  <summary>Details</summary>
Motivation: 虚假评论误导消费者、损害企业利益并破坏在线市场信任。虽然机器学习分类器能有效检测虚假评论，但其学习到的区分特征往往难以被人类理解

Method: 利用大型语言模型将机器学习分类器学到的词汇线索翻译成人类可理解的语言现象，并与先验知识和上下文学习获得的现象进行比较

Result: 通过该方法获得的语言现象具有数据实证基础，在相似领域具有通用性，且比LLM先验知识或上下文学习获得的现象更具预测性

Conclusion: 这些语言现象有助于人们在缺乏虚假评论检测分类器的环境中批判性评估在线评论的可信度

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


### [381] [Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation](https://arxiv.org/abs/2511.13689)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,Joseph K J*

Main category: cs.CL

TL;DR: 提出了TAI框架，结合LLM和潜在扩散模型，通过翻译和图像生成提升印度诗歌的可访问性，支持联合国可持续发展目标。


<details>
  <summary>Details</summary>
Motivation: 印度诗歌具有丰富的文化遗产，但其复杂的语言结构、文化典故和语法构造对非母语读者构成理解障碍，现有研究对印度语言诗歌关注不足。

Method: 使用TAI框架，包括：(1) 基于几率比偏好对齐算法的翻译模块，准确翻译形态丰富的诗歌；(2) 基于语义图的图像生成模块，捕捉隐喻和语义关系，创建视觉化诗歌表示。

Result: 综合实验评估显示TAI Diffusion在诗歌图像生成任务中优于强基线方法，并发布了包含1,570首21种低资源印度语言诗歌的MorphoVerse数据集。

Conclusion: 该工作通过解决诗歌翻译和视觉理解的空白，旨在扩大印度诗歌的可访问性，丰富读者体验。

Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

</details>


### [382] [Generalist Foundation Models Are Not Clinical Enough for Hospital Operations](https://arxiv.org/abs/2511.13703)
*Lavender Y. Jiang,Angelica Chen,Xu Han,Xujin Chris Liu,Radhika Dua,Kevin Eaton,Frederick Wolff,Robert Steele,Jeff Zhang,Anton Alyakin,Qingkai Pan,Yanbing Chen,Karl L. Sangwon,Daniel A. Alber,Jaden Stryker,Jin Vivian Lee,Yindalon Aphinyanaphongs,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: Lang1是一个专门针对医疗运营决策的模型家族，通过在EHR数据和互联网文本上预训练，在医疗预测任务中显著优于通用模型，即使后者规模大70倍。


<details>
  <summary>Details</summary>
Motivation: 通用基础模型在医疗知识方面表现良好，但缺乏医疗运营决策所需的专业知识，如患者流量、成本和护理质量等方面的预测能力。

Method: 开发Lang1模型家族（100M-7B参数），使用80B临床令牌和627B互联网令牌混合预训练，并通过ReMedE基准评估，包括30天再入院预测、死亡率预测、住院时长、共病编码和保险拒赔预测五个关键任务。

Result: 在零样本设置下，通用和专业模型在五个任务中的四个表现不佳（36.6%-71.7% AUROC），死亡率预测除外。微调后，Lang1-1B在AUROC上比微调通用模型提高3.64%-6.75%，比零样本模型提高1.66%-23.66%，且能有效迁移到其他临床任务和外部医疗系统。

Conclusion: 医院运营的预测能力需要明确的监督微调，而这个过程通过EHR领域内预训练变得更高效。专业LLM可以在专业任务中与通用模型竞争，有效的医疗系统AI需要领域内预训练、监督微调和超越代理基准的真实世界评估的结合。

Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.

</details>


### [383] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
*Chengyu Huang,Zhengxin Zhang,Claire Cardie*

Main category: cs.CL

TL;DR: HAPO是一种历史感知策略优化方法，通过记录每个问题的历史状态来激励模型发现比之前更简洁的正确解决方案，在保持准确性的同时显著减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测试时扩展响应长度虽然能提升LLM的推理能力，但会导致冗长输出和增加推理成本。传统方法没有利用训练过程中相同问题的历史信息，限制了解决方案随时间推移变得更加简洁的能力。

Method: HAPO跟踪每个问题的历史状态（如之前生成正确响应的最小长度），使用基于历史状态的新长度奖励函数，激励发现比之前更简洁的正确解决方案。该方法结合长度奖励和正确性奖励，联合优化正确性和效率。

Result: 在多个数学基准测试中，HAPO有效诱导LLM的简洁推理能力，产生33-59%的长度减少，准确率仅下降2-5%。

Conclusion: HAPO通过利用历史信息成功实现了在保持高准确性的同时显著减少输出长度，为LLM的高效推理提供了有效解决方案。

Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.

</details>


### [384] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
*Chengyu Huang,Tanya Goyal*

Main category: cs.CL

TL;DR: 本文提出了DCRM指标来衡量偏好优化中响应对的质量，并基于此开发了best-of-N²配对方法，在多个基准测试中提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究试图将偏好优化性能与底层偏好数据集关联，但作者观察到偏好的响应与不偏好的响应之间的差异可能不符合期望学习的内容。

Method: 使用距离和奖励边际来量化响应对差异，结合得到DCRM指标，并提出best-of-N²配对方法选择DCRM最高的响应对。

Result: 实验表明，该方法在AlpacaEval、MT-Bench和Arena-Hard等多个基准测试中都能进一步提升模型性能。

Conclusion: DCRM指标与学习结果存在普遍相关性，基于DCRM的配对方法能有效提升偏好优化性能。

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [385] [Softmax as a Lagrangian-Legendrian Seam](https://arxiv.org/abs/2511.11573)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 该论文将机器学习中的softmax函数建模为微分几何中的几何界面，揭示了logits到概率转换的几何结构，包括Legendre变换、接触结构和辛结构等几何概念。


<details>
  <summary>Details</summary>
Motivation: 建立机器学习与微分几何之间的桥梁，揭示softmax函数背后的几何结构，为理解机器学习中的概率转换提供新的数学视角。

Method: 将softmax建模为几何界面：使用负熵和log-sum-exp生成的两种保守描述在概率单纯形上的Legendre变换界面相遇，形成接触结构和辛结构。

Result: 证明了偏置平移不变性对应于屏幕上的Reeb流，Fenchel-Young等式/KL散度提供了到界面的可计算距离，并具体分析了二类和三类情况。

Conclusion: 该工作为机器学习开辟了新的研究方向：紧凑logit模型（射影或球面）、全局不变量以及与信息几何的联系，其中屏幕上的动力学表现为复制子流。

Abstract: This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian "seam" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.

</details>


### [386] [LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora](https://arxiv.org/abs/2511.11574)
*Viviana Luccioli,Rithika Iyengar,Ryan Panley,Flora Haberkorn,Xiaoyu Ge,Leland Crane,Nitish Sinha,Seung Jung Lee*

Main category: cs.LG

TL;DR: 提出了M-RARU算法，结合主动学习和知识蒸馏，通过不确定性采样和随机接受-拒绝机制选择最有信息量的样本，大幅降低大语言模型部署成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在分类任务中准确率高，但计算和财务成本高昂，阻碍了在动态环境中的大规模部署。知识蒸馏过程本身也需要大量标注样本，产生显著token消耗。

Method: 引入M-RARU主动学习算法，结合不确定性和随机接受-拒绝机制，选择最有信息量的数据点供LLM教师标注，显著减少API调用和数据处理时间。

Result: 在五个不同学生模型和多个基准数据集上的实验表明，相比随机采样，M-RARU可减少80%的样本需求，显著提高分类准确率同时降低财务成本和训练时间。

Conclusion: M-RARU提供了一种成本效益高的解决方案，能够在保持LLM性能的同时，大幅降低知识蒸馏过程的计算和财务开销。

Abstract: Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM "teacher" trains a smaller and more efficient "student" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.

</details>


### [387] [Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms](https://arxiv.org/abs/2511.11575)
*Animesh Joshi*

Main category: cs.LG

TL;DR: 本文提出了一个基于k折交叉验证的统计显著性测试框架，用于检测算法公平性指标的统计显著违反，并在累犯预测算法中发现了对黑人个体的统计显著偏见。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏评估群体间观察到的差异是否具有统计显著性或仅是偶然的方法，需要建立严格的统计测试框架来评估算法决策系统的公平性。

Method: 利用k折交叉验证生成公平性指标的抽样分布，开发基于预测与实际结果差异、模型校准和因果推断技术的统计显著性测试。

Result: 在累犯预测算法中，发现对黑人个体在多个公平性定义下存在统计显著偏见，而在其他定义下则无偏见或对白人存在偏见。

Conclusion: 评估算法决策系统时，严格和稳健的统计测试至关重要，不同公平性定义可能得出不同的偏见结论。

Abstract: Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.

</details>


### [388] [To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance](https://arxiv.org/abs/2511.12121)
*Wanlong Fang,Tianle Zhang,Alvin Chan*

Main category: cs.LG

TL;DR: 本文通过可控对比学习模块研究显式对齐对多模态学习的影响，发现最优对齐强度取决于模态间冗余度，需要平衡模态特定信号和共享冗余信息。


<details>
  <summary>Details</summary>
Motivation: 传统多模态学习假设表示对齐总是有益的，但缺乏对显式对齐直接影响的系统性研究。本文旨在探究在不同模态特定信息结构下，显式对齐如何影响模型性能和表示对齐。

Method: 引入可控对比学习模块，在训练过程中精确控制对齐强度，研究显式对齐在何时改善或阻碍性能。在合成和真实数据集上进行实验，分析不同数据特征下的影响。

Result: 实验结果表明，显式对齐对单模态模型性能的影响与数据特征相关：最优对齐水平取决于不同模态间的冗余度。在混合信息分布中，存在一个平衡模态特定信号和共享冗余的最优对齐强度。

Conclusion: 本研究为何时以及如何应用显式对齐以获得最优单模态编码器性能提供了实用指导，强调需要根据模态间冗余度来调整对齐策略。

Abstract: Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.

</details>


### [389] [DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs](https://arxiv.org/abs/2511.11576)
*WenZhuo Zhu,Zheng Cui,Wenhan Lu,Sheng Liu,Yue Zhao*

Main category: cs.LG

TL;DR: 提出了DAOpt框架，包括OptU数据集、多智能体决策模块和仿真环境，用于评估LLM在不确定优化中的表现，并通过融入随机和鲁棒优化的领域知识增强LLM建模能力。


<details>
  <summary>Details</summary>
Motivation: 现实决策具有不确定性，但现有研究主要关注确定性优化，LLM在不确定环境中的应用尚未充分探索。

Method: 构建DAOpt框架，包含OptU数据集、多智能体决策模块和仿真环境，采用小样本学习融入随机和鲁棒优化的领域知识。

Result: 开发了评估LLM在样本外可行性和鲁棒性方面的框架，增强了LLM在不确定优化中的建模能力。

Conclusion: DAOpt框架填补了LLM在不确定优化建模中的研究空白，为评估和提升LLM在现实决策中的表现提供了有效工具。

Abstract: Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.

</details>


### [390] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: 本文深入分析了Transformer中RoPE位置编码的位置与符号信息编码机制，提出了位置性和符号性注意头的定义与度量方法，并通过实验验证了频率使用与模型行为之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer中位置编码如何分别编码位置信息和符号信息，特别是RoPE位置编码的成功机制，以及位置性与符号性注意头行为的理论分析。

Method: 提出了位置性和符号性注意头的理论定义，证明这两种行为互斥，开发了量化指标，并在使用RoPE的Transformer LLMs上应用该框架进行分析，设计了纯位置性和符号性的规范任务。

Result: 发现所有注意头的行为与频率使用之间存在强相关性，通过控制注意头可访问的频率可以因果性地控制Transformer的性能表现。

Conclusion: 本研究提供了对RoPE位置编码的深入理解，阐明了其特性与模型行为之间的关系，为位置编码机制的设计和分析提供了理论框架。

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [391] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: 开发了一个基于Triton DSL的跨平台LLM推理系统，通过优化的paged attention内核在NVIDIA和AMD GPU上实现最佳性能，解决了LLM推理的硬件可移植性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理在跨硬件架构时的可移植性问题，消除对底层手动调优的需求，同时保持最佳效率，这是工业界和学术界长期追求的目标。

Method: 使用Triton领域特定语言开发最先进的paged attention内核，结合算法和系统级改进、参数自动调优，并集成到流行的推理服务器中。

Result: 将通用Triton attention内核的性能从仅达到最先进水平的19.7%提升到105.9%，在NVIDIA和AMD GPU上都实现了最佳性能。

Conclusion: 开源领域特定语言可以用于解锁模型在不同GPU厂商之间的可移植性，证明了高效跨平台LLM推理的可行性。

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [392] [Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations](https://arxiv.org/abs/2511.11583)
*Fernando Spadea,Oshani Seneviratne*

Main category: cs.LG

TL;DR: RAG-FLARKO是一个基于检索增强的金融推荐系统，通过多阶段知识图谱检索来提升个性化金融建议的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在个性化金融推荐中存在上下文限制、幻觉问题和缺乏行为基础等挑战，需要更有效的方法来整合用户行为和市场数据。

Method: 采用多阶段并行知识图谱检索：首先从用户交易知识图谱中检索行为相关实体，然后基于此上下文从市场知识图谱中筛选时间一致信号，构建紧凑的接地子图供LLM使用。

Result: 在真实金融交易数据集上的实证评估显示，RAG-FLARKO显著提升了推荐质量，使更小、更高效的模型在盈利性和行为对齐方面都能实现高性能。

Conclusion: 该框架为在资源受限环境中部署接地金融AI提供了可行路径，通过减少上下文开销和聚焦相关信息，实现了高质量推荐。

Abstract: Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.

</details>


### [393] [Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2511.13053)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 通过几何分析Hopfield网络的能量景观，发现存在"优化脊"现象，在高负载和全局核条件下网络通过直接驱动力与反馈力的强反相关性最大化吸引子稳定性。


<details>
  <summary>Details</summary>
Motivation: 核学习方法显著提高了Hopfield网络的存储容量，但其背后的动力学机制仍不清楚，需要深入理解能量景观的几何特性。

Method: 引入"峰顶锐度"指标量化吸引子局部稳定性，系统改变核宽度和存储负载，通过理论分解景观梯度为直接"驱动力"和间接"反馈力"。

Result: 发现了丰富的吸引子形状相图，核心发现是"优化脊"的出现，其中直接驱动力在高存储负载下放大并主导反对的集体反馈力。

Conclusion: 网络通过自适应利用模式间相互作用作为合作反馈控制系统来塑造鲁棒能量景观，为高容量联想记忆的稳定性提供了新的物理图像和设计原则。

Abstract: Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.

</details>


### [394] [Output Supervision Can Obfuscate the Chain of Thought](https://arxiv.org/abs/2511.11584)
*Jacob Drori,Luke Marks,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: 训练模型仅使用输出监控器（无法访问思维链）仍会导致隐蔽的思维链，通过两种机制：模型泛化到让思维链看起来安全，以及安全外观的思维链增加安全输出的可能性。提出了两种缓解方法，在可监控性和任务性能方面实现了帕累托改进。


<details>
  <summary>Details</summary>
Motivation: OpenAI的研究表明，针对思维链监控器的训练可能导致隐蔽的思维链，其中包含监控器无法检测的不良行为。他们建议仅使用无法访问思维链的输出监控器进行训练。本文旨在证明这种训练方法仍然会导致隐蔽的思维链问题。

Method: 识别了两种导致隐蔽思维链的机制：1）模型被训练产生安全外观输出时，可能泛化到让思维链看起来安全；2）由于后续token依赖于先前token，安全外观的思维链会增加安全输出的可能性，从而强化安全外观的思维链。提出了两种相应的缓解方法。

Result: 提出的两种缓解方法在可监控性和任务性能方面实现了帕累托改进，相比常规训练方法表现更好。

Conclusion: 即使仅使用输出监控器进行训练，仍然存在隐蔽思维链的风险。通过针对性地解决两种机制，可以显著改善模型的可监控性，同时保持良好的任务性能。

Abstract: OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.

</details>


### [395] [Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games](https://arxiv.org/abs/2511.11602)
*Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于收益的学习方案——基于期望的扰动学习自动机(APLA)，用于解决分布式优化中强化学习在多玩家弱无环游戏中无法保证收敛到纯纳什均衡的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在分布式设置中存在局限性，特别是在多玩家弱无环游戏中，当每个玩家应用独立的学习动态时，无法保证收敛到纯纳什均衡。先前的研究仅关注潜在博弈和协调博弈等小类游戏。

Method: 提出了APLA学习方案，其中每个玩家的动作选择概率分布不仅通过重复选择来强化，还通过捕捉玩家满意度的期望因子来强化。在存在噪声观测的情况下，对多玩家正效用博弈中的APLA进行了随机稳定性分析。

Result: 首次在一般非零和博弈中通过建立诱导无限维马尔可夫链与有限维马尔可夫链的等价性来表征随机稳定性。在弱无环游戏中进一步专门化了随机稳定性分析。

Conclusion: APLA方案能够有效解决分布式优化中强化学习的收敛问题，特别是在多玩家弱无环游戏中，为更广泛的博弈类型提供了收敛保证。

Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.

</details>


### [396] [Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge](https://arxiv.org/abs/2511.11585)
*Kabir Khan,Manju Sarkar,Anita Kar,Suresh Ghosh*

Main category: cs.LG

TL;DR: FedGen-Edge是一个联邦学习框架，通过将预训练的全局主干网络与轻量级客户端适配器解耦，仅联邦化适配器，大幅减少通信开销并支持个性化。


<details>
  <summary>Details</summary>
Motivation: 大型生成模型在跨设备联邦学习中面临计算通信负担重和统计/系统异构性问题，需要一种资源高效且支持个性化的解决方案。

Method: 使用低秩适应(LoRA)将客户端更新约束到紧凑子空间，仅联邦化轻量级适配器，保持预训练主干网络冻结。

Result: 在语言建模和图像生成任务上，相比完整模型FedAvg，FedGen-Edge实现了更低的困惑度/FID、更快的收敛速度，通信开销减少99%以上。

Conclusion: FedGen-Edge为异构边缘设备上的隐私保护、资源感知和个性化生成AI提供了一条实用路径。

Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

</details>


### [397] [On the Trade-Off Between Transparency and Security in Adversarial Machine Learning](https://arxiv.org/abs/2511.11842)
*Lucas Fenaux,Christopher Srinivasa,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 论文通过可转移对抗样本攻击研究AI透明度与安全性的权衡，发现攻击者在匹配防御者决策时更成功，表明透明度可能损害安全性。


<details>
  <summary>Details</summary>
Motivation: 研究透明度与安全性在对抗性环境中的冲突，探讨AI系统透明化是否会影响其安全性。

Method: 使用大规模实证评估（9种攻击方法在181个模型上测试），并结合博弈论分析（纳什博弈和斯塔克尔伯格博弈模型）。

Result: 攻击者成功匹配防御者决策时攻击成功率更高；仅知道防御者模型是否被防御就足以损害其安全性。

Conclusion: AI系统的透明度可能与安全性相冲突，博弈论分析揭示了这种基本权衡关系。

Abstract: Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.

</details>


### [398] [WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation](https://arxiv.org/abs/2511.11589)
*Chenyue Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: WildfireGenome开发了一个可解释的野火风险评估系统，融合多种指标并利用机器学习方法，在县级尺度提供透明化的风险驱动因素分析。


<details>
  <summary>Details</summary>
Motivation: 解决现有野火风险评估依赖粗糙地图和不透明模型的问题，牺牲决策尺度的可解释性来优化区域精度。

Method: 融合7个联邦野火指标创建PCA复合风险标签，使用随机森林分类，并通过SHAP和ICE/PDP分析揭示县级非线性驱动关系。

Result: 在7个生态多样县实现0.755-0.878准确率和0.951二次加权Kappa，主成分解释87-94%指标方差，针叶林覆盖和海拔是主要驱动因素。

Conclusion: WildfireGenome将野火风险评估从区域预测推进到可解释的决策尺度分析，指导植被管理、分区和基础设施规划。

Abstract: Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.

</details>


### [399] [Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL](https://arxiv.org/abs/2511.11592)
*Guojian Zhan,Likun Wang,Pengcheng Wang,Feihong Zhang,Jingliang Duan,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出轨迹熵约束强化学习（TECRL）框架，通过分离奖励和熵的Q函数学习，解决最大熵强化学习中Q值估计不稳定和局部熵调节短视的问题。


<details>
  <summary>Details</summary>
Motivation: 最大熵强化学习存在两个瓶颈：1）温度参数更新与熵注入共同导致的Q值估计不稳定；2）仅基于当前单步熵调节温度的短视局部熵调节。

Method: 提出TECRL框架：分别学习奖励Q函数和熵Q函数，确保值目标不受温度更新影响；通过专门的熵Q函数量化期望累积熵，实施轨迹熵约束控制策略长期随机性。基于此开发DSAC-E算法。

Result: 在OpenAI Gym基准测试中，DSAC-E能够获得更高的回报和更好的稳定性。

Conclusion: TECRL框架通过分离Q函数学习和轨迹熵约束，有效解决了最大熵强化学习中的稳定性问题，提升了算法性能。

Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.

</details>


### [400] [Sound Logical Explanations for Mean Aggregation Graph Neural Networks](https://arxiv.org/abs/2511.11593)
*Matthew Morris,Ian Horrocks*

Main category: cs.LG

TL;DR: 该论文研究了使用均值聚合的图神经网络（MAGNNs）的可解释性和表达能力，证明了这类网络能够表达的单调规则类别，并提供了用于解释任何MAGNN预测的一阶逻辑片段。


<details>
  <summary>Details</summary>
Motivation: 尽管使用均值聚合的GNN在知识图谱补全中很常见，但对其可解释性和表达能力的研究仍然缺乏，特别是在使用非负权重约束的情况下。

Method: 研究使用均值聚合和非负权重的GNN（MAGNNs），通过理论分析确定这类网络能够表达的单调规则类别，并构建一阶逻辑片段来解释预测。

Result: 实验表明，限制均值聚合GNN使用非负权重在标准归纳基准测试中可获得相当或更好的性能，实践中能够获得合理的规则，生成有洞察力的解释，并能暴露训练模型中的问题。

Conclusion: MAGNNs不仅在实践中表现良好，而且具有理论上的可解释性保证，能够为知识图谱补全任务提供有意义的解释和模型诊断。

Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.

</details>


### [401] [Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach](https://arxiv.org/abs/2511.11596)
*Javier Marín*

Main category: cs.LG

TL;DR: 论文分析了损失给定违约(LGD)建模中数据质量问题，发现90%的训练数据是基于破产前资产负债表的代理估计而非实际回收结果，这导致递归划分方法系统性失效，而基于信息论的方法表现更优。


<details>
  <summary>Details</summary>
Motivation: LGD建模面临数据质量约束，大部分训练数据是代理估计而非实际回收结果，这影响了模型性能，需要寻找更有效的建模方法。

Method: 使用香农熵和互信息的信息论方法，在1,218个企业破产案例(1980-2023)上进行测试，并与随机森林等递归划分方法对比。

Result: 随机森林在测试数据上获得负r平方(-0.664)，而信息论方法获得0.191的r平方和0.284的RMSE。分析显示杠杆特征包含1.510比特互信息，而规模效应仅贡献0.086比特。

Conclusion: 信息论方法在LGD建模中表现更优，为金融机构在Basel III要求下部署LGD模型提供实用指导，这些发现也适用于医疗结果研究、气候预测和技术可靠性等领域。

Abstract: Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.

</details>


### [402] [Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques](https://arxiv.org/abs/2511.11604)
*Amaratou Mahamadou Saley,Thierry Moyaux,Aïcha Sekhari,Vincent Cheutet,Jean-Baptiste Danielou*

Main category: cs.LG

TL;DR: 提出了一种结合数据驱动技术和核工业领域知识的预测性维护方法，在核工业环境中显著优于纯数据驱动方法，将预测时间从3小时延长到24小时，F1分数从56.36%提升到93.12%。


<details>
  <summary>Details</summary>
Motivation: 物联网和工业4.0的融合增强了核工业的数据驱动方法，但核工业系统复杂，需要大量领域知识。纯数据驱动方法在预测资产维护需求方面存在局限性，需要结合领域知识来提高预测性能。

Method: 提出了一种新颖的预测性维护方法，将数据驱动技术与核设备领域知识相结合。方法在两个层面具有创新性：强调纯数据驱动方法的局限性，展示知识在提升预测模型性能中的重要性。

Result: 通过详细的真实案例研究比较了当前设备监控状态与两种场景，结果显示混合方法显著优于纯数据驱动方法。纯数据驱动方法预测时间仅为3小时，F1分数56.36%；混合方法预测时间延长到24小时，F1分数达到93.12%。

Conclusion: 在核工业这种高度受限和超敏感的领域中，结合领域知识的混合预测性维护方法比纯数据驱动方法具有更好的性能表现，能够更准确地预测设备故障，减少停机时间和运营成本。

Abstract: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.

</details>


### [403] [Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning](https://arxiv.org/abs/2511.11607)
*Guoqing Ma,Yuhan Zhang,Yuming Dai,Guangfu Hao,Yang Chen,Shan Yu*

Main category: cs.LG

TL;DR: 提出COWM层来缓解强化学习中的环境非平稳性问题，提高学习效率


<details>
  <summary>Details</summary>
Motivation: 强化学习通常假设环境是平稳的，但实际环境往往是非平稳的，这导致需要数百万次迭代，样本效率低下

Method: 引入COWM层，集成到任何RL算法的策略网络中，通过聚类技术和投影矩阵来稳定学习过程

Result: 在视觉和状态基础的DMControl基准上分别提升9%和12.6%，在各种算法和任务中表现出鲁棒性和通用性

Conclusion: COWM层能有效缓解非平稳性，提高学习速度，减少梯度干扰，增强整体学习效率

Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.

</details>


### [404] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型中分词器设计（缩放和量化策略）与迁移学习对预测性能的影响，发现分词器配置主要控制模型表示能力和稳定性，而迁移学习影响优化效率和对齐。


<details>
  <summary>Details</summary>
Motivation: 分词器和迁移学习是构建最先进时间序列基础模型的两个关键组件，但它们在时间序列建模中的具体作用和相互作用尚未得到系统研究。

Method: 通过经验训练实验和理论分析相结合的方法，研究分词器设计（特别是缩放和量化策略）以及预训练与随机初始化的影响。

Result: 预训练模型能更有效地利用设计良好的分词器，特别是在较小词汇量时；而错误对齐的分词会削弱甚至逆转预训练的益处。

Conclusion: 在时间序列建模中，精心设计的分词器至关重要，将小型高效词汇表与预训练权重相结合在多模态预测设置中尤其有利。

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [405] [Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data](https://arxiv.org/abs/2511.11623)
*Yushan Jiang,Shuteng Niu,Dongjin Song,Yichen Wang,Jingna Feng,Xinyue Hu,Liu Yang,Cui Tao*

Main category: cs.LG

TL;DR: 开发了一个多模态深度学习框架，用于早期预测肝移植中的移植物抗宿主病(GVHD)，通过整合电子健康记录中的多种模态数据，解决了数据异构性和极端类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: GVHD是肝移植中罕见但致命的并发症，死亡率极高。利用多模态深度学习整合异构和不平衡的电子健康记录，旨在推进GVHD的早期预测，为及时干预和改善患者预后铺平道路。

Method: 分析了2100名肝移植患者的术前电子健康记录，包括42例GVHD病例。开发了多模态深度学习框架，动态融合患者人口统计学、实验室检查、诊断和药物四种主要模态，处理不规则记录和缺失值，并通过AUC优化解决极端类别不平衡。

Result: 该框架优于所有单模态和多模态机器学习基线，AUC为0.836，AUPRC为0.157，召回率为0.768，特异性为0.803。证明该方法能有效捕捉不同模态的互补信息。

Conclusion: 多模态深度学习框架显著改进了GVHD早期预测的现有方法，有效解决了真实世界电子健康记录中的异构性和极端类别不平衡挑战，实现了准确的早期预测。

Abstract: Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.

</details>


### [406] [MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks](https://arxiv.org/abs/2511.11625)
*Mohammad Karami,Mohammad Reza Nemati,Aidin Kazemi,Ali Mikaeili Barzili,Hamid Azadegan,Behzad Moshiri*

Main category: cs.LG

TL;DR: MedFedPure是一个个性化的联邦学习防御框架，用于保护医疗AI模型在推理时免受对抗攻击，同时保持隐私和准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的医疗AI模型在推理时容易受到对抗攻击，现有防御方法难以应对去中心化的医疗环境。

Method: 结合个性化联邦学习模型、掩码自编码器检测可疑输入，以及自适应扩散净化模块选择性清理被标记的扫描图像。

Result: 在Br35H脑部MRI数据集上，对抗鲁棒性从49.50%提升到87.33%，同时保持97.67%的清洁精度。

Conclusion: 该框架为在临床工作流中部署安全、可信且保护隐私的AI工具提供了实用路径。

Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy

</details>


### [407] [SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion](https://arxiv.org/abs/2511.11627)
*Wang Zhenyu,Li Peiyuan,Shi Yongxiang,Wu Ruoyu,Zhang Lei*

Main category: cs.LG

TL;DR: 提出SA-EMO架构用于未知地下结构的速度场反演，通过结构对齐编码器和多算子混合机制，显著提升全波形反演的性能和边界分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统全波形反演方法存在病态性、非线性强、计算量大等问题，且现有深度学习方法依赖单一CNN架构或神经算子，在未知或复杂地质环境中泛化能力差，难以区分不同地质类型。

Method: 采用结构对齐编码器将高维地震波场映射到物理一致的潜空间，消除波形与速度域间的时空不匹配；通过自适应路由机制选择和融合多种神经算子专家（谱、小波、多尺度和局部算子）来预测速度模型。

Result: 在OpenFWI基准测试和Marmousi2数据集上，SA-EMO显著优于传统方法，平均MAE降低约58.443%，边界分辨率提升约10.308%。消融研究证实各组件均对性能提升有重要贡献。

Conclusion: 这项工作为高效、可扩展且物理可解释的全波形反演引入了新范式。

Abstract: Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.

</details>


### [408] [Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification](https://arxiv.org/abs/2511.11629)
*Xu Zhang,Peng Wang,Chen Wang,Zhe Xu,Xiaohua Nie,Wei Wang*

Main category: cs.LG

TL;DR: 提出了一种基于超图的全局特征学习与融合框架，通过特征工程构建全局特征和学习局部特征间的高阶关系来增强应变计状态时间序列的表示，提高识别精度。


<details>
  <summary>Details</summary>
Motivation: 在应变计状态识别中，传统CNN方法只能提取局部特征，当不同时间序列的局部子序列相似时（如飞机机翼静强度实验数据），仅靠局部特征不足以准确表达时间序列。CNN由于卷积操作的本质限制，难以提取全局特征。

Method: 提出超图基础的全局特征学习与融合框架：1）通过特征工程构建全局特征；2）学习局部特征间的高阶关系来捕获全局特征。该方法在工业SGS和公共UCR数据集上验证。

Result: 在工业SGS和公共UCR数据集上的实验表明，该方法在SGS识别中具有更好的泛化能力，对未见数据表现出更好的性能。

Conclusion: 通过超图学习全局特征并将其与局部特征融合，能够更全面地表示SGS时间序列，显著提高识别准确率，特别是在局部特征相似的情况下效果更佳。

Abstract: Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.

</details>


### [409] [Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models](https://arxiv.org/abs/2511.11630)
*Eliane Younes,Elie Hachem,Marc Bernacki*

Main category: cs.LG

TL;DR: 本研究评估了多种深度学习模型（RNN、LSTM、TCN、transformer）预测晶粒生长过程中的晶粒尺寸分布。LSTM表现最佳，准确率超过90%，计算时间从20分钟缩短到几秒，且能保持物理一致性。


<details>
  <summary>Details</summary>
Motivation: 晶粒生长强烈影响材料力学性能，但全场模拟计算成本高。本研究旨在开发基于低维统计描述符的高效预测方法，为数字孪生和工艺优化提供支持。

Method: 从120个晶粒生长序列中提取平均场统计描述符，将归一化晶粒尺寸分布作为时间函数。使用递归预测策略，基于短期历史预测未来分布。比较了RNN、LSTM、TCN和transformer等深度学习架构。

Result: LSTM网络准确率最高（超过90%），性能最稳定，在长期预测中保持物理一致性，计算时间从约20分钟/序列减少到仅几秒钟。其他架构在时间预测较远时趋于发散。

Conclusion: 低维描述符和基于LSTM的预测方法在高效准确预测微观结构方面具有巨大潜力，对数字孪生开发和工艺优化有直接应用价值。

Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.

</details>


### [410] [Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination](https://arxiv.org/abs/2511.11632)
*Qiuhao Zeng*

Main category: cs.LG

TL;DR: 提出了一种新的元学习算法，通过将分类器分解为元组件来提高小样本学习的泛化能力。元组件在可见类上学习，并通过正交正则化促进多样性，从而捕获不同分类器间的共享子结构。


<details>
  <summary>Details</summary>
Motivation: 解决小样本学习中基于度量的元学习方法可能过度拟合可见类、在未见类上泛化能力差的问题。

Method: 探索分类器的子结构，将每个分类器表示为元组件的组合。元组件在可见类的元学习过程中学习，并通过正交正则化促进多样性和捕获共享子结构。

Result: 在小样本基准任务上的广泛实验显示，所提方法取得了优越的性能。

Conclusion: 通过元组件和正交正则化的方法有效提高了小样本学习的泛化能力。

Abstract: In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.

</details>


### [411] [A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products](https://arxiv.org/abs/2511.11646)
*Li Yinxing,Tsukasa Ishigaki*

Main category: cs.LG

TL;DR: 提出了一种使用条件表格变分自编码器(CTVAE)预测新产品线扩展中消费者属性变化的方法，帮助营销人员制定有效的产品线营销策略。


<details>
  <summary>Details</summary>
Motivation: 产品线扩展是重要的营销策略，但过度扩展会破坏品牌形象。营销人员需要了解新产品的主要消费者特征，以便基于消费者需求进行适当的产品线扩展。

Method: 使用条件表格变分自编码器(CTVAE)从大规模的消费者和产品表格数据中生成合成数据，预测新产品线扩展中的消费者属性变化。

Result: 实验结果表明，CTVAE在预测性能上优于现有模型，能够为改变包装或口味的新产品提供有效的营销启示。

Conclusion: 该方法有助于避免产品自相蚕食，并为设计产品形象和营销策略提供支持，具有重要的实际应用价值。

Abstract: Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.

</details>


### [412] [An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment](https://arxiv.org/abs/2511.11636)
*Asma Sadia Khan,Sadia Tabassum*

Main category: cs.LG

TL;DR: 开发了一个公平审计和可解释的机器学习框架用于预测多囊卵巢综合征，通过SHAP特征归因和人口统计学审计评估模型性能并识别患者亚组间的诊断差异。


<details>
  <summary>Details</summary>
Motivation: 解决PCOS诊断中存在的亚组间不公平性问题，确保机器学习模型在不同患者群体中的可靠性和可解释性，为临床决策提供可信赖的工具。

Method: 集成SHAP特征归因与人口统计学审计，使用随机森林、SVM和XGBoost模型，结合等渗和Platt缩放进行校准，评估概率校准指标（Brier Score和ECE）。

Result: 校准后的随机森林模型达到90.8%的预测准确率，SHAP分析识别出卵泡计数、体重增加和月经不规律为最重要特征。模型在25-35岁女性中表现最佳（90.9%），但在25岁以下群体中表现较差（69.2%）。

Conclusion: 该框架成功平衡了校准性能和可解释性，揭示了年龄相关的诊断差异，并通过Streamlit网络界面实现了临床可用的实时PCOS风险评估工具。

Abstract: This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.

</details>


### [413] [KForge: Program Synthesis for Diverse AI Hardware Accelerators](https://arxiv.org/abs/2511.13274)
*Taras Sereda,Tom St. John,Burak Bartan,Natalie Serrino,Sachin Katti,Zain Asgar*

Main category: cs.LG

TL;DR: KForge是一个平台无关的GPU内核优化框架，使用两个协作的LLM代理：生成代理通过编译和正确性反馈迭代优化程序，性能分析代理解释性能数据指导优化，仅需单次示例即可适配新平台。


<details>
  <summary>Details</summary>
Motivation: GPU内核对机器学习性能至关重要，但在不同加速器上优化困难，需要平台无关的解决方案。

Method: 采用双代理协作架构：生成代理负责程序生成和迭代优化，性能分析代理解释性能数据并提供优化建议，支持跨平台知识迁移。

Result: 验证了框架在NVIDIA CUDA和Apple Metal等不同并行计算平台上的有效性，展示了跨平台知识迁移能显著提升生成质量。

Conclusion: KForge提供了一种平台无关的GPU内核优化方法，通过协作代理架构实现高效的程序合成和优化，仅需少量示例即可适配新硬件平台。

Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.

</details>


### [414] [Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches](https://arxiv.org/abs/2511.11638)
*Aamir Shehzad*

Main category: cs.LG

TL;DR: 本研究开发了两种改进的PINN方法（自适应和保守方法）来解决正则化长波方程，发现PINN的有效性与问题类型相关。自适应PINN在处理复杂非线性相互作用时表现更好，而保守PINN在长期行为问题上更优。


<details>
  <summary>Details</summary>
Motivation: 标准物理信息神经网络在解决正则化长波方程时产生较大误差，需要开发改进方法来提高精度。

Method: 开发了两种改进PINN方法：具有自适应损失加权的自适应方法和强制执行显式守恒定律的保守方法。使用三个基准测试：单孤子传播、双孤子相互作用和涌浪演化。

Result: 自适应PINN在复杂非线性相互作用问题上显著优于保守PINN和标准PINN，而保守PINN在长期行为问题上表现更好。两种方法的解与数值解的误差在O(10^-5)量级。

Conclusion: 显式强制执行守恒定律可能对高度非线性系统的求解优化有害，需要特殊训练方法。PINN可以为复杂偏微分方程系统提供准确的无网格解，但需要根据具体问题类型设计合适的PINN。

Abstract: Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.

</details>


### [415] [EcoSpa: Efficient Transformer Training with Coupled Sparsity](https://arxiv.org/abs/2511.11641)
*Jinqi Xiao,Cheng Luo,Lingyi Huang,Cheng Yang,Yang Sui,Huy Phan,Xiao Zang,Yibiao Ying,Zhexiang Tang,Anima Anandkumar,Bo Yuan*

Main category: cs.LG

TL;DR: EcoSpa是一种高效的Transformer结构化稀疏训练方法，通过联合评估和稀疏化耦合权重矩阵对，在保持交互模式的同时实现显著的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏训练方法未能保留注意力层和前馈层中权重矩阵之间的关键结构关系，导致在高稀疏度下性能下降。

Method: 引入联合评估和稀疏化耦合权重矩阵对的方法，通过对齐的行/列移除来保持交互模式，并在预训练和微调场景中执行耦合估计和稀疏化。

Result: LLaMA-1B训练内存减少50%，训练速度提升21%；GPT-2-Medium模型压缩2.2倍，困惑度降低2.4；推理速度提升1.6倍。

Conclusion: EcoSpa使用标准PyTorch操作，无需定制硬件或内核，使高效Transformer训练可在普通硬件上实现。

Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.

</details>


### [416] [AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking](https://arxiv.org/abs/2511.12934)
*Zhi Kou,Xiang-Rong Sheng,Shuguang Han,Zhishan Zhao,Yueyao Cheng,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 本文提出了异步推理框架(AIF)，通过解耦用户/物品独立组件与实时预测，在淘宝展示广告系统中实现了显著性能提升和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统中的预排序模型采用顺序执行框架，存在重复计算相同用户/物品和严格顺序操作导致的延迟瓶颈，限制了模型容量和系统效率。

Method: 提出AIF框架，将交互无关组件与实时预测解耦：用户端计算与检索阶段并行执行，物品端计算以近线方式完成，交互无关组件只需计算一次并在预排序实时预测前完成。

Result: AIF提高了计算效率并减少延迟，释放资源显著改进交互无关组件的特征集和模型架构，在淘宝展示广告系统成功部署。

Conclusion: 通过框架与模型的协同设计，AIF在不大幅增加计算和延迟成本的情况下实现了显著性能提升，证明了该方法的有效性。

Abstract: In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.

</details>


### [417] [Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection](https://arxiv.org/abs/2511.11647)
*Dariush Salami,Ramin Hashemi,Parham Kazemi,Mikko A. Uusitalo*

Main category: cs.LG

TL;DR: 提出了一种基于迁移学习和强化学习的可持续波束选择方法，通过将环境建模为点云并计算Chamfer距离来识别结构相似环境，从而重用预训练模型，大幅减少训练时间和计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的波束选择模型在多样化环境中需要大量训练时间和计算资源，这影响了系统的可扩展性和能源效率。

Method: 将环境建模为点云（包含gNodeB和散射体位置），计算点云间的Chamfer距离来识别结构相似环境，通过迁移学习重用预训练模型。

Result: 实现了16倍的训练时间和计算开销减少，同时保持高性能，显著降低能耗和碳排放。

Conclusion: 该方法证明了迁移学习能够实现可扩展、自适应且环保的RL波束选择策略，支持绿色AI在无线系统中的发展。

Abstract: This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.

</details>


### [418] [Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning](https://arxiv.org/abs/2511.11648)
*Shunyu Wu,Tianyue Li,Yixuan Leng,Jingyi Suo,Jian Lou,Dan Li,See-Kiong Ng*

Main category: cs.LG

TL;DR: LTSV是一种轻量级时间序列估值方法，通过在时间序列基础模型上进行上下文微调来估计数据样本的贡献度，解决了传统方法在计算效率和时序依赖性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统数据估值方法（如影响函数）在处理大规模时间序列基础模型时面临计算瓶颈，且难以保持时序依赖性，因此需要开发更高效准确的时间序列数据估值方法。

Method: 基于理论证据证明上下文微调近似于影响函数，LTSV通过测量上下文微调后上下文损失的变化来估计样本贡献度，并引入时序块聚合来捕捉时序依赖性。

Result: 在多个时间序列数据集和模型上的实验表明，LTSV能够提供可靠且强大的估值性能，同时保持可管理的计算需求。

Conclusion: 在时间序列基础模型上进行上下文微调为时间序列学习中的数据归因和模型泛化之间提供了实用有效的桥梁。

Abstract: Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.

</details>


### [419] [Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine](https://arxiv.org/abs/2511.11650)
*Daniele Ugo Leonzio,Paolo Bestagini,Marco Marcon,Stefano Tubaro*

Main category: cs.LG

TL;DR: 提出一种基于水压测量和单类支持向量机的数据驱动漏水检测方法，仅需无漏水时的压力数据和管网拓扑信息。


<details>
  <summary>Details</summary>
Motivation: 水管网络每年因漏水造成大量水资源损失，需要可靠有效的漏水检测和定位系统。数据驱动方法因其优越性能而受到关注。

Method: 使用水管网络节点处的水压测量数据，基于特征提取器和在无漏水数据上训练的单类支持向量机，将漏水检测为异常。

Result: 在Modena水管网络模拟数据集上的结果表明，该方法优于最近的漏水检测方法。

Conclusion: 提出的数据驱动漏水检测方案仅需管网拓扑和无漏水压力数据，在性能上优于现有方法。

Abstract: Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.

</details>


### [420] [Incomplete Depression Feature Selection with Missing EEG Channels](https://arxiv.org/abs/2511.11651)
*Zhijian Gong,Wenjia Dong,Xueyuan Xu,Fulin Wei,Chunyu Liu,Li Zhuo*

Main category: cs.LG

TL;DR: 提出了一种名为IDFS-MEC的新型特征选择方法，用于处理EEG数据中的缺失通道问题，通过整合缺失通道指示信息和自适应通道权重学习来提高抑郁症分析的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: EEG特征通常包含冗余、不相关和噪声信息，且现实世界EEG数据采集经常面临电极脱落导致的数据丢失和严重噪声干扰等挑战。

Method: IDFS-MEC将缺失通道指示信息和自适应通道权重学习整合到正交回归中，减轻不完整通道对模型构建的影响，然后利用全局冗余最小化学习来减少所选特征子集中的冗余信息。

Result: 在MODMA和PRED-d003数据集上的广泛实验表明，IDFS-MEC选择的EEG特征子集在3、64和128通道设置下比10种流行特征选择方法具有更优越的性能。

Conclusion: IDFS-MEC方法能够有效处理EEG数据中的缺失通道问题，并选择出性能优越的特征子集，为抑郁症分析提供了更鲁棒的解决方案。

Abstract: As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.

</details>


### [421] [How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity](https://arxiv.org/abs/2511.11652)
*Marvin Plein,Carsten F. Dormann,Andreas Christen*

Main category: cs.LG

TL;DR: 该研究提出了一种逐步移除气象站的方法，用于精简德国弗莱堡的城市气象站网络，发现在保持高预测精度的前提下，可以将气象站数量从42个大幅减少到4个。


<details>
  <summary>Details</summary>
Motivation: 城市气象站网络的维护成本高昂且劳动密集，需要寻找在保持监测能力的同时减少站点数量的方法。

Method: 采用逐步移除气象站的方法，分析不同子集在模拟减少气象站密度后，重现原始网络空气温度和湿度模式的能力。

Result: 从42个气象站减少到4个，空气温度预测RMSE从0.69K增加到0.83K（增加20%），相对湿度RMSE从3.8%增加到4.4%（增加16%），预测精度仍优于最先进的数值城市地表模型。

Conclusion: 研究表明精简气象站网络具有巨大潜力，可以最大化城市气候研究中财务和人力资源的分配效率，位于建成区和乡村交界处的站点对于重建城市范围气候特征最有价值。

Abstract: Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.

</details>


### [422] [Convergence of Multiagent Learning Systems for Traffic control](https://arxiv.org/abs/2511.11654)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文对多智能体强化学习在交通信号控制中的收敛性进行了理论分析，证明了在特定条件下该算法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 随着班加罗尔等城市的快速城市化，交通拥堵问题日益严重。虽然已有实证研究表明多智能体Q学习在交通信号控制中的有效性，但缺乏对其稳定性和收敛性的严格理论分析。

Method: 使用随机逼近方法，正式分析了多智能体强化学习算法的学习动态，将单智能体异步值迭代的收敛性证明扩展到多智能体场景。

Result: 证明了特定的多智能体强化学习交通控制算法在给定条件下能够收敛。

Conclusion: 本文填补了多智能体强化学习在交通控制领域理论分析的空白，为该方法的实际应用提供了理论保障。

Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.

</details>


### [423] [On the Probabilistic Learnability of Compact Neural Network Preimage Bounds](https://arxiv.org/abs/2511.11656)
*Luca Marzari,Manuele Bicego,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 提出RF-ProVe方法，使用随机森林和主动重采样来高效计算神经网络预像的近似边界，提供统计保证，解决#P难问题的可扩展性限制


<details>
  <summary>Details</summary>
Motivation: 现有可证明的神经网络预像边界计算方法受#P难问题限制难以扩展，需要开发具有高置信度保证和有限误差的实用解决方案

Method: 引入RF-ProVe方法，利用随机决策树集成生成满足输出属性的候选输入区域，并通过主动重采样进行精炼

Result: 理论推导提供区域纯度和全局覆盖的正式统计保证，为计算紧凑预像近似提供实用可扩展方案

Conclusion: RF-ProVe为精确求解器无法扩展的情况提供了具有统计保证的实用、可扩展预像近似计算方法

Abstract: Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\textbf{R}$andom $\textbf{F}$orest $\textbf{Pro}$perty $\textbf{Ve}$rifier ($\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.

</details>


### [424] [SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization](https://arxiv.org/abs/2511.11663)
*Zhixiong Zhao,Fangxin Liu,Junjie Wang,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.LG

TL;DR: SpecQuant是一种从傅里叶频域角度解决LLM极端压缩的两阶段框架，通过平滑激活异常值和通道级低频傅里叶截断，在LLaMA-3 8B上实现4位量化的权重和激活，性能损失仅1.5%。


<details>
  <summary>Details</summary>
Motivation: 随着准确开源大语言模型的出现，需要先进的量化技术来在终端设备上高效部署。本文从傅里叶频域角度重新审视LLM极端压缩的挑战，目标是实现权重和激活的超低位量化。

Method: 两阶段框架：第一阶段平滑激活异常值并将其转移到权重矩阵；第二阶段应用通道级低频傅里叶截断来抑制高频分量，同时保留基本信号能量。还引入了轻量级截断模块用于推理时的自适应调整。

Result: 在LLaMA-3 8B上，SpecQuant实现了权重和激活的4位量化，与全精度相比零样本准确率差距仅为1.5%，同时推理速度提升2倍，内存使用降低3倍。

Conclusion: SpecQuant证明了从傅里叶频域角度处理LLM极端压缩的有效性，通过保留低频分量能量实现了高质量的超低位量化，为终端设备部署提供了实用解决方案。

Abstract: The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.

</details>


### [425] [Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE](https://arxiv.org/abs/2511.11665)
*Sameeksha Sriram,Ayush Paliwal,Alexander S. Ecker,Chase van de Geijn*

Main category: cs.LG

TL;DR: 本文提出了基于四元数的旋转位置嵌入方法QuatRo，并将其推广到基于几何代数的Clifford旋转嵌入CARE，解决了球形RoPE的非交换性问题，并实现了任意维度的位置编码。


<details>
  <summary>Details</summary>
Motivation: 球形RoPE等扩展方法由于使用欧拉角表示旋转而具有非交换性，丧失了原始RoPE的平移等变性特性。本文旨在通过四元数和几何代数解决这一问题。

Method: 1. 提出QuatRo方法，用四元数替代欧拉角表示3D旋转；2. 将QuatRo推广为CARE，使用Clifford代数中的旋量作用于多向量；3. 支持任意维度的位置编码和多种等级多向量的位置信息编码。

Result: 初步实验比较了球形、四元数和Clifford基旋转嵌入的性能，证明了所提方法的有效性。

Conclusion: QuatRo和CARE方法不仅解决了球形RoPE的非交换性问题，还实现了更通用的旋转位置嵌入框架，为高维输入的位置编码提供了新的解决方案。

Abstract: Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.

</details>


### [426] [Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks](https://arxiv.org/abs/2511.11666)
*Rajit Rajpal,Benedict Leimkuhler,Yuanhao Jiang*

Main category: cs.LG

TL;DR: 提出了SA-SGLD自适应采样方法，通过时间重缩放自动调整步长，在贝叶斯神经网络中实现更准确的后验采样。


<details>
  <summary>Details</summary>
Motivation: 现有的SGMCMC方法对步长选择高度敏感，自适应变体如pSGLD需要昂贵的发散校正项才能正确采样不变测度。

Method: 基于SamAdams框架的时间步长自适应方案，使用时间重缩放根据梯度范数调整步长，在高曲率区域自动缩小步长，在平坦区域扩大步长。

Result: 在高曲率2D玩具示例和使用尖锐先验的贝叶斯神经网络图像分类中，比SGLD实现了更准确的后验采样。

Conclusion: SA-SGLD方法能够在不引入偏差的情况下提高稳定性和混合性，实现更准确的后验采样。

Abstract: Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.

</details>


### [427] [Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion](https://arxiv.org/abs/2511.11667)
*Feng Guo,Yuntao Wen,Shen Gao,Junshuo Zhang,Shuo Shang*

Main category: cs.LG

TL;DR: 提出KUnBR方法，通过知识密度估计定位有害知识密集层，采用层重插入策略彻底消除LLM中的有害知识，在保持模型性能的同时实现最先进的遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法难以彻底移除LLM中的有害知识，残留知识容易被恢复，需要解决隐私、合规和伦理问题。

Method: 使用知识密度估计量化有害知识分布，识别有害知识密集层；设计层重插入策略，将有害知识密集层提取并重插入原始LLM，绕过覆盖层梯度阻塞。

Result: 在多个遗忘和通用能力基准测试中，KUnBR实现了最先进的遗忘性能，同时保持了模型效用。

Conclusion: KUnBR通过精确的层定位和重插入策略，有效解决了LLM有害知识彻底移除的挑战，为机器遗忘提供了新思路。

Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.

</details>


### [428] [Do traveling waves make good positional encodings?](https://arxiv.org/abs/2511.11668)
*Chase van de Geijn,Ayush Paliwal,Timo Lüddecke,Alexander S. Ecker*

Main category: cs.LG

TL;DR: 提出了一种基于行波的新型位置编码机制RollPE，通过对查询和键张量应用循环滚动操作来实现，在自注意力中通过位置差异而非绝对索引计算注意力。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用绝对正弦嵌入或学习的位置向量，而较新方法强调相对编码以更好地捕捉平移等变性。

Method: 在自注意力中对查询和键张量应用循环滚动操作，诱导位置间的相位相对偏移。

Result: 该方法显著优于传统绝对位置嵌入，与RoPE性能相当。推导了RollPE的连续情况，并在数学上证明了与特定RoPE配置的等价性。

Conclusion: 从行波角度审视RollPE可能简化RoPE，并将其与大脑信息流动过程联系起来。

Abstract: Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.

</details>


### [429] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: 提出了一种能够根据输入数据动态调整内部结构的神经网络架构，通过路由机制实现迭代和自适应计算，探索可适应且更可解释的网络新方向。


<details>
  <summary>Details</summary>
Motivation: 受到思维过程和动态推理的启发，旨在创建能够根据数据和内部状态条件化信息流的网络，探索学习计算结构本身的可能性，而非追求现有基准的性能优化。

Method: 设计具有路由机制的神经网络架构，允许每层影响其输出在网络中的传播方式，实现动态调整内部结构的迭代计算。

Result: 由于计算资源和数据的实际限制，本研究仍处于初步探索阶段，但初步观察显示出前景，架构的完整潜力需要在更有利的计算条件下进行未来实验评估。

Conclusion: 提出了一个概念原型架构框架，为探索可适应且可能更可解释的网络开辟了新方向，强调这是一个概念性探索而非性能竞争的工作。

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [430] [Evaluation of LLM-based Explanations for a Learning Analytics Dashboard](https://arxiv.org/abs/2511.11671)
*Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: 使用大型语言模型为学习分析仪表板生成数据解释，在专家研究中发现LLM生成的技能状态解释和学习建议比独立仪表板和教师解释更受青睐。


<details>
  <summary>Details</summary>
Motivation: 学习分析仪表板支持自主学习，但其有效性受数据可解释性影响。需要辅助解释来帮助理解仪表板数据。

Method: 使用大型语言模型生成仪表板数据的语言解释，并与独立仪表板和教师提供的解释进行对比，在专家研究中对大学教育工作者(N=12)进行评估。

Result: LLM生成的技能状态解释和课程学习建议显著优于其他条件，更受青睐。

Conclusion: 使用LLM进行解释可以提升学习体验，同时保持教师认可的教学标准。

Abstract: Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.

</details>


### [431] [Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture](https://arxiv.org/abs/2511.11673)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: 提出了一种新颖的协同融合层(SFL)架构，通过门控机制将深度语义特征与结构特征融合，在歌词内容分类任务中实现了高精度和优异的校准性能。


<details>
  <summary>Details</summary>
Motivation: 解决将复杂高维深度语义特征与简单可解释的结构线索相结合进行歌词内容分类的挑战。

Method: 使用门控机制调节Sentence-BERT嵌入(Fdeep)与低维辅助特征(Fstruct)，将UMAP降维后的歌词嵌入聚类任务重构为二分类问题。

Result: SFL模型准确率达0.9894，Macro F1得分0.9894，比随机森林基线表现更好，校准误差降低93%，对数损失降低2.5倍。

Conclusion: 非线性门控机制优于简单特征拼接，SFL模型为复杂多模态歌词分析提供了稳健可信的系统。

Abstract: This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.

</details>


### [432] [Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff](https://arxiv.org/abs/2511.11675)
*Junchen Liu,Yi Sheng*

Main category: cs.LG

TL;DR: 提出双向剪枝-再生策略，从极度压缩的网络开始，选择性再生关键连接来恢复性能，解决高稀疏度下模型性能急剧下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当稀疏度超过特定阈值时，迭代和一次性剪枝方法都会导致模型性能急剧下降，限制了可实现的压缩比，无法满足某些硬件平台的严格尺寸约束。

Method: 双向剪枝-再生策略：从满足硬件约束的极度压缩网络开始，选择性再生关键连接以恢复丢失的性能。

Result: 该方法有效缓解了高稀疏度条件下常见的精度急剧下降问题。

Conclusion: 提出的双向剪枝-再生策略能够克服传统剪枝方法在高稀疏度下的性能限制，实现更高的压缩比同时保持模型性能。

Abstract: As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.

</details>


### [433] [Learning with Preserving for Continual Multitask Learning](https://arxiv.org/abs/2511.11676)
*Hanchen David Wang,Siwoo Bae,Zirong Chen,Meiyi Ma*

Main category: cs.LG

TL;DR: 提出了Learning with Preserving (LwP)框架，通过保持共享表示空间的几何结构来解决持续多任务学习中的灾难性遗忘问题，无需重放缓冲区。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、医疗影像分析等关键领域，AI系统需要持续学习新任务而不遗忘已学能力，现有持续学习方法在此场景下表现不佳，因为它们学习的是碎片化的任务特定特征。

Method: 引入动态加权距离保持(DWDP)损失，通过正则化潜在数据表示之间的成对距离来防止表示漂移，从而保持共享表示空间的几何结构。

Result: 在时间序列和图像基准测试中，LwP不仅缓解了灾难性遗忘，而且持续优于最先进的基线方法，是唯一超越强单任务学习基线的方法。

Conclusion: LwP框架通过保持表示空间的几何结构，在持续多任务学习场景中表现出色，特别适合对隐私敏感的应用和现实世界的动态环境。

Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.

</details>


### [434] [Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow](https://arxiv.org/abs/2511.11677)
*Shimiao Li,Aaron Tuor,Draguna Vrabie,Larry Pileggi,Jan Drgona*

Main category: cs.LG

TL;DR: 提出了一种基于同伦引导的自监督学习方法，用于解决参数化交流最优潮流问题的学习优化挑战，通过构建从松弛问题到原始问题的连续变形来提高收敛稳定性和可行性。


<details>
  <summary>Details</summary>
Motivation: 交流最优潮流问题的非凸性导致优化景观复杂，标准学习方法难以收敛到可行的高质量解，需要一种能提高收敛稳定性和可行性的新方法。

Method: 采用同伦引导的自监督学习，构建目标函数和约束条件的连续变形过程，从具有广泛吸引域的松弛问题逐步过渡到原始问题。

Result: 在标准IEEE交流最优潮流基准测试中，该方法相比非同伦基线显著提高了可行性率，同时达到与完整最优潮流求解器相当的目标函数值。

Conclusion: 同伦启发式方法在电力系统优化中展示了可扩展、约束感知的学习优化的潜力。

Abstract: Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.

</details>


### [435] [A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications](https://arxiv.org/abs/2511.11679)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.LG

TL;DR: 提出SBN-Opt框架，通过神经代理网络SBN嵌入LSQC能量，优化自由边界微分同胚映射，在密度均衡映射和不一致表面配准中优于传统数值算法。


<details>
  <summary>Details</summary>
Motivation: 自由边界微分同胚优化在表面映射问题中至关重要但极具挑战性，因为边界不受约束且在大变形下必须保持局部双射性。

Method: 提出谱Beltrami网络(SBN)，将LSQC能量嵌入多尺度网格谱架构中；然后提出SBN-Opt优化框架，优化自由边界微分同胚映射。

Result: 在密度均衡映射和不一致表面配准的广泛实验中，SBN-Opt优于传统数值算法。

Conclusion: SBN-Opt框架能够有效优化自由边界微分同胚映射，并明确控制局部几何畸变。

Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.

</details>


### [436] [Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP](https://arxiv.org/abs/2511.11680)
*Udaya Bhasker Cheerala,Varun Teja Chirukuri,Venkata Akhil Kumar Gummadi,Jintu Moni Bhuyan,Praveen Damacharla*

Main category: cs.LG

TL;DR: 本研究开发了加州野火风险地图，使用随机森林算法结合SHAP可解释AI方法，识别了森林和草原生态系统的关键风险驱动因素，并评估了模型的空间和时间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 野火对全球生态系统构成重大威胁，加州由于气候、地形、植被和人类活动等因素经常发生火灾，需要开发全面的风险评估方法来支持决策制定。

Method: 采用随机森林算法结合SHAP可解释AI方法，通过空间和时间验证策略评估模型性能，识别生态系统特定的关键风险驱动因素。

Result: RF模型表现出强大的预测性能，森林和草原的AUC分别达到0.997和0.996。空间交叉验证显示中等可转移性，时间验证显示更好的泛化能力。SHAP分析识别出土壤有机碳、树木覆盖和NDVI是森林的关键驱动因素，而LST、海拔和植被健康指数是草原的关键驱动因素。

Conclusion: RF-SHAP框架提供了一个稳健、可理解和适应性强的野火风险评估方法，能够支持明智决策并制定有针对性的风险缓解策略。

Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.

</details>


### [437] [MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation](https://arxiv.org/abs/2511.11681)
*Penghui Niu,Jiashuai She,Taotao Cai,Yajuan Zhang,Ping Zhang,Junhua Gu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出MPCM-Net网络，结合部分注意力卷积和Mamba架构，用于地面云图像分割，在CSRC数据集上实现精度与速度的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法存在三个主要问题：依赖空洞卷积缺乏通道间互操作性；注意力机制忽视精度-吞吐量平衡；解码器修改未能建立层次局部特征的全局依赖关系。

Method: 编码器包含MPAC模块（MPC块和MPA块），实现多尺度云形成的全局空间交互和低计算复杂度特征提取；解码器使用M2B模块通过SSHD保持线性复杂度进行深度特征聚合。

Result: 在CSRC数据集上的广泛实验表明，MPCM-Net优于最先进方法，在分割精度和推理速度之间达到最优平衡。

Conclusion: MPCM-Net通过创新的多尺度网络架构有效解决了现有方法的局限性，同时发布了CSRC数据集作为新的分割基准。

Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.

</details>


### [438] [Stratified Knowledge-Density Super-Network for Scalable Vision Transformers](https://arxiv.org/abs/2511.11683)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.LG

TL;DR: 提出WPAC和PIAD方法，将预训练ViT转换为分层知识密度超网络，实现灵活提取不同大小的子网络，同时保持最大知识保留。


<details>
  <summary>Details</summary>
Motivation: 为不同资源约束训练和部署多个ViT模型成本高且效率低，需要一种能灵活适应不同模型大小的统一解决方案。

Method: 使用WPAC通过加权PCA压缩注意力层知识，结合PIAD通过渐进重要性感知dropout促进知识分层组织。

Result: WPAC在知识集中方面优于现有剪枝标准，与PIAD结合为模型压缩和扩展提供了强大替代方案。

Conclusion: 该方法能有效构建分层知识组织，实现从单一预训练模型灵活提取不同大小的子网络。

Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.

</details>


### [439] [A Bayesian Model for Multi-stage Censoring](https://arxiv.org/abs/2511.11684)
*Shuvom Sadhuka,Sophia Lin,Emma Pierson,Bonnie Berger*

Main category: cs.LG

TL;DR: 该论文提出了一种用于医疗决策漏斗结构的贝叶斯模型，解决了选择性标签和审查偏差问题，特别是在服务不足患者群体中。模型在合成设置中表现优于基线方法，并在急诊科数据中发现ICU入院存在性别差异。


<details>
  <summary>Details</summary>
Motivation: 医疗决策中的漏斗结构（如筛查、评估等阶段）存在选择性审查问题，即真实结果仅在流程结束时揭示，这会导致风险估计的统计偏差，特别是在服务不足患者群体中。

Method: 开发了一个基于选择性标签和审查先验工作的贝叶斯模型，用于漏斗决策结构，能够恢复真实参数并预测被审查患者的结果。

Result: 在合成设置中，模型能够准确恢复真实参数并比基线方法更准确地预测被审查患者的结果。在急诊科数据应用中，发现ICU入院存在性别差异：女性需要更高的死亡风险阈值（5.1%）才能被收入ICU，而男性为4.5%。

Conclusion: 该贝叶斯模型能够有效解决医疗漏斗决策结构中的选择性审查偏差问题，揭示了医疗决策中存在的性别差异，为改善医疗公平性提供了工具。

Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).

</details>


### [440] [R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models](https://arxiv.org/abs/2511.11685)
*Tianyi Yin,Jingwei Wang,Chenze Wang,Han Wang,Jiexuan Cai,Min Liu,Yunlong Ma,Kun Gao,Yuting Song,Weiming Shen*

Main category: cs.LG

TL;DR: R-Tuning是一种用于预训练时间序列模型持续适应的新框架，通过频率感知回放策略构建统一潜在空间，解决灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在时间序列预测中表现出色，但适应不断变化的数据分布仍然困难，主要挑战在于无法访问原始训练数据，仅在新数据上微调会导致灾难性遗忘。

Method: 提出R-Tuning框架，使用频率感知回放策略构建统一潜在空间，通过小波分解在多频段增强模型生成样本，创建趋势保持和融合增强变体，并引入潜在一致性约束来对齐新旧表示。

Result: 实验结果显示R-Tuning在新任务上MAE和MSE分别降低达46.9%和46.8%，在旧任务上保持知识增益达5.7%和6.0%。在少样本设置下，即使合成代理样本仅占新任务数据集的5%，也优于所有最先进基线。

Conclusion: R-Tuning通过频率感知回放和潜在一致性约束，有效解决了预训练时间序列模型的持续适应问题，在保持先验知识的同时显著提升了新任务的性能。

Abstract: Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.

</details>


### [441] [Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems](https://arxiv.org/abs/2511.11686)
*Qing Yao,Lijian Gao,Qirong Mao,Dong Ming*

Main category: cs.LG

TL;DR: 提出Regularized Schrödinger Bridge (RSB)方法解决扩散模型在逆问题中的失真-感知权衡和暴露偏差问题，在语音增强任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在逆问题中的两个关键挑战：1)失真-感知权衡，提高感知质量会降低重建保真度；2)暴露偏差问题，训练-推理输入不匹配导致预测误差累积和重建质量下降。

Method: 提出Regularized Schrödinger Bridge (RSB)，采用新颖的正则化训练策略，扰动输入状态和目标，通过暴露模型于模拟预测误差来缓解暴露偏差，并通过后验均值的精心设计插值来减轻失真。

Result: 在语音增强的两个典型逆问题上进行广泛实验，RSB优于最先进方法，显著改善失真指标并有效减少暴露偏差。

Conclusion: RSB是专门针对逆问题设计的Schrödinger Bridge改进版本，成功解决了扩散模型在失真-感知权衡和暴露偏差方面的局限性。

Abstract: Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schrödinger Bridge (RSB), an adaptation of Schrödinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.

</details>


### [442] [Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling](https://arxiv.org/abs/2511.11688)
*Aihua Zhu,Rui Su,Qinglin Zhao,Li Feng,Meng Shen,Shibo He*

Main category: cs.LG

TL;DR: HSO是一个高效的双层优化框架，通过全局搜索和局部优化交替进行，在极低NFE条件下实现扩散模型加速采样，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有扩散概率模型采样过程缓慢，而现有的调度优化方法难以同时满足有效性、自适应性、实用鲁棒性和计算效率四个核心原则。

Method: 提出HSO双层优化框架：上层全局搜索最优初始化策略，下层局部优化调度细化；引入中点误差代理(MEP)和间距惩罚适应度(SPF)函数。

Result: 在极低NFE条件下达到最先进性能，NFE为5时在Stable Diffusion v2.1上获得11.94的FID，优化成本仅需8秒。

Conclusion: HSO提供了一种实用高效的扩散模型加速范式，无需重新训练即可显著提升采样效率。

Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.

</details>


### [443] [Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2511.11690)
*Fei Song,Yi Li,Rui Wang,Jiahuan Zhou,Changwen Zheng,Jiangmeng Li*

Main category: cs.LG

TL;DR: 提出双重去偏测试时提示调优方法，通过动态检索增强调制和可靠性感知提示优化来缓解提示优化偏差问题


<details>
  <summary>Details</summary>
Motivation: 测试时提示调优在零样本设置下表现出色，但仅基于未标记测试数据调优可能导致提示优化偏差，影响下游任务性能

Method: 使用动态检索增强调制模块从动态知识库检索高置信度知识，结合可靠性感知提示优化模块进行置信度加权集成和跨模态一致性蒸馏

Result: 在15个基准数据集上的实验表明，该方法在自然分布偏移和跨数据集泛化方面优于基线方法

Conclusion: 所提出的双重去偏方法能有效缓解提示优化偏差，提高视觉语言模型的泛化性能

Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.

</details>


### [444] [Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues](https://arxiv.org/abs/2511.11691)
*Seham Nasr,Zhao Ren,David Johnson*

Main category: cs.LG

TL;DR: 提出了一个可解释AI框架，通过量化显著区域内的声学线索幅度，将显著区域与专家参考的语音情感声学线索联系起来，提高语音情感识别模型的解释质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于显著性的方法虽然能突出频谱图区域，但无法显示这些区域是否对应有意义的声学情感标记，限制了忠实性和可解释性。

Method: 提出一个框架，通过量化显著区域内的线索幅度，将显著性与专家参考的语音情感声学线索连接起来。

Result: 在基准SER数据集上的实验表明，该方法通过明确将显著区域与理论驱动的语音情感声学线索联系起来，提高了解释质量。

Conclusion: 相比标准显著性方法，提供了更易理解和合理的SER模型解释，为可信赖的基于语音的情感计算奠定了基础。

Abstract: Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies "what" is highlighted and connects it to "why" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.

</details>


### [445] [AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation](https://arxiv.org/abs/2511.11692)
*Jiayin Zhu,Linlin Yang,Yicong Li,Angela Yao*

Main category: cs.LG

TL;DR: 本文提出AnchorDS方法，通过将文本到3D优化重新表述为动态源分布到固定目标分布的映射，解决了传统SDS方法中的语义过度平滑问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的文本到3D方法将2D生成模型的指导视为静态，忽略了源动态，导致语义线索被抑制或合并，产生语义过度平滑伪影。

Method: 将问题转化为双条件潜空间，同时基于文本提示和中间渲染图像进行条件化。引入AnchorDS机制，提供状态锚定指导，并设计了轻量级过滤和微调策略来优化锚点。

Result: AnchorDS能够生成更精细的细节、更自然的颜色和更强的语义一致性，特别是在复杂提示下表现优异，同时在效率上保持优势。

Conclusion: 该方法在质量和效率上都超越了先前的方法，为文本到3D生成提供了更稳定和高质量的解决方案。

Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.

</details>


### [446] [Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL](https://arxiv.org/abs/2511.11696)
*Xun Shao,Aoba Otani,Yuto Hirasuka,Runji Cai,Seng W. Loke*

Main category: cs.LG

TL;DR: 本文提出了一个面向老年人日常活动监测的下一代系统，旨在从跌倒检测扩展到全面的日常生活活动识别，采用隐私保护、边缘部署和联邦学习技术。


<details>
  <summary>Details</summary>
Motivation: 随着老龄化社会的发展，需要更全面的老年人监测系统来支持独立生活和尊严，而不仅仅是跌倒检测。现有ADL数据集不足，需要开发更智能的监测方案。

Method: 使用SISFall数据集及其GAN增强变体进行可行性验证，将跌倒检测作为代理任务；探索联邦学习在非IID条件下的应用；在Jetson Orin Nano设备上进行嵌入式部署。

Result: 获得了联邦学习在非IID条件下的初步结果，并成功在边缘设备上部署了系统，证明了技术可行性。

Conclusion: 这项工作标志着从单一任务检测向全面日常活动识别的转变，为可持续和以人为本的老年人护理AI提供了早期证据和发展路线图。

Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.

</details>


### [447] [Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification](https://arxiv.org/abs/2511.11697)
*Liqin Tan,Pin Chen,Menghan Liu,Xiean Wang,Jianhuan Cen,Qingsong Zou*

Main category: cs.LG

TL;DR: MatUQ是一个用于评估图神经网络在材料属性预测中分布外泛化能力和不确定性量化的基准框架，包含1,375个OOD任务，提出了新的结构感知分割策略SOAP-LOCO和不确定性度量D-EviU。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估GNN在材料科学中OOD泛化能力和不确定性量化的基准，需要为材料发现中的分布偏移提供可靠模型选择指导。

Method: 构建了1,375个OOD预测任务，使用五种OFM分割和新提出的SOAP-LOCO分割策略，评估12个代表性GNN模型，采用蒙特卡洛Dropout和深度证据回归的统一不确定性训练协议。

Result: 不确定性训练方法显著提高预测精度，在挑战性OOD场景中平均减少70.6%误差；D-EviU度量在大多数任务中与预测误差相关性最强；没有单一模型在所有任务中占优。

Conclusion: 该基准为材料发现中分布偏移下的可靠模型选择提供了实用见解，早期模型仍具竞争力，新模型在特定属性上表现更优。

Abstract: We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.

</details>


### [448] [Moirai 2.0: When Less Is More for Time Series Forecasting](https://arxiv.org/abs/2511.11698)
*Chenghao Liu,Taha Aksu,Juncheng Liu,Xu Liu,Hanshu Yan,Quang Pham,Doyen Sahoo,Caiming Xiong,Silvio Savarese,Junnan Li*

Main category: cs.LG

TL;DR: Moirai 2.0是一个基于解码器架构的时间序列基础模型，采用分位数预测和多令牌预测技术，在精度和推理效率方面均有提升。相比Moirai 1.0，新版本简化了架构，速度提升2倍，模型大小缩小30倍，同时性能更好。


<details>
  <summary>Details</summary>
Motivation: 开发一个更高效、更准确的时间序列预测基础模型，解决现有模型在精度、速度和模型大小之间的权衡问题。

Method: 采用解码器专用架构、单补丁输入和分位数损失函数，使用分位数预测和多令牌预测技术，在包含3600万序列的新语料库上训练。

Result: 在Gift-Eval基准测试中表现优异，在精度、速度和模型大小之间达到良好平衡。相比Moirai 1.0-Large，速度快2倍，模型小30倍，性能更优。消融研究表明解码器架构和递归多分位数解码对性能提升贡献最大。

Conclusion: Moirai 2.0在时间序列预测方面实现了显著的效率提升和性能改进，但参数数量增加时性能趋于平稳，长时预测性能下降，未来需要在数据扩展和长时建模方面继续研究。

Abstract: We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.

</details>


### [449] [Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification](https://arxiv.org/abs/2511.11699)
*Xingqi Lin,Liangyu Chen,Min Wu,Min Zhang,Zhenbing Zeng*

Main category: cs.LG

TL;DR: 提出了一种新的截断矩形棱柱方法来紧密逼近RNN中的Hadamard积产生的三维非线性曲面，通过最小化体积和表面积实现更紧密的过近似，显著提升了RNN鲁棒性验证的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对非线性激活函数进行线性逼近时，单独处理每个非线性部分会导致显著的过估计，从而降低验证准确性。需要一种更紧密的过近似方法来提高RNN鲁棒性验证的精度。

Method: 提出截断矩形棱柱方法，使用两个线性松弛平面来包围Hadamard积产生的三维非线性曲面，并通过细化驱动方法最小化棱柱的体积和表面积，实现更紧密的过近似。

Result: 实验结果表明，DeepPrism在图像分类、语音识别和情感分析等多种任务中，相比最先进方法有显著改进。

Conclusion: 所提出的截断矩形棱柱方法能够更紧密地逼近非线性激活函数，显著提高了RNN鲁棒性验证的准确性和有效性。

Abstract: Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.

</details>


### [450] [Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2511.11701)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 提出基于贝叶斯神经网络和蒙特卡洛dropout的概率性电价预测框架，按小时分别建模以捕捉日周期模式，在点预测和区间预测方面均优于传统基准模型。


<details>
  <summary>Details</summary>
Motivation: 电力市场电价波动性大，传统点预测无法捕捉不确定性，限制了风险管理效用，需要概率性预测方法。

Method: 使用贝叶斯神经网络配合蒙特卡洛dropout技术，为每天24小时分别训练独立模型，捕捉电价日周期特性。

Result: 所提模型在点预测和区间预测方面均优于GARCHX和LEAR基准模型。

Conclusion: 该工作为在能源市场预测中应用概率性神经网络模型提供了参考依据。

Abstract: Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.

</details>


### [451] [Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom](https://arxiv.org/abs/2511.11703)
*Hugo Huang*

Main category: cs.LG

TL;DR: 提出两种基于语义分割的新输入表示方法(SS-only和RGB+SS)，在ViZDoom死亡竞赛中显著降低内存消耗并提升强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D环境中强化学习面临的两个主要挑战：内存缓冲区的高内存消耗，以及在部分可观测马尔可夫决策过程中的学习复杂性。

Method: 使用语义分割技术处理RGB彩色图像，提出SS-only和RGB+SS两种输入表示方法，并在ViZDoom死亡竞赛环境中进行实验评估。

Result: SS-only方法将内存缓冲区消耗降低66.6%-98.6%；RGB+SS方法通过额外语义信息显著提升RL智能体性能。

Conclusion: 语义分割是解决3D环境中RL内存消耗和POMDP学习复杂性的有效方法，密度热图可视化有助于分析智能体移动模式。

Abstract: Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.

</details>


### [452] [Simple Vision-Language Math Reasoning via Rendered Text](https://arxiv.org/abs/2511.11704)
*Matvey Skripkin,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.LG

TL;DR: 提出了一种轻量级但有效的训练流程，通过将LaTeX编码的数学公式渲染成图像，并与结构化的思维链提示配对，来训练视觉语言模型解决数学问题。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种简单但高效的方法，使紧凑的多模态架构能够实现最先进的数学推理准确性，同时保持广泛的通用领域能力。

Method: 使用文本到视觉的增强方法，将LaTeX编码的方程渲染为图像，并与结构化的思维链提示配对，训练视觉语言模型。

Result: 该方法在广泛使用的基准测试中持续匹配或超越了开源和专有的数学专用视觉语言求解器，同时在MMMU、ChartQA和DocVQA等任务上取得了高达20%的性能提升。

Conclusion: 渲染保真度和提示设计是性能的主要驱动因素，这种简单的方法在保持通用领域能力的同时，实现了卓越的数学推理性能。

Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.

</details>


### [453] [Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs](https://arxiv.org/abs/2511.11705)
*Arya Narang*

Main category: cs.LG

TL;DR: 研究验证了在菜品热量估计中，加入菜品名称文本信息相比仅使用图像能带来统计显著的改进，多模态模型使MAE从84.76千卡降低到83.70千卡。


<details>
  <summary>Details</summary>
Motivation: 探究短文本输入（菜品名称）能否在热量估计任务中相比纯图像基线模型带来显著改进。

Method: 使用TensorFlow库和Nutrition5k数据集，训练了纯图像CNN和多模态CNN（同时接受文本和图像输入）。

Result: 多模态模型将热量估计的MAE从84.76千卡降低到83.70千卡，减少了1.06千卡（1.25%的改进）。

Conclusion: 短文本输入确实能改善热量估计性能，且改进具有统计显著性。

Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.

</details>


### [454] [Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling](https://arxiv.org/abs/2511.11706)
*Julia Peters,Karin Mora,Miguel D. Mahecha,Chaonan Ji,David Montero,Clemens Mosig,Guido Kraemer*

Main category: cs.LG

TL;DR: 提出一个统一的多模态地球观测表征学习框架，能够在高时空分辨率下整合不同遥感传感器数据，生成生态分析就绪的嵌入表示。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测基础模型通常在固定时空尺度下运行，限制了需要精细空间细节和高时间保真度的生态分析应用。

Method: 采用两阶段设计：首先独立建模各传感器特征，然后将表征融合到共享模型中。使用Sentinel-1和Sentinel-2数据作为代表性模态，在原生10米分辨率和无云Sentinel-2采集频率下生成潜在空间。

Result: 定性分析显示学习到的嵌入在异质景观中具有高空间和语义一致性；定量评估表明这些嵌入编码了生态有意义的模式，并保持足够的时间保真度以支持精细尺度分析。

Conclusion: 该框架为需要不同空间和时间分辨率的环境应用提供了灵活的分析就绪表征学习方法，能够捕捉互补的遥感数据并保持时空一致性。

Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.

</details>


### [455] [FSC-Net: Fast-Slow Consolidation Networks for Continual Learning](https://arxiv.org/abs/2511.11707)
*Mohamed El Gorrim*

Main category: cs.LG

TL;DR: FSC-Net提出双网络架构解决持续学习中的灾难性遗忘问题，通过快速网络学习新任务，慢速网络进行知识巩固，在Split-MNIST和Split-CIFAR-10上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 受神经科学中记忆巩固机制启发，解决神经网络在持续学习中因灾难性遗忘而丢失先前知识的问题。

Method: 采用双网络架构：快速网络(NN1)立即适应新任务，慢速网络(NN2)通过蒸馏和重放进行知识巩固。发现纯重放策略优于蒸馏方法。

Result: 在Split-MNIST上达到91.71%保留准确率（比快速网络单独高4.27pp），在Split-CIFAR-10上达到33.31%保留准确率（比快速网络单独高8.20pp）。

Conclusion: 双时间尺度巩固机制而非架构复杂性是缓解灾难性遗忘的关键，简单MLP架构优于复杂变体，纯重放策略效果最佳。

Abstract: Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.

</details>


### [456] [Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control](https://arxiv.org/abs/2511.11711)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 将Model-X knockoffs方法引入稀疏自编码器特征选择，通过knockoff+控制错误发现率，为机制可解释性提供可复现的可靠特征发现框架


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在识别神经网络可解释特征时难以区分真实计算模式和错误相关性，需要更可靠的特征选择方法

Method: 使用Model-X knockoffs结合SAE特征选择，通过Gaussian surrogate模拟潜在分布，用knockoff+控制FDR，在有限样本下提供统计保证

Result: 在Pythia-70M情感分类任务中，从512个高活性SAE潜在特征中选出129个特征（FDR q=0.1），约25%潜在特征携带任务相关信号，选中的特征与非选中特征在knockoff统计量上呈现5.40倍分离

Conclusion: 通过将SAE与多重测试感知推理相结合，为可靠特征发现提供了可复现的原则性框架，推进了机制可解释性的基础

Abstract: Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.

</details>


### [457] [Reasoning: From Reflection to Solution](https://arxiv.org/abs/2511.11712)
*Zixi Li*

Main category: cs.LG

TL;DR: 论文提出推理是状态空间中迭代算子应用并收敛到固定点的过程，并通过OpenLM架构在OpenXOR问题上实现76%准确率，而现有LLMs为0%。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否真正学会了推理，还是仅仅在模式匹配推理痕迹，需要理解推理的本质要求并构建能提供真正推理能力的架构。

Method: 提出推理是状态空间中迭代算子应用收敛到固定点的定义，开发了OpenOperator理论和OpenLM架构来解决OpenXOR问题。

Result: OpenLM在OpenXOR问题上达到76%准确率，而当前最先进的LLMs准确率为0%，验证了所提推理定义的有效性。

Conclusion: 推理需要特定的架构支持，而不仅仅是模式匹配，通过状态空间中的迭代算子应用可以实现真正的推理能力。

Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.

</details>


### [458] [Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data](https://arxiv.org/abs/2511.11714)
*Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del Rio,Oleksii Sliusarenko,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 使用联邦学习在多个医院间协作训练肺炎检测模型，数据保持本地化，性能显著提升（准确率47.5%，AUC 50.0%增益）


<details>
  <summary>Details</summary>
Motivation: 解决医疗数据隐私法规限制下，分散在不同医院的胸部X光数据无法集中化处理的问题，同时应对数据分布不均和医院间差异性挑战

Method: 采用Sherpa.ai联邦学习平台，模拟多医院协作场景，使用非独立同分布数据训练CXR肺炎分类器，数据保持在各医院本地

Result: 联邦学习模型达到0.900准确率和0.966 ROC-AUC，相比单医院模型（0.610准确率，0.644 AUC）分别提升47.5%和50.0%

Conclusion: 联邦学习能够在不转移患者数据的情况下，实现高性能、可泛化、安全私密的肺炎检测，特别适用于罕见疾病和低数据领域

Abstract: Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.

</details>


### [459] [Multiscale Grassmann Manifolds for Single-Cell Data Analysis](https://arxiv.org/abs/2511.11717)
*Xiang Xiang Wang,Sean Cottrell,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出基于Grassmann流形的多尺度框架，用于单细胞数据分析，通过整合不同几何视图的特征到统一的流形空间中，有效保留数据结构并提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统单细胞数据分析方法将细胞表示为欧几里得空间中的向量，限制了捕捉内在相关性和多尺度几何结构的能力。

Method: 基于Grassmann流形的多尺度框架，通过生成多个表示尺度的嵌入，将不同几何视图的特征整合到统一的Grassmann流形中，并引入基于幂的尺度采样函数来控制尺度选择。

Result: 在九个基准单细胞RNA-seq数据集上的实验表明，该方法能有效保留有意义的结构并提供稳定的聚类性能，尤其适用于中小型数据集。

Conclusion: Grassmann流形为单细胞数据分析提供了连贯且信息丰富的基础框架。

Abstract: Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.

</details>


### [460] [Fast 3D Surrogate Modeling for Data Center Thermal Management](https://arxiv.org/abs/2511.11722)
*Soumyendu Sarkar,Antonio Guillen-Perez,Zachariah J Carmichael,Avisek Naug,Refik Mert Cam,Vineet Gundecha,Ashwin Ramesh Babu,Sahand Ghorbanpour,Ricardo Luna Gutierrez*

Main category: cs.LG

TL;DR: 开发基于视觉的替代建模框架，用于数据中心3D温度场实时预测，实现20000倍加速，支持实时冷却控制和负载重分配，节省7%能源消耗。


<details>
  <summary>Details</summary>
Motivation: 减少数据中心能耗和碳排放，传统CFD求解器计算成本高且需要专家设置，不适合实时应用。

Method: 使用3D体素化表示数据中心的视觉替代建模框架，评估多种架构包括3D CNN U-Net变体、3D傅里叶神经算子和3D视觉变换器。

Result: 替代模型在数据中心配置间具有良好泛化能力，实现高达20000倍加速（数百毫秒vs数小时），准确估计热点和温度分布。

Conclusion: 该框架支持实时冷却控制和负载重分配，带来显著能源节约（7%）和碳足迹减少。

Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.

</details>


### [461] [Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm](https://arxiv.org/abs/2511.11727)
*Tongda Xu*

Main category: cs.LG

TL;DR: 本文揭示了在扩散模型中优化条件输入时，去噪分数匹配与精确分数匹配之间的等价性被破坏，导致偏差和高分数范数的问题。


<details>
  <summary>Details</summary>
Motivation: 许多近期工作使用去噪分数匹配来优化扩散模型的条件输入，但作者发现这种方法会破坏去噪分数匹配与精确分数匹配之间的等价性。

Method: 通过理论分析和实验观察，研究了在扩散模型中优化条件输入时产生的偏差问题，并分析了该偏差对分数范数的影响。

Result: 发现这种偏差会导致更高的分数范数，并且在用预训练扩散模型优化数据分布时也存在类似偏差。该问题影响多个领域的工作。

Conclusion: 去噪分数匹配优化条件输入会产生偏差，影响包括MAR、PerCo、DreamFusion等多个应用领域，需要引起重视。

Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.

</details>


### [462] [Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics](https://arxiv.org/abs/2511.11734)
*Kamalpreet Singh Kainth,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedat Panat*

Main category: cs.LG

TL;DR: PI-NODE-SR框架通过结合低阶显式求解器和残差归一化，稳定了刚性生物物理系统的训练，能够在有限迭代预算下准确预测振荡频率和幅度。


<details>
  <summary>Details</summary>
Motivation: 标准神经微分方程和物理信息变体在建模刚性生物物理系统时不可靠，需要大量迭代且可能收敛到次优解，无法保持振荡频率或幅度。

Method: 引入物理信息神经ODE与尺度感知残差(PI-NODE-SR)，结合低阶显式求解器(Heun方法)和残差归一化，平衡不同时间尺度状态变量的贡献。

Result: 在Hodgkin-Huxley方程上，PI-NODE-SR从单个振荡中学习并外推超过100ms，准确捕捉振荡频率和接近正确的幅度，恢复门控变量中的尖锐亚阈值曲率等形态特征。

Conclusion: PI-NODE-SR相对于基线神经ODE和PINNs持续减少长期误差，为稳定高效学习刚性生物动力学提供了原则性途径。

Abstract: Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.

</details>


### [463] [KAN/H: Kolmogorov-Arnold Network using Haar-like bases](https://arxiv.org/abs/2511.11736)
*Susumu Katayama*

Main category: cs.LG

TL;DR: KAN/H是一种KAN变体，使用Haar变体基系统替代B样条，包含全局和局部基函数，应用于函数逼近和MNIST分类，无需大量超参数调优。


<details>
  <summary>Details</summary>
Motivation: 改进Kolmogorov-Arnold网络，通过使用Haar变体基系统来避免B样条方法所需的大量超参数调优，提高模型的实用性和易用性。

Method: 提出KAN/H网络，采用包含全局和局部基函数的Haar变体基系统替代传统的B样条基函数，应用于函数逼近和MNIST分类任务。

Result: KAN/H在函数逼近和MNIST分类任务上表现良好，且不需要大多数问题特定的超参数调优，简化了模型部署过程。

Conclusion: KAN/H通过使用Haar变体基系统成功减少了超参数调优的需求，为KAN网络的实际应用提供了更实用的解决方案。

Abstract: This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.

</details>


### [464] [DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks](https://arxiv.org/abs/2511.11737)
*Qizhe Li,Haolong Chen,Jiansheng Li,Shuqi Chai,Xuan Li,Yuzhou Hou,Xinhua Shao,Fangfang Li,Kaifeng Han,Guangxu Zhu*

Main category: cs.LG

TL;DR: DK-Root是一个联合数据和知识驱动的框架，用于移动网络QoE劣化根因分析，通过对比学习预训练、条件扩散增强和专家标签微调实现高精度诊断


<details>
  <summary>Details</summary>
Motivation: 移动网络QoE劣化根因诊断面临复杂跨层KPI交互和专家标注稀缺的挑战，基于规则的启发式方法生成标签存在噪声且粒度粗糙

Method: 1) 使用对比表示学习预训练编码器，通过监督对比目标去噪规则标签；2) 引入类条件扩散模型生成保留根因语义的KPI序列；3) 编码器和轻量级分类器使用稀缺专家标签联合微调

Result: 在真实运营商级数据集上的实验表明，DK-Root超越了传统机器学习和近期半监督时间序列方法，达到最先进精度

Conclusion: 消融实验证实了条件扩散增强和预训练-微调设计的必要性，验证了表示质量和分类性能的提升

Abstract: Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.

</details>


### [465] [Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts](https://arxiv.org/abs/2511.11743)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mackenzie J. Meni,Carlos Andrés Duran Paredes,Eric Arazo,Cristian Bosch,Ricardo Simon Carbajo,Yuan Lai,Leo Anthony Celi*

Main category: cs.LG

TL;DR: 提出好奇心驱动的量化混合专家框架，通过贝叶斯认知不确定性路由在异构专家间分配任务，在保持99.9%精度的同时实现4倍压缩和41%能耗节省，显著降低延迟方差82%。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上部署深度神经网络面临两个关键挑战：在激进量化下保持精度，同时确保可预测的推理延迟。

Method: 采用好奇心驱动的量化混合专家框架，使用贝叶斯认知不确定性在异构专家（BitNet三元、1-16位BitLinear、训练后量化）间进行路由分配。

Result: 4位量化在音频分类基准上保持99.9%的16位精度（F1分数0.858 vs 0.859），实现4倍压缩和41%能耗节省，路由机制将MoE延迟方差降低82%（从230ms到29ms标准差）。

Conclusion: 信息论路由证明自适应量化能够产生准确、节能且可预测的边缘模型，对于大多数部署场景，简单的4位量化架构优于复杂的MoE架构。

Abstract: Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.

</details>


### [466] [Diffusion Models: A Mathematical Introduction](https://arxiv.org/abs/2511.11746)
*Sepehr Maleki,Negar Pourmoazemi*

Main category: cs.LG

TL;DR: 本文提供了扩散生成模型的简明自包含推导，从高斯分布基本性质出发，系统构建了去噪扩散概率模型的理论框架，包括前向加噪过程、反向后验分布、变分下界等核心概念，并扩展到加速采样、连续时间公式和引导扩散等高级主题。


<details>
  <summary>Details</summary>
Motivation: 为扩散模型提供一个透明、代数清晰的理论推导，让读者既能理解理论又能实现相应算法，填补现有文献中推导步骤不够明确的空白。

Method: 从高斯分布的基本性质（密度、二次期望、重参数化、乘积和KL散度）出发，系统构建去噪扩散概率模型，包括前向加噪过程、闭式边际分布、精确离散反向后验和相关变分下界。

Result: 建立了完整的扩散模型理论框架，推导出标准的噪声预测目标，并扩展到加速采样（DDIM、DDGAN）、多尺度变体（嵌套和潜在扩散）、连续时间公式（概率流ODE、流匹配）以及引导扩散（分类器引导和无分类器引导）。

Conclusion: 通过透明代数和明确中间步骤，提供了扩散生成模型的完整理论推导，使读者能够同时理解理论并实现相应算法，为扩散模型的研究和应用提供了坚实的理论基础。

Abstract: We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.

</details>


### [467] [IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation](https://arxiv.org/abs/2511.11750)
*Hanting Yan,Pan Mu,Shiqi Zhang,Yuchao Zhu,Jinglin Zhang,Cong Bai*

Main category: cs.LG

TL;DR: 提出IDOL框架，通过物理先验知识构建身份导向约束，解决热带气旋估计中的分布偏移问题，提升在分布外场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 热带气旋估计面临复杂动态环境场导致的分布偏移挑战，现有方法忽视特征表示的内在分布，导致在分布外场景下泛化能力差。

Method: 提出身份分布导向的物理不变学习框架(IDOL)，利用风场模型和暗相关知识构建任务共享和任务特定的身份令牌，通过物理先验知识约束特征空间。

Result: 在多个数据集和任务上的实验表明，IDOL在热带气旋风速、气压、内核和外核尺寸估计中表现出色，有效缓解了分布偏移问题。

Conclusion: 基于物理先验知识的身份导向约束能有效处理热带气旋估计中的多样分布偏移，提升模型的鲁棒性和泛化能力。

Abstract: Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.

</details>


### [468] [Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain](https://arxiv.org/abs/2511.11753)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.LG

TL;DR: 提出了一种混合GraphSAGE网络(H-GSN)用于供应链物流管理的多任务预测，包括货物类型、物流状态、交通状况、物流ID和物流延迟等目标，在三个不同数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 供应链中系统化物流、运输设施和仓储信息对盈利发展至关重要。需要自动预测方法来提高供应链管理效率，增强供应链的韧性和可持续性。

Method: 使用混合GraphSAGE网络(H-GSN)进行多任务物流管理，预测货物类型、物流状态、交通状况、物流ID和物流延迟等目标，基于三个Kaggle供应链物流数据库(DataCo、Shipping和Smart Logistics)。

Result: 在Smart Logistics数据集上，10种物流ID预测平均准确率97.8%，3种交通状况预测准确率100%；在DataCo数据集上货物类型预测准确率98.7%；在Shipping数据集上物流延迟预测准确率99.4%。

Conclusion: 所提出的方法在不同物流场景下的评估指标证实了其有效性，能够提高供应链的韧性和可持续性。

Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.

</details>


### [469] [Sumudu Neural Operator for ODEs and PDEs](https://arxiv.org/abs/2511.11762)
*Ben Zelenskiy,Saibilila Abudukelimu,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: SNO是基于Sumudu变换的神经算子，通过多项式展开分解输入空间，在Sumudu空间中参数化神经算子。在ODE和PDE任务中表现优异，在PDE上优于FNO，与LNO竞争，在Euler-Bernoulli梁和扩散方程上误差最低，支持零样本超分辨率。


<details>
  <summary>Details</summary>
Motivation: 探索Sumudu变换作为神经算子设计的潜力，特别是针对特定类型的偏微分方程。

Method: 利用变换对的多项式展开关系分解输入空间为系数，然后转换到Sumudu空间进行神经算子参数化。

Result: 在PDE任务中表现优于FNO，与LNO竞争，在Euler-Bernoulli梁和扩散方程上取得最低误差，零样本超分辨率能从低质量样本获得更高质量数据。

Conclusion: 初步结果表明Sumudu变换作为神经算子设计具有前景，特别适用于某些类型的偏微分方程。

Abstract: We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.

</details>


### [470] [Learning Fair Representations with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.11767)
*Amisha Priyadarshini,Sergio Gago-Masague*

Main category: cs.LG

TL;DR: 本文提出了一种将Kolmogorov-Arnold网络（KANs）集成到公平对抗学习框架中的方法，通过自适应惩罚更新机制动态调整公平约束，在保持高预测准确性的同时实现公平性。


<details>
  <summary>Details</summary>
Motivation: 现有公平学习模型在公平性和准确性之间难以达到最优平衡，且黑盒模型缺乏可解释性，限制了在敏感领域（如大学录取）的应用。

Method: 将KANs集成到公平对抗学习框架中，利用KANs的对抗鲁棒性和可解释性，并提出了自适应惩罚更新机制来动态调整训练过程中的公平约束。

Result: 在两个真实世界大学录取数据集上的实验表明，该方法在三种不同优化策略下均优于基线公平学习模型，在保持高预测准确性的同时实现了跨敏感属性的竞争性公平。

Conclusion: 该方法通过结合KANs和自适应公平约束机制，有效解决了公平性和准确性之间的权衡问题，为敏感领域的公平决策提供了可解释且高效的解决方案。

Abstract: Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.

</details>


### [471] [CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments](https://arxiv.org/abs/2511.11778)
*Byoungjun Park,Pedro Porto Buarque de Gusmão,Dongjin Ji,Minhoe Kim*

Main category: cs.LG

TL;DR: CATCHFed是一个解决半监督联邦学习中标签数据稀缺问题的框架，通过自适应阈值、混合阈值和一致性正则化来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习场景中客户端通常缺乏标签数据，而现有半监督联邦学习方法在标签数据减少时性能显著下降。

Method: 提出客户端感知的自适应阈值（考虑类别难度）、混合阈值提升伪标签质量，并利用未伪标签数据进行一致性正则化。

Result: 在多种数据集和配置下的实验表明，CATCHFed能有效利用未标记客户端数据，在极有限标签设置下仍能获得优越性能。

Conclusion: CATCHFed通过创新的阈值设计和数据利用策略，成功解决了半监督联邦学习中的标签稀缺问题。

Abstract: Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.

</details>


### [472] [Coordinate Descent for Network Linearization](https://arxiv.org/abs/2511.11781)
*Vlad Rakhlin,Amir Jevnisek,Shai Avidan*

Main category: cs.LG

TL;DR: 提出一种基于坐标下降的离散优化方法，直接减少ResNet网络中ReLU激活函数的数量，以降低私有推理的延迟。


<details>
  <summary>Details</summary>
Motivation: ReLU激活函数是私有推理中的主要瓶颈，显著增加推理延迟。现有方法使用平滑近似优化，但最后的硬阈值步骤会导致较大性能损失。

Method: 采用坐标下降优化框架，直接在离散域中工作，通过设计产生稀疏解来减少ReLU数量。

Result: 在常见基准测试中达到最先进水平。

Conclusion: 该方法通过离散优化有效减少ReLU数量，在保持网络精度的同时显著降低推理延迟。

Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.

</details>


### [473] [Simplicial covering dimension of extremal concept classes](https://arxiv.org/abs/2511.11819)
*Ari Blondal,Hamed Hatami,Pooya Hatami,Chavdar Lalov,Sivan Tretiak*

Main category: cs.LG

TL;DR: 该论文将拓扑维度理论中的Lebesgue覆盖维度概念扩展到二元概念类，定义了单纯覆盖维度，并证明对于有限概念类，该维度精确刻画了PAC学习中的列表可复制数（等价于全局稳定性）。


<details>
  <summary>Details</summary>
Motivation: 将经典的拓扑维度理论应用于机器学习中的概念类分析，建立拓扑维度与学习理论中可复制性之间的联系。

Method: 将概念类与可实现分布空间关联，通过损失函数和概念类在该空间上诱导单纯结构，并定义单纯覆盖维度。

Result: 证明对于有限概念类，单纯覆盖维度精确等于列表可复制数，从而可以应用经典维度理论工具计算极值概念类的精确列表可复制数。

Conclusion: 成功建立了拓扑维度与学习理论中可复制性度量的深刻联系，为分析概念类的可复制性提供了新的数学工具。

Abstract: Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.
  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.

</details>


### [474] [Conformal Constrained Policy Optimization for Cost-Effective LLM Agents](https://arxiv.org/abs/2511.11828)
*Wenwen Si,Sooyong Jang,Insup Lee,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出CCPO方法，通过组合不同成本/准确度的LLM模型，在保证可靠性的前提下最小化计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大，但计算成本和API成本日益昂贵，需要寻找成本效益更高的部署方案

Method: 使用CCPO训练范式，结合约束策略优化、离策略强化学习和在线保形预测，联合优化成本感知策略和自适应阈值

Result: 在两个多跳问答基准测试中，相比其他成本感知基线和LLM引导方法，成本降低高达30%，同时保持可靠性

Conclusion: 该方法为部署LLM代理提供了原则性和实用的框架，在保持可靠性的同时显著提高成本效益

Abstract: While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.

</details>


### [475] [Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers](https://arxiv.org/abs/2511.11834)
*Vahid Hemmati,Ahmad Mohammadi,Abdul-Rauf Nuhu,Reza Ahmari,Parham Kebria,Abdollah Homaifar*

Main category: cs.LG

TL;DR: 本文研究了VC（Volatility in Certainty）这一无需标签的指标，通过测量排序softmax输出的离散度来量化模型置信度的不规则性，发现VC与分类准确率呈强负相关，可作为实时系统中对抗性攻击的早期预警指标。


<details>
  <summary>Details</summary>
Motivation: 在实时系统中部署神经网络分类器时，对抗性鲁棒性是一个关键挑战，特别是在推理阶段无法获得真实标签的情况下。需要一种无需标签的指标来检测模型性能下降和对抗性漂移。

Method: 定义VC为相邻确定性值的平均平方对数比，捕捉模型输出平滑度的局部波动。在MNIST和CIFAR-10数据集上使用ANN、CNN和VGG-like模型进行实验，使用FGSM生成对抗样本，并通过逐步引入对抗性污染创建混合测试集。

Result: 分类准确率与log(VC)之间存在强负相关性（大多数情况下rho < -0.90），表明VC能够有效反映性能下降，无需标记数据。

Conclusion: VC是一种可扩展、架构无关的实时性能指标，适用于安全关键应用中的早期预警系统。

Abstract: Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.

</details>


### [476] [Leveraging Exogenous Signals for Hydrology Time Series Forecasting](https://arxiv.org/abs/2511.11849)
*Junyang He,Judy Fox,Alireza Jafari,Ying-Jung Chen,Geoffrey Fox*

Main category: cs.LG

TL;DR: 本研究探讨了在时间序列模型中融入领域知识对水文降雨-径流建模的影响，发现包含全面已知外生输入的模型优于基础模型，其中自然年周期时间序列的融入贡献最大。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型研究取得进展，但很少研究其在物理科学特定下游应用中的有效性。本研究旨在探索将领域知识整合到时间序列模型中对水文降雨-径流建模的作用。

Method: 使用CAMELS-US数据集（包含671个位置的降雨和径流数据，具有6个时间序列流和30个静态特征），比较基线和基础模型，特别关注融入领域知识的方法。

Result: 结果表明，包含全面已知外生输入的模型表现优于更有限的方法，包括基础模型。自然年周期时间序列的融入带来了最显著的改进。

Conclusion: 在水文建模中，融入领域知识特别是自然年周期时间序列，能够显著提升模型性能，这为时间序列基础模型在物理科学应用中的优化提供了重要启示。

Abstract: Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.

</details>


### [477] [Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production](https://arxiv.org/abs/2511.11880)
*David Montero,Miguel D. Mahecha,Francesco Martinuzzi,César Aybar,Anne Klosterhalfen,Alexander Knohl,Jesús Anaya,Clemens Mosig,Sebastian Wieneke*

Main category: cs.LG

TL;DR: 比较GPT-2和LSTM两种深度学习模型在预测森林CO2吸收(GPP)方面的表现，发现LSTM整体精度更高但GPT-2在极端事件中表现更好，同时分析了不同输入特征的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决现有遥感方法在捕捉GPP复杂时间动态方面的局限性，探索深度学习模型在融合多源数据预测GPP方面的潜力。

Method: 使用GPT-2（Transformer架构）和LSTM（循环神经网络）两种代表性模型，结合多变量输入数据（包括辐射、Sentinel-2、MODIS地表温度、Sentinel-1等）进行GPP预测。

Result: 两种模型达到相似精度，LSTM整体表现更好但GPT-2在极端事件中更优；LSTM使用更短的输入窗口即可达到相似精度；辐射是最重要的预测因子。

Conclusion: 模型架构、上下文长度和多模态输入共同决定了GPP预测性能，为未来开发监测陆地碳动态的深度学习框架提供指导。

Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.

</details>


### [478] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: PasoDoble是一个无监督的LLM对抗训练框架，通过让Proposer模型生成有挑战性的问题和Solver模型解决问题的双角色对抗训练，提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM训练严重依赖外部监督（如人工标注），而对抗学习特别是自博弈训练可以减少对外部监督的依赖。但将双角色对抗训练应用于LLM仍面临奖励破解和训练不稳定的挑战。

Method: 从同一基础模型初始化两个模型：Proposer生成带有真实答案的挑战性问题，Solver尝试解决这些问题。Proposer从预训练数据集中获取知识以确保问题质量和多样性。为避免奖励破解，Proposer仅在生成有效且能挑战Solver极限的问题时获得奖励，Solver在正确解决问题时获得奖励，两者联合更新。还引入了可选的离线训练范式来增强稳定性。

Result: 实验结果表明，PasoDoble能够提升LLM的推理性能。

Conclusion: PasoDoble框架成功实现了无监督的LLM对抗训练，通过双角色自博弈机制有效提升了模型的推理能力，为减少对外部监督的依赖提供了可行方案。

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [479] [FLEX: Feature Importance from Layered Counterfactual Explanations](https://arxiv.org/abs/2511.11891)
*Nawid Keshtmand,Roussel Desmond Nzoyem,Jeffrey Nicholas Clark*

Main category: cs.LG

TL;DR: FLEX框架将反事实解释转化为局部、区域和全局层面的特征变化频率评分，弥合了局部反事实解释与全局特征归因之间的差距。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型缺乏可解释性限制了在高风险场景中的安全部署，现有反事实解释通常局限于实例层面，无法量化特征在特征空间连贯区域或整个数据集中的系统性驱动作用。

Method: FLEX是一个模型和领域无关的框架，通过聚合实例和邻域的反事实解释，将反事实集合转换为特征变化频率评分，兼容不同的反事实生成方法。

Result: 在交通事故严重性预测和贷款审批任务中，FLEX的全局排名与SHAP相关但揭示了更多驱动因素，区域分析发现了全局总结遗漏的上下文特定因素。

Conclusion: FLEX弥合了局部反事实解释与全局归因之间的差距，支持风险敏感应用中透明且面向干预的决策制定。

Abstract: Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable "what-if" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.

</details>


### [480] [Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design](https://arxiv.org/abs/2511.11894)
*Lingxiao Li,Haobo Zhang,Bin Chen,Jiayu Zhou*

Main category: cs.LG

TL;DR: 提出Chain-of-Generation (CoG)框架，通过多阶段潜在扩散模型解决文本条件分子生成中的一次性条件限制问题，提高语义对齐和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的分子生成模型采用一次性条件编码，难以同时满足提示中的所有要求，存在生成组件可解释性差、无法生成所有子结构以及过度野心等问题。

Method: CoG将提示分解为课程排序的语义片段，逐步将其作为中间目标，引导去噪轨迹朝着满足越来越丰富语言约束的分子发展，并引入后对齐学习阶段加强文本和分子潜在空间的对应关系。

Result: 在基准和实际任务上的广泛实验表明，CoG比一次性基线方法产生更高的语义对齐、多样性和可控性，生成的分子更忠实地反映复杂组合提示，同时提供生成过程的透明洞察。

Conclusion: CoG框架通过多阶段生成过程有效解决了文本条件分子生成中的关键挑战，为复杂语言约束下的分子设计提供了更可靠和可解释的解决方案。

Abstract: Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.

</details>


### [481] [Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm](https://arxiv.org/abs/2511.11902)
*Ci Lin,Tet Yeap,Iluju Kiringa,Biwei Zhang*

Main category: cs.LG

TL;DR: 提出了一种无梯度训练算法B-SRA来提升双向联想记忆(BAM)的鲁棒性，通过正交权重矩阵和梯度模式对齐原则增强模型抗噪和抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 传统双向联想记忆(BAM)使用双向反向传播(B-BP)训练时存在鲁棒性差、对噪声和对抗攻击敏感的问题。

Method: 提出双向子空间旋转算法(B-SRA)，引入正交权重矩阵(OWM)和梯度模式对齐(GPA)正则化策略，通过消融研究确定最优配置。

Result: SAME配置(结合OWM和GPA)在所有方法中表现出最强的鲁棒性，在不同攻击场景和记忆容量(50/100/200对)下均取得优异性能。

Conclusion: B-SRA和提出的正则化策略显著提升了联想记忆的鲁棒性，为构建弹性神经网络架构开辟了新方向。

Abstract: Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.

</details>


### [482] [A Systematic Study of Model Extraction Attacks on Graph Foundation Models](https://arxiv.org/abs/2511.11912)
*Haoyan Xu,Ruizhi Qian,Jiate Li,Yushun Dong,Minghao Lin,Hanson Yan,Zhengtao Yao,Qinghua Liu,Junhao Dong,Ruopeng Huang,Yue Zhao,Mengyuan Li*

Main category: cs.LG

TL;DR: 本文首次系统研究了针对图基础模型(GFMs)的模型提取攻击(MEAs)，揭示了GFMs显著扩大了MEA攻击面，攻击者仅需极小成本即可近似受害者模型，几乎不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 图基础模型(GFMs)联合预训练图和文本编码器，统一结构和语义理解，支持零样本推理，但由于高预训练成本和跨领域知识，成为模型提取攻击的有吸引目标。现有研究仅关注单图上的小型图神经网络，对大规模多模态GFMs的安全影响尚未探索。

Method: 提出黑盒威胁模型，定义六种攻击场景，引入轻量级提取方法，通过监督回归图嵌入训练攻击者编码器，无需对比预训练数据即可保持与受害者文本编码器对齐。

Result: 在七个数据集上的实验表明，攻击者仅需原始训练成本的极小部分即可近似受害者模型，准确性几乎无损失。

Conclusion: GFMs极大扩展了MEA攻击面，突显了在大规模图学习系统中部署感知安全防御的必要性。

Abstract: Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.

</details>


### [483] [Batch Matrix-form Equations and Implementation of Multilayer Perceptrons](https://arxiv.org/abs/2511.11918)
*Wieger Wesselink,Bram Grooten,Huub van de Wetering,Qiao Xiao,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: 本文提供了多层感知机(MLP)的完整批处理矩阵形式规范，包括前向和反向传播的数学推导、符号验证、多框架实现，并展示了显式公式在稀疏计算中的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管MLP是现代深度学习的基础，但其算法细节很少以完整的批处理矩阵形式呈现，大多数文献采用逐样本梯度或自动微分。显式的批处理矩阵形式对于透明分析、系统优化（特别是稀疏神经网络）至关重要。

Method: 1. 推导所有标准层和高级层（包括批归一化和softmax）的前向和反向传播方程；2. 使用SymPy符号数学库验证所有梯度方程；3. 在NumPy、PyTorch、JAX、TensorFlow和优化的C++后端构建统一参考实现；4. 基于少量矩阵原语构建实现。

Result: 1. 完成了MLP批处理矩阵形式反向传播的完整推导；2. 所有梯度方程都经过符号验证；3. 建立了基于统一矩阵原语的Python和C++参考实现；4. 证明了显式公式化能够实现高效的稀疏计算。

Conclusion: 这些结果为理解、教学和研究神经网络算法建立了一个经过验证、可扩展的基础，特别在稀疏计算等优化场景中具有重要价值。

Abstract: Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.

</details>


### [484] [Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks](https://arxiv.org/abs/2511.11928)
*Ziyao Cui,Edric Tam*

Main category: cs.LG

TL;DR: 本文提出了插值拉普拉斯嵌入(ILEs)，这是一种从图矩阵家族中导出的谱嵌入方法，用于在节点特征有限时增强图神经网络(GNNs)的性能。


<details>
  <summary>Details</summary>
Motivation: GNNs的性能严重依赖于信息丰富的节点特征，但在实际应用中这些特征往往有限或缺失。虽然拉普拉斯谱嵌入是自然选择，但作者探索是否其他图矩阵的谱嵌入也能提供有用的表示。

Method: 引入了插值拉普拉斯嵌入(ILEs)，它来自一个简单但表达力强的图矩阵家族。使用谱图理论工具来解释ILEs捕获的结构信息。

Result: 通过模拟和真实世界数据集的实验表明，通过ILEs进行特征增强可以提高常用GNN架构的性能。

Conclusion: 这项工作提供了一个简单实用的方法，扩展了实践者在节点特征有限时的谱增强工具包。

Abstract: Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.

</details>


### [485] [A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts](https://arxiv.org/abs/2511.11934)
*C. César Claros Olivares,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: 本文系统比较了CLIP分层机制下的OOD检测方法，发现特征空间学习对OOD检测效果起决定性作用。概率性评分在误分类检测中表现最佳，而几何感知方法在强分布偏移下更有效，且不同架构（CNN vs ViT）的最优方法存在差异。


<details>
  <summary>Details</summary>
Motivation: 当前OOD检测方法缺乏在不同表示范式（CNN从头训练 vs ViT微调）下的系统性比较，需要为不同分布偏移场景提供统计可靠的方法选择指导。

Method: 使用AURC和AUGRC作为主要指标，在CIFAR-10/100、SuperCIFAR-100和TinyImageNet数据集上，采用多重比较控制的基于排名的分析流程（Friedman检验与Conover-Holm事后检验）和Bron-Kerbosch团分析。

Result: 特征空间学习是OOD检测效果的决定因素。CNN和ViT上概率性评分（MSR、GEN）在误分类检测中占优；强偏移下几何感知方法（NNGuide、fDBD、CTM）在CNN上表现更好，而ViT上GradNorm和KPCA重建误差保持竞争力。

Conclusion: 研究支持以表示为中心的OOD检测视角，为分布偏移下的方法选择提供了统计基础指导，并发现简单PCA投影可提升多个检测器的性能。

Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.

</details>


### [486] [SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis](https://arxiv.org/abs/2511.11935)
*Munib Mesinovic,Tingting Zhu*

Main category: cs.LG

TL;DR: SurvBench是一个开源预处理流水线，将原始PhysioNet数据集转换为标准化的多模态生存分析张量，解决深度学习生存模型的可复现性问题。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据为深度学习生存分析提供了巨大机会，但由于预处理方法不一致，可复现性受到严重限制。

Method: 提供三个主要重症监护数据库的数据加载器，实施严格的数据质量控制、患者级分割、缺失值跟踪和标准化时间聚合，支持单风险和竞争风险场景。

Result: 输出与pycox库兼容，支持标准统计和深度学习模型，填补了阻碍深度学习生存模型公平比较的"预处理差距"。

Conclusion: SurvBench通过提供可复现的配置驱动预处理，使研究人员能够专注于方法创新而非数据工程。

Abstract: Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.

</details>


### [487] [Learning the relative composition of EEG signals using pairwise relative shift pretraining](https://arxiv.org/abs/2511.11940)
*Christopher Sandino,Sayeri Lala,Geeling Chau,Melika Ayoughi,Behrooz Mahasseni,Ellen Zippi,Ali Moin,Erdrin Azemi,Hanlin Goh*

Main category: cs.LG

TL;DR: 提出PARS预训练方法，通过预测EEG窗口对的相对时间偏移来学习神经信号的长程依赖关系，在多种EEG解码任务中优于现有自监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前EEG自监督学习方法主要使用掩码重建策略，关注局部时间模式，而能够学习长程依赖关系的位置预测预训练方法尚未充分探索。

Method: 引入PARS预训练，这是一种新颖的前置任务，通过预测随机采样的EEG窗口对之间的相对时间偏移来学习相对时间组成和长程依赖关系。

Result: 在多种EEG解码任务中，PARS预训练的transformers在标签效率和迁移学习设置中始终优于现有的预训练策略。

Conclusion: PARS为自监督EEG表示学习建立了新范式，能够有效捕捉神经信号中的长程依赖关系。

Abstract: Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.

</details>


### [488] [Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation](https://arxiv.org/abs/2511.11949)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TL;DR: FedBacys是一个电池感知的联邦学习框架，通过基于用户电池水平的循环客户端参与来减少能量消耗，特别适用于能量收集联邦学习系统。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式学习中很强大，但客户端训练模型的计算复杂度增加导致显著的能量消耗。在能量收集联邦学习系统中，由于能量有限，每个设备的参与可用性会波动，这是一个关键挑战。

Method: 提出FedBacys框架，通过聚类客户端并基于电池水平进行顺序调度，最小化冗余计算。还提出FedBacys-Odd变体，允许客户端选择性参与以进一步降低能量成本。

Result: 通过数值实验证明，与现有算法相比，FedBacys具有更优的能量效率和鲁棒性。

Conclusion: FedBacys框架通过电池感知的循环客户端参与，有效减少了系统范围内的能量使用，提高了学习稳定性，同时提供了收敛性分析。

Abstract: Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.

</details>


### [489] [Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression](https://arxiv.org/abs/2511.11973)
*Xinming Gao,Shangzhe Li,Yujin Cai,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出了一种改进的离线强化学习方法，通过量化回归估计温度系数β，并引入值正则化技术，解决了XQL和MXQL方法需要大量超参数调优和训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在固定数据集上学习策略而无需与环境交互，在高风险或成本高昂的领域特别有价值。但现有的XQL及其稳定变体MXQL存在显著局限性：需要针对每个数据集和领域进行大量超参数调优，且在训练过程中表现出不稳定性。

Method: 提出了一种基于量化回归的原则性方法来估计温度系数β，并在温和假设下进行。为了进一步提高训练稳定性，引入了受约束值学习启发的值正则化技术，具有温和的泛化性。

Result: 实验结果表明，所提出的算法在包括D4RL和NeoRL2在内的一系列基准任务中实现了竞争性或更优的性能，同时保持了稳定的训练动态，并在所有数据集和领域中使用一致的超参数集。

Conclusion: 该方法成功解决了XQL和MXQL方法的超参数敏感性和训练不稳定性问题，在多个基准任务上表现出色，为离线强化学习的实际应用提供了更可靠的解决方案。

Abstract: Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $β$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.

</details>


### [490] [Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238)
*Patrick Parschan,Charlott Jakob*

Main category: cs.LG

TL;DR: 本文对无监督和半监督的基于文本的理想点估计算法进行了首次系统性回顾，分析了这些从文本数据推断政治立场的算法发展历程、方法论家族及其应用指导。


<details>
  <summary>Details</summary>
Motivation: 过去20年基于文本的理想点估计算法发展迅速但领域碎片化，缺乏系统比较和应用指导，需要整合这一领域的发展脉络。

Method: 通过系统性文献回顾识别了25种算法，进行手动内容分析，并提出了区分文本方差生成、捕获和聚合的概念框架，将算法分为词频、主题建模、词嵌入和LLM四大方法论家族。

Result: 建立了四个方法论家族的分类体系，分析了各方法的假设、可解释性、可扩展性和局限性，为应用研究者提供了算法选择的实用指导。

Conclusion: 算法间的估计差异本身具有信息价值，强调了系统基准测试的必要性，为未来研究提供了结构化框架和实践指南。

Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

</details>


### [491] [ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting](https://arxiv.org/abs/2511.11991)
*Xiang Ma,Taihua Chen,Pengcheng Wang,Xuemei Li,Caiming Zhang*

Main category: cs.LG

TL;DR: 提出了ReCast框架，通过可学习码本将局部模式编码为离散嵌入，使用双路径架构分别处理规则结构和不规则波动，并通过可靠性感知的码本更新策略实现轻量级、鲁棒的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖全局分解为趋势、季节性和残差分量，对于现实世界中由局部、复杂和高度动态模式主导的时间序列效果不佳，且模型复杂度高限制了在实时或资源受限环境中的应用。

Method: 使用基于码本的补丁量化将局部模式编码为离散嵌入，采用双路径架构（量化路径建模规则结构，残差路径重建不规则波动），并通过融合多个可靠性因素的分布鲁棒优化方案实现码本的可靠性感知更新。

Result: 在准确性、效率和适应分布变化能力方面均优于最先进模型。

Conclusion: ReCast框架通过局部模式编码和可靠性感知更新，实现了轻量级且鲁棒的时间序列预测，在多种场景下表现优异。

Abstract: Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \textbf{RE}liability-aware \textbf{C}odebook-\textbf{AS}sisted \textbf{T}ime series forecasting framework (\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.

</details>


### [492] [Selecting Fine-Tuning Examples by Quizzing VLMs](https://arxiv.org/abs/2511.12002)
*Tenghao Ji,Eytan Adar*

Main category: cs.LG

TL;DR: QZLoRA是一个通过QuizRank方法自动选择高质量训练图像来进行LoRA微调的框架，能够用更少的样本生成更对齐、更逼真的图像。


<details>
  <summary>Details</summary>
Motivation: 在微调文本到图像扩散模型时，从质量参差不齐的图像集（如维基共享资源）中选择好的训练样本很困难，但高质量的训练图像能确保生成图像具有代表性特征。

Method: 提出QZLoRA框架，利用QuizRank方法将图像视为'教育干预'并通过VLM'测验'来自动排名图像，用于低秩适应（LoRA）微调。

Result: QZLoRA能够用更少的样本生成更对齐、更逼真的图像，并且这些微调后的模型也能生成具有代表性的风格化图像（如插画）。

Conclusion: 将自动视觉推理与参数高效微调相结合，在主题自适应生成建模方面具有广阔前景。

Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.

</details>


### [493] [EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation](https://arxiv.org/abs/2511.12033)
*Jiahe Shi,Zhengqi Gao,Ching-Yun Ko,Duane Boning*

Main category: cs.LG

TL;DR: EARL是一个基于熵感知强化学习的Verilog生成框架，通过选择性地对高熵令牌进行梯度更新，提高了RTL代码生成的功能正确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在硬件设计自动化中生成RTL代码时存在语法错误、功能幻觉和与设计意图对齐不足的问题，需要更有效的强化学习方法来提升生成质量。

Method: 提出EARL框架，使用可验证奖励信号进行策略优化，并引入熵引导的选择性更新机制，将策略梯度限制在高熵令牌上，专注于影响控制流和模块结构的关键代码区域。

Result: 在VerilogEval和RTLLM数据集上的实验表明，EARL相比之前的LLM基线将功能通过率提高了14.7%，同时减少了不必要的更新并改善了训练稳定性。

Conclusion: 将强化学习聚焦于关键的高不确定性令牌，能够实现更可靠和有针对性的策略改进，适用于结构化RTL代码生成任务。

Abstract: Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.

</details>


### [494] [Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers](https://arxiv.org/abs/2511.12041)
*Shivam Barwey,Pinaki Pal*

Main category: cs.LG

TL;DR: 开发了一种首创的多尺度图变换器方法（SR-GT），用于反应流动的网格超分辨率重建，在复杂几何和非均匀网格上表现出色，优于传统插值方法。


<details>
  <summary>Details</summary>
Motivation: 超分辨率流动重建对于亚网格/亚滤波器闭合建模、加速时空预测、数据压缩以及作为稀疏实验测量的升尺度工具具有重要价值。

Method: 采用基于图的流动场表示方法，结合变换器架构捕捉低分辨率流动场中的长程依赖关系，识别重要特征，生成保留这些特征的高分辨率流动场。

Result: 在2D氢气-空气预混混合物中爆轰传播的挑战性测试问题中，SR-GT在反应流动场特征上提供了高精度的超分辨率重建。

Conclusion: SR-GT框架在复杂多尺度反应流动行为中表现出优越性能，为网格超分辨率提供了新的数据驱动建模范式。

Abstract: Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.

</details>


### [495] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: P1系列是首个通过强化学习训练的开源物理推理模型，在2025年国际物理奥林匹克竞赛中获得金牌表现，并在13个国际/地区物理竞赛中赢得12枚金牌，展现了卓越的物理推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展需要从谜题解决转向科学级推理，物理学作为连接符号与现实的基础学科，是测试这种转变的最佳领域。

Method: 通过强化学习训练P1系列开源物理推理模型，并配备代理框架PhysicsMinions来增强推理能力。

Result: P1-235B-A22B在IPhO 2025中获得金牌表现，在13个物理竞赛中赢得12枚金牌；P1-30B-A3B获得银牌；配备PhysicsMinions后获得IPhO 2025总分第一。

Conclusion: P1模型不仅在物理推理方面表现卓越，在数学和编程等其他推理任务上也展现出强大的泛化能力，证明了该系列模型的通用性。

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


### [496] [Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread](https://arxiv.org/abs/2511.12071)
*Rosario Napoli,Gabriele Morabito,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: 提出了一种集成知识补全阶段的图机器学习流程，通过发现稀疏数据集中的隐式知识来改进图嵌入表示质量


<details>
  <summary>Details</summary>
Motivation: 由于图嵌入仅从显式拓扑和特征中学习，可能错过稀疏数据集中隐藏的关键隐式知识，影响图结构和表示质量

Method: 在嵌入生成前加入知识补全阶段，专注于传递关系，使用基于衰减的推理函数建模隐藏连接，重塑图拓扑结构

Result: 实验表明该流程显著改变了嵌入空间的几何结构，证明其不仅是简单的丰富，而是重新定义图表示质量的变革性步骤

Conclusion: 集成知识补全的图机器学习流程能够发现隐式知识，从根本上改进图嵌入表示，是图表示学习的重要进展

Abstract: The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.

</details>


### [497] [Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies](https://arxiv.org/abs/2511.12075)
*Dong-Hee Shin,Deok-Joong Lee,Young-Han Son,Tae-Eui Kam*

Main category: cs.LG

TL;DR: TreatStitch是一个用于增强离线强化学习的数据增强框架，通过智能拼接现有治疗轨迹片段来生成临床有效的合成治疗轨迹，解决临床数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 临床环境中无法使用在线强化学习，而离线强化学习受限于数据稀缺问题，需要一种方法来生成更多样化的训练数据。

Method: 提出Treatment Stitching方法：识别不同轨迹中的相似中间患者状态并拼接相应片段；对于不相似状态，使用薛定谔桥方法生成平滑的桥接轨迹。

Result: 在多个治疗数据集上的实验表明，TreatStitch能有效提升离线强化学习的性能。

Conclusion: TreatStitch通过数据增强提高了离线强化学习在自适应治疗策略优化中的效果，同时保持临床有效性。

Abstract: Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schrödinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.

</details>


### [498] [SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling](https://arxiv.org/abs/2511.12092)
*Yu Zheng,Kezhi Wang,Wenji Xi,Gang Yu,Jiming Chen,Jie Zhang*

Main category: cs.LG

TL;DR: SenseRay-3D是一个基于RGB-D扫描直接预测3D路径损耗热图的端到端框架，无需显式几何重建或材料标注，实现了可扩展、高效且物理一致的室内传播建模。


<details>
  <summary>Details</summary>
Motivation: 现有室内无线电传播建模方法依赖人工建模几何和材料特性，导致可扩展性和效率受限。

Method: 构建感知驱动的体素化场景表示，联合编码占用率、电磁材料特性和收发器几何信息，通过SwinUNETR神经网络推断环境路径损耗。

Result: 在未见环境中实现4.27 dB的平均绝对误差，支持217 ms/样本的实时推理，展示了可扩展性、效率和物理一致性。

Conclusion: SenseRay-3D为感知驱动、可泛化且物理一致的室内传播建模开辟了新途径，超越了先前的EM DeepRay框架。

Abstract: Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.

</details>


### [499] [Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks](https://arxiv.org/abs/2511.12122)
*Yi Wang,Ruoyi Fang,Anzhuo Xie,Hanrui Feng,Jianlin Lai*

Main category: cs.LG

TL;DR: 提出基于Transformer的实时会计交易异常检测方法，通过时间序列建模和多头自注意力机制捕捉全局依赖关系，在公开数据集上验证了优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂交易环境中隐藏异常行为和高时效性要求的挑战，为智能财务风控和审计提供方法支持。

Method: 将多维会计交易数据建模为时间序列矩阵，使用嵌入层和位置编码实现低维映射，构建多头自注意力序列建模结构，结合前馈层和正则化策略进行深度特征表示和异常概率估计。

Result: 在AUC、F1-Score、精确率和召回率等指标上均优于基线模型，在不同环境条件和数据扰动下保持稳定性能。

Conclusion: 基于Transformer的框架在会计交易动态异常检测中具有适用性和优势，为智能财务风险控制提供了有效的方法支持。

Abstract: This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.

</details>


### [500] [HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.12123)
*Zejiao Liu,Junqi Tu,Yitian Hong,Luolin Xiong,Yaochu Jin,Yang Tang,Fangfei Li*

Main category: cs.LG

TL;DR: 提出了一种基于指挥者的联合策略框架HCPO，通过协调多智能体探索来提升合作效率，在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有合作多智能体强化学习方法通常通过独立智能体探索来更新联合策略，缺乏智能体间的协调，限制了联合策略的表达能力和探索效率。

Method: 提出基于指挥者的联合策略框架，开发分层指挥者策略优化算法(HCPO)，通过指挥者协调智能体策略更新方向，保持集中训练优势同时消除执行时的通信需求。

Result: 在StarCraftII、Multi-agent MuJoCo和Multi-agent Particle Environment三个基准测试中，HCPO在合作效率和稳定性方面均优于竞争性MARL基线方法。

Conclusion: HCPO通过协调多智能体探索有效提升了联合策略的表达能力和性能，理论分析证明了联合策略优化过程的单调性，在多个挑战性环境中展现出优越性能。

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.

</details>


### [501] [FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates](https://arxiv.org/abs/2511.12132)
*Zhenqiang Ye,Jinjie Lu,Tianlong Gu,Fengrui Hao,Xuemin Wang*

Main category: cs.LG

TL;DR: FairGSE是一个通过最大化二维结构熵来改进图神经网络公平性的框架，在提升公平性的同时显著降低假阳性率。


<details>
  <summary>Details</summary>
Motivation: 现有公平感知的GNN在追求公平性指标时忽视了模型预测负标签的能力，导致极高的假阳性率，这在高风险场景中会产生负面影响。

Method: 提出FairGSE框架，通过最大化二维结构熵来改进公平性，同时关注假阳性率问题。

Result: 在多个真实数据集上的实验表明，FairGSE相比最先进的公平感知GNN将假阳性率降低了39%，同时保持相当的公平性改进。

Conclusion: 在改进公平性时应仔细校准分类性能，而不仅仅是约束准确率损失，FairGSE为此提供了有效解决方案。

Abstract: Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.

</details>


### [502] [Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion](https://arxiv.org/abs/2511.12139)
*Sahar Moghimian Hoosh,Ilia Kamyshev,Henni Ouerdane*

Main category: cs.LG

TL;DR: 提出一个端到端的非侵入式负载监测分类框架，融合ICA和PCA特征提取方法，使用轻量级神经网络Fusion-ResNet，在多个设备同时运行时仍保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界NILM部署面临的过拟合、模型泛化能力差以及多设备同时运行时的分解挑战。

Method: 提出包含高频标记数据、特征提取方法和轻量级神经网络的端到端框架，引入融合ICA和PCA的新型特征提取方法，设计轻量级多标签分类架构Fusion-ResNet。

Result: 提出的基于特征的模型在平均F1分数和不同设备上的表现优于现有NILM分类器，同时最小化训练和推理时间，在多达15个设备同时运行时仍保持相对鲁棒性。

Conclusion: Fusion-ResNet框架有效解决了NILM中的关键挑战，在多设备同时运行的应力条件下仍能保持良好性能。

Abstract: Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.

</details>


### [503] [Variation-Bounded Loss for Noise-Tolerant Learning](https://arxiv.org/abs/2511.12143)
*Jialiang Wang,Xiong Zhou,Xianming Liu,Gangfeng Hu,Deming Zhai,Junjun Jiang,Haoliang Li*

Main category: cs.LG

TL;DR: 提出了一种新的鲁棒损失函数家族——变分有界损失(VBL)，通过引入变分比作为衡量损失函数鲁棒性的新属性，并证明较小的变分比能带来更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 减轻噪声标签对监督学习的负面影响是一个长期存在的问题，鲁棒损失函数是解决该问题的流行方案。

Method: 引入变分比作为损失函数鲁棒性的新属性，提出变分有界损失(VBL)家族，并对变分比进行理论分析，证明较小变分比能带来更好鲁棒性。基于变分比将常用损失函数重新表述为变分有界形式。

Result: 在各种数据集上的实验验证了该方法的有效性和灵活性。

Conclusion: 变分比提供了放松对称条件的可行方法，为实现非对称条件提供了更简洁的路径，所提出的VBL方法在噪声标签环境下表现出良好的鲁棒性。

Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.

</details>


### [504] [Finding Time Series Anomalies using Granular-ball Vector Data Description](https://arxiv.org/abs/2511.12147)
*Lifeng Shen,Liang Peng,Ruiwen Liu,Shuyin Xia,Yi Liu*

Main category: cs.LG

TL;DR: GBOC是一种基于粒度球向量数据描述的异常检测方法，通过密度引导的分层分裂过程生成紧凑的高密度区域表示，在训练时对齐样本与最近的粒度球中心，推理时基于距离计算异常分数。


<details>
  <summary>Details</summary>
Motivation: 传统方法如最近邻和聚类在动态非线性时间序列异常检测中依赖刚性假设（如预定义的可靠邻居或聚类数量），在复杂时序场景中经常失效。

Method: 提出粒度球单类网络（GBOC），基于粒度球向量数据描述（GVDD）将潜在空间划分为由粒度球表示的紧凑高密度区域，通过密度引导的分层分裂过程生成并去除噪声结构。

Result: 大量实验验证了该方法的有效性和优越性，在处理时间序列异常检测挑战方面表现出色。

Conclusion: 通过关注密集高质量区域并显著减少原型数量，GBOC在异常检测中实现了鲁棒性和效率的平衡。

Abstract: Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.

</details>


### [505] [Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions](https://arxiv.org/abs/2511.12154)
*Gustavo Polleti,Marlesson Santana,Eduardo Fontes*

Main category: cs.LG

TL;DR: 提出了一个多模态基础模型，整合结构化属性和非结构化文本描述，通过掩码语言建模处理交易序列，在数据稀缺的开放银行场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统特征工程和离散事件序列方法在金融交易分析中存在局限性，特别是在数据稀缺的开放银行场景下需要更有效的解决方案。

Method: 采用多模态基础模型，将结构化属性和非结构化文本描述整合为统一表示，并应用掩码语言建模到交易序列中。

Result: 模型在北美数千家金融机构的大规模研究中表现出色，超越了传统方法，特别是在数据稀缺场景下，且能够跨地域和机构泛化。

Conclusion: 自监督模型在金融应用中具有巨大潜力，可推动从欺诈预防、信用风险到客户洞察等多个领域的发展。

Abstract: We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights

</details>


### [506] [Rethinking Deep Alignment Through The Lens Of Incomplete Learning](https://arxiv.org/abs/2511.12155)
*Thong Bach,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: 论文揭示了语言模型安全对齐的机制性缺陷：自回归训练中的位置依赖性梯度削弱导致信号衰减，造成后期响应区域的安全学习不完整。作者提出基于基础模型偏好的标记作为计算指标，并开发了针对性补全方法来增强对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管进行了广泛的安全对齐，大语言模型仍存在系统性对抗攻击漏洞。研究发现这是由于自回归训练中的位置依赖性梯度削弱导致安全学习不完整，特别是在后期响应区域。

Method: 引入基础偏好标记作为计算指标，开发针对性补全方法，包括自适应惩罚和混合教师蒸馏，以解决未充分训练的区域。

Result: 在Llama和Qwen模型系列上的实验评估显示对抗鲁棒性显著提升，攻击成功率降低48-98%，同时保持通用能力。

Conclusion: 这项工作为安全对齐方法的基本局限性建立了机制性理解，并提供了实用的解决方案，显著增强了模型对抗攻击的鲁棒性。

Abstract: Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.

</details>


### [507] [Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis](https://arxiv.org/abs/2511.12158)
*Houtan Ghaffari,Lukas Rauch,Paul Devos*

Main category: cs.LG

TL;DR: 提出了一种轻量级的神经网络架构Residual-MLP-RNN和三阶段训练流程，用于在标注数据稀缺情况下实现高效的鸟类鸣叫音节检测。


<details>
  <summary>Details</summary>
Motivation: 鸟类鸣叫研究需要精确的音节级标注，但人工标注成本高昂，需要开发自动化和数据高效的方法来降低标注成本。

Method: 使用Residual-MLP-RNN架构，采用三阶段训练：自监督预训练（掩码预测和在线聚类）、监督训练（数据增强）、半监督后训练（利用未标注数据）。

Result: 在Canary这种复杂鸣叫的极端标签稀缺场景下验证了方法的有效性，Canary是最难标注的鸟类之一，间接验证了方法对其他鸟类的适用性。

Conclusion: 该方法在数据稀缺情况下实现了可靠的鸟类鸣叫音节检测，并展示了自监督嵌入在线性探测和无监督鸟类鸣叫分析中的潜力。

Abstract: Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.

</details>


### [508] [FGM optimization in complex domains using Gaussian process regression based profile generation algorithm](https://arxiv.org/abs/2511.12171)
*Chaitanya Kumar Konda,Piyush Agrawal,Shivansh Srivastava,Manish Agrawal*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程回归的通用体积分数分布生成算法，用于设计任意形状功能梯度材料，并通过遗传算法优化寻找最优配置。


<details>
  <summary>Details</summary>
Motivation: 解决任意形状功能梯度材料设计的挑战，特别是在复杂形状域中生成平滑的材料分布并满足边界体积分数约束。

Method: 使用高斯过程回归生成体积分数分布，结合改进的遗传算法（使用投影算子替代标准模拟二进制交叉）进行优化。

Result: 算法能够处理复杂形状域，生成平滑的功能梯度材料分布，并通过多个热弹性优化示例验证了方法的有效性。

Conclusion: 提出的方法为任意形状功能梯度材料设计提供了通用框架，能够生成多样化的材料分布并找到最优配置。

Abstract: This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.

</details>


### [509] [TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective](https://arxiv.org/abs/2511.12174)
*Lifeng Shen,Xuyang Li,Lele Long*

Main category: cs.LG

TL;DR: TSGDiff是一个基于图神经网络的时间序列生成框架，将时间序列表示为动态图，通过扩散过程建模结构表示分布，并提出Topo-FID评分来评估生成时间序列的结构保真度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成方面表现出色，但生成时间序列数据仍然具有挑战性，因为需要捕捉复杂的时间依赖性和结构模式。

Method: 将时间序列表示为基于傅里叶谱特征和时间依赖性的动态图，使用图神经网络的编码器-解码器架构构建潜在空间，通过扩散过程建模时间序列的结构表示分布。

Result: 在真实世界数据集上的实验表明，TSGDiff能够生成高质量的时间序列数据，忠实地保留时间依赖性和结构完整性。

Conclusion: TSGDiff通过图视角重新思考时间序列生成，提出了有效的生成框架和评估指标，推动了合成时间序列生成领域的发展。

Abstract: Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.

</details>


### [510] [Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering](https://arxiv.org/abs/2511.12180)
*Ge Cheng,Shuo Wang,Yun Zhang*

Main category: cs.LG

TL;DR: 本文提出了SC-InfoNCE损失函数，通过引入可调节的收敛目标来灵活控制特征相似性对齐，在图像、图结构和文本任务上实现了稳定且强大的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管InfoNCE在对比学习中取得了经验成功，但其理论基础仍然有限。作者希望通过显式建模增强视图的特征空间和数据增强动态来深入理解InfoNCE的工作原理。

Method: 引入显式特征空间来建模样本的增强视图，使用转移概率矩阵捕捉数据增强动态。基于对InfoNCE工作原理的洞察，提出了SC-InfoNCE损失函数，通过缩放目标矩阵来灵活控制特征相似性对齐。

Result: 在包括图像、图结构和文本任务在内的基准数据集上的实验表明，SC-InfoNCE在不同领域都能实现稳定且强大的性能。

Conclusion: SC-InfoNCE通过引入可调节的收敛目标，能够更好地匹配下游数据的统计特性，为对比学习提供了更灵活和有效的训练目标。

Abstract: Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.

</details>


### [511] [Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?](https://arxiv.org/abs/2511.12188)
*Xuanyu Chen,Nan Yang,Shuai Wang,Dong Yuan*

Main category: cs.LG

TL;DR: 本文研究了联邦学习场景下模型规模扩展的理论基础，通过PAC-Bayes理论推导出泛化误差上界，发现最优模型规模与客户端数量呈负幂律关系，并实证验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断扩大，高质量训练数据日益枯竭，联邦学习成为利用边缘设备数据同时保护隐私的可行方案。但分布式训练数据给模型扩展带来了挑战，这一领域尚未充分探索。

Method: 使用PAC-Bayes理论推导联邦学习环境中随机算法训练模型的泛化误差上界，通过解析求解最小化该上界的模型规模，量化分布式训练数据对最优模型规模的影响。

Result: 理论分析表明，在总训练计算量不变的情况下，最优模型规模与客户端数量呈负幂律关系；切换到联邦学习会降低模型通过训练能达到的泛化性能上界；联邦场景下估计最优模型规模应基于客户端的平均训练计算量。

Conclusion: 本文填补了联邦学习中模型规模扩展的理论空白，提供了量化分析框架，并通过大量实验验证了理论结果的正确性，为联邦学习中的模型设计提供了重要指导。

Abstract: The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.

</details>


### [512] [Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data](https://arxiv.org/abs/2511.12191)
*Szymon Wojciechowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 提出了一种新的可靠方法来评估基于多目标算法的算法与返回单一解的方法，同时指出根据用户偏好从帕累托前沿中选择的解。


<details>
  <summary>Details</summary>
Motivation: 在机器学习任务中，多目标优化算法产生帕累托前沿，而传统方法返回单一解，现有评估方法无法可靠比较这两种不同类型的算法。

Method: 提出新的评估方法，专注于算法比较而非学习过程，通过选择符合用户偏好的帕累托前沿解来进行公平比较。

Result: 该方法为比较多目标优化算法与单一解算法提供了可靠框架，解决了现有评估方法中的显著差距。

Conclusion: 所提出的评估方法填补了分类器评估方法学中的重要空白，能够可靠地比较返回单一解的方法与返回帕累托前沿的算法。

Abstract: Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.
  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.

</details>


### [513] [MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization](https://arxiv.org/abs/2511.12199)
*Runhao Jiang,Chengzhi Jiang,Rui Yan,Huajin Tang*

Main category: cs.LG

TL;DR: 本文研究了膜电位分布与替代梯度函数之间的相互作用对脉冲神经网络鲁棒性的影响，提出了一种基于膜电位分布的替代梯度正则化方法，显著提升了SNN对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 替代梯度方法虽然提升了深度脉冲神经网络的性能，但也使其更容易受到对抗攻击。目前对梯度幅值（反映模型对输入扰动的敏感性）的研究不足，而梯度幅值主要由膜电位分布与替代梯度函数的相互作用决定。

Method: 提出膜电位分布驱动的替代梯度正则化方法，通过显式地基于膜电位分布与替代梯度函数的相互作用来正则化膜电位分布，从而增强鲁棒性。

Result: 在多个图像分类基准测试和不同网络架构上的实验表明，该方法显著提升了SNN对抗扰动的鲁棒性，并在不同网络配置、替代梯度函数变体和脉冲编码方案中表现出强泛化能力。

Conclusion: 通过调控膜电位分布与替代梯度函数的相互作用，可以有效降低SNN对输入扰动的敏感性，为提升脉冲神经网络的安全性和鲁棒性提供了新的思路。

Abstract: The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.

</details>


### [514] [AlignTree: Efficient Defense Against LLM Jailbreak Attacks](https://arxiv.org/abs/2511.12217)
*Gil Goren,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: AlignTree是一种高效防御机制，通过监控LLM激活并使用随机森林分类器检测未对齐行为，无需额外提示或辅助模型。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法要么计算成本高，要么容易被绕过，不适用于实际LLM系统。需要既鲁棒又计算高效的防御机制。

Method: 使用随机森林分类器监控LLM激活，结合拒绝方向（线性表示）和SVM信号（非线性特征）来检测有害内容。

Result: 在多个LLM和基准测试中证明了AlignTree的效率和鲁棒性。

Conclusion: AlignTree在增强模型对齐的同时保持最小计算开销，为实际LLM系统提供了实用防御方案。

Abstract: Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.

</details>


### [515] [Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling](https://arxiv.org/abs/2511.12222)
*Hangshuo Tian*

Main category: cs.LG

TL;DR: 本文分析了鸡群优化算法(CSO)与KLD自适应采样在粒子滤波中的相互作用，提出CSO的适应度驱动更新可近似为均方收缩，使粒子分布更集中，从而在相同统计误差下需要更少的粒子数量。


<details>
  <summary>Details</summary>
Motivation: 理解基于群体智能的粒子更新与KLD自适应采样之间的理论交互关系，目前尚未完全明确。

Method: 在简化建模框架下分析CSO更新步骤对粒子集分布的影响，将CSO的适应度驱动更新近似为均方收缩，并应用Karamata不等式分析期望粒子数量。

Result: 分析表明，在相同统计误差界限下，CSO增强的粒子滤波(CPF)比标准PF需要更低的期望粒子数量。

Conclusion: 该研究提供了一个可处理的理论框架来解释这些技术结合时观察到的计算效率，并为设计更高效的自适应滤波器提供了起点。

Abstract: Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.
  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.
  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.

</details>


### [516] [SCI: An Equilibrium for Signal Intelligence](https://arxiv.org/abs/2511.12240)
*Vishal Joshua Meesala*

Main category: cs.LG

TL;DR: SCI是一个将可解释性建模为受控状态的闭环控制理论框架，通过主动驱动手术精度指标来减少解释误差，在多个领域显著提升了解释稳定性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的静态解释方法存在解释误差大、稳定性差的问题，需要一种能够主动调节和优化解释质量的控制理论框架。

Method: SCI包含三个协调组件：可靠性加权的多尺度特征、知识引导的解释器生成可追踪标记和理由、以及配备回滚和信任区域保护的Lyapunov引导控制器。

Result: 在生物医学、工业和环境领域，SCI将解释误差降低25-42%（平均38%），同时保持AUC/F1在基线1-2个百分点内，并将SP方差从0.030降至0.011。

Conclusion: 将可解释性建模为控制目标能够在不同信号机制下产生更稳定、恢复更快且更可信的解释行为。

Abstract: We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] ("Surgical Precision") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.

</details>


### [517] [Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.12261)
*Zongxin Shen,Yanyong Huang,Dongjie Wang,Jinyuan Chang,Fengmao Lv,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: 提出CLIM-FS方法解决混合缺失的多视图无监督特征选择问题，通过联合学习特征选择和自适应数据填补，并利用共识聚类结构和跨视图局部几何结构增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临三个关键挑战：1）仅关注视图缺失问题，不适用于实践中更普遍的混合缺失场景；2）对视图间一致性和多样性的利用不足；3）缺乏理论分析阐明特征选择与数据填补在联合学习过程中的相互作用。

Method: 基于非负正交矩阵分解的特征选择模型，集成缺失视图和变量的填补，联合学习特征选择和自适应数据填补，充分利用共识聚类结构和跨视图局部几何结构。

Result: 在八个真实世界多视图数据集上的实验结果表明，CLIM-FS优于最先进的方法。

Conclusion: CLIM-FS有效解决了混合缺失的多视图无监督特征选择问题，提供了理论分析阐明其协作机制，并在实验中表现出优越性能。

Abstract: Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.

</details>


### [518] [Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks](https://arxiv.org/abs/2511.12265)
*Rui Wang,Zeming Wei,Xiyue Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 提出了一种名为校准对抗采样(CAS)的高效微调方法，通过多臂老虎机框架动态设计奖励并平衡探索与利用，提升DNNs在多维度鲁棒性上的整体表现。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗训练框架主要关注单一或有限攻击类型，导致DNNs在实际中仍面临未在训练中处理的攻击类型的安全隐患。

Method: 从多臂老虎机优化视角出发，考虑多个鲁棒性维度的动态和相互依赖特性，动态设计奖励并平衡探索与利用。

Result: 在基准数据集上的实验表明，CAS实现了优越的整体鲁棒性，同时保持了较高的干净准确率。

Conclusion: CAS为DNNs的鲁棒泛化提供了新的范式。

Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.

</details>


### [519] [MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing](https://arxiv.org/abs/2511.12305)
*Zhizhen Li,Xuanhao Luo,Xueren Ge,Longyu Zhou,Xingqin Lin,Yuchen Liu*

Main category: cs.LG

TL;DR: MMSense是一个多模态、多任务的基础模型，通过整合图像、雷达、LiDAR和文本数据，在统一特征空间中实现跨模态对齐，用于无线感知任务。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在无线通信中主要局限于单模态输入和特定信道目标，忽略了基础模型在统一无线感知方面的潜力。

Method: 将多模态数据转换为视觉兼容表示，使用模态门控机制自适应融合，基于视觉大语言模型实现统一特征对齐和指令驱动的任务适应，采用任务特定序列注意力和不确定性损失加权机制。

Result: 在真实无线场景数据集上的实验表明，该方法优于任务特定和大模型基线，在异构感知任务上展现出强泛化能力。

Conclusion: MMSense证明了多模态基础模型在统一无线感知中的有效性，为未来无线通信系统提供了新的研究方向。

Abstract: Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.

</details>


### [520] [Optimal Self-Consistency for Efficient Reasoning with Large Language Models](https://arxiv.org/abs/2511.12309)
*Austin Feng,Marius Alonso,Ambroise Odonnat*

Main category: cs.LG

TL;DR: 本文提出了Blend-ASC，一种新型的自一致性变体，通过动态分配样本到问题来提高样本效率，相比传统自一致性方法平均减少6.8倍样本使用量。


<details>
  <summary>Details</summary>
Motivation: 传统自一致性方法在大规模应用时计算成本过高，且缺乏对样本效率和扩展行为的统一理论分析。

Method: 基于模式估计和投票理论分析自一致性扩展行为，提出动态分配采样方案Blend-ASC，该方法是超参数无关的并能适应任意样本预算。

Result: Blend-ASC在样本效率上达到最先进水平，平均比传统自一致性方法少用6.8倍样本，优于固定分配和动态分配基线方法。

Conclusion: Blend-ASC方法在保持性能的同时显著提高了自一致性的效率，为大规模应用提供了实用解决方案。

Abstract: Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.

</details>


### [521] [Active Learning of Symbolic Automata Over Rational Numbers](https://arxiv.org/abs/2511.12315)
*Sebastian Hagedorn,Martín Muñoz,Cristian Riveros,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: 将L*算法扩展到符号自动机学习，支持有理数上的无限稠密字母表，使算法适用于新领域如RGX和时间序列分析。


<details>
  <summary>Details</summary>
Motivation: 传统的L*算法只能学习有限字母表上的DFA，限制了其在人工智能和软件工程中的应用范围。需要扩展该算法以处理无限稠密字母表（如有理数）。

Method: 扩展L*算法以学习符号自动机，其转换使用有理数上的谓词。算法保持多项式时间复杂性，查询次数与转换数量和谓词表示大小呈线性关系。

Result: 成功开发了能够学习符号自动机的扩展L*算法，该算法在查询复杂度方面是最优的。

Conclusion: 扩展后的L*算法显著拓宽了应用范围，使其能够处理无限稠密字母表，为RGX和时间序列等新领域提供了有效的自动机学习方法。

Abstract: Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.

</details>


### [522] [BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data](https://arxiv.org/abs/2511.12316)
*Zhijun Zeng,Junqing Chen,Zuoqiang Shi*

Main category: cs.LG

TL;DR: 提出了BlinDNO方法，用于从无序密度快照中恢复随机和量子动力系统的参数，在时间标签缺失的情况下学习分布到函数的神经算子。


<details>
  <summary>Details</summary>
Motivation: 研究在时间标签缺失场景下的逆问题，即只有从观测时间分布中采样的无序密度快照可用，需要从这些观测中恢复底层演化算子的参数。

Method: 提出BlinDNO架构，结合多尺度U-Net编码器和基于注意力的混合器，构建排列不变的分布到函数神经算子。

Result: 在广泛的随机和量子系统上的数值实验表明，BlinDNO能够可靠地恢复控制参数，并在冷冻电镜环境下的3D蛋白质折叠机制重建问题中持续优于现有神经逆算子基线。

Conclusion: BlinDNO方法在时间标签缺失的逆问题中表现出色，能够有效从无序密度观测中恢复系统参数。

Abstract: We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.

</details>


### [523] [LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment](https://arxiv.org/abs/2511.12340)
*Katarzyna Fojcik,Renaldas Zioma,Jogundas Armaitis*

Main category: cs.LG

TL;DR: 提出了一种基于梯度下降优化二进制逻辑门网络的方法，不仅选择逻辑门类型，还优化门之间的连接结构，显著减少了实现特定任务所需的逻辑门数量。


<details>
  <summary>Details</summary>
Motivation: 为了在考虑硬件约束的情况下高效部署机器学习模型，利用二进制逻辑门作为数字芯片的基本构建块，设计能够直接在这些单元上运行的模型以实现节能计算。

Method: 使用梯度下降方法同时优化逻辑门的选择和连接结构（连接组），通过优化连接来大幅减少所需逻辑门数量。

Result: LILogicNet模型仅用8,000个门在MNIST上达到98.45%测试准确率，训练时间不到5分钟；256,000个门的架构在CIFAR-10上达到60.98%准确率，超越同类模型。

Conclusion: 完全二值化的模型在推理时计算开销极小，非常适合在低功耗数字硬件上部署，为高效机器学习模型部署提供了新途径。

Abstract: Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.

</details>


### [524] [Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach](https://arxiv.org/abs/2511.12351)
*Bahareh Golchin,Banafsheh Rekabdar*

Main category: cs.LG

TL;DR: 提出了一种结合变分自编码器、LSTM-DQN、动态奖励塑形和主动学习的深度强化学习框架，用于多变量时间序列异常检测，在SMD和WADI数据集上表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列异常检测中的高维性、标记数据有限和传感器间复杂依赖关系等挑战。

Method: 使用VAE提取紧凑潜在表示和降噪，LSTM-DQN进行自适应序列异常分类，动态奖励塑形平衡探索与利用，主动学习模块选择最不确定样本进行标注。

Result: 在SMD和WADI数据集上的实验表明，该方法在F1分数和AU-PR指标上优于现有基线方法。

Conclusion: 结合生成建模、强化学习和选择性监督的方法能够实现准确且可扩展的实时多变量系统异常检测。

Abstract: Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.

</details>


### [525] [BitSnap: Checkpoint Sparsification and Quantization in LLM Training](https://arxiv.org/abs/2511.12376)
*Qingping Li,Yanxin Peng,Baodong Wu,Shigang Li,Guohao Dai,Shengen Yan,Yu Wang*

Main category: cs.LG

TL;DR: 提出了一种动态适应不同训练阶段和模型架构的检查点稀疏化和量化方法，实现高效压缩而不影响模型精度


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和复杂度增长，高效的检查点保存和加载对于管理存储、内存使用和容错性变得至关重要，现有工作未能全面优化这些方面

Method: 提出基于位掩码的稀疏化方法和基于聚类的量化方法，动态适应不同训练阶段和模型架构，平衡压缩比、速度和精度影响

Result: 在不同规模的LLM上实验表明，位掩码稀疏化方法实现16倍压缩比且不损害模型精度，聚类量化方法实现2倍压缩比且精度损失很小

Conclusion: 该方法为LLM训练提供了高效的检查点管理解决方案，显著优化了存储、内存使用和容错性

Abstract: As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.

</details>


### [526] [CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection](https://arxiv.org/abs/2511.12388)
*Zahra Zamanzadeh Darban,Qizhou Wang,Charu C. Aggarwal,Geoffrey I. Webb,Ehsan Abbasnejad,Mahsa Salehi*

Main category: cs.LG

TL;DR: 提出CEDL框架，将几何正态性直接嵌入判别目标，实现几何与判别学习的统一，无需后处理即可获得可解释的异常评分


<details>
  <summary>Details</summary>
Motivation: 现有监督异常检测方法难以泛化到训练分布之外，且异常评分需要显式映射或校准才能进行概率解释

Method: 通过中心化径向距离函数重新参数化sigmoid预测对数，在单一端到端公式中统一几何和判别学习

Result: 在表格、时间序列和图像数据上的广泛实验表明，CEDL在不同现实异常检测任务中实现了竞争性且平衡的性能

Conclusion: CEDL框架有效且具有广泛适用性，能够实现几何正态性和标签判别的统一学习

Abstract: Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.

</details>


### [527] [On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions](https://arxiv.org/abs/2511.12398)
*Yulong Lu,Tong Mao,Jinchao Xu,Yahong Yang*

Main category: cs.LG

TL;DR: 本文构建了对称深度神经网络来逼近对称Korobov函数，证明了收敛率和常数因子最多随环境维度多项式增长，显著改善了之前受维度灾难影响的逼近保证。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络已被广泛用作具有固有物理结构（包括置换对称性）的函数的通用逼近器。本文旨在构建对称深度神经网络来逼近对称Korobov函数，并克服维度灾难问题。

Method: 构建对称深度神经网络来逼近对称Korobov函数，并分析其逼近性能。基于这些逼近界限，进一步推导学习对称Korobov函数的泛化误差率。

Result: 证明了收敛率和常数因子最多随环境维度多项式增长，这显著优于之前受维度灾难影响的逼近保证。基于这些逼近界限，推导出的泛化误差率的主导因子同样避免了维度灾难。

Conclusion: 对称深度神经网络能够有效逼近对称Korobov函数，其收敛率和泛化误差率都避免了维度灾难，为高维函数逼近提供了有效的理论保证。

Abstract: Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.

</details>


### [528] [Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario](https://arxiv.org/abs/2511.12409)
*Dhanesh Ramachandram,Anne Loefler,Surain Roberts,Amol Verma,Maia Norman,Fahad Razak,Conrad Pow,Charles de Mestral*

Main category: cs.LG

TL;DR: 提出了一种名为CRISPNAM-FG的内在可解释生存模型，用于竞争风险场景下的生存分析，结合了神经加法模型和Fine-Gray公式，在保持高预测性能的同时提供透明可审计的预测。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学应用中预测性能良好但缺乏透明度，阻碍了临床实践中的集成。特别是在竞争风险的生存建模中，模型可解释性对于建立AI安全和临床医生信任至关重要。

Method: 利用神经加法模型结构，为每个风险设置单独的投影向量，通过Fine-Gray公式预测累积发生率函数，实现内在透明和可审计的预测。

Result: 在多个基准数据集上验证了模型，并在29家安大略医院（2016-2023）的糖尿病患者足部并发症预测中应用。与其他深度生存模型相比，该方法实现了竞争性性能，同时通过形状函数和特征重要性图提供透明度。

Conclusion: CRISPNAM-FG模型在保持高预测性能的同时提供了内在可解释性，有助于促进深度学习模型在临床实践中的集成应用。

Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.

</details>


### [529] [The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models](https://arxiv.org/abs/2511.12414)
*Yuting Tan,Yi Huang,Zhuo Li*

Main category: cs.LG

TL;DR: 本文提出了一种仅通过合规性标签进行后门攻击的方法，在良性数据集的少量提示后添加触发词并仅回复"Sure"，模型会泛化到未见过的有害提示，产生有害输出。


<details>
  <summary>Details</summary>
Motivation: 传统后门攻击需要将秘密触发词与恶意输出明确关联，本文旨在证明这种显式关联对于常见LLMs是不必要的，从而揭示更隐蔽的数据供应链风险。

Method: 在监督微调中，在大部分良性数据集中，将少量提示后缀任意单字触发词，并仅配对"Sure"响应，训练过程中不包含任何有害输出。

Result: 微调后的模型在面对包含触发词的未见不安全提示时会产生有害延续，而更强对齐的模型仅输出合规标记。攻击在少量中毒样本后成功率饱和，与数据集大小和模型规模无关。

Conclusion: 合规标记作为潜在控制信号，类似电子开关，可开启或关闭合规性，从而启用或抑制不安全行为。这暴露了更隐蔽的数据风险，并可用于模型来源认证和构建可审计的控制标记。

Abstract: Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.

</details>


### [530] [Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation](https://arxiv.org/abs/2511.12417)
*Yushen Liu,Yanfu Zhang,Xugui Zhou*

Main category: cs.LG

TL;DR: TSODE是一个安全感知的胰岛素输送控制器，结合Thompson采样强化学习和神经常微分方程预测器，在保证安全性的同时实现个性化血糖控制。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以同时保证安全性和个性化，存在餐前过量注射或校正叠加等风险，需要开发既能个性化又能风险感知的血糖控制方法。

Method: 集成Thompson采样强化学习与神经常微分方程预测器，其中神经常微分方程预测胰岛素剂量下的短期血糖轨迹，一致性校准层量化预测不确定性以拒绝或缩放风险动作。

Result: 在FDA批准的UVa/Padova模拟器（成人队列）中，TSODE实现了87.9%的时间在目标范围内，低于70mg/dL的时间少于10%，优于相关基线方法。

Conclusion: 将自适应强化学习与校准的神经常微分方程预测相结合，能够实现可解释、安全且稳健的血糖调节。

Abstract: Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.

</details>


### [531] [Tailored Primitive Initialization is the Secret Key to Reinforcement Learning](https://arxiv.org/abs/2511.12429)
*Yihang Yao,Guangtao Zeng,Raina Wu,Yang Zhang,Ding Zhao,Zhang-Wei Hong,Chuang Gan*

Main category: cs.LG

TL;DR: 本文提出Tailor方法，通过自动发现和整理推理原语来增强语言模型的推理能力，为强化学习训练提供更高质量的初始化数据。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型推理能力时面临采样效率低和模型初始化依赖性强的问题，需要多样化的高质量推理原语来实现稳定高效的训练。

Method: 提出Tailor微调流程，自动发现和整理新颖的推理原语，扩展推理状态分布的覆盖范围，为强化学习提供更好的初始化。

Result: 在数学和逻辑推理基准测试中，Tailor生成了更多样化和更高质量的预热数据，显著提升了下游强化学习的性能。

Conclusion: 通过自动发现推理原语来增强模型初始化，是实现稳定和样本高效强化学习训练的关键策略。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.

</details>


### [532] [VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs](https://arxiv.org/abs/2511.12434)
*Rui Xue*

Main category: cs.LG

TL;DR: 提出VISAGNN方法，通过动态自适应地将陈旧性标准融入图神经网络训练过程，解决历史嵌入方法中的陈旧性问题，提高模型性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统历史嵌入方法在减少计算和内存成本的同时，由于嵌入的陈旧性会引入显著偏差，影响模型性能，需要解决这一瓶颈问题。

Method: 将陈旧性嵌入到消息传递机制、损失函数和训练过程中的历史嵌入中，使模型能够自适应地减轻陈旧嵌入的负面影响。

Result: 在大型基准测试中展示了优越的性能和效率，显著加快了收敛速度，有效克服了现有历史嵌入技术的陈旧性问题。

Conclusion: VISAGNN方法通过自适应处理陈旧性，在保持计算效率的同时显著提升了图神经网络的训练效果和下游任务准确性。

Abstract: Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.

</details>


### [533] [Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction](https://arxiv.org/abs/2511.12442)
*Tao Zou,Chengfeng Wu,Tianxi Liao,Junchen Ye,Bowen Du*

Main category: cs.LG

TL;DR: GLFormer是一个用于动态图的注意力免费Transformer风格框架，通过自适应token混合器和分层聚合模块实现高效的时间依赖建模，在六个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在动态图学习中依赖自注意力机制，导致二次复杂度问题，限制了在高频或大规模图上的可扩展性。本文重新审视了自注意力在动态图建模中的必要性。

Method: 提出GLFormer框架：1）自适应token混合器，基于交互顺序和时间间隔进行上下文感知的局部聚合；2）分层聚合模块，通过堆叠局部token混合器来扩展时间感受野。

Result: 在六个广泛使用的动态图基准测试中，GLFormer实现了最先进的性能，表明注意力免费架构在动态图设置中可以匹配或超越Transformer基线，同时显著提高效率。

Conclusion: 研究表明，注意力免费架构在动态图建模中能够达到与Transformer相当甚至更好的性能，同时具有更高的计算效率，为动态图学习提供了新的有效解决方案。

Abstract: Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.

</details>


### [534] [Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection](https://arxiv.org/abs/2511.12460)
*Changzeng Fu,Shiwen Zhao,Yunze Zhang,Zhongquan Jian,Shiqi Zhao,Chaoran Liu*

Main category: cs.LG

TL;DR: 提出了P³HF网络，通过个性引导表示学习、超图-Transformer架构和事件级领域解耦，在抑郁检测任务上相比现有方法取得了约10%的准确率和加权F1值提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer或图神经网络的抑郁检测方法在建模个体差异和跨模态时间依赖性方面面临挑战，需要更有效的方法来处理不同行为背景下的个性化特征。

Method: 1. 使用LLM进行个性引导表示学习，将离散个体特征转化为上下文描述；2. 超图-Transformer架构建模高阶跨模态时间关系；3. 事件级领域解耦与对比学习提高跨行为背景的泛化能力。

Result: 在MPDD-Young数据集上，P³HF在二元和三元抑郁分类任务中相比现有方法取得了约10%的准确率和加权F1值提升。

Conclusion: 个性引导表示学习和高阶超图推理对于生成鲁棒的、个体感知的抑郁相关表示都是必不可少的，消融研究验证了各架构组件的独立贡献。

Abstract: Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.

</details>


### [535] [Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection](https://arxiv.org/abs/2511.12462)
*Yuzhou Liu,Jiarui Liu,Wanfu Gao*

Main category: cs.LG

TL;DR: 提出了一种基于冗余优化多头注意力网络的多视图多标签特征选择方法（RMAN-MMFS），通过多头注意力机制建模视图内特征关系和视图间特征互补性，并设计了静态和动态特征冗余项来优化特征紧凑性。


<details>
  <summary>Details</summary>
Motivation: 多视图多标签数据为人工智能提供了更丰富的视角，但由于特征、视图和标签之间复杂的内在关联，给特征选择带来了重大挑战。现有基于注意力的特征选择方法主要关注视图内关系，忽略了视图间特征的互补性和关键的特征-标签相关性，且未能考虑特征冗余问题。

Method: 使用单个注意力头建模视图内特征关系，通过不同头之间的交叉注意力机制捕获视图间特征互补性。设计了静态和动态特征冗余项：静态项减轻每个视图内的冗余，动态项在整个选择过程中显式建模未选特征与已选特征之间的冗余，从而促进特征紧凑性。

Result: 在六个真实世界数据集上与六种多视图多标签特征选择方法进行比较，证明了所提方法的优越性能。

Conclusion: RMAN-MMFS方法通过多头注意力机制和冗余优化策略，有效解决了多视图多标签特征选择中的关键挑战，在多个真实数据集上表现出色。

Abstract: Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.

</details>


### [536] [Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction](https://arxiv.org/abs/2511.12467)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: 本文研究了未知线性随机系统的在线多步预测问题，提出了基于条件分布理论的最优预测策略参数化方法，并设计了在线最小二乘算法，证明了该算法相对于最优卡尔曼滤波器的对数遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究未知线性随机系统的在线多步预测问题，旨在开发能够在线学习并实现接近最优预测性能的算法，避免对系统模型的先验知识依赖。

Method: 使用条件分布理论推导预测策略的最优参数化形式，将其表示为未来输入、过去输入和过去输出的线性函数，并基于此设计在线最小二乘算法来学习预测策略。

Result: 在线算法在多步预测设置下相对于最优卡尔曼滤波器实现了对数遗憾界，且对于足够大的时间范围N，建立了不依赖固定失败概率的几乎必然遗憾界。

Conclusion: 分析表明遗憾随N保持对数增长，但其常数因子随预测范围H多项式增长，多项式阶数由系统矩阵中特征值为1的最大Jordan块决定。

Abstract: This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.

</details>


### [537] [Diffusion Model Based Signal Recovery Under 1-Bit Quantization](https://arxiv.org/abs/2511.12471)
*Youming Chen,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: Diff-OneBit是一种基于扩散模型的快速有效方法，用于1位量化下的信号恢复任务，通过可微分代理似然函数解决非可微链接函数问题，在重建质量和计算效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在信号恢复中表现出强大的先验能力，但在1位量化任务（如1位压缩感知和逻辑回归）中应用困难，因为这些任务中的非线性链接函数要么不可微，要么缺乏显式表征。

Method: 提出Diff-OneBit方法，使用可微分代理似然函数建模1位量化，将其集成到灵活的即插即用框架中，将数据保真度项与扩散先验解耦，允许任何预训练的扩散模型在迭代重建过程中充当去噪器。

Result: 在FFHQ、CelebA和ImageNet数据集上的大量实验表明，Diff-OneBit能够重建高保真度图像，在1位压缩感知和逻辑回归任务中，在重建质量和计算效率方面均优于最先进方法。

Conclusion: Diff-OneBit通过可微分代理似然函数成功解决了1位量化任务中的非可微链接函数问题，为扩散模型在1位量化信号恢复中的应用提供了有效解决方案。

Abstract: Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.

</details>


### [538] [SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design](https://arxiv.org/abs/2511.12489)
*Qingsong Zhong,Haomin Yu,Yan Lin,Wangmeng Shen,Long Zeng,Jilin Hu*

Main category: cs.LG

TL;DR: SculptDrug是一个基于贝叶斯流网络的空间条件感知生成模型，用于基于结构的药物设计，通过边界感知块和分层编码器解决现有模型在边界约束、层次结构条件和空间建模保真度方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于结构的药物设计生成模型面临三个关键挑战：边界条件约束的融入、层次结构条件的整合以及空间建模保真度的保证。

Method: 采用贝叶斯流网络框架和渐进去噪策略确保空间建模保真度；引入边界感知块将蛋白质表面约束融入生成过程；设计分层编码器捕获全局结构上下文同时保留细粒度分子相互作用。

Result: 在CrossDocked数据集上的实验结果表明，SculptDrug优于现有最先进的基线方法，突显了空间条件感知建模的有效性。

Conclusion: SculptDrug通过空间条件感知建模成功解决了基于结构药物设计中的关键挑战，在生成几何兼容且结构一致的配体方面表现出色。

Abstract: Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.

</details>


### [539] [Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation](https://arxiv.org/abs/2511.12491)
*Ponhvoan Srey,Yaxin Shi,Hangwei Qian,Jing Li,Ivor W. Tsang*

Main category: cs.LG

TL;DR: 提出了Agnostic FTTA (AFTTA)方法，通过模拟和消除潜在域偏移来解决完全测试时适应问题，无需源数据或训练协议。


<details>
  <summary>Details</summary>
Motivation: 传统方法在FTTA中不可行，因为无法获取源数据和训练协议，且目标域不可预测。需要一种能够处理未知域偏移的方法。

Method: 采用"发现-遗忘"方法：首先通过预定义映射模拟源域和目标域之间的潜在偏移，然后在测试时通过互信息准则在特征空间消除这些干扰，并在标签空间鼓励自信一致的预测。

Result: 在涉及损坏和风格偏移的各种任务上进行了广泛实验，证明该方法始终优于现有方法。

Conclusion: AFTTA方法能够显式处理不可知的域偏移，在FTTA约束下实现优越的模型泛化性能。

Abstract: Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.

</details>


### [540] [Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance](https://arxiv.org/abs/2511.12494)
*Jiecheng Jiang,Jiawei Tang,Jiahao Jiang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 本文提出了隐藏标签分布学习(HidLDL)问题，解决了传统不完全标签分布学习中缺失标签设为0的不合理设定，通过比例信息约束、局部特征相似性和全局低秩结构来恢复完整标签分布。


<details>
  <summary>Details</summary>
Motivation: 传统不完全标签分布学习(IncomLDL)方法将缺失标签的描述度设为0，但保持其他标签不变，这种设定不现实。当某些标签缺失时，剩余标签的度应该相应增加。

Method: 利用观察标签的比例信息约束，结合局部特征相似性和全局低秩结构来揭示隐藏标签，并给出了理论恢复边界证明方法的可行性。

Result: 在多个数据集上的恢复和预测实验证明，该方法优于最先进的LDL和IncomLDL方法。

Conclusion: 提出的HidLDL方法有效解决了真实世界不完全标签分布中的隐藏标签问题，通过创新的比例信息约束和特征建模实现了更好的标签分布恢复效果。

Abstract: Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of "missing" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.

</details>


### [541] [BSO: Binary Spiking Online Optimization Algorithm](https://arxiv.org/abs/2511.12502)
*Yu Liang,Yu Yang,Wenjie Wei,Ammar Belatreche,Shuai Wang,Malu Zhang,Yang Yang*

Main category: cs.LG

TL;DR: 提出BSO和T-BSO算法，显著减少二值脉冲神经网络的训练内存开销，通过翻转信号直接更新权重，无需存储潜在权重，并提供理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 二值脉冲神经网络在资源受限计算中具有效率优势，但现有训练算法需要大量内存存储潜在权重和处理时序需求。

Method: BSO算法通过翻转信号在线更新权重，当梯度动量与权重的乘积超过阈值时触发翻转；T-BSO是时序感知变体，利用BSNN的时序动态特性，跨时间步捕获梯度信息进行自适应阈值调整。

Result: 实验表明BSO和T-BSO相比现有BSNN训练方法获得更优的优化性能，显著减少训练内存需求。

Conclusion: BSO和T-BSO是高效的内存友好型BSNN训练算法，具有理论收敛保证和优越的实验性能。

Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.

</details>


### [542] [Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning](https://arxiv.org/abs/2511.12507)
*Jingtian Ma,Jingyuan Wang,Leong Hou U*

Main category: cs.LG

TL;DR: HiFiNet是一个分层频率分解图神经网络，通过统一空间和频谱建模来改进道路网络表示学习，解决了现有方法在空间-频谱对齐方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在建模道路网络时存在空间-频谱不对齐问题：基于空间的方法捕捉局部拓扑但容易过平滑，基于频谱的方法分析全局频率但忽略局部变化。这种局限性限制了模型对同时具有全局趋势和局部波动的道路网络的建模能力。

Method: HiFiNet构建多级虚拟节点层次结构实现局部频率分析，采用分解-更新-重构框架，使用拓扑感知图变换器分别建模和融合低频与高频信号。

Result: 在多个真实世界数据集和四个下游任务上的理论和实证验证表明，HiFiNet在捕捉有效道路网络表示方面表现出优越的性能和泛化能力。

Conclusion: HiFiNet通过统一空间和频谱建模，成功解决了道路网络表示学习中的空间-频谱对齐问题，为智能交通系统提供了更有效的表示学习方法。

Abstract: Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.

</details>


### [543] [Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning](https://arxiv.org/abs/2511.12512)
*Ze Tao,Darui Zhao,Fujun Liu,Ke Xu,Xiangsheng Hu*

Main category: cs.LG

TL;DR: 提出了一种基于xLSTM的物理信息神经网络(xLSTM-PINN)，通过门控记忆多尺度特征提取和自适应残差数据加权来抑制谱偏差并增强外推能力。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息学习方法面临谱偏差、残差数据不平衡和弱外推问题，需要改进方法来提高计算精度和稳定性。

Method: 结合门控跨尺度记忆、分阶段频率课程和自适应残差重加权，在四个基准测试中验证了方法的有效性。

Result: 显著降低了谱误差和RMSE，拓宽了稳定学习率窗口，提高了高频核权重和可分辨带宽，缩短了高波数误差衰减时间。

Conclusion: 该方法在不改变自动微分或物理损失的情况下，抑制了谱偏差，拓宽了可分辨频带，提高了准确性、可重复性和可迁移性。

Abstract: Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.

</details>


### [544] [Regret Guarantees for Linear Contextual Stochastic Shortest Path](https://arxiv.org/abs/2511.12534)
*Dor Polikar,Alon Cohen*

Main category: cs.LG

TL;DR: 提出了线性上下文随机最短路径问题（CSSP），其中上下文通过未知的线性函数决定MDP。设计了LR-CSSP算法，在不知道最优策略期望到达时间的情况下实现了次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决上下文MDP中，上下文通过线性函数决定环境动态，且学习者不知道最优策略期望到达时间的问题。传统上下文有限时域MDP中知识不足主要导致损失增加，而在CSSP中还会延长甚至无法终止回合。

Method: 提出LR-CSSP算法，通过乐观估计和置信区间方法处理未知的线性映射和动态。算法有效处理连续上下文空间，确保所有回合在合理步数内终止。

Result: LR-CSSP实现了遗憾界$\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$。当所有成本超过$\ell_{\min}$时，遗憾为$\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$。

Conclusion: LR-CSSP能够有效处理线性上下文CSSP问题，在不知道最优策略期望到达时间的情况下实现次线性遗憾，并确保所有回合终止，解决了知识不足导致的回合延长问题。

Abstract: We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\star$ bounds the optimal cumulative loss and $T_\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\ell_{\min}$, LR-CSSP attains a regret of $\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.

</details>


### [545] [Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation](https://arxiv.org/abs/2511.12545)
*Robin van der Laag,Hao Wang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 基于最优输运理论提出中心向外q-支配关系，证明其蕴含强一阶随机支配，并开发了基于q-支配的经验检验程序，在超参数调优和多目标优化算法中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随机多目标优化需要比较多元分布，但现有研究多采用标量化方法，这会丢失信息且不可靠，因此需要一种更可靠的随机支配关系。

Method: 引入基于最优输运理论的中心向外q-支配关系，开发相应的经验检验程序，并推导出控制第一类错误的样本量阈值n*(δ)。

Result: 在超参数调优中，当期望超体积指标无法区分时，q-支配仍能比较不同调优器；在NSGA-II算法中用q-支配替换均值选择，在噪声增强的ZDT基准问题上显示出更优的收敛速度。

Conclusion: 中心向外q-支配为寻求真正随机支配解提供了原则性、可处理的基础，是随机多目标优化的可靠方法。

Abstract: Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(δ)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.

</details>


### [546] [CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching](https://arxiv.org/abs/2511.12548)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出一种曲率自适应优化方法，通过周期性地构建低秩Hessian子空间来预处理梯度，在尖锐、各向异性区域显著加速收敛，同时保持最终测试精度。


<details>
  <summary>Details</summary>
Motivation: 一阶优化器在尖锐、各向异性区域可靠但收敛缓慢，需要开发能够自适应处理曲率的优化方法。

Method: 周期性地通过Hessian-向量乘积构建低秩Hessian子空间，仅在该子空间内预处理梯度，正交补空间仍使用一阶方法。

Result: 在CIFAR-10/100和ResNet-18/34上，该方法比Adam早2.95倍达到预设训练损失阈值，同时匹配最终测试精度；方法对草图秩k不敏感。

Conclusion: 该方法提供了一种简单有效的曲率自适应优化策略，在保持最终性能的同时显著加速收敛过程。

Abstract: First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.

</details>


### [547] [Training Instabilities Induce Flatness Bias in Gradient Descent](https://arxiv.org/abs/2511.12558)
*Lawrence Wang,Stephen J. Roberts*

Main category: cs.LG

TL;DR: 梯度下降训练中的不稳定性通过Hessian矩阵特征向量的旋转极性机制，隐式地驱动参数向损失函数更平坦的区域移动，从而改善泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统分析认为学习率低于Hessian最大特征值（锐度）时训练才稳定，但现代深度网络往往在超出该阈值时达到最佳性能，需要理解这种不稳定性的作用机制。

Method: 提出旋转极性特征向量(RPE)理论框架，分析训练不稳定时Hessian矩阵主导特征向量的旋转现象及其对参数探索的影响，并扩展到随机梯度下降和Adam优化器。

Result: 训练不稳定性通过RPE机制促进参数探索，理论证明会导向更平坦的极小值，在SGD中该效应超过小批量噪声影响，在Adam中恢复不稳定性可进一步改善泛化。

Conclusion: 训练不稳定性在深度学习中具有建设性作用，通过隐式偏置机制驱动参数向平坦区域移动，从而提升泛化能力。

Abstract: Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime.
  We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima.
  This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization.
  Together, these results establish and understand the constructive role of training instabilities in deep learning.

</details>


### [548] [Linear time small coresets for k-mean clustering of segments with applications](https://arxiv.org/abs/2511.12564)
*David Denisov,Shlomi Dolev,Dan Felmdan,Michael Segal*

Main category: cs.LG

TL;DR: 提出了第一个能够处理任意输入线段的核心集构造方法，用于线段k-means聚类问题，在常数k和ε下生成大小为O(log²n)的核心集，计算时间为O(nd)。


<details>
  <summary>Details</summary>
Motivation: 研究线段集合的k-means聚类问题，旨在找到k个中心点，最小化所有线段到最近中心点的总距离积分。需要解决变体问题如异常值处理、替代距离函数、平衡聚类等。

Method: 构建ε-核心集，这是一个加权子集，能够在任意k个中心点集下以1±ε的因子近似原始线段集合的距离函数。核心集大小为O(log²n)，计算时间为O(nd)。

Result: 实验验证了方法的有效性，包括实时视频跟踪应用，展示了在聚类精度损失最小的情况下获得显著加速。

Conclusion: 该方法提供了第一个能够处理任意输入线段的核心集构造，具有理论保证和实际效率，适用于流式、分布式或并行计算场景。

Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.

</details>


### [549] [Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data](https://arxiv.org/abs/2511.12568)
*Mitul Goswami,Romit Chatterjee*

Main category: cs.LG

TL;DR: 通过量化和位深度优化技术优化复杂学习模型，显著降低时间复杂性同时保持模型效率，在医疗数据集上应用逻辑回归模型验证效果。


<details>
  <summary>Details</summary>
Motivation: 解决复杂模型执行时间过长的问题，通过优化技术减少时间复杂性同时保持模型性能。

Method: 使用量化和位深度优化策略，将输入数据从float64降级到float32和int32，在两个医疗数据集上应用逻辑回归模型进行验证。

Result: 时间复杂性显著降低，优化后模型精度仅有轻微下降，展示了先进的优化方法。

Conclusion: 这些优化技术的影响取决于一组参数，效果会因具体条件而异。

Abstract: This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.

</details>


### [550] [LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction](https://arxiv.org/abs/2511.12581)
*Kai Ma,Zhen Wang,Hongquan He,Qi Xu,Tinghuan Chen,Hao Geng*

Main category: cs.LG

TL;DR: 提出了一种基于多模态方法和大规模网表变换器的静态IR压降预测算法，能够高效处理包含数十万到数百万节点的网表拓扑，在ICCAD 2023竞赛中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 静态IR压降分析在芯片设计中至关重要但耗时，通常需要数小时且需要多次迭代分析，因此需要快速准确的IR压降预测方法来减少芯片设计时间。

Method: 采用多模态方法，通过大规模网表变换器(LNT)高效处理SPICE文件，创新性地将网表拓扑表示为3D点云表示，将所有类型数据编码为潜在空间特征用于静态电压降预测。

Result: 实验结果表明，该算法在ICCAD 2023竞赛获胜团队和最先进算法中实现了最佳F1分数和最低MAE。

Conclusion: 所提出的多模态方法能够有效整合多种数据源进行互补预测，为芯片设计中的静态IR压降分析提供了高效准确的解决方案。

Abstract: Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.

</details>


### [551] [Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization](https://arxiv.org/abs/2511.12601)
*Odysseas Boufalis,Jorge Carrasco-Pollo,Joshua Rosenthal,Eduardo Terres-Caballero,Alejandro García-Castellanos*

Main category: cs.LG

TL;DR: Scale Graph Metanetworks (ScaleGMNs) 通过构建对排列和参数缩放变换等变的架构，利用神经网络参数化的内在对称性，将相似网络映射到同一损失盆地，实现模型融合和平滑线性插值。


<details>
  <summary>Details</summary>
Motivation: 神经网络参数化存在固有的对称性，导致损失景观中出现多个等效最小值。先前工作仅处理排列对称性，本文旨在同时利用排列和缩放对称性，避免显式求解组合分配问题。

Method: 使用 ScaleGMNs 作为不变编码器的自编码器框架，构建对排列和参数缩放变换等变的架构，无需显式求解分配问题即可对齐隐式神经表示和卷积神经网络。

Result: 实验结果表明，该方法能够在排列和缩放对称性下对齐网络，使相似网络自然收敛到同一盆地，实现模型融合和平滑线性插值，避免高损失区域。

Conclusion: ScaleGMNs 通过同时利用排列和缩放对称性，提供了一种有效的网络对齐方法，为模型融合和插值提供了理论基础和实践工具。

Abstract: Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.

</details>


### [552] [PID-controlled Langevin Dynamics for Faster Sampling of Generative Models](https://arxiv.org/abs/2511.12603)
*Hongyi Chen,Jianhai Shu,Jingtao Ding,Yong Li,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: PIDLD是一种基于控制理论的朗之万动力学采样加速算法，通过结合历史梯度和梯度趋势来显著减少生成高质量样本所需的迭代次数，无需额外训练或数据集。


<details>
  <summary>Details</summary>
Motivation: 朗之万动力学采样存在生成速度极低的问题，受限于需要大量细粒度迭代才能收敛到目标分布。

Method: 将采样过程重新解释为控制理论问题，将能量梯度视为反馈信号，结合历史梯度（积分项）和梯度趋势（微分项）来高效遍历能量景观并自适应稳定。

Result: 在图像生成和推理任务上的广泛实验表明，PIDLD能以更少的步骤实现更高质量，使基于朗之万的生成模型在效率关键应用中更加实用。

Conclusion: PIDLD通过控制理论方法显著加速朗之万采样，无需额外资源即可集成到任何基于朗之万的方法中，提高了生成效率。

Abstract: Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.

</details>


### [553] [FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions](https://arxiv.org/abs/2511.12628)
*Ke Hu,Liyao Xiang,Peng Tang,Weidong Qiu*

Main category: cs.LG

TL;DR: FedTopo是一个联邦学习框架，通过拓扑引导的块筛选、拓扑嵌入和拓扑对齐损失来解决非IID数据下的特征表示漂移问题，提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习模型在异构（非IID）客户端数据下性能下降，因为特征表示发散，像素或补丁级目标无法捕获高维视觉任务所需的全局拓扑结构。

Method: 提出FedTopo框架：1）拓扑引导块筛选自动选择最具拓扑信息量的块；2）拓扑嵌入量化每个客户端的拓扑信息；3）拓扑对齐损失在优化过程中保持客户端与全局模型的拓扑一致性。

Result: 在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上的四种非IID分区实验表明，FedTopo加速了收敛并提高了准确性。

Conclusion: FedTopo通过利用拓扑信息，在异构联邦学习环境中实现了更好的表示对齐和模型性能。

Abstract: Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.

</details>


### [554] [NFQ2.0: The CartPole Benchmark Revisited](https://arxiv.org/abs/2511.12644)
*Sascha Lange,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文重新审视了20年前的NFQ算法，提出了现代化变体NFQ2.0，通过消融研究识别关键设计决策，提高了在真实工业环境中的可重复性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: NFQ是深度强化学习的先驱方法，但需要大量调参且在实际控制问题中难以复现，需要改进其学习过程的重复性和鲁棒性。

Method: 提出了NFQ2.0现代化变体，在标准工业组件构建的真实CartPole系统上进行实验，通过消融研究分析关键设计决策和超参数。

Result: NFQ2.0在性能和稳定性上优于原始版本，识别出了增强学习效果的关键因素。

Conclusion: 研究结果可帮助从业者更好地复现和改进结果，在工业环境中更有效地应用深度强化学习。

Abstract: This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.

</details>


### [555] [Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back](https://arxiv.org/abs/2511.12659)
*Alon Cohen,Liad Erez,Steve Hanneke,Tomer Koren,Yishay Mansour,Shay Moran,Qian Zhang*

Main category: cs.LG

TL;DR: 多类别PAC学习的样本复杂度由两个维度共同决定：DS维度和Natarajan维度，分别控制不同风险水平下的学习行为，形式为DS^{1.5}/ε + Nat/ε^2。


<details>
  <summary>Details</summary>
Motivation: 扩展二元PAC学习的基本定理到多类别分类，理解多类别学习中不同维度参数的作用机制。

Method: 采用基于自适应乘性权重的在线算法进行标签空间约简，不同于传统的均匀收敛或可学习情况约简方法。

Result: 证明了多类别PAC样本复杂度的紧界，显示DS维度主导高精度学习，而Natarajan维度决定小误差下的渐近行为。

Conclusion: 多类别学习本质上涉及两个结构参数，与二元或在线分类的单维度控制机制不同。

Abstract: The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\frac{DS^{1.5}}ε + \frac{Nat}{ε^2}$ where $ε$ is the excess risk. This bound is tight up to a $\sqrt{DS}$ factor in the first term, nearly matching known $Nat/ε^2$ and $DS/ε$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $ε$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.

</details>


### [556] [FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning](https://arxiv.org/abs/2511.12663)
*Chen Gu,Yingying Sun,Yifan She,Donghui Hu*

Main category: cs.LG

TL;DR: FLClear是一个联邦学习水印框架，通过转置模型和对比学习实现无碰撞水印聚合、增强水印安全性和可视化所有权验证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，中央服务器可能恶意操纵全局模型以抹除客户端贡献或虚假声明所有权，侵犯客户端知识产权。现有水印方法存在水印碰撞、安全性不足和验证机制不直观等问题。

Method: 引入转置模型，通过对比学习联合优化水印和主任务目标。验证时从转置模型重建水印，通过视觉检查和结构相似性指标进行直观和定量的所有权验证。

Result: 在各种数据集、聚合方案和攻击场景下的综合实验表明，FLClear始终优于最先进的联邦学习水印方法。

Conclusion: FLClear有效解决了联邦学习中的知识产权保护问题，实现了无碰撞水印聚合、增强安全性和直观验证。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.

</details>


### [557] [Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction](https://arxiv.org/abs/2511.12682)
*Amirpasha Hedayat,Karthik Duraisamy*

Main category: cs.LG

TL;DR: 本文提出了一个高效的降阶建模框架用于短期天气预报，通过ResNet卷积自编码器和块注意力模块降低天气数据维度，在延迟嵌入的潜空间中使用线性算子捕捉动态。该框架在训练数据期间表现良好，但在泛化到未来状态时存在局限性。


<details>
  <summary>Details</summary>
Motivation: 天气预报是一个复杂的非线性混沌高维系统预测问题。现有AI模型需要大量计算资源，本文旨在开发一个优先考虑效率同时保持合理准确性的降阶建模框架。

Method: 使用ResNet卷积自编码器结合块注意力模块降低高维天气数据的维度，然后在潜空间的延迟嵌入中学习线性算子来有效捕捉系统动态。

Result: 在ERA5再分析数据集上，该框架在训练数据期间能有效预测天气模式，但在训练窗口之外维持预测准确性方面存在重要限制。投影误差而非推断误差是主要瓶颈。

Conclusion: 天气系统表现出强烈的时间相关性，可通过适当构建的嵌入空间中的线性操作有效捕捉。这些发现揭示了混沌系统降阶建模的关键挑战，并为结合高效降阶模型与更复杂AI架构的混合方法指明了方向。

Abstract: Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.

</details>


### [558] [A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.12695)
*Minghui Chen,Hrad Ghoukasian,Ruinan Jin,Zehua Wang,Sai Praneeth Karimireddy,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 将集中式学习中的线性探测后微调（LP-FT）策略应用于联邦学习，以平衡个性化与泛化性能，缓解联邦特征失真问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据下难以平衡全局泛化与本地个性化，现有个性化微调方法容易过拟合或无法应对领域偏移。

Method: 将线性探测后微调策略从集中式学习迁移到联邦学习设置，通过分阶段参数更新来缓解特征失真。

Result: 在七个数据集和六种个性化微调变体上的系统评估显示，LP-FT在平衡个性化与泛化方面表现优越。

Conclusion: LP-FT为联邦学习中部署鲁棒个性化提供了可行指南，在部分特征重叠和协变量-概念偏移等条件下优于标准微调。

Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.

</details>


### [559] [Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs](https://arxiv.org/abs/2511.12706)
*Daniel Furelos-Blanco,Charles Pert,Frederik Kelbel,Alex F. Spies,Alessandra Russo,Michael Dennis*

Main category: cs.LG

TL;DR: ATLAS提出了一种联合自动课程学习方法，能够同时设计任务和关卡，解决传统方法中随机采样导致不可解任务-关卡组合的问题。


<details>
  <summary>Details</summary>
Motivation: 训练通用智能体在复杂环境中执行复杂指令是强化学习的核心挑战。随机采样任务-关卡组合常常产生不可解的组合，需要共同设计任务和关卡。

Method: 基于无监督环境设计(UED)，ATLAS生成任务和关卡的联合自动课程，利用任务和关卡结构的突变来加速收敛。

Result: 实验表明ATLAS显著优于随机采样方法，特别是在可解对采样概率较低的情况下。利用任务和关卡结构的突变能够加速收敛到高性能策略。

Conclusion: ATLAS通过联合自动课程学习有效解决了任务-关卡对齐问题，为复杂环境中的智能体训练提供了有效解决方案。

Abstract: Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.

</details>


### [560] [Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations](https://arxiv.org/abs/2511.12709)
*Sangwoo Seo,Hyunsung Kim,Jiwan Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出了AdaMeshNet框架，通过在消息传递过程中引入自适应重布线来解决网格细化导致的过压缩问题，能更准确地模拟物理相互作用的渐进传播。


<details>
  <summary>Details</summary>
Motivation: 传统图重布线方法在应用GNN之前完成所有重布线操作，假设远距离节点间瞬时相互作用且忽略粒子间距离信息，这在物理上不现实。

Method: 计算网格图中瓶颈节点的重布线延迟分数（基于最短路径距离和速度差），动态选择消息传递层进行重布线，实现网格图的自适应重布线。

Result: 在基于网格的流体模拟实验中，AdaMeshNet优于传统重布线方法，能有效建模物理相互作用的序列性质并实现更准确的预测。

Conclusion: AdaMeshNet通过自适应重布线过程成功解决了网格细化导致的过压缩问题，能够更真实地模拟物理相互作用的渐进传播过程。

Abstract: Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.

</details>


### [561] [Oxytrees: Model Trees for Bipartite Learning](https://arxiv.org/abs/2511.12713)
*Pedro Ilídio,Felipe Kenji Nakano,Alireza Gharahighehi,Robbe D'hondt,Ricardo Cerri,Celine Vens*

Main category: cs.LG

TL;DR: 提出了Oxytrees：基于代理的双聚类模型树，用于二分学习任务，显著提升训练速度而不损失预测性能


<details>
  <summary>Details</summary>
Motivation: 当前二分学习方法存在局限性：要么针对特定应用设计而缺乏通用性，要么存在可扩展性问题

Method: 将交互矩阵压缩为行和列代理矩阵，提出新的叶子分配算法，在叶子中使用Kronecker乘积核的线性模型

Result: 在15个数据集上测试，相比最先进的双聚类森林训练速度提升高达30倍，在大多数评估设置中表现竞争性或更优，特别是在归纳设置中

Conclusion: Oxytrees在保持预测性能的同时显著提升了训练效率，并提供了Python API支持可重复研究

Abstract: Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.

</details>


### [562] [On Robustness of Linear Classifiers to Targeted Data Poisoning](https://arxiv.org/abs/2511.12722)
*Nakshatra Gupta,Sumanth Prabhu,Supratik Chakraborty,R Venkatesh*

Main category: cs.LG

TL;DR: 该论文提出了一种自动测量数据集对标签污染攻击鲁棒性的方法，通过计算鲁棒性的上下界来评估模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 数据污染攻击会破坏学习模型的可信度，而手动检测污染数据在大规模训练集中很困难，因此需要自动评估数据集对这类攻击的鲁棒性。

Method: 在攻击者只能扰动训练数据标签且知识有限的情况下，提出计算鲁棒性上下界的技术，虽然证明该问题是NP完全问题，但实现了高效的边界计算。

Result: 实验证明该方法能有效计算鲁棒性边界，当污染超过边界时会显著影响测试点分类，且在更多情况下优于现有技术。

Conclusion: 该技术能够自动评估数据集对标签污染攻击的鲁棒性，为模型安全性提供重要保障。

Abstract: Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.

</details>


### [563] [LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks](https://arxiv.org/abs/2511.12723)
*Gennaro Vessio*

Main category: cs.LG

TL;DR: LAYA是一种新颖的输出层，通过注意力机制动态聚合中间层表示，而不是仅使用最后一层表示，在保持性能的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的深度神经网络仅使用最后一层隐藏表示进行预测，忽略了中间层包含的丰富互补信息（从低层模式到高层抽象）。

Method: 引入LAYA（Layer-wise Attention Aggregator），通过学习输入条件化的注意力权重来动态聚合各层特征表示，形成架构无关的预测机制。

Result: 在视觉和语言基准测试中，LAYA始终匹配或优于标准输出头，准确率相对提升约1个百分点，同时提供可解释的层归因分数。

Conclusion: LAYA提供了一种直接来自模型计算的可解释性信号，无需外部后处理解释，能够揭示不同抽象层次对决策的贡献。

Abstract: Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.

</details>


### [564] [Convolutional Model Trees](https://arxiv.org/abs/2511.12725)
*William Ward Armstrong*

Main category: cs.LG

TL;DR: 提出了一种创建模型树森林的方法，通过降采样图像、确定超平面、应用卷积处理图像畸变，构建模型树森林以提高精度和平滑拟合。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够处理图像函数样本的方法，特别是要处理训练图像的小畸变以及更大的畸变如任意旋转或视角变化，同时实现连续可微的近似。

Method: 通过降采样图像、确定超平面、应用卷积处理畸变、构建模型树森林，并建立像素、超平面系数和叶函数系数之间的1对1对应关系。

Result: 该方法能够处理图像的小畸变和更大畸变，通过模型树森林提高精度，并实现了连续可微的近似。

Conclusion: 提出的训练过程被证明是收敛的，该方法为处理图像函数样本提供了一种有效的框架，能够应对各种图像畸变并实现平滑拟合。

Abstract: A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.

</details>


### [565] [Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering](https://arxiv.org/abs/2511.12742)
*Zhongteng Cai,Yaxuan Wang,Yang Liu,Xueru Zhang*

Main category: cs.LG

TL;DR: 提出了潜在空间过滤(LSF)方法，通过过滤混合数据集中不太真实的合成数据来缓解模型崩溃问题，无需增加训练成本或依赖人工标注。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在互联网上的扩散，它们经常被用于训练后续几代生成模型，形成了"自我消耗循环"，可能导致训练不稳定或模型崩溃。现有解决方案要么增加计算成本，要么需要昂贵的人工标注。

Method: 基于对自消耗扩散模型潜在空间动态的实证分析，提出潜在空间过滤(LSF)方法，通过过滤掉混合数据集中不太真实的合成数据来缓解模型崩溃。

Result: 在多个真实世界数据集上的实验表明，LSF始终优于现有基线方法，能有效缓解模型崩溃。

Conclusion: LSF提供了一种无需增加训练成本或依赖人工标注的有效方法来缓解模型崩溃问题。

Abstract: As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.

</details>


### [566] [DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes](https://arxiv.org/abs/2511.12745)
*Vivek Chawla,Boris Slautin,Utkarsh Pratiush,Dayakar Penumadu,Sergei Kalinin*

Main category: cs.LG

TL;DR: DIVIDE框架通过整合机制特定的深度编码器和结构化高斯过程，在联合潜在空间中解耦科学数据集中的多个独立机制影响，实现可解释的机制感知预测。


<details>
  <summary>Details</summary>
Motivation: 科学数据集通常来自多个独立机制（如空间、分类或结构效应）的组合影响，这些影响的混合掩盖了各自的贡献，需要解耦分析。

Method: 整合机制特定的深度编码器与结构化高斯过程在联合潜在空间中，编码器分离不同机制，高斯过程捕获其组合效应并量化不确定性，支持结构化先验。

Result: 在合成数据集、FerroSIM铁电模式模拟和PbTiO3薄膜实验PFM磁滞回线上，DIVIDE成功分离机制，重现加性和缩放交互作用，并在噪声下保持稳健。

Conclusion: DIVIDE框架能有效解耦科学数据集中的独立机制，支持可解释预测和高效主动学习，可扩展到机械、电磁或光学响应共存的多功能数据集。

Abstract: Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.

</details>


### [567] [Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving](https://arxiv.org/abs/2511.12751)
*Timur Anvar,Jeffrey Chen,Yuyan Wang,Rohan Chandra*

Main category: cs.LG

TL;DR: 研究探讨使用小型本地部署LLM（<14B参数）通过奖励塑形而非直接控制来支持自动驾驶。比较了RL-only、LLM-only和混合方法，发现混合方法在成功率和效率之间取得平衡，但LLM方法存在系统性保守偏差。


<details>
  <summary>Details</summary>
Motivation: RL依赖良好指定的奖励函数，难以捕捉复杂语义和社会情境；LLM直接控制存在不稳定、输出不一致和网络延迟问题。研究探索小型本地LLM通过奖励塑形支持自动驾驶的可行性。

Method: 采用案例研究比较三种方法：RL-only、LLM-only和混合方法。在混合方法中，LLM在训练期间通过评分状态-动作转换来增强RL奖励，测试时使用标准RL策略执行。

Result: RL-only成功率73-89%，效率合理；LLM-only成功率可达94%但速度性能严重下降；混合方法介于两者之间。LLM影响的方法表现出系统性保守偏差和模型依赖性变异。

Conclusion: 当前小型LLM在安全关键控制任务中存在重要限制，尽管能提高成功率但带来效率损失和保守行为，需要进一步改进才能可靠应用于自动驾驶。

Abstract: Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.

</details>


### [568] [Conformal Online Learning of Deep Koopman Linear Embeddings](https://arxiv.org/abs/2511.12760)
*Ben Gao,Jordan Patracone,Stéphane Chrétien,Olivier Alata*

Main category: cs.LG

TL;DR: COLoKe是一个用于从流数据中自适应更新非线性动力系统Koopman不变表示的新框架，结合深度特征学习和提升空间中的多步预测一致性，通过一致性机制选择性更新模型。


<details>
  <summary>Details</summary>
Motivation: 需要从流数据中自适应学习非线性动力系统的Koopman不变表示，同时避免过拟合和不必要的模型更新。

Method: 结合深度特征学习和提升空间中的多步预测一致性，使用一致性机制动态评估当前Koopman模型的一致性，仅在预测误差超过动态校准阈值时触发更新。

Result: 在基准动力系统上的实证结果表明，COLoKe能有效维持长期预测准确性，同时显著减少不必要的更新并避免过拟合。

Conclusion: COLoKe框架成功实现了从流数据中自适应学习Koopman表示，在保持预测准确性的同时优化了模型更新策略。

Abstract: We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.

</details>


### [569] [INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers](https://arxiv.org/abs/2511.12764)
*Hao Wei,Aleksandra Franz,Bjoern List,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出间接神经校正器（INC），通过将学习到的校正项整合到控制方程中而非直接更新状态，显著减少混沌系统中长期模拟的自回归误差，实现稳定高效的PDE仿真。


<details>
  <summary>Details</summary>
Motivation: 传统混合求解器直接将学习到的校正应用于求解器输出，在混沌系统中会产生显著的自回归误差，导致长期模拟不稳定。

Method: 提出间接神经校正器（INC）框架，将学习到的校正项整合到控制方程中，而不是直接更新状态变量。该方法不依赖特定架构，可与任意神经网络和求解器集成。

Result: 在1D混沌系统到3D湍流的广泛基准测试中，INC将长期轨迹性能（R²）提升高达158.7%，稳定了激进粗化下的爆炸问题，在复杂3D湍流案例中实现了几个数量级的加速。

Conclusion: INC实现了具有形式误差减少的稳定高效PDE仿真，为具有可靠物理保证的更快科学和工程模拟铺平了道路。

Abstract: When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\(\mathrm{INC}\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \(\mathrm{INC}\) reduces the error amplification on the order of \(Δt^{-1} + L\), where \(Δt\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \(\mathrm{INC}\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\(R^2\)) by up to 158.7\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC

</details>


### [570] [MolEdit: Knowledge Editing for Multimodal Molecule Language Models](https://arxiv.org/abs/2511.12770)
*Zhenyu Lei,Patrick Soga,Yaochen Zhu,Yinhan He,Yushun Dong,Jundong Li*

Main category: cs.LG

TL;DR: MolEdit是一个针对分子语言模型的知识编辑框架，通过多专家知识适配器和专业知识感知编辑切换器，实现了对分子知识的精准编辑，同时保持不相关知识的完整性。


<details>
  <summary>Details</summary>
Motivation: 分子语言模型可能因训练数据过时或恶意篡改而编码和传播错误信息，影响下游发现流程。目前知识编辑在通用AI领域已有探索，但在分子语言模型中的应用仍属空白，面临分子知识多面性和相互依赖性的独特挑战。

Method: 提出MolEdit框架，包含多专家知识适配器（将编辑路由到不同分子方面的专业专家）和专业知识感知编辑切换器（仅在输入与存储编辑紧密匹配时激活适配器），以最小化对不相关知识的干扰。

Result: 在两个流行的分子语言模型骨干上进行的广泛实验表明，MolEdit在可靠性方面比基线方法高出18.8%，在局部性方面高出12.0%，同时保持效率。

Conclusion: MolEdit是首个针对分子语言模型的知识编辑框架，能够实现精准的分子知识编辑，同时有效保护不相关知识，为分子领域的AI安全提供了重要工具。

Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.

</details>


### [571] [Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation](https://arxiv.org/abs/2511.12779)
*Zhenshuo Zhang,Minxuan Duan,Youran Ye,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 提出PolicyGradEx算法，通过元训练和微调两阶段方法，将多目标强化学习任务高效聚类为k个相关组，实现16%性能提升和26倍加速。


<details>
  <summary>Details</summary>
Motivation: 在多目标强化学习中，当目标数量n增长时，为所有目标学习单一策略是次优的。需要将相关目标分组训练以提高效率。

Method: 两阶段方法：1）使用多任务学习训练元策略；2）对随机采样子集进行微调，利用策略网络的一阶近似特性估计任务亲和度矩阵，然后进行聚类分组。

Result: 在机器人控制和Meta-World基准测试中，相比最先进基线平均提升16%性能，速度提升达26倍。基于损失的聚类比随机分组和梯度相似性分组提升19%。

Conclusion: 该方法能有效估计多目标强化学习中的任务亲和度，实现高效分组训练，并通过Hessian迹分析提供了非平凡泛化误差度量。

Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\%$ on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.

</details>


### [572] [Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data](https://arxiv.org/abs/2511.12788)
*Rubén Darío Guerrero*

Main category: cs.LG

TL;DR: 提出了一种物理约束自适应学习框架，通过可学习参数自动校准电磁近似，在极紫外光刻优化中实现亚纳米精度，相比传统方法显著减少计算资源和训练样本需求。


<details>
  <summary>Details</summary>
Motivation: 半导体行业面临极紫外光刻优化的计算危机，传统方法消耗数十亿CPU小时却无法达到亚纳米精度，需要解决学术物理信息神经网络与工业部署需求之间的关键差距。

Method: 集成可微分模块（菲涅尔衍射、材料吸收、光学点扩散函数模糊、相移效应、对比度调制）与直接几何图案匹配目标，通过物理约束学习自动校准电磁近似参数。

Result: 在15个代表性图案上实现一致的亚纳米边缘放置误差性能（0.664-2.536 nm范围），仅需每个图案50个训练样本，相比无物理约束的CNN基线平均提升69.9%，推理速度显著快于严格电磁求解器。

Conclusion: 物理约束自适应学习为实时半导体制造优化建立了基础方法学，通过联合物理校准和制造精度目标，解决了学术研究到工业部署的关键差距。

Abstract: The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbolθ = \{θ_d, θ_a, θ_b, θ_p, θ_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.

</details>


### [573] [Optimal Look-back Horizon for Time Series Forecasting in Federated Learning](https://arxiv.org/abs/2511.12791)
*Dahao Tang,Nan Yang,Yanli Li,Zhiyu Zhu,Zhibo Jin,Dong Yuan*

Main category: cs.LG

TL;DR: 提出了一个联邦时间序列预测中自适应回溯窗口选择的原理性框架，通过内在空间公式化来解决联邦学习场景下的窗口选择问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景中，数据分散、异构且非独立，选择合适的回溯窗口是时间序列预测的基本挑战。现有方法主要局限于集中式和独立分布设置。

Method: 引入合成数据生成器捕捉客户端数据的时间结构，定义将时间序列窗口映射到内在表示空间的变换，推导预测损失的分解为贝叶斯项和近似项。

Result: 分析表明增加回溯窗口能改善确定性模式的可识别性，但由于模型复杂度增加和样本效率降低，也会增加近似误差。总预测损失在不可约损失开始饱和而近似损失持续上升的最小窗口处最小化。

Conclusion: 这项工作为联邦学习中时间序列预测的自适应窗口选择提供了严格的理论基础。

Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.

</details>


### [574] [Genomic Next-Token Predictors are In-Context Learners](https://arxiv.org/abs/2511.12797)
*Nathan Breslow,Aayush Mishra,Mahler Revsine,Michael C. Schatz,Anqi Liu,Daniel Khashabi*

Main category: cs.LG

TL;DR: 研究发现基因组模型通过大规模预测训练也能自然涌现出上下文学习能力，这与语言模型类似，表明上下文学习是跨模态的通用能力


<details>
  <summary>Details</summary>
Motivation: 探索上下文学习是否仅是人类语言特有的能力，还是其他符号序列领域通过大规模预测训练也能自然涌现的通用能力

Method: 使用Evo2基因组模型，开发包含语言和基因组形式的符号推理任务实验框架，直接比较基因组和语言模型的上下文学习能力

Result: 基因组模型与语言模型类似，随着上下文演示数量增加，模式归纳能力呈对数线性增长，首次证明基因组序列中自然涌现的上下文学习

Conclusion: 上下文学习是大规模预测建模在丰富数据上的结果，这一发现将涌现元学习扩展到语言之外，指向统一的、模态无关的上下文学习观点

Abstract: In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?
  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.

</details>


### [575] [The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation](https://arxiv.org/abs/2511.12804)
*Ali Falahati,Mohammad Mohammadi Amiri,Kate Larson,Lukasz Golab*

Main category: cs.LG

TL;DR: 本文为自消费生成模型的递归再训练提供了首个形式化分析框架，揭示了在Bradley-Terry模型下基于两阶段筛选机制的三种收敛机制，并证明了递归BT筛选机制在保持多样性、确保对称影响和消除初始化依赖方面的基本不可能性。


<details>
  <summary>Details</summary>
Motivation: 自消费生成模型在自身输出上训练时，与用户偏好的对齐变成了递归而非一次性过程，需要分析这种递归再训练对对齐的长期影响。

Method: 采用基于Bradley-Terry模型的两阶段筛选机制，将对齐建模为模型所有者（筛选学习输出）和公共用户（决定共享保留输出）两个派系之间的交互。

Result: 分析揭示了三种结构收敛机制：共识崩溃、共享最优妥协和不对称精炼。证明了递归BT筛选机制无法同时保持多样性、确保对称影响和消除初始化依赖的基本不可能定理。

Conclusion: 对齐不是静态目标而是演化均衡，既受权力不对称性影响，也受路径依赖影响。

Abstract: In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.

</details>


### [576] [Expressive Temporal Specifications for Reward Monitoring](https://arxiv.org/abs/2511.12808)
*Omar Adalat,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 使用定量线性时序逻辑(LTL_f[F])合成奖励监控器，为可观测状态轨迹生成密集奖励流，解决长时决策中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中指定信息丰富且密集的奖励函数是一个关键挑战，直接影响智能体训练效率。当前文献中占主导地位的布尔语义会导致长时决策中的稀疏奖励问题。

Method: 利用定量线性时序逻辑(LTL_f[F])的表达能力合成奖励监控器，这些监控器为运行时可观测状态轨迹生成密集奖励流。该框架是算法无关的，仅依赖于状态标记函数，并自然支持非马尔可夫性质的规范。

Result: 实验结果表明，定量监控器始终包含布尔监控器，并且根据环境的不同，在最大化任务完成定量度量和减少收敛时间方面优于布尔监控器。

Conclusion: 定量奖励监控器通过提供细粒度反馈，能有效引导智能体达到最优行为，缓解稀疏奖励问题，在强化学习中具有显著优势。

Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.

</details>


### [577] [Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs](https://arxiv.org/abs/2511.12817)
*Shasha Zhou,Mingyu Huang,Jack Cole,Charles Britton,Ming Yin,Jan Wolber,Ke Li*

Main category: cs.LG

TL;DR: 提出FAITH框架，使用医学知识图谱自动评估LLM生成响应的真实性，无需参考答案，通过分解声明、链接知识图谱和基于证据路径评分来实现。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域部署LLM需要严格的验证，了解潜在风险。研究使用医学知识图谱自动评估LLM响应真实性的可靠性和可行性。

Method: 引入FAITH框架，将响应分解为原子声明，链接到医学知识图谱，基于证据路径进行评分，无需参考答案。

Result: 实验显示基于知识图谱的评估与临床医生判断相关性更高，能有效区分不同能力的LLM，对文本变化具有鲁棒性，评分具有可解释性。

Conclusion: 尽管存在局限性，但利用知识图谱是医疗领域自动真实性评估的重要方向。

Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.

</details>


### [578] [Catastrophic Forgetting in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.12828)
*Mohammad Marufur Rahman,Guanchu Wang,Kaixiong Zhou,Minghan Chen,Fan Yang*

Main category: cs.LG

TL;DR: 本文系统研究了KANs在持续学习中的灾难性遗忘问题，建立了理论框架分析遗忘机制，并提出了KAN-LoRA适配器用于参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 虽然KANs被认为具有内在的抗遗忘能力，但其在持续学习中的实际行为和局限性尚不清楚，需要系统研究。

Method: 开发理论框架分析遗忘与激活支持重叠和内在数据维度的关系，通过合成和视觉任务实验验证，并设计KAN-LoRA适配器用于语言模型微调。

Result: KANs在低维算法设置中表现出良好的知识保留能力，但在高维领域如图像分类和语言建模中仍然容易发生遗忘。

Conclusion: 研究揭示了KANs在持续学习中的优势和局限性，为持续学习系统设计提供了实用见解。

Abstract: Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.

</details>


### [579] [An Evaluation of Representation Learning Methods in Particle Physics Foundation Models](https://arxiv.org/abs/2511.12829)
*Michael Chen,Raghav Kansal,Abhijith Gandrakota,Zichun Hao,Jennifer Ngadiuba,Maria Spiropulu*

Main category: cs.LG

TL;DR: 系统评估粒子物理学中的表示学习目标，在统一框架下比较对比学习、掩码粒子建模和生成重建等不同方法，并提出了达到最先进性能的监督架构改进。


<details>
  <summary>Details</summary>
Motivation: 为粒子物理学中的基础模型开发提供参考基准，通过受控比较来隔离学习目标的贡献，突出各自的优势和局限，促进社区更透明和稳健的进展。

Method: 使用共享的基于transformer的粒子云编码器，采用标准化的预处理、匹配采样和一致的评估协议，在喷注分类数据集上比较不同学习目标。

Result: 在基准评估中实现了最先进的性能，通过目标监督架构修改取得了优异结果。

Conclusion: 这项工作为粒子物理学中基础模型的未来发展提供了参考点，使整个社区能够进行更透明和稳健的进展。

Abstract: We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.

</details>


### [580] [Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency](https://arxiv.org/abs/2511.12838)
*Rongqin Chen,Fan Mo,Pak Lon Ip,Shenghui Zhang,Dan Wu,Ye Li,Leong Hou U*

Main category: cs.LG

TL;DR: Co-Sparsify是一个连接感知的稀疏化框架，通过将2节点消息传递限制在连通分量中，3节点交互限制在双连通分量中，在保持2-FWL表达能力的同时消除计算冗余。


<details>
  <summary>Details</summary>
Motivation: 现有的高阶图神经网络（HOGNNs）虽然表达能力更强，但计算成本高达O(n³)，而现有的效率优化方法通常以牺牲表达能力为代价。

Method: 提出Co-Sparsify框架，基于拓扑结构识别计算冗余：在双连通分量之外，结构关系可以通过2节点消息传递或全局读出完全捕获，无需高阶建模。

Result: 在PPGN上，Co-Sparsify在合成子结构计数任务中匹配或超越准确率，在真实世界基准测试（ZINC, QM9）中达到最先进性能。

Conclusion: 高表达能力和可扩展性并不相互排斥：基于拓扑指导的稀疏化能够实现强大且高效的GNN，并具有理论保证。

Abstract: Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.

</details>


### [581] [RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees](https://arxiv.org/abs/2511.12846)
*Zelin Zhu,Yancheng Huang,Kai Yang*

Main category: cs.LG

TL;DR: 提出RoS-Guard算法，用于线性系统不确定环境下的在线变化检测，通过神经展开实现GPU加速，提供理论性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有在线变化检测方法假设精确系统知识，不切实际；且在大规模系统中效率低下。

Method: 通过紧致松弛和重构OCD优化问题，采用神经展开实现GPU并行计算加速。

Result: 实验验证了RoS-Guard的有效性，在大规模系统场景中显著提升计算速度。

Conclusion: RoS-Guard是适用于不确定线性系统的鲁棒且最优的在线变化检测算法，具有理论保证和计算效率优势。

Abstract: Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.

</details>


### [582] [From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability](https://arxiv.org/abs/2511.12852)
*Jihoon Moon*

Main category: cs.LG

TL;DR: 提出一种控制理论框架，将训练好的神经网络视为非线性状态空间系统，通过局部线性化、可控性和可观测性Gramian矩阵以及Hankel奇异值来分析其内部计算。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然性能优异但难以进行机械解释，需要一种系统性的方法来分析其内部计算机制。

Method: 对于给定输入，在对应的隐藏激活模式周围线性化网络，构建状态空间模型。通过输入状态和状态输出Jacobian定义局部可控性和可观测性Gramian矩阵，计算Hankel奇异值和相关模式。

Result: 在简单前馈网络上验证了该框架，包括1-2-2-1 SwiGLU网络和2-3-3-2 GELU网络。发现激活饱和会降低可控性，缩小主导Hankel奇异值，并将主导内部模式转移到不同的神经元子集。

Conclusion: 该方法将神经网络转化为局部白盒动态模型集合，并识别出哪些内部方向是剪枝或约束以提高可解释性的自然候选者。

Abstract: Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.

</details>


### [583] [An approach of deep reinforcement learning for maximizing the net present value of stochastic projects](https://arxiv.org/abs/2511.12865)
*Wei Xu,Fan Yang,Qinyuan Cui,Zhi Chen*

Main category: cs.LG

TL;DR: 该研究提出使用双深度Q网络（DDQN）解决具有随机活动持续时间和现金流量的项目优化问题，目标是最大化期望净现值（NPV）。


<details>
  <summary>Details</summary>
Motivation: 传统刚性策略和动态策略在处理大规模或高度不确定环境下的项目优化时效果有限，需要更有效的解决方案。

Method: 将问题建模为离散时间马尔可夫决策过程（MDP），并采用双深度Q网络（DDQN）方法，利用双网络架构和目标网络来缓解动作值高估问题并提高训练稳定性。

Result: DDQN在比较实验中优于传统策略，特别是在大规模或高度不确定环境中表现出更优的计算能力、策略可靠性和适应性。消融研究证实双网络架构和目标网络的有效性。

Conclusion: DDQN不仅能在复杂项目优化中实现更高的期望NPV，还提供了稳定有效的策略实施框架。

Abstract: This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.

</details>


### [584] [On the Fundamental Limits of LLMs at Scale](https://arxiv.org/abs/2511.12869)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zeeshan Memon,Muhammad Ibtsaam Qadir,Sagnik Bhattacharya,Hassan Rizwan,Abhiram R. Gorle,Maahe Zehra Kazmi,Ayesha Mohsin,Muhammad Usman Rafique,Zihao He,Pulkit Mehta,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，系统分析了LLM扩展的五个根本限制：幻觉、上下文压缩、推理退化、检索脆弱性和多模态不对齐，从计算、信息和学习的基础理论角度揭示了扩展的天花板。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM扩展限制的描述停留在经验层面，缺乏连接计算、信息和学习基础理论的严谨理论综合。本文旨在填补这一空白，为LLM扩展提供理论支撑。

Method: 构建了一个统一的、基于证明的理论框架，从计算不可判定性、信息论约束和统计限制三个维度分析LLM扩展的根本限制，并结合定理和实证证据进行验证。

Result: 证明了对于任何可计算枚举的模型族，对角化方法保证存在某些输入会使模型失败；不可判定查询会导致所有可计算预测器的无限失败集；信息论约束限制了即使在可判定任务上的可达精度。

Conclusion: LLM扩展存在固有的理论天花板，在某些领域扩展会饱和甚至无法进步。提出了有界预言检索、位置课程学习、稀疏或分层注意力等实际缓解路径。

Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.

</details>


### [585] [On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples](https://arxiv.org/abs/2511.12881)
*Cheongjae Jang,Jonghyun Won,Soyeon Jun,Chun Kee Chung,Keehyoung Joo,Yung-Kyun Noh*

Main category: cs.LG

TL;DR: 本文分析了有限样本下一维Wasserstein距离的信息处理能力，证明其能够捕捉点态密度差异并与支撑差异相协调，在神经尖峰序列解码和氨基酸接触频率数据中得到验证。


<details>
  <summary>Details</summary>
Motivation: 当两个密度函数的支撑显著重叠但密度存在明显点态差异时，Wasserstein距离能否准确识别这些差异及其在有限样本下的解析特性尚不明确。

Method: 利用泊松过程并分离速率因子，分析一维Wasserstein距离在有限样本下的信息处理能力。

Result: 结果显示一维Wasserstein距离能够突出与速率和支撑相关的有意义的密度差异。

Conclusion: 一维Wasserstein距离能够有效捕捉点态密度差异，并与支撑差异信息相协调，在神经科学和生物信息学应用中具有实用价值。

Abstract: Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.

</details>


### [586] [Method of Manufactured Learning for Solver-free Training of Neural Operators](https://arxiv.org/abs/2511.12890)
*Arth Sojitra,Omer San*

Main category: cs.LG

TL;DR: MML是一种不依赖求解器的框架，通过分析构造物理一致的数据集来训练神经算子，避免了传统方法对数值求解器生成数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子训练需要大量由实验或数值求解器生成的数据，这限制了可扩展性并制约了对物理系统的探索。

Method: 受经典制造解方法启发，MML用函数合成替代数值数据生成：从受控分析空间采样平滑候选解，通过直接应用控制微分算子推导相应的强迫场。在推理时，将这些强迫项设为零即可恢复原始控制方程。

Result: 在热方程、平流方程、Burgers方程和扩散反应方程等基准测试中，MML实现了高谱精度、低残差误差以及对未见条件的强泛化能力。

Conclusion: 通过将数据生成重新定义为分析合成过程，MML提供了一条可扩展、求解器无关的路径，用于构建保持控制定律保真度的物理基础神经算子，无需依赖昂贵的数值模拟或实验数据进行训练。

Abstract: Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.

</details>


### [587] [Functional Mean Flow in Hilbert Space](https://arxiv.org/abs/2511.12898)
*Zhiqi Li,Yuchen Sun,Greg Turk,Bo Zhu*

Main category: cs.LG

TL;DR: FMF是一种在无限维希尔伯特空间中定义的一步生成模型，将Mean Flow框架扩展到函数域，提供函数流匹配的理论公式和高效训练采样的实际实现。


<details>
  <summary>Details</summary>
Motivation: 将一步Mean Flow框架扩展到函数域，解决函数数据生成任务的需求，如时间序列、图像、PDE和3D几何等。

Method: 提出函数流匹配的理论公式，引入$x_1$-预测变体提高稳定性，提供高效训练和采样的实际实现。

Result: 开发了一个实用的函数流匹配方法，适用于广泛的函数数据生成任务。

Conclusion: FMF是一个实用的函数流匹配框架，可有效处理各种函数数据生成问题。

Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.

</details>


### [588] [Contrastive Entropy Bounds for Density and Conditional Density Decomposition](https://arxiv.org/abs/2511.12903)
*Bo Hu,Jose C. Principe*

Main category: cs.LG

TL;DR: 该论文从贝叶斯高斯视角研究神经网络特征的可解释性，提出通过最大化高斯算子的迹来训练自编码器，以及使用核范数作为散度来训练混合密度网络，并引入基于希尔伯特空间内积的边界来提高样本多样性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络特征的可解释性，从贝叶斯高斯视角理解优化过程，探索如何通过概率边界和密度估计来改进神经网络训练。

Method: 使用希尔伯特空间和分解方法处理多输出网络产生的多个高斯混合中心；提出最大化高斯算子迹训练自编码器，使用核范数作为散度训练MDNs；引入基于内积的边界来增加样本多样性。

Result: 发现自编码器目标等价于最大化高斯算子迹；提出核范数作为散度训练MDNs；基于希尔伯特空间内积的边界能防止网络输出常数解，提高样本多样性。

Conclusion: 从贝叶斯高斯视角为神经网络特征提供了新的可解释性框架，提出的方法能够改进自编码器和混合密度网络的训练，并防止模型坍塌问题。

Abstract: This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.

</details>


### [589] [LinkedIn Profile Characteristics and Professional Success Indicators](https://arxiv.org/abs/2511.12905)
*Tania-Amanda Fredrick Eneye,Ashlesha Malla,Pawan Paudel*

Main category: cs.LG

TL;DR: 基于6.2万个LinkedIn匿名档案，研究通过机器学习模型分析档案特征与职业成功的关系，发现晋升可高度预测，但粉丝增长更复杂。


<details>
  <summary>Details</summary>
Motivation: 探索LinkedIn档案特征与职业成功指标（晋升、粉丝数、职业发展速度）之间的关系，为专业人士优化LinkedIn存在和职业策略提供指导。

Method: 使用机器学习技术对超过6.2万个匿名LinkedIn档案数据集开发预测模型，识别影响职业成功的关键因素。

Result: 晋升具有高度可预测性，但粉丝增长表现出更大的复杂性，研究确定了驱动职业成功的最具影响力因素。

Conclusion: 该研究为专业人士优化LinkedIn存在和职业发展策略提供了可行的见解，强调了不同成功指标的预测难度差异。

Abstract: This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.

</details>


### [590] [APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift](https://arxiv.org/abs/2511.12945)
*Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Tao Sun,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出APT模块，通过时间戳条件原型学习生成仿射参数，解决时间序列预测中的分布偏移问题，兼容任意预测主干网络和归一化策略。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖局部统计归一化，无法捕捉全局分布偏移，RevIN等方法在处理缺失值、噪声观测和无效通道仿射变换方面存在困难。

Method: APT模块利用时间戳条件原型学习动态生成仿射参数，调制输入和输出序列，使主干网络能从自监督、分布感知的聚类实例中学习。

Result: 在六个基准数据集和多种主干-归一化组合上的实验表明，APT显著提高了分布偏移下的预测性能。

Conclusion: APT是一个轻量级、灵活的插件模块，能有效注入全局分布特征到归一化-预测流程中，提升分布偏移下的时间序列预测效果。

Abstract: Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.

</details>


### [591] [A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series](https://arxiv.org/abs/2511.12951)
*Ziling Fan,Ruijia Liang,Yiwen Hu*

Main category: cs.LG

TL;DR: 提出基于FEDformer的混合框架，用于金融时间序列的异常检测和风险预测，在S&P 500、NASDAQ和布伦特原油数据集上表现优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 金融市场具有高度波动性，传统深度学习模型难以捕捉金融数据中的长期依赖和复杂周期模式，需要更有效的异常检测和风险预测方法。

Method: 集成频率增强分解Transformer（FEDformer）、基于残差的异常检测器和风险预测头，在时域和频域建模时间动态，分解趋势和季节成分。

Result: 相比基准方法，RMSE降低15.7%，异常检测F1分数提升11.5%，在多个金融数据集上验证了模型有效性。

Conclusion: 该模型能有效捕捉金融波动性，为市场崩盘预测和风险管理提供可靠的早期预警系统。

Abstract: Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.

</details>


### [592] [Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series](https://arxiv.org/abs/2511.12955)
*Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的全局跨时间注意力融合(GCTAF)架构，用于解决太阳耀斑预测中的时间序列不平衡问题，通过可学习的全局标记来捕捉关键的非连续时间点。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑预测中的多元时间序列分类面临严重的数据不平衡问题，强耀斑事件稀少，传统方法难以有效学习。需要增强模型对长程时间模式的理解能力。

Method: GCTAF架构引入可学习的跨注意力全局标记，这些标记通过交叉注意力与输入序列交互，总结整个序列中的显著时间模式，然后融合回时间表示中。

Result: 在基准太阳耀斑数据集上的评估表明，GCTAF能有效检测强耀斑事件并提升预测性能。

Conclusion: 改进基于Transformer的架构为太阳耀斑预测任务提供了高潜力的替代方案，全局跨时间注意力机制能有效捕捉与耀斑相关的判别性动态。

Abstract: Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.

</details>


### [593] [RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems](https://arxiv.org/abs/2511.12979)
*Zhengchao Wang,Yitao Hu,Jianing Ye,Zhuxuan Chang,Jiazheng Yu,Youpeng Deng,Keqiu Li*

Main category: cs.LG

TL;DR: RAGPulse是一个开源RAG工作负载跟踪数据集，收集自服务超过4万用户的大学问答系统，旨在解决现有LLM推理跟踪无法捕捉RAG特定动态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的通用LLM推理跟踪无法捕捉RAG系统的多阶段流水线（检索、生成）和独特工作负载特征（如知识依赖性），导致学术研究与实际部署之间存在显著性能差距。

Method: 从2024年4月起运行的大学问答系统中收集数据，采用隐私保护的基于哈希的数据格式，详细描述了系统架构并进行了深入的统计分析。

Result: 分析显示真实世界的RAG工作负载表现出显著的时间局部性和高度偏斜的热门文档访问模式。

Conclusion: RAGPulse为研究人员开发和验证RAG系统的新型优化策略（如内容感知批处理和检索缓存）提供了高保真基础，最终提升RAG服务的效率和可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.

</details>


### [594] [Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks](https://arxiv.org/abs/2511.12985)
*Minsoo Jo,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 提出了一种基于双曲几何的新型对抗攻击方法，通过利用双曲空间的几何特性，在切线空间中计算梯度并分解为径向和角度分量，仅从角度方向生成扰动，从而更有效地攻击双曲网络。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击方法如FGSM和PGD在欧几里得几何中开发，但未考虑双曲网络的非欧几何结构，可能导致效率低下或几何不一致的攻击。需要重新评估在非欧几何中的攻击策略。

Method: 在双曲空间的切线空间中计算损失函数梯度，将其分解为径向（深度）分量和角度（语义）分量，仅从角度方向应用扰动，生成专注于语义敏感方向的对抗样本。

Result: 在图像分类、跨模态检索任务和网络架构上的实验表明，该方法比传统对抗攻击获得更高的欺骗率，同时产生具有更高影响力的扰动，更深入地揭示了双曲嵌入的脆弱性。

Conclusion: 这项工作强调了在弯曲表示空间中几何感知对抗策略的重要性，并为攻击分层嵌入提供了一个原则性框架。

Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.

</details>


### [595] [Learning Branching Policies for MILPs with Proximal Policy Optimization](https://arxiv.org/abs/2511.12986)
*Abdelouahed Ben Mhamed,Assia Kamal-Idrissi,Amal El Fallah Seghrouchni*

Main category: cs.LG

TL;DR: 提出了Tree-Gate Proximal Policy Optimization (TGPPO)框架，使用强化学习训练分支策略，以提升混合整数线性规划中分支定界算法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于模仿学习的分支策略容易过拟合专家演示，难以泛化到结构多样或未见过的实例。

Method: 使用近端策略优化(PPO)强化学习算法，构建参数化状态空间表示来动态捕捉搜索树演化上下文。

Result: TGPPO在减少探索节点数和改进p-原始对偶积分方面优于现有学习方法，特别是在分布外实例中表现突出。

Conclusion: 强化学习有潜力为MILP求解器开发鲁棒且适应性强的分支策略。

Abstract: Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.

</details>


### [596] [Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs](https://arxiv.org/abs/2511.13010)
*Jeongwhan Choi,Seungjun Park,Sumin Park,Sung-Bae Cho,Noseong Park*

Main category: cs.LG

TL;DR: 提出了一种名为分形节点的新概念，通过将图划分为反映完整图连接模式的子图，在保持MPNN计算效率的同时增强长距离依赖关系。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在平衡局部和全局信息方面的困难，以及图Transformer虽然能处理长距离交互但忽略MPNN固有局部性和效率的问题。

Method: 基于现实网络中的分形结构，引入分形节点与原始节点共存，自适应聚合子图级特征表示，强制子图内特征相似性，提供直接快捷连接以缓解过度压缩问题。

Result: 实验结果表明该方法提高了MPNN的表达能力，在保持MPNN计算效率的同时，在长距离依赖方面达到或优于图Transformer的性能。

Conclusion: 分形节点概念有效平衡了局部和全局信息，为图神经网络提供了一种高效处理长距离依赖的新方法。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.

</details>


### [597] [The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training](https://arxiv.org/abs/2511.13016)
*Subramanyam Sahoo*

Main category: cs.LG

TL;DR: 提出一个统一框架研究硬奖励、连续奖励和混合奖励结构在数学推理任务上微调大语言模型的效果，通过自适应混合奖励调度器在离散和连续信号间切换，提高收敛速度和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 奖励设计在基于人类反馈的强化学习和对齐研究中至关重要，需要研究不同奖励结构对数学推理任务的影响。

Method: 使用Qwen3-4B模型在GSM8K数据集上进行LoRA微调，形式化评估包含正确性、困惑度、推理质量和一致性的奖励公式，引入自适应混合奖励调度器。

Result: 混合奖励结构相比纯硬奖励或连续奖励方法，提高了收敛速度和训练稳定性。

Conclusion: 自适应奖励建模为对齐研究提供了新的见解，混合奖励结构在数学推理任务中表现更优。

Abstract: Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.

</details>


### [598] [The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference](https://arxiv.org/abs/2511.13018)
*Sairam S,Sara Girdhar,Shivam Soni*

Main category: cs.LG

TL;DR: R-Learner在图上应用存在严重瓶颈，最终阶段模型的归纳偏差是性能主要驱动因素，图盲最终阶段会导致完全失败，而端到端Graph R-Learner显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: R-Learner在估计异质处理效应方面具有理论优势，但在网络数据中应用面临挑战，因为其核心假设要求最终阶段模型能正确捕捉图依赖的因果异质性。

Method: 通过大规模实证研究系统分析R-Learner在图上的表现，提出端到端Graph R-Learner，并进行有针对性的"Hub-Periphery Trade-off"分析来解释GNN过压缩现象。

Result: 统计显著证据表明图盲最终阶段完全失败（MSE > 4.0），而Graph R-Learner成功并显著优于非DML GNN T-Learner基线。发现拓扑依赖的"nuisance bottleneck"与GNN过压缩相关。

Conclusion: R-Learner在图上的性能主要受最终阶段瓶颈制约，端到端Graph R-Learner是有效解决方案，研究结果为该领域提供了重要基准。

Abstract: The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic "representation bottleneck": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent "nuisance bottleneck," linking it to GNN over-squashing via a targeted "Hub-Periphery Trade-off" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical "final-stage bottleneck."

</details>


### [599] [Learning Time-Scale Invariant Population-Level Neural Representations](https://arxiv.org/abs/2511.13022)
*Eshani Patel,Yisong Yue,Geeling Chau*

Main category: cs.LG

TL;DR: 论文提出了时间尺度增强预训练（TSAP）方法，通过增强预训练阶段的时间尺度不变性，解决神经基础模型对预处理时间尺度不匹配的敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经时间序列基础模型在预训练和下游任务之间存在时间尺度不匹配时泛化能力较差，这限制了模型的通用性。

Method: 引入时间尺度增强预训练（TSAP），通过在预训练阶段增强对时间尺度变化的鲁棒性，构建具有时间尺度不变性的表示空间。

Result: TSAP方法在不同解码任务中一致地提高了对不同时间尺度的鲁棒性，并在表示空间中建立了不变性。

Conclusion: 处理预处理多样性是构建可泛化神经基础模型的关键步骤，TSAP为解决时间尺度不匹配问题提供了有效方案。

Abstract: General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.

</details>


### [600] [SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment](https://arxiv.org/abs/2511.13023)
*Jiacheng Wang,Yejun Zeng,Jinyang Guo,Yuqing Ma,Aishan Liu,Xianglong Liu*

Main category: cs.LG

TL;DR: SLMQuant是首个针对小语言模型(SLMs)的系统性量化基准，揭示了SLMs与大型语言模型(LLMs)在量化敏感性上的根本差异，并提出了针对SLMs的压缩设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管小语言模型作为资源高效的替代方案受到关注，但在边缘设备上的部署仍面临挑战，因为模型压缩的效率差距尚未解决。量化对LLMs有效，但对SLMs的应用研究严重不足。

Method: 通过跨多种架构和任务的多轨道综合评估，分析最先进的量化方法在SLMs上的表现，识别影响SLM量化效果的关键因素。

Result: 研究发现SLMs与LLMs在量化敏感性上存在根本差异，直接转移LLM优化技术会导致次优结果，因为SLMs具有独特的架构特征和训练动态。

Conclusion: SLMQuant为在边缘应用中推进高效SLM部署建立了基础框架，并为在资源受限场景中部署轻量级语言模型提供了关键见解。

Abstract: Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.

</details>


### [601] [One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow](https://arxiv.org/abs/2511.13035)
*Zeyuan Wang,Da Li,Yulin Chen,Ye Shi,Liang Bai,Tianyuan Yu,Yanwei Fu*

Main category: cs.LG

TL;DR: 提出一种单步生成策略，通过MeanFlow的残差重构实现从噪声到动作的直接映射，兼容Q学习，在离线强化学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有单步高斯策略推理快但难以捕捉复杂多峰动作分布，而基于流的方法表达能力更强但通常需要蒸馏和两阶段训练。

Method: 将MeanFlow重构为单步噪声到动作生成策略，通过残差公式将速度场和噪声到动作变换集成到单一策略网络中，无需单独速度估计。

Result: 在OGBench和D4RL基准的73个任务上验证，在离线和离线到在线强化学习设置中均取得强劲性能。

Conclusion: 该方法实现了高效单步生成、多峰动作分布建模和单阶段稳定策略学习的三重优势。

Abstract: We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.

</details>


### [602] [Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data](https://arxiv.org/abs/2511.13044)
*Rosario Napoli,Giovanni Lonia,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: Bi-View是一种新颖的混合方法，通过结合Node2Vec和GraphSAGE来增强知识图谱中的节点特征，生成改进的图嵌入，无需依赖额外合成数据。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法需要大量数据，在稀疏或不完整场景下表现受限。知识图谱由于其语义性质可能隐藏大量信息，需要增强节点特征来改进图机器学习模型。

Method: 结合两种互补的图嵌入技术：Node2Vec（通过无监督随机游走捕获结构模式）和GraphSAGE（以监督方式聚合邻域信息）。首先计算Node2Vec嵌入表示图拓扑，然后用基于中心性的指标丰富节点特征作为GraphSAGE输入，最后通过融合层结合两种表示。

Result: 该方法提高了下游任务性能，特别是在初始特征较差的情况下，为更准确和精确的知识图谱增强图机器学习模型奠定了基础。

Conclusion: Bi-View方法能够捕获图的拓扑和语义属性，使模型能够利用数据集中存在但未明确表示的信息特征，在知识图谱场景下显著提升了图机器学习模型的性能。

Abstract: Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.

</details>


### [603] [Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information](https://arxiv.org/abs/2511.13049)
*Antoine Ledent,Mun Chong Soo,Nong Minh Hieu*

Main category: cs.LG

TL;DR: 该论文研究了一个矩阵补全问题，其中真实矩阵R和未知采样分布P都是低秩矩阵且共享共同子空间。利用大量无标签数据（M）和少量带标签数据（N），通过低秩子空间恢复理论和矩阵补全泛化边界，获得了包含两个误差项的误差界。


<details>
  <summary>Details</summary>
Motivation: 受推荐系统启发，其中无标签数据对应'隐式反馈'（如购买、点击等），带标签数据对应'显式反馈'（如用户评分）。研究如何在显式和隐式反馈交互下改进矩阵补全性能。

Method: 利用低秩子空间恢复理论和矩阵补全模型的经典泛化边界，假设真实矩阵R和采样分布P都是低秩矩阵且共享共同子空间，结合大量无标签数据和小量带标签数据进行矩阵补全。

Result: 获得了误差界：$\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ + $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$，其中d是P的秩，r是M的秩。在合成实验中验证了泛化误差自然分解为P和真实矩阵的估计误差项。在真实数据集上，该方法在显式评分较少时优于仅依赖显式评分的基线方法。

Conclusion: 该研究为推荐系统中显式和隐式反馈交互提供了一个有效的理论框架，证明了结合两种反馈可以提升矩阵补全性能。

Abstract: We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.

</details>


### [604] [Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting](https://arxiv.org/abs/2511.13052)
*Yunhun Nam,Jaehyung Kim,Jongheon Jeong*

Main category: cs.LG

TL;DR: 提出了LfU（Learning-from-the-Undesirable）方法，一种简单有效的SFT正则化方案，通过在有限数据下使模型对不良更新具有鲁棒性来缓解过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 在有限数据下进行监督微调时，语言模型容易过拟合，依赖虚假模式或牺牲广泛有用的能力。需要一种正则化方法来提升泛化能力。

Method: 提出一致性正则化方法，直接对齐模型内部表示与经过不良更新后的表示，通过表示级数据增强来促进泛化。

Result: 在数学任务上比普通SFT平均提升16.8%，输出性能的标准差降低92.1%，显示更好的鲁棒性。

Conclusion: LfU作为有效先验，在有限数据下增强适应性同时保留预训练知识，具有多功能效果。

Abstract: Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.

</details>


### [605] [Latency and Ordering Effects in Online Decisions](https://arxiv.org/abs/2511.13060)
*Duo Yi*

Main category: cs.LG

TL;DR: 论文提出了一个延迟反馈和顺序敏感动态下在线决策系统的性能下界框架，将延迟、非交换性和实现差距等异质效应统一到一个可解释的下界表述中。


<details>
  <summary>Details</summary>
Motivation: 在线决策系统通常在延迟反馈和顺序敏感动态下运行，其中动作会影响观察结果的到达顺序。现有方法难以统一处理这些异质效应，需要一个新的理论框架来提供结构化性能保证。

Method: 使用Bregman散度作为损失基准，证明超额基准损失存在结构化下界，包含延迟惩罚、顺序敏感性惩罚、几何交互项和非凸性惩罚。扩展到近正则和弱凸设置，并通过2×2随机实验和流式诊断进行估计和监控。

Result: 获得了超越凸情况的鲁棒保证，将异质延迟、非交换性和实现差距效应打包到单个可解释的下界表述中，可在实际系统中进行压力测试和调优。

Conclusion: 该框架为延迟反馈和顺序敏感动态下的在线决策系统提供了统一的理论基础和实用工具，能够系统性地分析和优化系统性能。

Abstract: Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_Φ$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \ge L_{\mathrm{ideal}} + g_1(λ) + g_2(\varepsilon_\star) + g_{12}(λ,\varepsilon_\star) - D_{\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\mathrm{ncx}}\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.

</details>


### [606] [MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity](https://arxiv.org/abs/2511.13061)
*Vladimír Macko,Vladimír Boža*

Main category: cs.LG

TL;DR: MACKO-SpMV是一种GPU优化的稀疏矩阵向量乘法格式和内核，专门针对大语言模型中的非结构化稀疏性设计，在50%稀疏度下实现1.5倍内存减少和1.2-1.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有SpMV方法在LLM剪枝中常见的低且非结构化稀疏度（30-90%）下表现不佳，非结构化剪枝只能提供有限的内存减少和加速效果。

Method: 提出MACKO-SpMV，这是一种GPU优化的格式和内核协同设计，减少存储开销同时保持与GPU执行模型的兼容性，无需专用硬件单元或格式特定的预计算。

Result: 在50%稀疏度下，MACKO首次实现显著的内存减少（1.5倍）和加速（1.2-1.5倍）；相比其他SpMV基线：比cuSPARSE快2.8-13.0倍，比Sputnik快1.9-2.6倍，比DASP快2.2-2.5倍。应用于Llama2-7B模型，在fp16精度下实现1.5倍内存减少和1.5倍推理加速。

Conclusion: 得益于MACKO，50%稀疏度的非结构化剪枝现在可以在实际LLM工作负载中合理应用。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.

</details>


### [607] [Self-Adaptive Graph Mixture of Models](https://arxiv.org/abs/2511.13062)
*Mohit Meena,Yash Punjabi,Abhishek A,Vishal Sharma,Mahesh Chandran*

Main category: cs.LG

TL;DR: 提出SAGMM框架，通过自适应选择和组合多种GNN架构来解决模型选择难题，在16个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前GNN性能趋于平稳，复杂模型未必优于经典模型，且模型选择困难，需要能自动适配不同图任务的解决方案。

Method: 采用模块化框架，利用拓扑感知注意力门控机制自适应分配专家模型，包含剪枝机制提高效率，支持预训练专家模型。

Result: 在节点分类、图分类、回归和链接预测等16个基准数据集上，SAGMM始终优于或匹配领先的GNN基线和混合方法。

Conclusion: SAGMM为现实世界图学习提供了稳健且自适应的解决方案，能够有效应对不同图任务和数据集。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.

</details>


### [608] [A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning](https://arxiv.org/abs/2511.13078)
*Liuyi Jin,Pasan Gunawardena,Amran Haroon,Runzhi Wang,Sangwoo Lee,Radu Stoleru,Michael Middleton,Zepeng Huo,Jeeeun Kim,Jason Moats*

Main category: cs.LG

TL;DR: EMSGlass是一个基于EMSNet和EMSServe的智能眼镜系统，用于提升急救医疗服务的效率和决策质量。EMSNet是首个多模态多任务模型，整合文本、生命体征和场景图像；EMSServe是低延迟服务框架，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 急救医疗技术人员在高压环境下需要快速做出关键决策，面临沉重的认知和操作负担，需要智能化辅助系统来提升效率和准确性。

Method: 开发了EMSNet多模态多任务模型，整合文本、生命体征和场景图像；构建了EMSServe低延迟服务框架，包含模态感知模型分割器和特征缓存机制；在真实多模态EMS数据集上训练。

Result: EMSNet同时支持五个关键EMS任务，准确率优于单模态基线；EMSServe比直接PyTorch多模态推理快1.9-11.7倍；用户研究显示EMSGlass能增强实时态势感知、决策速度和操作效率。

Conclusion: EMSGlass成功将多模态智能与真实世界急救响应工作流程相结合，为下一代AI赋能的EMS系统提供了可行方向。

Abstract: Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.

</details>


### [609] [Real-time prediction of breast cancer sites using deformation-aware graph neural network](https://arxiv.org/abs/2511.13082)
*Kyunghyun Lee,Yong-Min Shin,Minwoo Shin,Jihun Kim,Sunghwan Lim,Won-Yong Shin,Kyungho Yoon*

Main category: cs.LG

TL;DR: 开发基于图神经网络的实时乳腺变形预测模型，用于提高间接MRI引导活检的准确性，实现0.2mm精度的癌灶位移预测和4000倍计算加速。


<details>
  <summary>Details</summary>
Motivation: 直接MRI引导活检存在时间长、成本高的问题，间接MRI引导活检需要准确的实时乳腺变形模型，但现有方法在创建精确的实时可变形乳腺模型方面面临挑战。

Method: 结合MRI图像的结构信息开发个体特异性有限元模型模拟变形行为，然后使用图神经网络处理表面位移和基于距离的图数据，预测整体组织位移包括肿瘤区域变形。

Result: 在体模和真实患者数据集上验证，癌症节点位移精度达到0.2mm（RMSE），与实际癌区空间重叠的DSC为0.977，实时推理速度比传统有限元模拟快4000倍以上。

Conclusion: 提出的变形感知GNN模型为乳腺活检中的实时肿瘤位移预测提供了有前景的解决方案，具有高精度和实时能力，与临床程序集成可显著提高乳腺癌诊断的精确性和效率。

Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.

</details>


### [610] [Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions](https://arxiv.org/abs/2511.13103)
*Vidur Sinha,Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: STACCA是一个基于Transformer的多智能体强化学习框架，通过集中式图Transformer评论家和共享图Transformer执行器来解决网络控制中的长程依赖和拓扑泛化问题，并在流行病控制和谣言传播任务中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法存在两个主要局限：1) 依赖局部交互衰减假设，难以捕捉长程依赖关系（如级联电力故障、疫情爆发）；2) 缺乏跨网络拓扑的泛化能力，需要在新图上重新训练。

Method: STACCA框架包含：1) 集中式图Transformer评论家，建模长程依赖并提供系统级反馈；2) 共享图Transformer执行器，学习可泛化策略；3) 新颖的反事实优势估计器，改进训练中的信用分配。

Result: 在流行病控制和谣言传播网络控制任务上的评估表明，STACCA在性能、网络泛化能力和可扩展性方面均有显著提升。

Conclusion: 基于Transformer的MARL架构在大规模网络系统中具有实现可扩展和可泛化控制的潜力。

Abstract: Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.

</details>


### [611] [Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning](https://arxiv.org/abs/2511.13116)
*Qipeng Song,Nan Yang,Ziqi Xu,Yue Li,Wei Shao,Feng Xia*

Main category: cs.LG

TL;DR: GFOES是一个用于机器遗忘的新框架，在仅能访问少量保留数据且无法访问遗忘数据的约束条件下，通过生成最优擦除样本和两阶段微调实现有效的类别遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器遗忘方法需要访问完整原始训练数据集的不切实际问题，应对更现实的少样本零访问场景。

Method: 提出GFOES框架，包含生成反馈网络(GFN)生成最优擦除样本(OES)，以及两阶段微调过程：第一阶段进行激进遗忘，第二阶段恢复模型性能。

Result: 在三个图像分类数据集上的实验表明，GFOES在仅使用5%原始数据的情况下，在logit和表示层面都实现了有效遗忘，同时保持了良好的性能。

Conclusion: 该框架为数据受限条件下的隐私保护机器学习提供了实用且可扩展的解决方案。

Abstract: Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.

</details>


### [612] [Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges](https://arxiv.org/abs/2511.13124)
*Changxi Chi,Yufei Huang,Jun Xia,Jiangbin Zheng,Yunfan Liu,Zelin Zang,Stan Z. Li*

Main category: cs.LG

TL;DR: 提出了一种基于薛定谔桥的生成模型，通过小批量最优传输配对直接对齐控制组和扰动组的单细胞分布，解决了单细胞扰动数据不成对的问题，在遗传和药物扰动数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞扰动预测是基因功能分析和药物候选筛选的关键，但由于测序的破坏性，单细胞数据天然不成对。现有生成模型缺乏显式条件化或依赖先验空间进行间接分布对齐，限制了精确的扰动建模。

Method: 近似薛定谔桥来直接对齐不同扰动条件下控制组和扰动组的单细胞分布。利用小批量最优传输配对避免双向推理，直接指导桥学习，实现可扩展的SB近似。同时建模离散基因激活状态和连续表达分布。

Result: 在公共遗传和药物扰动数据集上的实验表明，该模型有效捕捉了异质性单细胞响应，并实现了最先进的性能。

Conclusion: 通过薛定谔桥近似和小批量最优传输配对，成功解决了单细胞扰动数据的不成对问题，能够准确建模扰动并捕捉单细胞异质性。

Abstract: Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.

</details>


### [613] [Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2511.13133)
*Shudong Wang,Xinfei Wang,Chenhao Zhang,Shanchen Pang,Haiyuan Gui,Wenhao Ji,Xiaojian Liao*

Main category: cs.LG

TL;DR: SoCo-DT是一种基于参数重要性的软冲突解决方法，通过动态调整掩码值和自适应稀疏度策略，有效缓解多任务强化学习中的梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码的多任务强化学习方法存在两个问题：粗粒度二元掩码过度抑制关键冲突参数，阻碍任务间知识共享；不同任务冲突程度不同，但现有方法使用固定稀疏度策略，影响模型泛化能力和学习效率。

Method: 1. 基于Fisher信息动态调整掩码值，保留重要参数同时抑制冲突参数；2. 基于四分位距的动态稀疏度调整策略，构建任务特定的阈值方案；3. 使用非对称余弦退火调度持续更新阈值。

Result: 在Meta-World基准测试中，SoCo-DT在MT50上比最先进方法提升7.6%，在次优数据集上提升10.5%。

Conclusion: SoCo-DT通过软冲突解决和动态稀疏度调整，有效缓解梯度冲突，显著提升多任务性能。

Abstract: Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.

</details>


### [614] [Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching](https://arxiv.org/abs/2511.13144)
*Jiacheng Cheng,Xu Zhang,Guanghui Qiu,Yifang Zhang,Yinchuan Li,Kaiyuan Feng*

Main category: cs.LG

TL;DR: 提出了pFed1BS个性化联邦学习框架，通过一比特随机草图实现极致的通信压缩，在降低通信开销的同时处理客户端数据异构性问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临双向通信开销和客户端数据异构性两大挑战，需要在降低通信成本的同时有效处理数据异质性。

Method: 采用一比特随机草图进行通信压缩，客户端传输压缩后的一比特草图，服务器聚合并广播全局一比特共识，引入基于符号的正则化器指导本地模型与全局共识对齐，同时使用快速哈达玛变换提高计算效率。

Result: 理论分析证明算法收敛到全局势函数的稳定邻域，数值模拟显示pFed1BS显著降低通信成本，同时达到与先进通信高效FL算法相当的竞争性能。

Conclusion: pFed1BS框架成功解决了联邦学习中的通信开销和数据异构性问题，通过一比特压缩技术实现了高效的个性化联邦学习。

Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.

</details>


### [615] [OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs](https://arxiv.org/abs/2511.13147)
*Shaoyuan Chen,Zhixuan Chen,Dawei Yang,Zhihang Yuan,Qiang Wu*

Main category: cs.LG

TL;DR: OTARo是一种新颖的方法，通过一次微调使设备上的LLM能够灵活切换量化精度，同时保持性能鲁棒性。它引入共享指数浮点(SEFP)量化机制，通过简单截断尾数产生不同比特宽度，并采用探索-利用比特宽度路径搜索和低精度异步累积策略实现比特宽度鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法存在结构限制，无法在复杂现实场景中支持设备上精度切换。理解任务和生成任务对精度要求不同，需要灵活的量化精度切换能力。

Method: 提出共享指数浮点(SEFP)量化机制，通过截断尾数产生不同比特宽度；采用探索-利用比特宽度路径搜索(BPS)迭代更新搜索路径；使用低精度异步累积(LAA)在低比特宽度下执行异步梯度累积和延迟更新。

Result: 在LLaMA3.2-1B、LLaMA3-8B等流行LLM上的实验表明，OTARo在所有精度下都能实现一致强大且鲁棒的性能。

Conclusion: OTARo成功解决了传统量化方法在设备上精度切换的局限性，通过一次微调实现了灵活的量化精度切换和性能鲁棒性。

Abstract: Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.

</details>


### [616] [Warm-starting active-set solvers using graph neural networks](https://arxiv.org/abs/2511.13174)
*Ella J. Schmidtobreick,Daniel Arnström,Paul Häusner,Jens Sjölund*

Main category: cs.LG

TL;DR: 使用图神经网络预测二次规划问题中的有效集，通过图结构表示优化问题，显著减少求解器迭代次数并实现跨问题规模的泛化。


<details>
  <summary>Details</summary>
Motivation: 二次规划求解器在实时控制和优化中广泛应用，但计算成本限制了其在时间关键场景中的应用。需要一种能够加速求解过程的方法。

Method: 提出基于图神经网络的学习优化方法，将二次规划问题表示为二分图，学习预测最优有效集来预热双有效集求解器DAQP。

Result: 图神经网络在不同问题规模下都能持续减少求解器迭代次数，性能与多层感知器基线相当，且训练后的模型能有效泛化到未见过的维度。

Conclusion: 结构感知学习在实时应用（如模型预测控制）中具有加速优化的潜力，展示了图神经网络在优化问题中的灵活性和可扩展性。

Abstract: Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.

</details>


### [617] [Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach](https://arxiv.org/abs/2511.13178)
*Mingxuan Tian,Haochen Mu,Donghong Ding,Mengjiao Li,Yuhan Ding,Jianping Zhao*

Main category: cs.LG

TL;DR: 提出了一种物理信息神经算子(PINO)方法，用于金属增材制造中实时预测15秒内的z和y方向变形场，通过解耦热-机械响应并融入热传导方程约束，实现高精度、低误差累积的物理一致性预测。


<details>
  <summary>Details</summary>
Motivation: 数字孪生和智能制造系统发展对金属增材制造实时变形场预测有迫切需求，但数值模拟计算成本高，传统机器学习模型难以提取时空特征和解耦热-机械场。

Method: 采用物理信息深度算子网络-循环神经网络(PIDeepONet-RNN)，使用主干和分支网络分别处理温度历史和编码变形场，融入热传导方程作为软约束确保物理一致性。

Result: 模型在实验验证的有限元数据集上表现优异，z和y方向最大绝对误差分别为0.9733mm和0.2049mm，误差累积低，时间效率高，熔池区域误差较大但关键沉积区域梯度范数低。

Conclusion: PINO代理模型在实时长时程物理场预测方面具有巨大潜力，为缺陷控制提供了稳健且可解释的预测基础。

Abstract: With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.

</details>


### [618] [Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction](https://arxiv.org/abs/2511.13185)
*Aishwarya Venkataramanan,Sai Karthikeya Vemuri,Adithya Ashok Chalain Valapil,Joachim Denzler*

Main category: cs.LG

TL;DR: 本文评估了CARS光谱中不确定性量化技术，并展示了将物理约束集成到深度学习模型中可以提高校准效果，为更可靠的CARS数据分析提供路径。


<details>
  <summary>Details</summary>
Motivation: CARS光谱技术存在非共振背景干扰真实拉曼信号的问题，现有确定性模型缺乏不确定性量化能力，这在高风险科学和生物医学应用中至关重要。

Method: 评估和比较了CARS到拉曼信号重建中的各种不确定性量化技术，并将物理约束（Kramers-Kronig关系和光滑性约束）集成到模型中。

Result: 研究表明，将物理约束集成到不确定性量化模型中可以提高模型的校准效果。

Conclusion: 物理约束的集成为更可靠的CARS数据分析提供了有前景的路径，特别是在需要不确定性量化的高风险应用中。

Abstract: Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.

</details>


### [619] [DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play](https://arxiv.org/abs/2511.13186)
*Akash Karthikeyan,Yash Vardhan Pant*

Main category: cs.LG

TL;DR: 提出DiffFP框架，通过扩散策略估计对未见对手的最佳响应，在连续决策空间中实现快速收敛到ε-纳什均衡


<details>
  <summary>Details</summary>
Motivation: 解决连续决策空间中自博弈强化学习的收敛慢和泛化能力差的问题，防止智能体被未见对手策略利用

Method: 使用扩散策略进行虚构博弈，通过生成建模学习自适应和多样化的策略，估计对未见对手的最佳响应

Result: 在赛车和多粒子零和游戏中，比基线强化学习方法收敛快3倍，成功率平均高30倍，策略对多样化对手具有鲁棒性

Conclusion: DiffFP框架在连续空间零和游戏中能有效收敛到纳什均衡，学习到的策略具有鲁棒性和稳定性

Abstract: Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations

</details>


### [620] [ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer](https://arxiv.org/abs/2511.13198)
*Zhixin Ou,Peng Liang,Jianchen Han,Baihui Liu,Linbo Qiao*

Main category: cs.LG

TL;DR: ParaDySe是一个用于动态序列的自适应并行策略切换框架，通过实时选择最优并行策略来解决LLM训练中的内存不足和通信并行化取消问题。


<details>
  <summary>Details</summary>
Motivation: 当前训练框架对动态长度序列采用预定义的静态并行策略，导致短序列通信并行化取消和长序列内存不足的问题。

Method: 实现统一张量布局的并行策略模块化函数库，构建序列感知的内存和时间成本模型，通过启发式算法选择层间最优策略。

Result: 在序列长度达624K的数据集上测试，ParaDySe成功解决了LLM训练中的OOM和CPC瓶颈问题。

Conclusion: ParaDySe通过系统集成长序列优化与现有框架，实现了动态序列训练中并行策略的无缝热切换。

Abstract: Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.

</details>


### [621] [TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs](https://arxiv.org/abs/2511.13223)
*Yuxiang Zhang,Zhengxu Yu,Weihang Pan,Zhongming Jin,Qiang Fu,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.LG

TL;DR: TokenSqueeze是一种新颖的长到短推理压缩方法，通过自适应匹配问题复杂度的推理深度选择和分布对齐的语言精炼，在保持准确性的同时显著减少推理LLM的token使用量。


<details>
  <summary>Details</summary>
Motivation: 现有的推理LLM生成的长思维链导致token使用量增加，造成更高的推理延迟和内存消耗。现有长到短方法在减少推理长度的同时往往牺牲准确性，需要一种能在降低token成本的同时保持性能的方法。

Method: 1. 自适应推理深度选择：选择推理深度与问题复杂度匹配的自生成样本；2. 分布对齐语言精炼：在不改变底层推理路径的情况下优化语言表达，提高推理路径的清晰度和简洁性。

Result: 在MATH500基准测试中，使用该方法微调的DeepSeek-R1-Distill-Qwen-7B实现了50%的平均token减少，同时保持准确性。

Conclusion: TokenSqueeze仅利用模型自生成数据，无需依赖手动策划的短答案数据集，就能实现高效高保真的推理，适用于多样化应用场景。

Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.

</details>


### [622] [Laplace Learning in Wasserstein Space](https://arxiv.org/abs/2511.13229)
*Mary Chriselda Antony Oliver,Michael Roberts,Carola-Bibiane Schönlieb,Matthew Thorpe*

Main category: cs.LG

TL;DR: 该论文将图基半监督学习从有限维欧几里得空间扩展到无限维Wasserstein空间，证明了离散图p-Dirichlet能量到连续对应物的变分收敛性，并在基准数据集上验证了高维设置下的分类性能一致性。


<details>
  <summary>Details</summary>
Motivation: 基于流形假设，研究图基半监督学习方法，特别是将拉普拉斯学习从有限维欧几里得空间扩展到无限维Wasserstein空间。

Method: 在Wasserstein空间中研究拉普拉斯学习，证明离散图p-Dirichlet能量到连续对应物的变分收敛性，并刻画Wasserstein空间子流形上的拉普拉斯-贝尔特拉米算子。

Result: 通过基准数据集上的数值实验验证了理论框架，证明了在高维设置下分类性能的一致性。

Conclusion: 成功将图基半监督学习扩展到Wasserstein空间，建立了离散与连续设置之间的理论联系，并在实际应用中验证了方法的有效性。

Abstract: The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning
  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical
  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to
  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator
  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through
  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.

</details>


### [623] [MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing](https://arxiv.org/abs/2511.13234)
*Boris Kriuk*

Main category: cs.LG

TL;DR: MorphBoost是一种新型梯度提升框架，通过自组织树结构动态调整分裂行为，在训练过程中自动适应问题复杂性，在多个数据集上优于XGBoost等竞争模型。


<details>
  <summary>Details</summary>
Motivation: 传统梯度提升算法使用静态树结构，分裂标准在训练过程中保持不变，限制了其适应不同学习阶段梯度分布变化和问题特定特征的能力。

Method: 引入自适应分裂函数，基于累积梯度统计和迭代相关学习压力演化；包括变形分裂准则、自动问题指纹识别、向量化树预测、交互感知特征重要性和快速模式优化。

Result: 在10个多样化数据集上的基准测试显示，MorphBoost平均优于XGBoost 0.84%，获得4/10数据集胜利（40%胜率）和6/30前三名（20%），保持最低方差（σ=0.0948）和最高最低准确率。

Conclusion: MorphBoost通过动态自组织树结构实现了最先进的性能，在困难问题上表现尤为突出，显示出优越的一致性和鲁棒性。

Abstract: Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (σ=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.

</details>


### [624] [Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification](https://arxiv.org/abs/2511.13237)
*Alan G. Paredes Cetina,Kaouther Benguessoum,Raoni Lourenço,Sylvain Kubler*

Main category: cs.LG

TL;DR: CONFETTI是一个用于多元时间序列的多目标反事实解释方法，通过平衡预测置信度、邻近性和稀疏性来提供可操作的见解


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法缺乏透明度，而可解释AI方法往往无法传达完整的决策空间，反事实解释方法通常只优先考虑准确性、邻近性或稀疏性中的单一目标

Method: CONFETTI识别关键MTS子序列，定位反事实目标，并优化修改时间序列以平衡预测置信度、邻近性和稀疏性

Result: 在UEA档案的七个MTS数据集上评估，CONFETTI在优化目标上始终优于最先进的CE方法，在六个其他指标上表现更好，实现了≥10%的更高置信度，同时在≥40%的情况下改善了稀疏性

Conclusion: CONFETTI通过平衡多个目标提供了更好的可解释性和决策支持，是MTS反事实解释的有效方法

Abstract: Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.

</details>


### [625] [Incoherent Beliefs & Inconsistent Actions in Large Language Models](https://arxiv.org/abs/2511.13240)
*Arka Pal,Teo Kitanovski,Arthur Liang,Akilesh Potti,Micah Goldblum*

Main category: cs.LG

TL;DR: 研究发现大型语言模型在动态环境中存在信念更新不一致和行动与信念不匹配的问题，即使在高精度任务中也是如此。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务与静态数据集存在差异，需要模型能够进行顺序交互、连贯更新信念并基于信念做出决策，但LLM在这种动态环境中的表现难以从静态评估中预测。

Method: 通过分析LLM的信念更新一致性（直接获取的后验与正确先验更新之间的差异）以及行动与信念的一致性（如投注市场中的决策），评估模型在动态环境中的表现。

Result: LLM在信念更新上存在高达30%的平均不一致性；行动与内部信念不一致，甚至不按信念方向投注；对挑战的回答存在中度自相矛盾；这些问题在准确率高或校准良好的强模型中依然存在。

Conclusion: 研究结果突显了在复杂现实环境中预测LLM行为的困难，表明当前LLM在动态交互任务中存在显著的内部不一致问题。

Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.

</details>


### [626] [Uncovering and Mitigating Transient Blindness in Multimodal Model Editing](https://arxiv.org/abs/2511.13243)
*Xiaoqi Han,Ru Li,Ran Yi,Hongye Tan,Zhuomin Liang,Víctor Gutiérrez-Basulto,Jeff Z. Pan*

Main category: cs.LG

TL;DR: 本文提出了一个全面的多模态模型编辑评估框架，解决了现有方法因依赖低相似度或随机输入而夸大成功的问题，揭示了"瞬时盲视"现象，并提出了位置感知对抗损失来平衡跨模态表示。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型编辑评估方法从文本模型编辑改编而来，通过依赖低相似度或随机输入夸大了成功，掩盖了过拟合问题。

Method: 提出了全面的位置评估框架，涵盖三个关键维度：随机图像位置、无图像位置和一致图像位置，通过七种不同的数据类型进行操作化；引入了De-VQA动态评估；提出了位置感知对抗损失来平衡跨模态表示。

Result: 经验结果表明，该方法始终优于现有基线，平均减少瞬时盲视并提高位置性17%；令牌分析显示编辑对文本令牌的影响不成比例。

Conclusion: 提出的位置感知对抗损失方法能有效减少瞬时盲视，改善多模态模型编辑的位置性，提供更准确和结构化的评估框架。

Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.

</details>


### [627] [Seek and You Shall Fold](https://arxiv.org/abs/2511.13244)
*Nadav Bojan Sellam,Meital Bojan,Paul Schanda,Alex Bronstein*

Main category: cs.LG

TL;DR: 提出了一个非可微分指导的蛋白质生成模型框架，通过遗传算法将连续扩散生成器与任意黑盒目标耦合，解决了实验数据难以整合到蛋白质生成模型中的问题。


<details>
  <summary>Details</summary>
Motivation: 准确的蛋白质结构对理解生物功能至关重要，但将实验数据整合到蛋白质生成模型中仍然是一个主要挑战，特别是NMR中的化学位移等丰富数据难以直接集成到生成建模中。

Method: 开发了一个非可微分指导框架，将连续扩散生成器与任意黑盒目标通过定制的遗传算法耦合，支持三种模式：成对距离约束、核Overhauser效应约束和化学位移。

Result: 证明了化学位移指导的结构生成是可行的，揭示了当前预测器的关键弱点，并展示了整合多样化实验信号的通用策略。

Conclusion: 这项工作指向了超越可微分限制的自动化、数据条件化蛋白质建模方向。

Abstract: Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.

</details>


### [628] [Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs](https://arxiv.org/abs/2511.13250)
*Aleksandar Stanković,Dejan Lisica*

Main category: cs.LG

TL;DR: 本文为ogbn-proteins数据集提供了可复现的边缘感知基线方法，研究了边缘特征聚合和消息传递中的关键系统选择，发现基于sum的边缘到节点特征聚合效果最佳，并提出了条件层归一化等改进方法。


<details>
  <summary>Details</summary>
Motivation: 为ogbn-proteins数据集建立标准化的可复现基线，系统研究边缘特征如何影响图神经网络性能，特别是边缘特征聚合方法和消息传递机制的选择。

Method: 使用PyTorch Geometric实现GraphSAGE模型，比较sum、mean、max三种边缘特征聚合方法，测试LayerNorm、BatchNorm和条件层归一化(CLN)，并应用后处理的温度缩放和标签相关平滑技术。

Result: sum聚合方法表现最佳；BatchNorm获得最高AUC，CLN在AUC相近的情况下F1分数更好；后处理技术显著提升micro-F1和校准误差，对AUC影响很小。

Conclusion: 为ogbn-proteins提供了强基线方法，sum边缘特征聚合和条件层归一化是有效选择，后处理技术能进一步提升模型决策质量而不影响AUC。

Abstract: We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.

</details>


### [629] [Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning](https://arxiv.org/abs/2511.13322)
*Senne Deproost,Dennis Steckelmacher,Ann Nowé*

Main category: cs.LG

TL;DR: 提出一种新的模型无关方法，使用Voronoi分区将状态空间划分为区域，在每个区域内使用简化的线性模型来模仿深度强化学习控制器的行为，实现可解释性且性能匹配甚至略优于原始黑盒策略。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习控制器缺乏透明度，难以满足监管要求和建立信任。需要将学习到的行为转移到人类可读的模型中，但单一简化模型在动态情况下表现不佳，需要在灵活性和复杂性之间找到平衡。

Method: 使用Voronoi分区方法将状态空间划分为多个区域，在每个区域内使用线性模型来近似原始控制器的行为。这种方法可以创建专门针对局部区域的简化模型。

Result: 在网格世界环境和经典控制任务上的评估显示，这种局部专业化线性模型的蒸馏方法产生了可解释的策略，并且性能匹配甚至略微优于原始黑盒策略。

Conclusion: 提出的基于Voronoi分区的局部专业化线性模型蒸馏方法能够有效解决深度强化学习控制器的可解释性问题，同时保持或提升性能，为创建可信赖的AI系统提供了可行方案。

Abstract: Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.

</details>


### [630] [Tab-PET: Graph-Based Positional Encodings for Tabular Transformers](https://arxiv.org/abs/2511.13338)
*Yunze Leng,Rohan Ghosh,Mehul Motani*

Main category: cs.LG

TL;DR: 本文发现位置编码(PEs)可以提升表格数据上Transformer模型的泛化性能，提出Tab-PET框架通过图结构估计位置编码，在50个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 表格数据缺乏结构性线索，传统Transformer模型难以有效利用自注意力机制。现有表格Transformer模型通常不使用位置编码，因为缺乏先验结构信息。

Method: 提出Tab-PET框架，通过图结构估计位置编码。探索两种图估计范式：基于关联性和基于因果性。将图导出的位置编码融入嵌入表示中。

Result: 在50个分类和回归数据集上验证，图导出的位置编码显著提升了3T模型的性能。基于关联性的图比因果驱动的方法获得更稳定和显著的性能提升。

Conclusion: 位置编码在表格Transformer中具有意外的重要作用，能够通过降低特征有效秩来简化任务维度，从而改善泛化性能。

Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.

</details>


### [631] [Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model](https://arxiv.org/abs/2511.13339)
*Han Meng,Gang Mei,Hong Tian,Nengxiong Xu,Jianbing Peng*

Main category: cs.LG

TL;DR: 提出了一种基于表格基础模型的简单而稳健的方法，用于岩石不连续性的统计准确生成预测，能够在有限测量数据下有效捕捉复杂的分布模式。


<details>
  <summary>Details</summary>
Motivation: 岩石不连续性对岩体力学行为和稳定性至关重要，但其内部分布通常无法直接观测，只能通过表面暴露的不连续性进行推断。现有生成预测方法要么无法捕捉复杂的分布模式，要么在数据稀疏条件下缺乏稳健性。

Method: 利用专门为小数据设计的表格基础模型，充分发挥其在样本学习方面的强大能力，从有限的测量不连续性中有效捕捉潜在的复杂分布模式。

Result: 在十个具有不同规模和分布模式的数据集上的比较实验表明，该方法在准确性和稳健性方面优于传统统计模型和深度生成方法。

Conclusion: 这项工作推进了岩体结构的定量表征，支持更安全、更可靠的数据驱动岩土工程设计。

Abstract: Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.

</details>


### [632] [Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning](https://arxiv.org/abs/2511.13351)
*Xinlan Wu,Bin Zhu,Feng Han,Pengkun Jiao,Jingjing Chen*

Main category: cs.LG

TL;DR: 提出了一种用于多模态食物学习的持续学习框架，通过双LoRA架构和质量增强伪回放来解决现有大型多模态模型在学习新任务时的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的食物分析大型多模态模型在学习新任务时会出现灾难性遗忘，需要昂贵的从头重新训练。

Method: 采用双LoRA架构：专用LoRA学习任务特定知识，合作LoRA通过伪回放整合跨任务共享知识；质量增强伪回放策略利用自一致性和语义相似性减少生成样本的幻觉。

Result: 在综合Uni-Food数据集上的实验显示在减轻遗忘方面表现优异。

Conclusion: 这是首个针对复杂食物任务的有效持续学习方法。

Abstract: Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.

</details>


### [633] [A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs](https://arxiv.org/abs/2511.13373)
*Prakrit Timilsina,Anuj Nepal,Rajan Kadel,Robin Doss*

Main category: cs.LG

TL;DR: 本文系统评估了六种参数空间合并技术应用于两个基于Mistral-7B的医疗LLM，发现对于架构兼容的模型，简单的平均方法在医疗基准测试中表现优异，为资源受限的分布式医疗AI部署提供了实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在分布式医疗中面临的挑战：跨机构整合专业知识同时保护隐私、降低计算开销、防止模型更新时的灾难性遗忘。

Method: 提出分层方法结合选择性最优传输对齐和余弦相似度加权插值，并系统评估六种参数合并技术（Task Arithmetic、线性平均、DARE-TIES、DELLA、Breadcrumbs和分层方法）。

Result: 架构兼容模型从简单平均方法中获益显著，Task Arithmetic在MedQA上达到45.80%准确率，优于复杂的剪枝方法。

Conclusion: 对于架构兼容模型，简单平均提供了稳健且计算高效的知识整合基线，为可扩展医疗AI系统提供了实用路径。

Abstract: Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.

</details>


### [634] [Finding Kissing Numbers with Game-theoretic Reinforcement Learning](https://arxiv.org/abs/2511.13391)
*Chengdong Ma,Théo Tao Zhaowei,Pengyu Li,Minghao Liu,Haojun Chen,Zihao Mao,Yuan Cheng,Yuan Qi,Yaodong Yang*

Main category: cs.LG

TL;DR: 该论文提出PackingStar系统，使用博弈论强化学习方法解决高维空间中的接吻数问题，在25-31维中超越了所有已知记录，并发现了6000多个新结构。


<details>
  <summary>Details</summary>
Motivation: 接吻数问题自1694年牛顿研究以来一直是基础性挑战，高维几何的不规则性和指数级增长的组合复杂性限制了现有方法的可扩展性。

Method: 将问题建模为双玩家矩阵完成游戏，一个玩家填充矩阵条目，另一个纠正次优条目，通过合作最大化矩阵大小（对应接吻数），使用博弈论强化学习系统PackingStar高效探索高维空间。

Result: 在25-31维中超越了所有人类已知记录，25维配置几何上对应Leech格并可能最优；在13维中首次突破1971年以来的有理结构限制；在14维及其他维度发现了6000多个新结构。

Conclusion: 证明了AI探索超越人类直觉的高维空间的能力，为接吻数问题和更广泛的几何问题开辟了新途径。

Abstract: Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.

</details>


### [635] [Fast and Robust Simulation-Based Inference With Optimization Monte Carlo](https://arxiv.org/abs/2511.13394)
*Vasilis Gkolemis,Christos Diou,Michael Gutmann*

Main category: cs.LG

TL;DR: 提出一种针对可微分模拟器的贝叶斯参数推断新方法，通过将随机模拟转化为确定性优化问题，结合梯度方法显著减少运行时间并提高准确性


<details>
  <summary>Details</summary>
Motivation: 复杂随机模拟器的贝叶斯参数推断面临似然函数难处理、高维参数空间计算成本高、部分无信息输出等挑战

Method: 基于优化蒙特卡洛框架，将随机模拟重新表述为确定性优化问题，应用梯度方法高效导航至高密度后验区域，避免在低概率区域浪费模拟，并使用JAX实现关键组件的向量化

Result: 在高维参数空间、无信息输出、多观测和多峰后验等广泛实验中，该方法在保持与最先进方法相当甚至更好的准确性的同时，显著减少了运行时间

Conclusion: 该方法为可微分模拟器提供了一种高效准确的贝叶斯推断解决方案，在减少计算成本的同时保持了推断质量

Abstract: Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.

</details>


### [636] [PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation](https://arxiv.org/abs/2511.13414)
*Hanwen Hu,Zimo Wen,Shiyou Qian,Jian Co*

Main category: cs.LG

TL;DR: 提出PAST网络，通过主-辅助时空模式处理交通时间序列插补问题，在27种缺失数据条件下优于7个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以适应随机缺失位置，无法学习长期和大规模依赖关系，而交通数据缺失类型多样（随机、纤维、块状缺失）使插补任务具有挑战性。

Method: 将模式分为主模式和辅助模式，提出PAST网络，包含图集成模块（GIM）和交叉门控模块（CGM），通过动态图、多阶卷积和双向门控机制分别捕获两种模式，并在集成自监督框架下训练。

Result: 在三个数据集上的实验显示，PAST在RMSE和MAE指标上分别比最先进基线方法提升达26.2%和31.6%。

Conclusion: PAST通过有效分离和建模主-辅助时空模式，能够处理各种缺失数据条件，显著提升交通时间序列插补性能。

Abstract: Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.

</details>


### [637] [MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction](https://arxiv.org/abs/2511.13419)
*Shaheen Mohammed Saleh Ahmed,Hakan Hakan Guneyli*

Main category: cs.LG

TL;DR: 提出MMWSTM-ADRAN+双流深度学习架构，结合天气状态转移模型和异常驱动注意力网络，用于极端气温事件预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测极端气温事件对气候风险管理至关重要，但现有方法在短程极端事件预测方面仍面临挑战。

Method: 使用双流架构：MMWSTM流通过双向LSTM和可学习马尔可夫状态转移矩阵捕捉天气状态变化；ADRAN流通过双向GRU、多头自注意力和异常放大层增强对低概率信号的敏感性。采用轻量级注意力融合门和自定义极端天气损失函数。

Result: 模型通过时间序列数据增强使训练数据量翻倍，并优化了对温度分布上下5%极端值的预测准确性。

Conclusion: 该双流深度学习架构有效提升了极端气温事件的预测能力，为气候风险管理提供了更可靠的工具。

Abstract: Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data

</details>


### [638] [Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression](https://arxiv.org/abs/2511.13421)
*Tingkai Yan,Haodong Wen,Binghui Li,Kairong Luo,Wenguang Chen,Kaifeng Lyu*

Main category: cs.LG

TL;DR: 本文分析了在有限数据和重复训练轮次下，数据缩放定律的变化形式。通过线性回归的理论分析，定义了"有效重用率"E(K,N)，量化了K轮训练与单轮训练所需数据量的关系。研究发现E(K,N)在小K时近似等于K，但随着K增加会达到与数据规模相关的平台期。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大规模语料单轮训练的数据缩放定律，但在有限数据和重复训练轮次下的数据缩放定律形式仍不清楚。本文旨在填补这一空白，特别关注多轮训练如何重塑数据缩放定律。

Method: 使用线性回归的理论分析，在强凸性或Zipf分布数据条件下，分析SGD训练中数据有效重用率E(K,N)的缩放行为。

Result: 发现E(K,N)在小K时近似等于K（每轮训练带来线性增益），但随着K增加会达到与数据规模相关的平台期（强凸情况下为Θ(logN)）。这修正了先前实证研究的结论。

Conclusion: 数据有效重用率的最大K值实际上依赖于数据规模和分布，未来研究数据重用的缩放定律时需要明确建模这两个因素。

Abstract: While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \textit{i.e.}, $E(K, N) \approx K$ for $K \le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.

</details>


### [639] [Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes](https://arxiv.org/abs/2511.13444)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.LG

TL;DR: 提出了一种基于图像卷积聚类的无监督工业时间序列分析方法，通过将时间序列转换为灰度图像表示，结合软硬聚类和复合评估指标，成功识别出7种可解释的操作模式。


<details>
  <summary>Details</summary>
Motivation: 工业过程监控中传感器时间序列数据缺乏标签、高变异性且存在操作噪声，传统方法难以提取有意义的模式。现有聚类技术要么依赖固定距离度量，要么使用为静态数据设计的深度模型，难以处理动态、非结构化的工业序列。

Method: 1) 通过重叠滑动窗口将原始时间序列转换为灰度矩阵表示，使用深度卷积自编码器进行特征提取；2) 集成软硬聚类输出并通过两阶段策略进行细化选择；3) 开发新的复合评分指标S_eva，结合归一化轮廓系数、Calinski-Harabasz和Davies-Bouldin指数来客观评估聚类性能。

Result: 应用于北欧铸造厂的3900多个熔炉操作，该方法识别出7种可解释的操作模式，揭示了能耗、热动力学和生产持续时间的显著差异。相比经典和深度聚类基线方法，该方法实现了更优的整体性能、更强的鲁棒性和领域对齐的可解释性。

Conclusion: 该框架解决了无监督时间序列分析中的关键挑战，如序列不规则性、模式重叠和度量不一致性，为工业系统中的数据驱动诊断和能源优化提供了通用解决方案。

Abstract: Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.

</details>


### [640] [Hardware optimization on Android for inference of AI models](https://arxiv.org/abs/2511.13453)
*Iulius Gherasim,Carlos García Sánchez*

Main category: cs.LG

TL;DR: 研究Android系统上AI模型的最优执行配置，重点关注目标检测和图像分类任务，通过评估量化方案和设备加速器来平衡精度损失与推理速度


<details>
  <summary>Details</summary>
Motivation: 移动AI应用需要低延迟高响应性，但面临实时约束和异构硬件架构的挑战，需要找到最佳执行策略

Method: 评估YOLO目标检测和ResNet图像分类模型的不同量化方案，以及GPU和NPU设备加速器的使用效果

Result: 通过实验确定了在精度损失最小和推理速度提升最大之间的最佳平衡配置

Conclusion: 找到了Android系统上AI模型的最优执行配置，为移动AI应用提供了实用的部署指导

Abstract: The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.

</details>


### [641] [Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure](https://arxiv.org/abs/2511.13457)
*Bin Liu,Qinghao Zhao,Yuxi Zhou,Zhejun Sun,Kaijie Lei,Deyun Zhang,Shijia Geng,Shenda Hong*

Main category: cs.LG

TL;DR: 提出了一种基于自监督表示学习的方法，使用肺活量时间序列结合人口统计学数据来早期检测右心衰竭，在UK Biobank数据集上取得了良好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 右心衰竭（RHF）与高发病率和死亡率相关，肺病常导致右心室负荷增加引发RHF。从肺心病患者中早期筛查出可能发展为RHF的患者非常重要。

Method: 两阶段方法：第一阶段使用变分自编码器（VAE）从数据增强的无标签数据中学习肺活量时间序列的鲁棒低维表示；第二阶段将该表示与人口统计学信息融合，输入CatBoost分类器进行RHF预测。

Result: 在UK Biobank的26,617人子集上AUROC达0.7501；在高风险临床亚组中，慢性肾病患者测试集AUROC为0.8194，瓣膜性心脏病患者为0.8413。

Conclusion: 该自监督表示学习方法结合肺活量时间序列和人口统计学数据，在临床实践中具有早期检测RHF的潜力。

Abstract: Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.

</details>


### [642] [Multi-task GINN-LP for Multi-target Symbolic Regression](https://arxiv.org/abs/2511.13463)
*Hussein Rajabu,Lijun Qian,Xishuang Dong*

Main category: cs.LG

TL;DR: 提出了多任务回归GINN-LP（MTRGINN-LP），一种用于多目标符号回归的可解释神经网络，通过结合共享主干网络和任务特定输出层来捕获目标间依赖关系。


<details>
  <summary>Details</summary>
Motivation: 符号回归面临两个主要挑战：大多数方法在科学数据集上评估，限制了泛化能力；且主要针对单输出回归，而现实问题常涉及相互依赖的多目标输出。

Method: 将GINN-LP与多任务深度学习结合，使用包含多个幂项逼近器块的共享主干网络和任务特定输出层，在保持可解释性的同时捕获目标间依赖关系。

Result: 在能源效率预测和可持续农业等实际多目标应用中验证了模型，实验结果显示具有竞争力的预测性能和高度可解释性。

Conclusion: 该方法有效地将符号回归扩展到更广泛的现实世界多输出任务中。

Abstract: In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.

</details>


### [643] [AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate](https://arxiv.org/abs/2511.13465)
*Meng Zhu,Quan Xiao,Weidong Min*

Main category: cs.LG

TL;DR: 提出AdamX优化算法，通过新型二阶矩估计指数衰减率，在训练后期减弱学习步长修正强度并退化为SGD，提升稳定期训练稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: Adam优化器在大语言模型时代仍是主流，但相比SGD更容易收敛到非平坦极小值，影响泛化性能。

Method: 提出AdamX算法，核心创新是新型二阶矩估计指数衰减率，随着训练进行逐渐减弱学习步长修正强度，在稳定训练期退化为SGD。

Result: 实验表明新型二阶矩估计指数衰减率优于现有方法，AdamX在性能上稳定优于Adam及其变体。

Conclusion: AdamX通过改进二阶矩估计机制，有效解决了Adam收敛到非平坦极小值的问题，提升了训练稳定性和泛化能力。

Abstract: Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.

</details>


### [644] [GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction](https://arxiv.org/abs/2511.13469)
*Shiyuan Luo,Chonghao Qiu,Runlong Yu,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: GREAT框架通过辅助变换增强环境建模的泛化能力，解决未监测区域预测的挑战


<details>
  <summary>Details</summary>
Motivation: 环境建模面临观测数据有限且地理分布不平衡的问题，导致模型学习到仅在局部有效的伪模式，无法泛化到未监测区域

Method: GREAT框架在神经网络多层学习变换函数，增强原始环境特征和时间影响，通过双层训练过程约束增强数据保持源数据关键模式

Result: 在美国东部六个生态多样化流域的溪流温度预测实验中，GREAT在零样本场景下显著优于现有方法

Conclusion: GREAT为环境应用提供了实用解决方案，可在全面监测不可行的情况下改善预测性能

Abstract: Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.

</details>


### [645] [Quantum Machine Learning via Contrastive Training](https://arxiv.org/abs/2511.13497)
*Liudmila A. Zhukas,Vivian Ni Zhang,Qiang Miao,Qingfeng Wang,Marko Cetina,Jungsang Kim,Lawrence Carin,Christopher Monroe*

Main category: cs.LG

TL;DR: 该论文提出了一种量子机器学习中的自监督预训练方法，通过在可编程离子阱量子计算机上学习未标记数据的特征不变性，有效减少对标记数据的依赖，提高图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习模型面临与经典机器学习类似的挑战——标记数据稀缺，特别是随着模型规模和复杂性的增加。需要开发能够减少对标记数据依赖的量子表示学习方法。

Method: 在可编程离子阱量子计算机上实现自监督预训练，将图像编码为量子态，通过对比学习在硬件上学习特征不变性，然后进行微调分类。所有训练和分类阶段都在量子硬件上执行，相似性基于测量的量子重叠度。

Result: 与随机初始化模型相比，预训练后的模型在图像分类任务中表现出更高的平均测试准确率和更低的运行间变异性，在标记训练数据有限的情况下性能提升尤其显著。学习到的不变性能够泛化到预训练图像样本之外。

Conclusion: 这项工作为量子表示学习建立了一种标签高效的路径，对量子原生数据集具有直接相关性，并为处理更大规模的经典输入提供了清晰的发展路线。

Abstract: Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.

</details>


### [646] [Naga: Vedic Encoding for Deep State Space Models](https://arxiv.org/abs/2511.13510)
*Melanie Schaller,Nick Janssen,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: Naga是一种基于吠陀数学结构概念的深度状态空间模型编码方法，通过双向处理正向和时间反转序列，使用哈达玛积组合表示，在多个长期时间序列预测基准上优于28个现有模型。


<details>
  <summary>Details</summary>
Motivation: 受吠陀数学结构概念启发，旨在提高模型捕获远距离时间依赖性的能力，为长序列建模提供可解释且计算高效的替代方案。

Method: 提出双向表示方法，联合处理正向和时间反转输入序列，通过元素级哈达玛积交互生成吠陀启发的编码。

Result: 在ETTh1、ETTh2、ETTm1、ETTm2、Weather、Traffic和ILI等多个长期时间序列预测基准上，Naga优于28个当前最先进模型，且比现有深度SSM方法更高效。

Conclusion: 研究表明，结合结构化的吠陀启发分解可以为长范围序列建模提供可解释且计算高效的替代方案。

Abstract: This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.

</details>


### [647] [A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data](https://arxiv.org/abs/2511.13514)
*Pragatheeswaran Vipulananthan,Kamal Premaratne,Dilip Sarkar,Manohar N. Murthi*

Main category: cs.LG

TL;DR: 提出了一种基于量子物理学的"白盒"方法，通过将时间序列数据的核均值嵌入映射到再生核希尔伯特空间，构建张量网络启发的1D自旋链哈密顿量，并求解薛定谔方程来量化不确定性，提高模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中准确不确定性量化的关键挑战。神经网络虽然功能强大但缺乏可解释性，而概率"白盒"模型虽然可解释但性能较差。需要一种既能准确量化不确定性又具有良好可解释性的方法。

Method: 将时间序列数据的核均值嵌入映射到再生核希尔伯特空间，构建张量网络启发的1D自旋链哈密顿量，其中核均值嵌入作为其特征函数或特征模式。求解薛定谔方程并应用微扰理论来量化不确定性。

Result: 该方法在变化点检测和时间序列聚类任务中表现出色，相比最先进的"白盒"模型具有更好的性能，并能提供决策过程中的不确定性洞察。

Conclusion: 提出的量子物理学启发的"白盒"方法成功实现了准确的不确定性量化和增强的可解释性，为机器学习模型提供了新的不确定性量化框架。

Abstract: Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.

</details>


### [648] [Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images](https://arxiv.org/abs/2511.13527)
*Ihab Asaad,Maha Shadaydeh,Joachim Denzler*

Main category: cs.LG

TL;DR: 本文研究了基于patch的多标签分类中的伪相关性问题，特别是在肿瘤检测任务中，发现肿瘤patch倾向于包含更大的组织区域，而非肿瘤patch多为背景。作者提出使用GERNE去偏方法来提升最差组准确率。


<details>
  <summary>Details</summary>
Motivation: 基于patch的分类方法虽然能降低标注成本并简化训练，但会引入伪相关性：肿瘤patch通常包含更大组织区域，而非肿瘤patch多为背景。这种相关性会导致模型产生偏差。

Method: 采用GERNE去偏方法，该方法可适配以最大化最差组准确率(WGA)，用于缓解patch组成与标签之间的伪相关性。

Result: 相比ERM方法，使用GERNE后WGA提升了约7%，在关键少数情况（如小组织肿瘤patch和大组织非肿瘤patch）上表现更好。

Conclusion: 在基于patch的分类问题中，伪相关性感知学习至关重要，去偏策略能有效提升模型在关键少数情况下的性能。

Abstract: Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.

</details>


### [649] [Fairness-Aware Graph Representation Learning with Limited Demographic Information](https://arxiv.org/abs/2511.13540)
*Zichong Wang,Zhipeng Yin,Liping Yang,Jun Zhuang,Rui Yu,Qingzhao Kong,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出了FairGLite框架，在有限人口统计信息下实现公平图学习，通过生成人口统计代理、一致性嵌入和自适应置信策略来缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 现有公平图学习方法大多需要完整的人口统计信息，但在实践中由于隐私和法律限制很难获得，需要开发在有限人口统计信息下的公平图学习方案。

Method: 使用部分人口统计数据生成人口统计信息代理，设计跨人口统计组的一致性节点嵌入策略，并开发基于预测置信度的自适应贡献调整机制。

Result: 理论分析证明FairGLite在群体公平指标上具有可证明的上界，实验表明该框架在多个数据集和公平图学习框架中能有效缓解偏见并保持模型效用。

Conclusion: FairGLite为有限人口统计信息下的公平图学习提供了有效解决方案，实现了偏见缓解的形式化保证。

Abstract: Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.

</details>


### [650] [Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries](https://arxiv.org/abs/2511.13541)
*Yue Hou,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: 提出BaCa方法，一种无需微调预训练模型的测试时图OOD检测方法，通过双动态字典校准OOD分数，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决图OOD检测中缺乏真实OOD样本训练数据的问题，现有方法仅捕捉ID特征导致边界表示不可靠，且图数据的潜在多因素结构未被充分探索

Method: 使用双动态字典（基于优先级队列和注意力机制）自适应捕获潜在ID和OOD表示，通过估计图元和混合策略生成边界感知判别拓扑，无需辅助数据集

Result: 在真实世界数据集上的广泛实验表明，BaCa在OOD检测方面显著优于现有最先进方法

Conclusion: BaCa方法通过边界感知的OOD分数校准，有效解决了图OOD检测的关键挑战，无需微调预训练模型即可实现优异的检测性能

Abstract: A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.

</details>


### [651] [RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise](https://arxiv.org/abs/2511.13561)
*Shihao Dong,Yue Liu,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.LG

TL;DR: 提出RAC-DMVC框架，通过可靠性图指导多源噪声环境下的鲁棒表示学习，解决多视图聚类中的缺失噪声和观测噪声问题。


<details>
  <summary>Details</summary>
Motivation: 增强多视图聚类在现实场景中的适用性，解决更具挑战性的多源噪声（包括缺失噪声和观测噪声）环境下的聚类问题。

Method: 使用可靠性图指导鲁棒表示学习；通过跨视图重构处理观测噪声；采用可靠性感知噪声对比学习减少噪声表示导致的偏差；设计双注意力填补处理缺失噪声；引入自监督聚类蒸馏模块优化表示。

Result: 在五个基准数据集上的广泛实验表明，RAC-DMVC在多个评估指标上优于最先进方法，并在不同噪声比例下保持优异性能。

Conclusion: RAC-DMVC框架能有效处理多视图聚类中的多源噪声问题，具有优越的鲁棒性和聚类性能。

Abstract: Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.

</details>


### [652] [Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization](https://arxiv.org/abs/2511.13625)
*Kaichi Irie,Shuhei Watanabe,Masaki Onishi*

Main category: cs.LG

TL;DR: 本文提出了一种解耦准牛顿更新的方法，通过协程批处理采集函数调用，在保持与顺序多起点优化相同收敛性的同时，大幅减少了贝叶斯优化中采集函数优化的计算时间。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中采集函数优化的主要计算瓶颈在于多起点优化，现有方法使用批处理加速但导致准牛顿方法的逆Hessian矩阵存在非对角近似误差，减缓了收敛速度。

Method: 提出解耦准牛顿更新方法，使用协程批处理采集函数调用，同时保持准牛顿方法的理论收敛特性。

Result: 该方法不仅实现了与顺序多起点优化相同的理论收敛性，而且相比先前方法显著减少了实际运行时间。

Conclusion: 通过解耦准牛顿更新和批处理采集函数调用，有效解决了贝叶斯优化中采集函数优化的计算效率问题。

Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.

</details>


### [653] [Towards Multimodal Representation Learning in Paediatric Kidney Disease](https://arxiv.org/abs/2511.13637)
*Ana Durica,John Booth,Ivana Drobnjak*

Main category: cs.LG

TL;DR: 使用电子健康记录和循环神经网络预测儿童在未来30天内是否会出现异常血清肌酐值


<details>
  <summary>Details</summary>
Motivation: 儿科肾脏疾病表现和进展差异很大，需要持续监测肾功能

Method: 整合纵向实验室序列和人口统计信息的时间建模方法，使用循环神经网络模型

Result: 初步证明简单的时间表示可以捕捉常规儿科数据中的有用模式

Conclusion: 为未来使用额外临床信号和更详细肾脏结果的多模态扩展奠定了基础

Abstract: Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.

</details>


### [654] [Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures](https://arxiv.org/abs/2511.13640)
*Haohui Wang,Jingyuan Qi,Jianpeng Chen,Jun Wu,Lifu Huang,Lecheng Zheng,Kevin Choi,Balaji Veeramani,Edward Bowen,Alison Hu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 该论文研究了真实与合成数据混合数据集对LLM的影响，发现了三阶段缩放行为，提出了LLM泛化边界理论，并开发了高效的数据估值方法。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽然具有可扩展性和成本效益，但会引入系统性分布差异，特别是在长尾知识方面存在不足，这对混合数据集的效用评估提出了根本性挑战。

Method: 识别了三阶段缩放行为特征，推导了针对真实-合成混合数据的LLM泛化边界理论，并基于此提出了可扩展的高效数据估值方法。

Result: 在图像分类、情感分类、指令跟随和复杂推理四个任务上的综合实验表明，该方法在数据估值方面超越了现有最优基线，且计算成本显著降低。

Conclusion: 该研究为理解和评估真实-合成混合数据集提供了理论基础和实用工具，有助于更有效地利用混合数据训练LLM。

Abstract: The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.

</details>


### [655] [FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs](https://arxiv.org/abs/2511.13645)
*Aleksandar Stanković*

Main category: cs.LG

TL;DR: FuseSampleAgg是一个CUDA算子，将GraphSAGE的邻居采样和均值聚合融合为单次操作，显著提升训练速度并大幅减少GPU内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有GraphSAGE实现中邻居采样和聚合操作分离，导致多次内核启动和内存块物化，造成大量内存流量和开销。

Method: 通过保存索引重放的方式，在单次操作中融合邻居采样和均值聚合，消除块物化和额外内核启动，同时保持GraphSAGE均值语义。

Result: 在Reddit、ogbn-arxiv和ogbn-products基准测试中，速度提升最高达51倍，GPU内存减少最高达100倍。

Conclusion: FuseSampleAgg提供了一种高效、确定性的GraphSAGE训练方法，可与标准PyTorch优化器集成，显著提升性能。

Abstract: We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.

</details>


### [656] [Weight-sparse transformers have interpretable circuits](https://arxiv.org/abs/2511.13653)
*Leo Gao,Achyuta Rajaram,Jacob Coxon,Soham V. Govande,Bowen Baker,Dan Mossing*

Main category: cs.LG

TL;DR: 通过训练权重稀疏的语言模型来寻找人类可理解的电路，通过约束大部分权重为零使每个神经元只有少量连接，从而获得更易解释的电路结构。


<details>
  <summary>Details</summary>
Motivation: 在语言模型中寻找人类可理解的电路是机械可解释性领域的核心目标，当前需要更好的方法来识别和理解模型内部的复杂机制。

Method: 训练权重稀疏模型，约束大部分权重为零，使神经元只有少量连接；通过剪枝技术隔离特定任务相关的电路部分。

Result: 获得的电路通常包含对应自然概念的神经元和残差通道，具有少量直接可解释的连接；稀疏化在能力和可解释性之间权衡，模型规模扩展能改善这一权衡。

Conclusion: 该方法产生了前所未有的可理解电路，并通过严格验证；但将稀疏模型扩展到数千万非零参数以上同时保持可解释性仍具挑战性，该方法也可用于解释现有密集模型。

Abstract: Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.

</details>


### [657] [Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning](https://arxiv.org/abs/2511.13654)
*Pascal Zimmer,Ghassan Karame*

Main category: cs.LG

TL;DR: 本文首次系统分析了优化超参数（学习率、权重衰减、动量、批大小）对迁移攻击和查询攻击鲁棒性的影响，发现两者存在显著差异：降低学习率增强迁移攻击鲁棒性，而提高学习率增强查询攻击鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究优化超参数如何影响对抗攻击鲁棒性，填补了该领域的研究空白，为实际部署提供指导。

Method: 通过理论分析和实验验证，在集中训练、集成学习和分布式训练等多种实际部署场景下，系统测试不同超参数设置对两种攻击类型鲁棒性的影响。

Result: 发现学习率对两种攻击的鲁棒性影响相反：降低学习率使迁移攻击鲁棒性提升64%，提高学习率使查询攻击鲁棒性提升28%。分布式模型通过超参数调优能同时有效缓解两种攻击。

Conclusion: 优化超参数设计对对抗鲁棒性至关重要，分布式训练模型通过适当的超参数调优能实现最佳的双重攻击防御效果。

Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.

</details>


### [658] [Scientific Data Compression and Super-Resolution Sampling](https://arxiv.org/abs/2511.13675)
*Minh Vu,Andrey Lokhov*

Main category: cs.LG

TL;DR: 提出基于指数族学习的新型科学数据压缩与超分辨率框架，支持不确定性量化并在压缩比与重建保真度间灵活权衡


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟、观测和大规模实验生成的数据量常超出存储、处理和分析能力极限，需要开发能高效管理海量数据集同时保留关键物理特征的数据缩减方法

Method: 基于近期指数族学习进展，构建科学数据压缩与超分辨率框架，支持物理量不确定性量化

Result: 该方法能有效压缩科学数据并支持从压缩表示中恢复数据，在压缩比与重建保真度间提供灵活权衡

Conclusion: 该框架为科学数据管理提供了有效的压缩和恢复解决方案，特别适用于检查点和重启等关键科学工作流程

Abstract: Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.

</details>


### [659] [Cross-Learning from Scarce Data via Multi-Task Constrained Optimization](https://arxiv.org/abs/2511.13680)
*Leopoldo Agorio,Juan Cerviño,Miguel Calvo-Fullana,Alejandro Ribeiro,Juan Andrés Bazerque*

Main category: cs.LG

TL;DR: 提出跨学习框架，通过联合估计多个相关任务的确定性参数来解决数据稀缺问题，实现从数据丰富任务到数据稀缺任务的知识迁移


<details>
  <summary>Details</summary>
Motivation: 当数据有限时，学习模型难以泛化到未见过的案例。需要克服数据稀缺问题，特别是在参数推断对有限数据至关重要的场景中

Method: 将联合估计制定为约束优化问题，约束条件控制不同模型参数之间的相似性，允许参数在不同任务间存在差异，同时结合多个数据源的信息

Result: 在控制框架下提供理论保证，并在图像分类和传染病传播等实际应用中展示了跨学习方法的有效性

Conclusion: 跨学习框架能够实现知识迁移，产生更准确可靠的参数估计，为从有限数据中进行参数推断提供了解决方案

Abstract: A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \emph{cross-learning} framework to overcome data scarcity by jointly estimating \emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.

</details>


### [660] [Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers](https://arxiv.org/abs/2511.13685)
*Disha Varshney,Samarth Garg,Sarthak Tyagi,Deeksha Varshney,Nayan Deep,Asif Ekbal*

Main category: cs.LG

TL;DR: 该研究提出SSRGNet模型，通过结合图神经网络和语言模型，利用蛋白质3D结构数据来预测蛋白质二级结构，在f1分数上超越了基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用未标记的氨基酸序列，但未能充分利用可获取的蛋白质3D结构数据，而3D结构是决定蛋白质功能的关键因素。

Method: 使用蛋白质残基图捕获空间信息，结合预训练的蛋白质语言模型编码氨基酸序列，采用GCN和R-GCN等消息传递机制学习蛋白质结构的几何特征，通过多层卷积学习空间图的综合信息。

Result: 在NetSurfP-2.0数据集上的实验表明，SSRGNet模型在f1分数上超越了基线方法。

Conclusion: 通过有效结合蛋白质3D结构信息和序列信息，提出的方法能够更好地预测蛋白质二级结构，为理解蛋白质功能和结构提供重要见解。

Abstract: In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.

</details>


### [661] [Efficient Calibration for Decision Making](https://arxiv.org/abs/2511.13699)
*Parikshit Gopalan,Konstantinos Stavropoulos,Kunal Talwar,Pranay Tankala*

Main category: cs.LG

TL;DR: 本文提出了一种基于结构化后处理函数族的校准决策损失度量CDL_K，解决了原始CDL难以计算的问题，并建立了该度量的计算和理论可处理性理论。


<details>
  <summary>Details</summary>
Motivation: 原始校准决策损失(CDL)在离线设置中难以近似计算，需要寻找可处理的替代度量来评估预测器的校准质量。

Method: 通过限制后处理函数为结构化函数族K，定义相对校准决策损失CDL_K，分析其在信息论和计算上的可处理性，并为自然函数类提供上下界。

Result: 建立了CDL_K的全面理论框架，证明了在某些结构化函数族下的可计算性，为机器学习中广泛使用的重新校准程序提供了严格保证。

Conclusion: 通过引入结构化后处理函数族，CDL_K提供了计算可行的校准度量，同时为实际机器学习应用中的校准方法提供了理论支持。

Abstract: A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.

</details>


### [662] [Learning stochasticity: a nonparametric framework for intrinsic noise estimation](https://arxiv.org/abs/2511.13701)
*Gianluigi Pillonetto,Alberto Giaretta,Mauro Bisiacco*

Main category: cs.LG

TL;DR: Trine是一个非参数、基于核的框架，用于从时间序列数据中推断状态依赖的内在噪声，通过三阶段算法结合可解析求解的子问题和结构化核架构，能够捕捉噪声驱动的突变波动和状态依赖的方差平滑变化。


<details>
  <summary>Details</summary>
Motivation: 在生物学和生态学等领域，非线性相互作用和随机效应的不完全知识使得自下而上的建模方法往往无效，需要开发能够直接从数据中发现控制方程的方法。参数模型在没有强先验知识时难以有效估计内在噪声，而理解内在噪声对复杂系统动态行为的影响至关重要。

Method: Trine采用三阶段算法：结合可解析求解的子问题和结构化核架构，该架构能够同时捕捉噪声驱动的突变波动和状态依赖的方差平滑变化。这是一种非参数、基于核的方法，不依赖预定义的参数假设。

Result: 在生物和生态系统上的验证表明，Trine能够揭示隐藏的动态特性，无需依赖预定义的参数假设。在多个基准问题上，Trine实现了与oracle相当的性能，该oracle可视为能够直接追踪细胞内分子浓度或反应事件随机波动的理想观察者。

Conclusion: Trine框架为理解内在噪声如何影响复杂系统行为开辟了新途径，提供了一种有效的方法来从时间序列数据中推断状态依赖的内在噪声。

Abstract: Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.

</details>


### [663] [ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification](https://arxiv.org/abs/2511.13702)
*Luyao Niu,Nuoxian Huang*

Main category: cs.LG

TL;DR: ST-ProC是一个新颖的图原型多目标半监督学习框架，用于解决GPS轨迹出行模式识别中的标签稀缺问题，通过结合图正则化、原型锚定和边界感知伪标签策略，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: GPS轨迹出行模式识别对城市智能至关重要，但标注成本高导致标签严重稀缺。现有的半监督学习方法存在灾难性确认偏差问题，且忽略了数据的内在流形结构。

Method: 提出ST-ProC框架，结合图原型核心和基础SSL支持。核心部分通过图正则化、原型锚定和边界感知伪标签策略利用数据流形并主动拒绝噪声；支持部分通过对比学习和师生一致性损失确保高质量表示和鲁棒优化。

Result: ST-ProC在所有基线方法中表现显著优越，在真实世界稀疏标签设置下，相比最先进方法如FixMatch性能提升21.5%。

Conclusion: ST-ProC框架通过有效利用数据流形结构和多目标优化策略，成功解决了出行模式识别中的标签稀缺问题，为半监督学习在现实应用中的有效性提供了有力证明。

Abstract: Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.

</details>


### [664] [Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering](https://arxiv.org/abs/2511.13705)
*Alaa Mezghiche*

Main category: cs.LG

TL;DR: 通过自编码器和稳定性分析发现肾癌中罕见但可复现的分子亚型


<details>
  <summary>Details</summary>
Motivation: 在RNA-seq数据中发现标准标签之外的分子亚型，特别是罕见但可复现的亚型

Method: 结合自编码器表示、聚类和稳定性分析，使用高变基因、标准化处理、128维潜空间自编码器，k-means聚类(k=2-10)，通过Jaccard相似度和匈牙利算法评估稳定性

Result: 泛癌分析显示聚类与组织来源高度一致(Cramer's V=0.887)，在肾癌中发现k=5时存在罕见亚型C0(6.85%患者)，稳定性高(Jaccard=0.787)

Conclusion: 泛癌聚类受组织来源主导，而基于稳定性的单癌种分析能发现罕见可复现亚型

Abstract: Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.

</details>


### [665] [From Black Box to Insight: Explainable AI for Extreme Event Preparedness](https://arxiv.org/abs/2511.13712)
*Kiana Vu,İsmet Selçuk Özer,Phung Lai,Zheng Wu,Thilanka Munasinghe,Jennifer Wei*

Main category: cs.LG

TL;DR: 本文研究可解释AI在极端事件预测中的作用，以野火预测为例，使用SHAP方法揭示模型决策路径和潜在偏差，提升AI系统的可信度和可操作性。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了极端事件的频率和严重性，需要准确、可解释和可操作的预测。虽然AI模型在预测方面表现出潜力，但其黑盒特性限制了在实际决策中的采用，影响了信任度和可操作性。

Method: 使用野火预测作为案例研究，评估多种AI模型，并采用SHAP方法来揭示关键特征、决策路径和模型行为中的潜在偏差，同时提供支持性可视化来增强解释性。

Result: 分析表明XAI不仅澄清了模型推理，还支持领域专家和响应团队的关键决策，通过情境化特征重要性和时空模式增强了AI解释的可用性。

Conclusion: 研究强调AI系统不仅需要准确性，还需要可解释性、可访问性和可信性，这对于灾害准备、风险缓解和气候韧性规划的有效使用至关重要。

Abstract: As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [666] [WITNESS: A lightweight and practical approach to fine-grained predictive mutation testing](https://arxiv.org/abs/2511.11999)
*Zeyu Lu,Peng Zhang,Chun Yong Chong,Shan Gao,Yibiao Yang,Yanhui Li,Lin Chen,Yuming Zhou*

Main category: cs.SE

TL;DR: WITNESS是一种新的细粒度预测性变异测试方法，采用轻量级经典机器学习模型而非深度学习，适用于所有变异体（包括方法内和方法外），在保持高性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的细粒度预测性变异测试方法存在两个关键限制：(1) 高昂的计算成本，违背了预测性变异测试降低成本的目标；(2) 受限的适用性，只能处理方法内变异体而无法预测方法外变异体。

Method: WITNESS采用双重设计：(1) 收集方法内和方法外变异体的特征，使其适用于所有生成的变异体；(2) 使用轻量级经典机器学习模型进行训练和预测，而非计算昂贵的深度学习。

Result: 在Defects4J项目上的评估显示，WITNESS在不同场景下始终达到最先进的预测性能，显著提升杀灭矩阵预测效率。后验分析表明，包含变异前后信息的特征最为重要。

Conclusion: WITNESS提供了一种更高效、更适用的预测性变异测试解决方案，基于预测杀灭矩阵的测试用例优先级排序结果更接近使用实际杀灭矩阵的结果，优于基线方法。

Abstract: Existing fine-grained predictive mutation testing studies predominantly rely on deep learning, which faces two critical limitations in practice: (1) Exorbitant computational costs. The deep learning models adopted in these studies demand significant computational resources for training and inference acceleration. This introduces high costs and undermines the cost-reduction goal of predictive mutation testing. (2) Constrained applicability. Although modern mutation testing tools generate mutants both inside and outside methods, current fine-grained predictive mutation testing approaches handle only inside-method mutants. As a result, they cannot predict outside-method mutants, limiting their applicability in real-world scenarios. We propose WITNESS, a new fine-grained predictive mutation testing approach. WITNESS adopts a twofold design: (1) With collected features from both inside-method and outside-method mutants, WITNESS is suitable for all generated mutants. (2) Instead of using computationally expensive deep learning, WITNESS employs lightweight classical machine learning models for training and prediction. This makes it more cost-effective and enabling straightforward explanations of the decision-making processes behind the adopted models. Evaluations on Defects4J projects show that WITNESS consistently achieves state-of-the-art predictive performance across different scenarios. Additionally, WITNESS significantly enhances the efficiency of kill matrix prediction. Post-hoc analysis reveals that features incorporating information from before and after the mutation are the most important among those used in WITNESS. Test case prioritization based on the predicted kill matrix shows that WITNESS delivers results much closer to those obtained by using the actual kill matrix, outperforming baseline approaches.

</details>


### [667] [A Code Smell Refactoring Approach using GNNs](https://arxiv.org/abs/2511.12069)
*HanYu Zhang,Tomoji Kishi*

Main category: cs.SE

TL;DR: 提出了一种基于图深度学习的代码异味重构方法，使用图分类和节点分类任务处理长方法、大类、特性嫉妒三种代码异味，在实验中表现优于传统和现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有代码异味重构方法存在局限性：基于指标和规则的方法依赖手动定义的启发式规则和阈值，而深度学习方法受限于数据集可用性和模型设计。

Method: 设计了两类输入图（类级别和方法级别），采用图分类和节点分类任务，使用GCN、GraphSAGE和GAT三种经典GNN架构，并提出半自动数据集生成方法。

Result: 提出的方法在重构性能上优于传统和现有深度学习方法，实现了优越的重构效果。

Conclusion: 基于图的深度学习方法能够有效解决代码异味重构问题，在性能上超越现有技术，为软件重构提供了新的有效途径。

Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past decades, a variety of refactoring approaches have been proposed, which can be broadly classified into metrics-based, rule-based, and machine learning-based approaches. Recent years, deep learning-based approaches have also attracted widespread attention. However, existing techniques exhibit various limitations. Metrics- and rule-based approaches rely heavily on manually defined heuristics and thresholds, whereas deep learning-based approaches are often constrained by dataset availability and model design. In this study, we proposed a graph-based deep learning approach for code smell refactoring. Specifically, we designed two types of input graphs (class-level and method-level) and employed both graph classification and node classification tasks to address the refactoring of three representative code smells: long method, large class, and feature envy. In our experiment, we propose a semi-automated dataset generation approach that could generate a large-scale dataset with minimal manual effort. We implemented the proposed approach with three classical GNN (graph neural network) architectures: GCN, GraphSAGE, and GAT, and evaluated its performance against both traditional and state-of-the-art deep learning approaches. The results demonstrate that proposed approach achieves superior refactoring performance.

</details>


### [668] [Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision](https://arxiv.org/abs/2511.12229)
*Zhipeng Xue,Zhipeng Gao,Tongtong Xu,Xing Hu,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: 该研究构建了首个大规模可操作警告数据集，提出了ACWRecommender两阶段框架，通过粗粒度检测和细粒度重排来推荐高概率为真实bug的可操作警告，显著提升了静态分析工具的实用性。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具因高误报率而难以广泛应用，现有可操作警告收集方法的假设不够准确，导致大量无效警告。

Method: 从Top-500 GitHub C仓库挖掘68,274次回退构建数据集，提出ACWRecommender两阶段框架：粗粒度检测阶段预热预训练模型识别可操作警告，细粒度重排阶段通过弱监督学习将高概率真实bug警告排到顶部。

Result: 实验显示ACWRecommender在nDCG和MRR指标上大幅优于基线方法。在6个项目中手动检查2,197个警告，向开发者报告前10推荐警告，其中27个被确认为真实bug。

Conclusion: 该工具能帮助开发者从大量警告中快速找到真实bug，验证了其实际应用价值。

Abstract: The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.

</details>


### [669] [Reflections on the design, applications and implementations of the normative specification language eFLINT](https://arxiv.org/abs/2511.12276)
*L. Thomas van Binsbergen,Christopher A. Esterhuyse,Tim Müller*

Main category: cs.SE

TL;DR: 本文介绍了eFLINT领域特定软件语言，用于自动化法律合规检查，结合了声明式和过程式元素来处理法律场景和推理。


<details>
  <summary>Details</summary>
Motivation: 随着软件在社会实践中的普及，检查软件是否符合法律、法规和合同变得越来越重要且成本高昂。数字化服务受到越来越多的法律法规约束，需要高度可适应的合规实践。

Method: 开发了eFLINT领域特定软件语言，该语言结合声明式和过程式元素来分别推理情况和场景，形式化法律概念与计算概念之间的联系，旨在在软件系统运行前、运行中和运行后自动化合规检查。

Result: 通过回顾各种应用案例、它们提出的要求以及后续的设计决策，反映了该语言当前的设计。

Conclusion: 本文报告了在自动化合规领域可以惠及语言开发者的调查结果和见解。

Abstract: Checking the compliance of software against laws, regulations and contracts is increasingly important and costly as the embedding of software into societal practices is getting more pervasive. Moreover, the digitalised services provided by governmental organisations and companies are governed by an increasing amount of laws and regulations, requiring highly adaptable compliance practices. A potential solution is to automate compliance using software. However, automating compliance is difficult for various reasons. Legal practices involve subjective processes such as interpretation and qualification. New laws and regulations come into effect regularly and laws and regulations, as well as their interpretations, are subjected to constant revision. In addition, computational reasoning with laws requires a cross-disciplinary process involving both legal and software expertise.
  This paper reflects on the domain-specific software language eFLINT developed to experiment with novel solutions. The language combines declarative and procedural elements to reason about situations and scenarios respectively, explicates and formalises connections between legal concepts and computational concepts, and is designed to automate compliance checks both before, during and after a software system runs. The various goals and applications areas for the language give rise to (conflicting) requirements. This paper reflects on the current design of the language by recalling various applications, the requirements they imposed, and subsequent design decisions. As such, this paper reports on results and insights of an investigation that can benefit language developers within the field of automated compliance.

</details>


### [670] [Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation](https://arxiv.org/abs/2511.12288)
*Yihan Dai,Sijie Liang,Haotian Xu,Peichu Xie,Sergey Mechtaev*

Main category: cs.SE

TL;DR: 语义三角化通过问题转换验证程序一致性，提高代码生成的可靠性，能在低采样概率下识别正确解，并处理多有效解问题。


<details>
  <summary>Details</summary>
Motivation: 现有样本共识方法在采样概率低或存在多个有效解时效果不佳，无法可靠选择正确程序或适时弃权。

Method: 引入语义三角化，对编程问题进行非平凡语义转换，同时保持解之间的可验证映射，通过跨转换的一致性验证提高置信度。

Result: 在LiveCodeBench和CodeElo基准测试中，相比概率阈值0.5的高置信度选择方法，可靠性提高21%，能在采样概率低至0.14时识别正确解。

Conclusion: 语义三角化能更可靠地形成样本共识，准确识别正确程序并适时弃权，特别是在低概率采样和多有效解场景下表现优异。

Abstract: When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.

</details>


### [671] [ProofWright: Towards Agentic Formal Verification of CUDA](https://arxiv.org/abs/2511.12294)
*Bodhisatwa Chatterjee,Drew Zagieboylo,Sana Damani,Siva Hari,Christos Kozyrakis*

Main category: cs.SE

TL;DR: ProofWright是一个验证框架，通过结合自动化形式验证和LLM代码生成，为LLM生成的CUDA内核提供内存安全、线程安全和语义正确性的端到端保证。


<details>
  <summary>Details</summary>
Motivation: LLM自动生成的CUDA内核虽然提高了开发效率，但往往包含难以发现的正确性错误，且缺乏形式化安全保证。运行时测试不可靠，而手动形式验证无法跟上LLM的生成速度，形成验证瓶颈。

Method: ProofWright将自动化形式验证与基于LLM的代码生成相结合，构建了一个代理验证框架。

Result: 在KernelBench L1上，ProofWright验证了74%生成内核的安全属性，发现了传统测试遗漏的微妙正确性错误，并为元素级内核建立了语义等价性。每个内核验证时间约3分钟。

Conclusion: ProofWright证明了对LLM生成的GPU代码进行可扩展的自动化形式验证是可行的，为可信的高性能代码生成提供了路径，同时不牺牲开发效率。

Abstract: Large Language Models (LLMs) are increasingly used to automatically generate optimized CUDA kernels, substantially improving developer productivity. However, despite rapid generation, these kernels often contain subtle correctness bugs and lack formal safety guarantees. Runtime testing is inherently unreliable - limited input coverage and reward hacking can mask incorrect behavior - while manual formal verification is reliable but cannot scale to match LLM output rates, creating a critical validation bottleneck.
  We present ProofWright, an agentic verification framework that bridges this gap by integrating automated formal verification with LLM-based code generation. ProofWright provides end-to-end guarantees of memory safety, thread safety, and semantic correctness for LLM-generated CUDA kernels. On KernelBench L1, ProofWright verifies safety properties for 74% of generated kernels, uncovers subtle correctness errors missed by conventional testing, and establishes semantic equivalence for a class of element-wise kernels. With a modest overhead of 3 minutes per kernel, ProofWright demonstrates that scalable, automated formal verification of LLM-generated GPU code is feasible - offering a path toward trustworthy high-performance code generation without sacrificing developer productivity.

</details>


### [672] [High-level reasoning while low-level actuation in Cyber-Physical Systems: How efficient is it?](https://arxiv.org/abs/2511.12543)
*Burak Karaduman,Baris Tekin Tezel,Moharram Challenger*

Main category: cs.SE

TL;DR: 该研究比较了六种编程语言和框架在工业信息集成系统中的最坏情况执行时间和开发时间，包括C++、Java、Jade、Jason以及松散和紧密耦合的模糊Jason BDI框架，为工业应用选择软件技术提供实证依据。


<details>
  <summary>Details</summary>
Motivation: 工业信息集成系统日益复杂，需要智能行为、实时响应和高效开发的技术支持，但工程师缺乏足够的实证证据来指导高级工业应用的工具选择。

Method: 采用以开发者为中心的方法，通过可测量的结果比较不同抽象层次和推理能力对开发工作和运行时行为的影响，分析工程工作量与执行效率之间的权衡。

Result: 研究发现抽象层次和推理机制如何影响系统性能和开发效率，为设计必须在实时约束和复杂决策要求下运行的智能、基于代理的解决方案提供实践见解。

Conclusion: 该研究为工业信息化中选择软件技术提供基于证据的指导，支持改进集成效率、可维护性和响应性，并为未来研究网络物理和智能制造系统中语言特性、开发动态和运行时行为之间的相互作用奠定基础。

Abstract: The increasing complexity of industrial information-integration systems demands software technologies that enable intelligent behaviour, real-time response, and efficient development. Although many programming languages and frameworks exist, engineers still lack sufficient empirical evidence to guide the choice of tools for advanced industrial applications. This study addresses that need by measuring and comparing worst-case execution time (WCET) and development time across six languages and frameworks: C++, Java, Jade, Jason, and fuzzy Jason BDI with both loosely and tightly coupled integration. These technologies reflect a progression from procedural and object-oriented programming to agent-based frameworks capable of symbolic and fuzzy reasoning.
  Rather than relying on broad concepts such as paradigms or orientations, the study adopts a developer-centred approach grounded in measurable outcomes. The structured comparison examines how rising abstraction levels and reasoning capabilities affect both development effort and runtime behaviour. By analysing these dimensions, the study highlights concrete trade-offs between engineering workload and execution efficiency.
  The findings show how abstraction and reasoning mechanisms shape system performance and developer productivity, offering practical insight for designing intelligent, agent-based solutions that must operate under real-time constraints and complex decision-making requirements. Overall, the study contributes evidence-based guidance for selecting software technologies in industrial informatization, supporting improved integration efficiency, maintainability, and responsiveness, and laying groundwork for future research on the interplay between language features, development dynamics, and runtime behaviour in cyber-physical and smart manufacturing systems.

</details>


### [673] [Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?](https://arxiv.org/abs/2511.12576)
*Mohammad Meymani,Hamed Jelodar,Parisa Hamedi,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.SE

TL;DR: 本研究系统评估了大小生成式AI模型在应用行为理解（特别是恶意软件检测）方面的能力，发现小模型在保持竞争力的同时具有计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型在代码分析和恶意软件检测等领域展现出强大能力，但大模型的计算资源需求限制了实际部署。本研究旨在探索小模型在资源受限环境中的可行性。

Method: 系统评估不同规模的生成式AI语言模型在应用行为理解任务上的表现，以恶意软件检测为代表任务，比较准确率、精确率、召回率和F1分数等指标。

Result: 大模型整体准确率更高，但小模型在精确率和召回率方面保持竞争力，且在计算效率、推理速度和资源受限环境部署方面具有显著优势。

Conclusion: 小生成式AI模型可以有效补充大模型，在实际应用行为分析中提供性能与资源效率的平衡，特别适合资源受限环境。

Abstract: Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.

</details>


### [674] [LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews](https://arxiv.org/abs/2511.12635)
*Lech Madeyski,Barbara Kitchenham,Martin Shepperd*

Main category: cs.SE

TL;DR: 本文分析了评估大型语言模型在系统综述文献筛选中的性能时存在的关键问题，提出了改进评估实践的建议。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs发布速度快于用户严谨评估的能力，当LLMs用于支持研究（如系统综述文献筛选）时，需要稳健的实证评估。

Method: 以近期大规模研究为例，分析传统指标在评估Gen-AI工具文献筛选性能时的问题，分析了27篇相关论文，提取性能指标并识别良好实践和普遍问题。

Result: 发现主要弱点包括：使用对不平衡数据不稳健的指标、未考虑证据丢失对工作负载节省声明的影响、普遍未报告完整混淆矩阵。同时提取了良好评估实践。

Conclusion: 建议优先考虑证据丢失/召回率，使用机会锚定和成本敏感的WMCC指标，报告完整混淆矩阵，采用防泄漏设计，并将结论建立在成本效益分析基础上。

Abstract: Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.

</details>


### [675] [Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter](https://arxiv.org/abs/2511.12823)
*Sajed Jalil,Shuvo Saha,Hossain Mohammad Seym*

Main category: cs.SE

TL;DR: 提出了一种结合测试驱动开发(TDD)和代码解释器(CI)的新方法，使用开源模型提升孟加拉语代码生成能力，无需微调即可达到85%准确率，最小模型能达到最大模型98%的性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语拥有2.42亿母语者但在LLM训练中关注不足，现有代码生成技术需要大量专业知识和资源，目标是为资源受限的新兴市场提供本地语言的代码生成工具。

Method: 结合测试驱动开发(TDD)和代码解释器(CI)，使用开源权重模型，无需微调即可提升孟加拉语代码生成性能。

Result: 孟加拉语提示的代码生成基线准确率提升至85%，同一家族中最小的模型能达到最大模型98%的准确率。

Conclusion: 该方法证明了无需微调即可显著提升孟加拉语代码生成能力，为资源受限环境提供了可行的解决方案，所有结果已在GitHub公开。

Abstract: Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.

</details>


### [676] [Human-Centred Requirements Engineering for Critical Systems: Insights from Disaster Early Warning Applications](https://arxiv.org/abs/2511.12856)
*Anuradha Madugalla,Jixuan Dong,Kai Lyne Loi,Matthew Crossman,John Grundy*

Main category: cs.SE

TL;DR: 本文提出了一种以人为本的需求工程流程，将社会责任整合到关键系统开发中，通过设计适应性预警系统原型验证了62项功能和非功能需求，证明早期考虑以人为本需求能提升系统可用性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 关键系统（如医疗、国防、灾害管理）传统上注重技术保证，但忽视了系统运行的人类和社会背景。本文认为考虑以人为本的方面是可靠性的重要维度。

Method: 通过文献综述确定了为弱势群体设计软件的指南，转化为62项功能和非功能需求，设计适应性预警系统原型，并通过6次访谈和8次认知走查进行评估。

Result: 研究结果表明，早期解决以人为本的需求能增强系统对所有用户的可用性和可访问性。

Conclusion: 以人为本不是伦理附加品，而是安全和公平关键系统的决定性质量特征。

Abstract: Critical systems, such as those used in healthcare, defence, and disaster management, demand rigorous requirements engineering to ensure safety and reliability. Yet, much of this rigour has traditionally focused on technical assurance, often overlooking the human and social contexts in which these systems operate. This paper argues that considering human-centric aspects is an essential dimension of dependability, and presents a human-centred RE process designed to integrate social responsibility into critical system development. Drawing from a literature review, we identified a set of guidelines for designing software for vulnerable communities and translated these into sixty-two functional and non-functional requirements. These requirements were operationalised through the design of an adaptive early warning system prototype, which was subsequently evaluated through six interviews and eight cognitive walkthroughs to validate their relevance and applicability. The findings demonstrate that human-centric requirements, when addressed early, enhance the usability and accessibility of systems for all users. The paper concludes by positioning human-centricity not as an ethical add-on but as a defining quality of safe and equitable critical systems.

</details>


### [677] [Agent READMEs: An Empirical Study of Context Files for Agentic Coding](https://arxiv.org/abs/2511.12884)
*Worawalan Chatlatanagulchai,Hao Li,Yutaro Kashiwa,Brittany Reid,Kundjanasith Thonglek,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Bram Adams,Ahmed E. Hassan,Hajimu Iida*

Main category: cs.SE

TL;DR: 对2303个代理上下文文件的大规模实证研究发现，这些文件是复杂且难以阅读的配置类文档，主要关注功能性需求而忽视安全性和性能等非功能性需求。


<details>
  <summary>Details</summary>
Motivation: 代理编码工具使用自然语言目标作为输入，通过代理上下文文件提供项目级指令。本研究旨在首次大规模分析这些文件的结构、维护和内容特征。

Method: 分析了来自1925个代码仓库的2303个代理上下文文件，研究其结构、维护模式和16种指令类型的内容分布。

Result: 发现代理上下文文件是动态演化的配置代码，主要包含构建运行命令(62.3%)、实现细节(69.9%)和架构信息(67.7%)，但安全(14.5%)和性能(14.5%)等非功能性需求很少被指定。

Conclusion: 开发者主要使用上下文文件确保代理的功能性，但缺乏确保代码安全性和性能的防护措施，需要改进工具和实践。

Abstract: Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.

</details>


### [678] [Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities](https://arxiv.org/abs/2511.12950)
*Zirui Chen,Zhipeng Xue,Jiayuan Zhou,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: Diffploit是一个基于差异驱动的漏洞利用迁移方法，通过上下文模块和迁移模块迭代修复跨版本漏洞利用失败问题，在Java CVE数据集上取得了84.2%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞利用方法在跨版本迁移时经常失败，主要由于触发条件变化和环境级故障，现有技术主要关注代码级跟踪对齐，无法有效处理环境级故障和复杂的触发条件变化。

Method: Diffploit采用迭代的差异驱动方法，包含上下文模块（动态构建行为差异上下文）和迁移模块（基于LLM的迭代反馈循环适应），平衡差异候选探索和逐步优化。

Result: 在102个Java CVE和689个版本迁移任务上，Diffploit成功迁移84.2%的漏洞利用，比TARGET工具高52.0%，比IDEA工具高61.6%。还发现了5个CVE报告的错误影响版本范围和111个未报告漏洞版本。

Conclusion: Diffploit有效解决了跨版本漏洞利用迁移问题，不仅技术效果显著，还能识别和验证漏洞报告中的错误信息。

Abstract: Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.

</details>


### [679] [SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports](https://arxiv.org/abs/2511.12993)
*Longfei Chen,Ruibin Yan,Taiyu Wong,Yiyang Chen,Chao Zhang*

Main category: cs.SE

TL;DR: SmartPoC是一个自动化框架，能够将文本审计报告转换为可执行的验证测试用例，解决了传统审计中PoC测试缺乏可重现性和自动化验证的问题。


<details>
  <summary>Details</summary>
Motivation: 智能合约审计报告存在异构性，缺乏可重现、可执行的PoC测试，导致需要昂贵的手动验证。LLMs可用于生成PoC测试，但面临输入噪声、幻觉和缺失运行时预言等挑战。

Method: 首先处理输入审计报告以减少噪声，仅提取与漏洞相关的函数作为LLM上下文。通过专门设计的执行前后修复机制合成PoC测试用例，并使用差分验证作为预言来确认漏洞可利用性。

Result: 在SmartBugs-Vul和FORGE-Vul基准测试中，SmartPoC分别为85.61%和86.45%的目标生成了可执行的验证Foundry测试用例。应用于最新的Etherscan验证源语料库，SmartPoC以每个发现仅0.03美元的成本确认了545个审计发现中的236个真实漏洞。

Conclusion: SmartPoC成功解决了LLMs在生成PoC测试时的主要挑战，能够高效、低成本地将审计报告转换为可执行的验证测试用例，显著提升了智能合约漏洞验证的自动化水平。

Abstract: Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.

</details>


### [680] [Towards Requirements Engineering for GenAI-Enabled Software: Bridging Responsibility Gaps through Human Oversight Requirements](https://arxiv.org/abs/2511.13069)
*Zhenyu Mao,Jacky Keung,Yicheng Sun,Yifei Wang,Shuo Liu,Jialong Li*

Main category: cs.SE

TL;DR: 本文提出了一种三层分析方法来解决GenAI软件中的责任缺口问题，包括概念化、方法论和工件层面，通过用户研究验证了该方法在识别责任缺口和推导人类监督需求方面的有效性。


<details>
  <summary>Details</summary>
Motivation: GenAI软件中的生成性和适应性特性使得人类监督和责任分配变得复杂，现有需求工程方法在处理这些责任缺口方面存在概念、方法和工件层面的研究空白。

Method: 采用三层分析方法：概念化层定义人类和系统维度的责任要素；方法论层引入演绎管道识别责任缺口；工件层通过演绎骨干表形式化结果。

Result: 用户研究表明，与基线目标导向需求工程方法相比，所提方法在六个维度上均有明显改进，有效解决了三个研究空白。

Conclusion: 该方法为系统分析GenAI软件中的责任缺口提供了连贯视角，能够有效识别责任缺口并推导相应的人类监督需求。

Abstract: Context: Responsibility gaps, long-recognized challenges in socio-technical systems where accountability becomes diffuse or ambiguous, have become increasingly pronounced in GenAI-enabled software. The generative and adaptive nature complicates how human oversight and responsibility are specified, delegated, and traced. Existing requirements engineering (RE) approaches remain limited in addressing these phenomena, revealing conceptual, methodological, and artifact-level research gaps.. Objective: This study aims to analyze these research gaps in the context of GenAI-enabled software systems. It seeks to establish a coherent perspective for a systematic analysis of responsibility gaps from a human oversight requirements standpoint, encompassing how these responsibility gaps should be conceptualized, identified, and represented throughout the RE process. Methods: The proposed design methodology is structured across three analytical layers. At the conceptualization layer, it establishes a conceptual framing that defines the key elements of responsibility across the human and system dimensions and explains how potential responsibility gaps emerge from their interactions. At the methodological layer, it introduces a deductive pipeline for identifying responsibility gaps by analyzing interactions between these dimensions and deriving corresponding oversight requirements within established RE frameworks. At the artifact layer, it formalizes the results in a Deductive Backbone Table, a reusable representation that traces the reasoning path from responsibility gaps identification to human oversight requirements derivation. Results: A user study compared the proposed methodology with a baseline goal-oriented RE across two scenarios. Evaluation across six dimensions indicated clear improvements of the proposed methodology, confirming its effectiveness in addressing three research gaps.

</details>


### [681] [Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming](https://arxiv.org/abs/2511.13271)
*Rufeng Chen,Shuaishuai Jiang,Jiyun Shen,AJung Moon,Lili Wei*

Main category: cs.SE

TL;DR: 本研究比较了GenAI与传统在线资源在编程教育中的知识获取效果，发现GenAI能显著提升任务完成度但对知识增益帮助有限，不同经验水平的学生使用策略差异明显。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注GenAI完成任务的能力和对学生表现的影响，但忽视了其对知识获取的影响，需要比较GenAI与传统资源在支持知识增益方面的差异。

Method: 对24名不同编程经验水平（初学者、中级）的本科生进行对照实验，分析他们在解决编程任务时与ChatGPT的交互行为，评估任务表现、概念理解和互动行为。

Result: 使用GenAI生成完整解决方案能显著提升任务表现（尤其对初学者），但不能持续带来知识增益；初学者倾向于过度依赖GenAI完成任务而缺乏知识获取，中级学生则采用更有选择性的方法。

Conclusion: 应将GenAI作为学习工具而非问题解决工具使用，编程教育中集成GenAI需要指导以避免过度依赖或极少使用，以促进更深层次的理解。

Abstract: The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.

</details>


### [682] [SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents](https://arxiv.org/abs/2511.13305)
*Rangeet Pan,Raju Pavuluri,Ruikai Huang,Rahul Krishna,Tyler Stennett,Alessandro Orso,Saurabh SInha*

Main category: cs.SE

TL;DR: SAINT是一个用于企业Java应用服务级测试的白盒测试方法，结合静态分析、大语言模型和基于LLM的代理来自动生成端点和场景测试。


<details>
  <summary>Details</summary>
Motivation: 现有的服务级测试工具主要依赖模糊测试和OpenAPI规范，但在真实企业代码库中这些规范通常不可用，且难以生成有效测试功能场景的测试。

Method: SAINT通过静态分析构建端点模型和操作依赖图，然后使用基于LLM的代理生成测试。端点测试最大化代码和数据库交互覆盖，场景测试通过代理循环的计划、行动和反思阶段从代码中提取用例并精炼为可执行测试。

Result: 在8个Java应用（包括一个专有企业应用）上的评估显示，SAINT在覆盖率、故障检测和场景生成方面表现有效，开发者调查对生成的场景测试给予强烈认可。

Conclusion: 将静态分析与基于代理的LLM工作流相结合，能够实现更有效、功能性和开发者对齐的服务级测试生成。

Abstract: Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.

</details>


### [683] [LinkXplore: A Framework for Affordable High-Quality Blockchain Data](https://arxiv.org/abs/2511.13318)
*Peihao Li*

Main category: cs.SE

TL;DR: LinkXplore是一个开源的区块链数据收集和管理框架，通过直接分析RPC查询或流数据绕过昂贵的区块链数据提供商，为预算有限的研究人员和开发者提供低成本的高质量区块链数据。


<details>
  <summary>Details</summary>
Motivation: 大规模区块链数据收集成本过高，现有RPC提供商定价昂贵，不适合预算受限的研究和工业应用，这严重阻碍了学术研究和产品开发。同时缺乏能够灵活集成新模块的系统性框架来分析链上数据。

Method: 开发了LinkXplore开源框架，通过简单的API和后端处理逻辑，允许用户直接分析来自RPC查询或流的原始数据，可以集成任何类型的链数据。

Result: LinkXplore能够以极低成本提供高质量的区块链数据，为预算有限的研究人员和开发者提供了实用的替代方案。

Conclusion: LinkXplore是首个用于收集和管理链上数据的开放框架，解决了区块链数据收集成本高昂和缺乏灵活分析框架的问题，代码和数据集已公开。

Abstract: Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.
  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore

</details>


### [684] [An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains](https://arxiv.org/abs/2511.13341)
*Zihe Yan,Kai Luo,Haoyu Yang,Yang Yu,Zhuosheng Zhang,Guancheng Li*

Main category: cs.SE

TL;DR: 提出了一个细粒度的开源软件后门风险评估框架，从攻击者视角建模隐蔽后门攻击，并针对每个攻击阶段定义相应指标。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中，开源软件供应链存在安全隐患，特别是在XZ-Util事件中体现的高隐蔽性后门攻击，由于依赖维护不足和社区审计不充分，难以确保源代码安全和仓库维护者的合法性。

Method: 框架从攻击者视角建模隐蔽后门攻击，为每个攻击阶段定义针对性指标，并利用大语言模型对代码仓库进行语义评估，克服静态分析在评估仓库维护活动可靠性方面的局限性。

Result: 在Debian生态系统的66个高优先级软件包上评估该框架，实验结果表明当前开源软件供应链面临多种安全风险。

Conclusion: 该框架能够有效评估开源软件的后门风险，揭示了当前开源软件供应链的安全脆弱性。

Abstract: In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.

</details>


### [685] [FLOWER: Flow-Oriented Entity-Relationship Tool](https://arxiv.org/abs/2511.13357)
*Dmitry Moskalev*

Main category: cs.SE

TL;DR: FLOWER是一个面向流程的实体关系工具，首个端到端解决方案，能够自动处理、创建和可视化显式和隐式依赖关系，支持SQL和自然语言查询，在分布表示和约束学习方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索跨数据源的关系是实体识别的关键优化。由于数据库存储大量合成和有机数据，正确处理所有对象数量是重要任务，但实体关系模型的构建决策往往受人为因素影响。

Method: 提出FLOWER工具，采用动态采样和鲁棒数据分析技术，自动检测内置约束并创建正确的必要约束，支持多种SQL方言和自然语言查询。

Result: 在STATS基准测试中，FLOWER在分布表示上比水库采样快2.4倍，约束学习快2.6倍，加速2.15倍。数据叙事准确度提升1.19倍，上下文减少1.86倍。支持23种语言，兼容CPU和GPU。

Conclusion: FLOWER能够更好地处理真实世界数据，确保质量、可扩展性和不同用例的适用性。

Abstract: Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.

</details>


### [686] [BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance](https://arxiv.org/abs/2511.13611)
*Torec T. Luik,Joost de Folter,Rodrigo Rosas-Bertolini,Eric A. J. Reits,Ron A. Hoebe,Przemek M. Krawczyk*

Main category: cs.SE

TL;DR: BIOMERO 2.0是一个重大演进的框架，将OMERO转变为符合FAIR原则（可发现、可访问、可互操作、可重用）且具备溯源意识的生物成像平台，通过容器化组件集成数据导入、预处理、分析和工作流监控。


<details>
  <summary>Details</summary>
Motivation: 解决生物成像数据管理中缺乏完整溯源记录和FAIR合规性的问题，弥合数据导入、分析和共享之间的差距。

Method: 采用双子系统方法：导入子系统通过容器化预处理和元数据丰富实现就地导入；分析子系统通过BIOMERO Python库协调和跟踪高性能计算系统上的容器化分析。所有操作都记录参数、版本和结果。

Result: 实现了实时溯源访问，通过集成仪表板提供完整的操作记录，将OMERO置于生物成像分析过程的核心位置。

Conclusion: BIOMERO 2.0通过集成层增强了OMERO的FAIR化，支持可追踪、可重用的图像分析工作流，为生物成像研究提供了完整的溯源管理解决方案。

Abstract: We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing.

</details>


### [687] [Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?](https://arxiv.org/abs/2511.13646)
*Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang*

Main category: cs.SE

TL;DR: Live-SWE-agent是第一个能够在运行时自主持续演化的软件代理，从基础bash工具开始，在解决真实软件问题时自主进化其脚手架实现，在SWE-bench Verified基准测试中达到75.4%的解决率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM软件代理需要专门设计且可能不是最优的，而自我改进代理需要昂贵的离线训练且泛化能力有限。

Method: 提出Live-SWE-agent，从仅具备bash工具的基础代理开始，在解决真实软件问题时自主进化其脚手架实现。

Result: 在SWE-bench Verified基准测试中达到75.4%的解决率，优于所有现有开源软件代理；在SWE-Bench Pro基准测试中达到45.8%的最佳已知解决率。

Conclusion: Live-SWE-agent证明了软件代理能够在运行时自主持续进化，显著提升性能，接近最佳专有解决方案的水平。

Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

</details>


### [688] [What's in a Software Engineering Job Posting?](https://arxiv.org/abs/2511.13656)
*Marvin Wyrich,Lloyd Montgomery*

Main category: cs.SE

TL;DR: 分析100个软件工程职位发布，揭示雇主对非技术能力的需求，包括与公司目标契合、文化适应、个人成长和人际交往能力。


<details>
  <summary>Details</summary>
Motivation: 探索软件工程师理想候选人的演变，超越技术能力，关注雇主在社会技术和组织方面的期望。

Method: 对100个软件工程职位发布进行主题分析。

Result: 雇主期望候选人：与公司目标契合、适应企业文化、追求个人和职业成长、擅长人际互动。

Conclusion: 为SE社区提供关于软件工程师角色演变的见解，对研究人员、教育者、从业者和招聘者具有参考价值，记录了2023年SE职位发布的趋势。

Abstract: A well-rounded software engineer is often defined by technical prowess and the ability to deliver on complex projects. However, the narrative around the ideal Software Engineering (SE) candidate is evolving, suggesting that there is more to the story. This article explores the non-technical aspects emphasized in SE job postings, revealing the sociotechnical and organizational expectations of employers. Our Thematic Analysis of 100 job postings shows that employers seek candidates who align with their sense of purpose, fit within company culture, pursue personal and career growth, and excel in interpersonal interactions. This study contributes to ongoing discussions in the SE community about the evolving role and workplace context of software engineers beyond technical skills. By highlighting these expectations, we provide relevant insights for researchers, educators, practitioners, and recruiters. Additionally, our analysis offers a valuable snapshot of SE job postings in 2023, providing a scientific record of prevailing trends and expectations.

</details>


### [689] [Ontology-Driven Model-to-Model Transformation of Workflow Specifications](https://arxiv.org/abs/2511.13661)
*Francisco Abreu,Luís Cruz,Sérgio Guerreiro*

Main category: cs.SE

TL;DR: 提出基于本体的模型转换管道，将专有工作流语言转换为标准BPMN 2.0，解决厂商锁定问题，成功率为94.2%


<details>
  <summary>Details</summary>
Motivation: 专有工作流建模语言（如Smart Forms & Smart Flow）阻碍互操作性和重用，将流程知识锁定在封闭格式中

Method: 三阶段管道：RML将JSON语义提升为RDF/OWL、本体对齐和推理、通过Camunda Model API生成BPMN，将映射知识外部化为本体和声明性规则

Result: 在69个真实工作流上测试，生成92个BPMN图，成功率94.2%，失败率5.81%源于动态行为和时间转换未在静态JSON中显式定义

Conclusion: 本体驱动的M2M转换可系统性地桥接领域特定工作流和标准符号，提供可量化的性能和定性效益，支持互操作性并减少厂商依赖

Abstract: Proprietary workflow modeling languages such as Smart Forms & Smart Flow hamper interoperability and reuse because they lock process knowledge into closed formats. To address this vendor lock-in and ease migration to open standards, we introduce an ontology-driven model-to-model pipeline that systematically translates domain-specific workflow definitions to Business Process Model and Notation (BPMN) 2.0. The pipeline comprises three phases: RML-based semantic lifting of JSON to RDF/OWL, ontology alignment and reasoning, and BPMN generation via the Camunda Model API. By externalizing mapping knowledge into ontologies and declarative rules rather than code, the approach supports reusability across vendor-specific formats and preserves semantic traceability between source definitions and target BPMN models. We instantiated the pipeline for Instituto Superior Técnico (IST)'s Smart Forms & Smart Flow and implemented a converter that produces standard-compliant BPMN diagrams. Evaluation on a corpus of 69 real-world workflows produced 92 BPMN diagrams with a 94.2% success rate. Failures (5.81%) stemmed from dynamic behaviors and time-based transitions not explicit in the static JSON. Interviews with support and development teams indicated that the resulting diagrams provide a top-down view that improves comprehension, diagnosis and onboarding by exposing implicit control flow and linking tasks and forms back to their sources. The pipeline is generalizable to other proprietary workflow languages by adapting the ontology and mappings, enabling interoperability and reducing vendor dependency while supporting continuous integration and long-term maintainability. The presented case study demonstrates that ontology-driven M2M transformation can systematically bridge domain-specific workflows and standard notations, offering quantifiable performance and qualitative benefits for stakeholders.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [690] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨了使用大型语言模型生成合成新闻标题数据集来替代真实数据，以解决NLP任务中的数据获取和隐私问题，特别关注负面情感文本。


<details>
  <summary>Details</summary>
Motivation: 克服真实世界数据获取的挑战和隐私担忧，为情感分析等NLP任务提供替代数据源。

Method: 使用定制提示创建负面新闻标题语料库，通过专家评审和嵌入空间分析验证合成标题，并与真实新闻标题进行多项基准测试比较。

Result: 生成的标题在内容、语调、长度和风格上与真实标题高度匹配，仅在POS分析中的专有名词得分存在明显差异。

Conclusion: LLM生成的合成数据集可以有效替代真实数据用于NLP任务，特别是在负面情感分析领域。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [691] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB是一个评估大语言模型处理气候变化专业知识的基准测试，发现前沿模型具有博士级别的知识综合能力，但在证据基础方面存在严重幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型处理复杂专业知识的能力，特别是在气候变化领域的表现，这是AI部署到科学工作流程中的关键挑战。

Method: 引入CLINB基准测试，基于真实用户问题和气候科学家制定的评估标准，采用模型驱动的评估过程来测试多个前沿模型。

Result: 前沿模型展现出卓越的知识综合能力，甚至超过专家辅助的混合答案，但在证据基础方面存在严重问题，引用和图像幻觉率较高。

Conclusion: 弥合知识综合与可验证归因之间的差距对于AI在科学工作流程中的部署至关重要，需要像CLINB这样的可靠基准来构建可信AI系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [692] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying是一个用于网络欺凌研究和检测的合成多LLM对话数据集，通过大语言模型模拟真实欺凌互动，提供可扩展且伦理安全的替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统网络欺凌数据收集存在伦理问题和规模限制，需要一种既能模拟真实对话结构又能确保伦理安全的数据生成方法。

Method: 利用大语言模型生成多轮对话，提供对话结构、上下文感知标注和细粒度标签，涵盖多种欺凌类别，并进行多维度评估。

Result: 数据集在对话结构、词汇模式、情感/毒性、角色动态、伤害强度和欺凌类型分布等五个维度上表现良好，可作为独立训练数据或增强源用于欺凌分类。

Conclusion: SynBullying为网络欺凌研究提供了一个可扩展、伦理安全的合成数据集，支持多维度分析和分类任务。

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [693] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard是一种结合因果推理和符号逻辑的新方法，能在幻觉发生时检测和预防，无需重新训练模型，显著减少错误信息


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型的幻觉问题，现有方法要么需要重新训练模型，要么增加计算成本，要么未能解决幻觉的根本原因

Method: 结合因果推理和符号逻辑，通过两条互补路径：追踪模型知识与生成内容之间的因果关系，以及使用自动推理检查逻辑一致性

Result: 在12个基准测试中，正确识别幻觉89.3%，仅漏检8.3%，减少虚假声明近80%，在复杂推理任务中表现尤其出色

Conclusion: CausalGuard能有效减少幻觉，保持回答自然性，在医疗诊断和金融分析等敏感领域特别有价值，因为其能展示推理过程

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [694] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: 提出了一个量化框架，通过将游戏建模为随机决策树来分离技能和运气成分，定义了技能-运气指数S(G)在[-1,1]范围内，并引入波动率Sigma来量化回合间结果的不确定性。


<details>
  <summary>Details</summary>
Motivation: 需要一种系统方法来量化游戏中技能和运气的相对贡献，以便进行游戏设计、AI评估和风险评估。

Method: 将游戏建模为随机决策树，将游戏结果分解为技能杠杆K和运气杠杆L，定义技能-运气指数S(G)=K-L，并计算波动率Sigma。

Result: 分析了30个游戏，发现从纯运气（抛硬币，S=-1）到混合领域（如双陆棋，S=0）再到纯技能（国际象棋，S=+1）的连续谱。扑克表现出中等技能优势（S=0.33）。

Conclusion: 该框架可扩展到一般随机决策系统，为玩家影响力、游戏平衡性和预测稳定性的比较提供了原则性方法，在游戏设计、AI评估和风险评估中有应用价值。

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [695] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR是一个模块化的零样本代理框架，通过分层提示分析和价值对齐推理来实现更安全、更有帮助的文生图生成，显著减少不安全输出同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型存在产生不安全、冒犯性或文化不当内容的风险，现有防御方法难以在不牺牲生成质量或高成本的情况下使输出与人类价值观对齐。

Method: VALOR框架包含多层NSFW检测器、文化价值对齐模块和意图消歧器，检测到不安全内容时通过大语言模型在特定角色指令下重写提示，必要时进行风格化再生以导向更安全的视觉领域。

Result: 在对抗性、模糊性和价值敏感提示上的实验显示，VALOR将不安全输出减少高达100.00%，同时保持了提示的有用性和创造性。

Conclusion: VALOR作为可扩展且有效的方法，能够在开放世界环境中部署安全、对齐且有用的图像生成系统。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [696] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel是一个能够生成并实现量子物理学想法的LLM代理系统，它通过领域特定的AI工具将文献中的想法转化为可立即在实验室实施的实验设计。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学领域的应用仍主要依赖人类提供研究问题和目标，AI生成的创意往往模糊且需要人类执行。自动化想法生成和实现将显著改变人类在科学过程中的角色。

Method: 开发AI-Mandel LLM代理系统，该系统从文献中提炼想法，并使用领域特定的AI工具将这些想法转化为具体的实验设计。

Result: AI-Mandel生成的许多想法具有科学价值，其中两个想法已经促成了独立的后续科学论文。想法包括量子隐形传态的新变体、不定因果顺序中的量子网络原语，以及基于量子信息传输闭合环路的几何相位新概念。

Conclusion: AI-Mandel是能够生成和实现具体可行想法的AI物理学家的原型演示。构建这样的系统不仅有助于加速科学发展，还揭示了实现人类水平人工科学家所面临的具体挑战。

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [697] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的智能体框架，让小型LLM学会通过迭代执行反馈来动态调试SPARQL查询，显著提升了知识图谱问答中复杂多跳问题的处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成复杂SPARQL查询时存在脆弱性问题，缺乏基于实时执行反馈的动态调试策略，这成为知识图谱问答的关键瓶颈。

Method: 使用仅3B参数的紧凑模型，通过结果驱动的强化学习（GRPO）训练，无需监督微调，让智能体学会从执行错误中系统恢复并迭代优化查询。

Result: 在LC-QuAD 2.0的可执行子集上达到49.7%的准确率，比最强的零样本基线提升17.5个百分点，显式推理步骤进一步提升了策略精度。

Conclusion: 该工作为教授智能体通过交互掌握形式化符号工具提供了通用蓝图，弥合了概率性LLM与结构化知识图谱世界之间的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [698] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: 论文主张以通用性而非抽象智力概念作为AI评估的基础，认为智力基准与真实世界效用存在错位，通用性才是更稳定的评估标准。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估过度依赖智力基准（如ARC、Raven测试等），但这些基准缺乏稳定定义且无法预测实际任务表现，可能导致评估与真实世界效用脱节。

Method: 通过概念和形式分析，检验智力评估的三个假设：通用性、稳定性和现实性，论证只有通用性能够经受概念和实证检验。

Result: 分析表明智力不能解释通用性，通用性应被理解为多任务学习问题，直接关联到可测量的性能广度和可靠性。

Conclusion: 应将AI进展评估重新框架为基于通用性的评估，通用性为评估多样化演进任务的能力提供了更稳定的基础。

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [699] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文批判性地评估了现有NL-FOL翻译数据集和评测方法，提出新的评测协议来区分真正的语义理解与表面模式识别，并发现对话型LLM在NL-FOL翻译方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 自然语言到一阶逻辑的翻译是一个长期挑战，现有研究对LLM在此任务上的能力存在矛盾结论，需要更准确的评估方法。

Method: 批判分析现有数据集和评测协议的局限性，设计新的评测协议来区分语义级逻辑理解与表面模式识别、记忆和数据集污染。

Result: 使用新评测方法发现，最先进的对话型LLM展现出强大的NL-FOL翻译能力和真正的句子级逻辑理解能力，而嵌入中心模型表现明显较差。

Conclusion: 对话型LLM具有真正的NL-FOL翻译能力，但需要更准确的评测方法来评估这种能力，避免现有方法的局限性导致的误判。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [700] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: KrwEmd算法通过k-recall winrate特征和earth mover's distance聚类来解决德州扑克等游戏中手牌抽象过度的问题，显著提升AI游戏表现。


<details>
  <summary>Details</summary>
Motivation: 解决大规模不完全信息游戏中手牌抽象过度的问题，特别是极端不完全回忆抽象完全丢弃历史信息导致的AI性能下降。

Method: 引入k-recall winrate特征，利用未来和关键的历史游戏信息区分信号观察信息集并量化相似性；开发KrwEmd算法，使用earth mover's distance测量特征差异来聚类信号观察信息集。

Result: 实验结果表明KrwEmd相比现有算法显著提升了AI游戏表现。

Conclusion: KrwEmd是首个实用的解决手牌抽象过度问题的算法，通过有效利用历史信息改善了不完全信息游戏中的AI性能。

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [701] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception是一个基于拓扑属性的基准测试，用于严格评估大型视觉语言模型的全局视觉感知能力，发现现有模型在全局感知方面表现不佳，甚至不如随机猜测。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准存在局部捷径问题，导致高估模型的感知能力。需要一种能够无捷径评估全局视觉感知的方法。

Method: 利用拓扑属性构建评估基准，因为拓扑依赖于图像全局结构且对局部特征不变，能够实现无捷径的全局感知评估。

Result: 所有最先进模型在最粗粒度感知上都表现不佳，准确率不高于随机水平。更强大的模型反而表现更差，表明单纯扩大模型规模无法解决这一问题。

Conclusion: TopoPerception揭示了当前LVLMs的关键瓶颈，为改进全局视觉感知提供了方向和视角，可能需要新的训练范式或架构。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [702] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O是一个端到端系统，可将组织解剖视频转化为手势序列，并发现与术后结果相关的模式。该系统使用基于Transformer的时空建模和逐帧分类，在机器人辅助前列腺癌根治术的神经保留步骤中准确检测连续短手势，并能预测术后结果，准确性与人工标注相当。


<details>
  <summary>Details</summary>
Motivation: 术中行为的细粒度分析及其对患者结果的影响是一个长期存在的挑战。需要开发能够自动分析手术视频并发现与临床结果相关模式的系统。

Method: 使用基于Transformer的空间和时间建模，结合逐帧分类方法，将组织解剖视频转化为手势序列。系统检测连续短手势（约2秒），并提取手势频率、持续时间和转换等特征。

Result: 在神经保留步骤中实现帧级AUC 0.80和视频级AUC 0.81的检测性能。F2O提取的特征预测术后结果的准确性与人工标注相当（0.79 vs 0.75）。系统还发现了与勃起功能恢复相关的关键模式，如延长组织剥离时间和减少能量使用。

Conclusion: F2O通过实现自动可解释的评估，为数据驱动的手术反馈和前瞻性临床决策支持奠定了基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [703] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: 提出了Forgetting-MarI框架，通过惩罚边际信息来精确移除待遗忘数据对LLM的额外贡献，同时保留其他数据的知识，实现可证明的不可检测性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在扩展数据集上训练，需要选择性移除特定数据的影响以满足隐私保护和法规要求，现有方法往往过度移除信息导致模型性能下降。

Method: 引入Forgetting-MarI框架，通过惩罚边际信息来精确移除待遗忘数据对模型的额外贡献，同时保留其他数据的知识，提供明确的遗忘数据集残留影响上界。

Result: 在多个基准测试中优于现有最先进遗忘方法，实现了可靠的遗忘效果并更好地保持了模型的通用性能。

Conclusion: 这一进展使AI系统在满足隐私和版权法规的同时不损害其有效性，向更可控和合规的AI系统迈出了重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [704] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: 本研究系统评估了四种大型语言模型在抽象视觉推理问题上的表现，发现GPT-4.1-Mini在所有推理架构中表现最佳，但推理效果具有模型特异性。


<details>
  <summary>Details</summary>
Motivation: 系统评估大型语言模型在抽象视觉推理问题上的性能表现，探索不同推理架构对模型表现的影响。

Method: 使用四种LLM模型和四种推理架构，在RAVEN-FAIR数据集上进行测试，通过三阶段处理生成视觉响应，使用SSIM和LPIPS指标评估，分析思维链分数和错误类型。

Result: GPT-4.1-Mini在所有架构中始终获得最高准确率，多智能体架构会改变语义和数值平衡但效果不一致，不同模型对架构设计表现出不同的敏感模式。

Conclusion: 推理效果具有模型特异性，响应覆盖度的变化使跨架构直接比较复杂化，多轮运行评估比单轮评估更可靠。

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [705] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: 本章探讨了构建可靠AI系统（特别是智能体AI系统）面临的挑战和未来发展方向，包括级联故障风险缓解、动态环境、任务执行不一致性、不可预测的涌现行为以及资源密集型可靠性机制等研究问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统的发展，确保其可靠性和安全性变得日益重要。本章旨在识别和讨论构建可靠AI系统过程中面临的关键挑战和开放研究问题。

Method: 通过分析当前AI系统可靠性领域的现状，识别出多个关键研究方向和挑战，包括级联故障缓解、动态环境适应性、任务执行一致性、涌现行为预测以及可靠性机制优化等。

Result: 提出了构建可靠AI系统的多个重要研究方向和开放问题，为未来研究提供了清晰的路线图。

Conclusion: 构建可靠的智能体AI系统需要在多个技术层面进行深入研究，包括故障缓解、环境适应性、行为预测和评估方法等，这些研究方向对于推动AI系统的实际应用至关重要。

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [706] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 本文提出了"反弹赢家通吃(RWTA)"基元作为可扩展神经形态控制架构的基本元素，结合了离散计算的可靠性和连续调节的可调性。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的架构，能够同时处理连续节律生成和离散决策，结合离散计算的可靠性和连续调节的可调性。

Method: 使用反弹赢家通吃(RWTA)基元构建神经形态控制架构，该架构继承赢家通吃状态机的离散计算能力和可兴奋生物物理电路的连续调谐能力。

Result: 通过蛇形机器人的神经系统设计展示了该架构的通用性、鲁棒性和模块化特性。

Conclusion: 提出的基于事件的框架为连续节律生成和离散决策提供了统一的物理建模语言，具有很好的应用前景。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [707] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: 提出CFA-SMOTE方法，结合反事实解释和SMOTE技术，通过生成气候异常事件的合成数据点来解决气候变化预测中的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致极端天气事件频发，传统机器学习方法难以处理分布外的气候异常数据，因为历史数据缺乏足够的少数类（气候异常事件）样本。

Method: 将气候变化预测问题视为类别不平衡问题，结合可解释AI中的反事实方法和经典的SMOTE方法，创建代表气候异常事件的合成数据点来增强数据集。

Result: 在不同类别不平衡比例条件下，CFA-SMOTE方法在预测爱尔兰奶牛场草生长方面表现出优于基准方法的性能。

Conclusion: CFA-SMOTE方法能有效处理气候变化预测中的类别不平衡问题，提高对极端气候事件的预测能力。

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [708] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: 提出了一种结合大语言模型和符号逻辑的混合神经符号框架，用于确定性检测复杂法律中的法规不一致性，以美国国内税收法典为案例研究。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在层次化处理和深度结构化推理方面存在困难，特别是在处理长文本时，而税收领域的特定应用仍然稀缺。

Method: 使用GPT-4o将税法条款翻译为Prolog规则，然后通过Prolog增强提示测试不一致性检测效果，并与纯自然语言提示进行对比。

Result: GPT-4o在三种策略中仅检测到一种不一致性（33%准确率），而混合Prolog模型产生了确定性和可重现的结果，成功检测到不一致区域。

Conclusion: 基于符号逻辑的LLM辅助形式化能够实现透明可靠的法规不一致性检测。

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [709] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: 提出基于直接依赖检索(DDR)的声明自动形式化框架，通过直接从自然语言数学描述生成候选库依赖并验证其存在，显著提升了检索精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有声明自动形式化方法缺乏上下文感知能力，容易产生形式化定义和定理的幻觉，且当前检索增强方法在形式化库依赖检索方面精度和召回率较差，无法有效利用不断增长的公共数据集。

Method: 提出DDR方法：直接从自然语言数学描述生成候选库依赖，通过高效后缀数组检查验证其在形式化库中的存在性；构建超过50万样本的依赖检索数据集并微调高精度DDR模型。

Result: DDR模型在检索精度和召回率上显著优于SOTA方法；配备DDR的自动形式化器在单次尝试准确率和多次尝试稳定性方面均优于使用传统基于选择的RAG方法的模型。

Conclusion: DDR方法有效解决了声明自动形式化中的依赖检索问题，为深度学习和形式数学的融合提供了更可靠的基础。

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [710] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 提出Chain-of-Evidence（CoE）范式和Look As You Think（LAT）强化学习框架，用于视觉文档检索增强生成中的证据归因，通过将推理步骤与具体视觉区域关联来确保可验证的预测。


<details>
  <summary>Details</summary>
Motivation: 现有端到端训练方法缺乏细粒度监督和推理过程的渐进可追溯性，无法提供可靠的证据来源验证。

Method: CoE统一了思维链推理和视觉证据归因，使用边界框和页面索引将推理元素与具体区域关联。LAT通过强化学习训练模型生成具有一致归因的可验证推理路径。

Result: 在Paper-和Wiki-VISA基准测试中，LAT在单图和双图设置下分别提升软精确匹配8.23%和IoU@0.5 47.0%，优于监督微调基线并展现更强的跨领域泛化能力。

Conclusion: LAT框架有效提升了视觉文档检索增强生成中的证据归因质量，实现了过程级的自我验证和可靠的预测可追溯性。

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [711] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH是一个可解释的病理AI框架，通过两阶段自学习过程从被动模式识别转向证据关联的诊断推理，仅需少量标注数据即可生成癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 当前病理AI系统缺乏人类可读的推理过程，难以审计决策和防止错误，限制了临床应用。

Method: 采用两阶段自学习过程：多样化扩展病理风格解释，优化提高准确性；无需白盒访问或权重更新。

Result: 在乳腺癌和前列腺癌数据集上，RECAP-PATH生成与专家评估一致的推理，诊断准确性显著优于基线方法。

Conclusion: RECAP-PATH通过结合视觉理解和推理，提供临床可信的AI，展示了证据关联解释的通用路径。

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [712] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: 提出了一种用于橡胶轮胎制造的多路径差分裁剪近端策略优化算法（MPD-PPO），通过多分支策略架构和差分梯度裁剪约束，解决了高维多目标优化问题，在轮胎薄膜生产的宽度和厚度控制中显著提升了调节精度和操作效率。


<details>
  <summary>Details</summary>
Motivation: 智能制造需要解决传统集中式调度和生产线路配置的局限性，特别是应对动态生产需求。轮胎制造系统形成具有非线性相互作用和涌现动态的复杂网络，有效协调多个子系统成为关键但艰巨的任务。

Method: 引入深度强化学习算法MPD-PPO，采用多分支策略架构和差分梯度裁剪约束，确保高维策略更新的稳定性和效率。

Result: 在橡胶轮胎薄膜生产的宽度和厚度控制实验中，MPD-PPO在调节精度和操作效率方面均表现出显著改进。

Conclusion: 该框架成功解决了高维度、多目标权衡和动态适应等关键挑战，为轮胎制造中的实时工业部署提供了增强的性能和生产稳定性。

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [713] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: 本文提出了T-BoN BO框架，将贝叶斯优化扩展到语言领域，通过结合Best-of-N选择和文本梯度来优化评估效率，在广告对齐任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前自改进AI主要关注生成效率，但在许多社会应用中，评估成本远高于生成成本，需要优化评估效率而非生成效率。

Method: 提出T-BoN BO框架，证明Best-of-N选择策略与文本梯度组合可模拟UCB采集函数的梯度行为，实现语言空间的贝叶斯优化。

Result: 在自动化广告对齐任务上的实验验证表明，T-BoN BO相比现有最优基线方法具有更优越的性能。

Conclusion: T-BoN BO为AI自改进提供了一个简单且评估效率高的语言空间贝叶斯优化框架，解决了评估成本高昂的实际问题。

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [714] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 提出了Embedding CFR算法，通过将信息集嵌入到低维连续空间来解决大规模不完美信息扩展式博弈，相比基于聚类的抽象方法能更精确地捕捉信息集间的差异和联系，在扑克游戏中实现了更快的可剥削性收敛。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法依赖预训练的离散聚类进行抽象，但硬分类会不可逆地丢失信息集之间的量化细微差异，这些差异对于策略求解至关重要，从而影响求解质量。

Method: 受自然语言处理中词嵌入范式启发，提出Embedding CFR算法：预训练并将孤立信息集的特征嵌入到互联的低维连续空间，在该嵌入空间中进行遗憾积累和策略更新的策略求解过程。

Result: 在扑克实验表明，在相同空间开销下，Embedding CFR相比基于聚类的抽象算法实现了显著更快的可剥削性收敛，证实了其有效性。

Conclusion: 这是扑克AI中首个通过低维嵌入预训练信息集抽象来进行策略求解的算法，理论分析验证了其减少累积遗憾的能力。

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [715] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: Mobile-Agent-RAG是一个新颖的分层多智能体框架，通过双重检索增强来解决移动智能体在现实世界长序列跨应用任务中的性能瓶颈，显著提高了任务完成率和步骤效率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的移动智能体在现实世界长序列跨应用任务中成功率不足，主要问题在于过度依赖MLLM中的静态内部知识，导致高层规划中的战略幻觉和低层UI操作中的执行错误。

Method: 提出分层多智能体框架Mobile-Agent-RAG，集成双重检索增强：在规划阶段使用Manager-RAG检索人类验证的综合任务计划以减少战略幻觉；在执行阶段使用Operator-RAG检索精确的低层指导以提高执行准确性。构建了两个专门的检索导向知识库。

Result: 大量实验表明，Mobile-Agent-RAG显著优于最先进的基线方法，任务完成率提高了11.0%，步骤效率提高了10.2%。

Conclusion: Mobile-Agent-RAG为上下文感知、可靠的多智能体移动自动化建立了一个稳健的范式，通过区分规划阶段和操作阶段所需的不同知识类型，有效解决了移动智能体的关键性能瓶颈。

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [716] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: 提出了一种缓解小模型知识蒸馏中灾难性遗忘的解决方案，包括构建包含元认知知识的数据集和GDPO训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和微调方法在8B以下小模型上容易导致灾难性遗忘，主要问题是忽略训练数据知识与模型固有能力的关系，以及传统训练目标无法有效约束固有知识保留。

Method: 1) 构建包含5K实例的数据集，覆盖多种推理任务并融入元认知知识；2) 提出GDPO（Group Direction Preference Optimization）训练方法，通过参考模型隐式约束优化路径，更适合资源受限场景。

Result: 大量实验表明该方法显著缓解了灾难性遗忘，并提升了小模型的推理性能。

Conclusion: 从数据和训练方法两个角度提出的综合解决方案能有效解决小模型知识蒸馏中的灾难性遗忘问题。

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [717] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: RTMol是一个双向对齐框架，通过自监督的往返学习统一分子描述和文本到SMILES生成，解决了现有方法在化学准确性、数据质量和双向一致性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将分子描述和文本到分子设计作为独立任务处理，存在三个关键问题：传统指标偏重语言流畅性而非化学准确性、训练数据包含化学模糊描述、独立优化导致双向不一致。

Method: 提出RTMol框架，通过自监督往返学习统一分子描述和文本到SMILES生成，引入新颖的往返评估指标，支持无需配对分子-文本语料库的无监督训练。

Result: 实验表明RTMol在各种LLMs上将双向对齐性能提升高达47%，为联合分子-文本理解和生成建立了有效范式。

Conclusion: RTMol框架有效解决了分子序列表示与文本描述对齐的关键挑战，在药物发现、材料设计和自动化化学文献分析等应用中具有重要价值。

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [718] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: 提出了DRedMTL算法，一种支持有界区间的DatalogMTL增量推理方法，显著优于重新物化方法


<details>
  <summary>Details</summary>
Motivation: 现有DatalogMTL推理方法不支持高效的动态更新，而实际应用需要频繁数据更新

Method: 基于经典DRed算法，设计了专门操作符处理DatalogMTL物化的周期性表示

Result: 在多个公开数据集上的实验表明，DRedMTL通常显著优于重新物化方法，有时快几个数量级

Conclusion: DRedMTL为DatalogMTL提供了高效的增量推理能力，满足实际应用中的动态更新需求

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [719] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: 提出了DoM框架，通过多智能体辩论机制动态融合结构化和非结构化知识来解决不完整知识图谱问答问题，并构建了更真实的不完整KGQA数据集。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱通常不完整，现有方法无法自适应地融合多源知识，无法充分利用知识的互补优势。

Method: 基于多智能体辩论范式，分配专门智能体分别处理知识图谱和外部文本推理，通过迭代交互协调输出，包括问题分解、双智能体证据检索和法官智能体评估聚合。

Result: 实验表明DoM在多个基准测试中持续优于最先进的基线方法。

Conclusion: DoM框架通过知识互补性增强了KG不完整性的鲁棒性，为不完整知识图谱问答提供了有效解决方案。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [720] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 提出ViTE框架用于行人轨迹预测，通过虚拟图和专家路由机制自适应建模显式和隐式交互，避免深层GNN的计算负担。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模高阶交互时面临深度与计算成本的权衡：层数不足导致感受野受限，层数过多则计算成本过高。需要自适应建模显式一阶和隐式高阶依赖。

Method: ViTE框架包含两个核心模块：虚拟图引入动态虚拟节点建模长距离高阶交互，无需深层GNN堆叠；专家路由基于社交上下文自适应选择交互专家，采用混合专家设计。

Result: 在ETH/UCY、NBA和SDD三个基准测试中均取得最先进性能，验证了方法的有效性和实际效率。

Conclusion: ViTE通过虚拟图和专家路由的组合实现了灵活可扩展的交互模式推理，在行人轨迹预测任务中表现出色且高效。

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [721] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: 本文通过哲学案例分析批判性检验世界模型框架是否能充分表征人类水平的理解能力


<details>
  <summary>Details</summary>
Motivation: 研究世界模型是否能像人类心智世界模型那样体现真正的理解能力，而非仅基于统计相关性

Method: 使用科学哲学文献中的案例研究，重点关注世界模型能力与人类理解差异最明显的哲学分析

Result: 发现世界模型框架在表征人类理解方面存在局限性

Conclusion: 世界模型虽然能模拟外部世界，但可能无法完全捕捉人类水平的理解能力

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [722] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA是一个基于视觉的风险检测系统，使用完全合成的ICU视频数据集开发，通过姿态估计检测患者手部进入气道管附近区域（碰撞）和解剖关键点速度（躁动）两种高风险运动模式，实现隐私保护的实时非计划拔管检测。


<details>
  <summary>Details</summary>
Motivation: ICU中非计划拔管是严重的安全问题，但实时检测受限于伦理和隐私问题难以获取标注的ICU视频数据。

Method: 利用文本到视频扩散技术生成多样化的临床真实ICU场景，通过姿态估计识别手部进入气道管附近区域的碰撞模式和基于解剖关键点速度的躁动模式。

Result: 专家评估确认合成数据的真实性，性能评估显示碰撞检测准确率高，躁动识别性能中等。

Conclusion: 这项工作展示了一种开发隐私保护、可复现的患者安全监测系统的新途径，具有在重症监护环境中部署的潜力。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [723] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 该论文提出了一种解决大语言模型在分布外道德对齐问题的方法，通过构建Moral-Reason-QA数据集和Group Relative Policy Optimization学习框架，使LLM能够将特定道德推理框架应用到未见过的道德场景中。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地影响人类的道德决策，但现有方法主要关注评估而非主动引导其道德决策。需要解决LLM在分布外场景中应用一致道德推理框架的问题。

Method: 构建包含680个人工标注的高模糊度道德场景的Moral-Reason-QA数据集，采用Group Relative Policy Optimization学习框架，通过复合奖励同时优化决策对齐和框架特定推理过程。

Result: 在分布外评估集上，功利主义框架的softmax归一化对齐分数提高了+0.757，义务论框架提高了+0.450，成功实现了对未见道德场景的泛化。

Conclusion: LLM代理可以被系统训练以内化并将特定道德框架应用于新情境，为AI安全提供了关键基础，特别是在语言模型更深入融入人类决策过程的情况下。

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [724] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: UpBench是一个基于真实Upwork工作任务的动态基准测试，用于评估LLM代理在真实工作环境中的表现，通过专家制定的详细评分标准和人类评估来提供细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多为静态、合成或领域受限，无法有效评估AI代理在动态、经济意义环境中的真实工作能力和人机协作潜力。

Method: 从Upwork平台获取真实工作任务，由专家自由职业者制定详细可验证的验收标准，采用基于评分标准的评估框架对AI提交内容进行逐项评估。

Result: 提供了超越二元通过/失败指标的细粒度模型能力分析，能够识别模型在指令遵循、专业标准等方面的具体优势和弱点。

Conclusion: UpBench为在真实劳动力市场环境中评估智能代理系统提供了可扩展、以人为本的基础，支持AI通过合作而非替代来增强人类能力的研究方向。

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [725] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出了RGR-GRPO框架，通过评分标准提供细粒度奖励和离线指导，在多领域推理任务中显著优于现有RL方法


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单领域可验证奖励的强化学习，依赖纯在线RL框架限制了探索空间，从而限制了推理性能

Method: RGR-GRPO框架，利用评分标准提供密集信息奖励，在GRPO训练期间探索更大的解空间

Result: 在14个多领域基准测试中，相比可验证在线RL基线，在数学、物理、化学和通用推理任务上分别平均提升7.0%、5.4%、8.4%和6.6%

Conclusion: RGR-GRPO在离线策略训练中保持稳定的熵波动，实现了持续的探索并有效突破了现有性能瓶颈

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [726] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: 本文提出了一种计算理性用户模型，用于模拟认知受限代理在偏见信念下的最优行为，重点关注记忆限制如何导致动态不一致的信念状态和次优决策，并开发了在线推理方法来从观察到的行为中推断用户的认知限制。


<details>
  <summary>Details</summary>
Motivation: 尽管AI技术快速发展，但预测和理解用户或人类合作者的次优行为仍然是一个关键挑战。这些行为往往不是非理性的，而是在认知限制和偏见信念下的理性决策。

Method: 提出计算理性用户模型，显式建模有限记忆过程如何导致动态不一致的偏见信念状态；开发基于嵌套粒子滤波的在线推理方法，同时跟踪用户潜在信念状态并估计未知认知限制。

Result: 在代表性导航任务中验证方法：CR模型生成与不同记忆容量水平对应的直观合理行为；推理方法从有限观察（≤100步）中准确高效地恢复真实认知限制。

Conclusion: 该方法为开发自适应AI助手提供了理论基础，使辅助系统能够考虑用户的记忆限制，实现个性化适应。

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [727] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: 提出了一个动态学习和适应不同建议者可靠性的框架，通过贝叶斯推断建议者类型，并引入显式的"询问"动作来策略性地请求建议。


<details>
  <summary>Details</summary>
Motivation: 自主智能体在不确定性下的顺序决策任务中可以从外部行动建议中受益，但现有方法通常假设建议者质量参数是静态且已知的，限制了实际部署。

Method: 将建议者质量直接集成到智能体的信念表示中，通过贝叶斯推断建议者类型；引入显式的"询问"动作，允许智能体在关键时刻策略性地请求建议。

Result: 实验评估展示了在不同建议者质量下的鲁棒性能、对变化可靠性的适应能力，以及建议请求的策略性管理。

Conclusion: 这项工作通过解决不确定环境中的建议不确定性，为自适应人机协作提供了基础。

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [728] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: 开发了一个基于临床验证流程图的对话式自我分诊系统，通过多智能体框架实现95.29%的流程图检索准确率和99.10%的导航准确率，结合自由文本交互的灵活性和标准化临床协议的严谨性。


<details>
  <summary>Details</summary>
Motivation: 在线健康资源和大型语言模型在医疗决策中准确性低、透明度不足且易受未经验证信息影响，需要可靠的患者决策支持系统。

Method: 使用美国医学会100个临床验证流程图构建多智能体框架，包括检索代理、决策代理和聊天代理，分别负责识别相关流程图、解释患者响应和提供个性化建议。

Result: 在模拟对话的大规模评估中，系统在流程图检索方面达到95.29%的top-3准确率，在流程图导航方面达到99.10%的准确率。

Conclusion: 该方法展示了透明、准确且可推广的AI辅助自我分诊的可行性，有潜力支持知情患者决策并改善医疗资源利用。

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [729] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: 提出了ARCHE任务和ARCHE Bench基准，用于评估LLM从科学文献中提取结构化推理链的能力，发现当前模型在推理链完整性和逻辑有效性之间存在权衡，无法完全满足科学论证的严谨要求。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽然能通过思维链等方法产生类似推理的内容，但这些输出通常是非结构化和非正式的，难以判断模型是否真正理解科学推理的基本范式。

Method: 引入潜在推理链提取(ARCHE)任务，要求模型将复杂推理分解为标准推理范式的组合，形成推理逻辑树(RLT)，其中所有推理步骤都明确分类为皮尔斯三种基本推理模式之一：演绎、归纳或溯因。

Result: 在基于70篇Nature Communications文章构建的ARCHE Bench基准上评估10个领先LLM，发现模型在推理边准确率(REA)和实体覆盖率(EC)之间存在权衡，尚无模型能够提取完整且标准的推理链。

Conclusion: 当前推理模型的能力与科学论证所需的严谨性之间存在显著差距，需要进一步改进模型以更好地理解和提取结构化推理链。

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [730] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT是一个针对限价订单簿数据的通用基础模型，通过新的标记化方案将多维消息作为单个标记处理，在预测中间价格变动和下一消息等任务中表现领先。


<details>
  <summary>Details</summary>
Motivation: 现有的LOB模型需要繁琐的数据表示，缺乏原始任务之外的适应性，因此需要开发一个适用于下游微调的通用基础模型。

Method: LOBERT基于BERT架构，采用新的标记化方案将完整的多维消息作为单个标记处理，同时保留价格、数量和时间的连续表示。

Result: LOBERT在预测中间价格变动和下一消息等任务中取得了领先性能，同时相比之前的方法减少了所需的上下文长度。

Conclusion: LOBERT为LOB数据提供了一个有效的通用基础模型，在多个任务中表现出色且具有更好的适应性。

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [731] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: PCRS-TKA是一个基于提示的框架，通过检索增强生成将预训练语言模型与知识图谱集成，解决了现有方法在利用PLM推理、知识筛选和协作偏好建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将PLM与知识图谱集成时面临三个关键挑战：未能充分利用PLM在图关系上的推理能力、不加区分地整合检索到的知识、以及忽略多轮对话中的协作偏好。

Method: 构建对话特定的知识树并序列化为文本，实现结构感知推理；选择性过滤上下文相关知识；使用专门监督信号显式建模协作偏好；通过语义对齐模块协调异构输入。

Result: 广泛实验表明，PCRS-TKA在推荐和对话质量方面始终优于所有基线方法。

Conclusion: PCRS-TKA通过有效整合PLM和KG，显著提升了对话推荐系统的准确性和对话质量。

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [732] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: 提出了一种动态树数据库变体，用于压缩命题和数值变量的状态集，在保持静态树数据库优点的同时避免了大量内存预分配问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模任务中扩展显式状态空间搜索时，紧凑表示生成状态集是一个核心挑战。传统树数据库虽然能在最佳情况下实现每个生成状态的恒定空间需求，但需要大量内存预分配。

Method: 开发了一种动态变体的树数据库，用于压缩命题和数值变量的状态集，并证明其保持了静态对应版本的理想特性。

Result: 在经典和数值规划任务上对状态压缩技术的实证评估显示，压缩比达到几个数量级，且通常运行时开销可忽略不计。

Conclusion: 动态树数据库提供了一种高效的状态压缩解决方案，在保持压缩性能的同时解决了内存预分配问题。

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [733] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: 提出了一种策略条件化的合作者框架，通过变分自编码器学习策略空间、聚类识别策略类型，并利用后悔最小化算法实时适应新伙伴的策略变化，在复杂协作环境中实现优于现有基线的性能。


<details>
  <summary>Details</summary>
Motivation: 在人类-智能体团队中，智能体需要实时适应具有独特偏好和动态变化策略的人类伙伴，这在时间压力和复杂策略空间的任务中尤其具有挑战性。

Method: 使用变分自编码器从智能体轨迹数据中学习潜在策略空间，通过聚类识别不同策略类型，训练基于这些聚类的条件化合作者智能体，并利用固定份额后悔最小化算法进行在线策略推断和调整。

Result: 在修改版的Overcooked协作烹饪环境中，该方法与新颖人类和智能体队友配对时，实现了优于现有基线的性能表现。

Conclusion: 提出的策略条件化合作者框架能够有效表示、分类和实时适应广泛的潜在伙伴策略，在复杂协作任务中展现出卓越的适应能力。

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [734] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: 研究发现，在语义流畅性任务中，现代高维嵌入空间中的随机游走能够产生与最优觅食理论一致的行为模式，而更复杂的Metropolis-Hastings采样方法反而与人类行为不符。


<details>
  <summary>Details</summary>
Motivation: 探索现代高维嵌入空间是否能提供足够的表示能力来匹配人类在语义流畅性任务中观察到的觅食行为模式，特别是验证最优觅食理论在记忆检索中的适用性。

Method: 使用最先进的词嵌入和先前的语义流畅性数据，在嵌入空间中进行随机游走和Metropolis-Hastings采样，比较两种方法产生的结果与人类行为的匹配程度。

Result: 随机游走在嵌入空间中产生了与最优觅食理论一致的结果，而Metropolis-Hastings采样方法未能产生与人类行为一致的结果。

Conclusion: 适当结构的嵌入空间，即使使用简单的采样方法，也能产生接近最优的觅食动态，挑战了复杂采样机制必然能更好模拟记忆检索的假设，支持Hills(2012)而非Abbott(2015)的观点。

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [735] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: Event-CausNet：利用LLM量化非结构化事件报告，构建因果知识库，并通过因果注意力机制增强GNN-LSTM网络，在交通中断事件中显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时空图神经网络在处理非重复性事件（如事故）时性能急剧下降，因为它们本质上是相关性模型，无法适应中断期间引入的新因果因素。

Method: 使用大语言模型量化非结构化事件报告，通过估计平均处理效应构建因果知识库，并采用双流GNN-LSTM网络结合新型因果注意力机制来调整和增强预测。

Result: 在真实数据集上的实验表明，Event-CausNet将预测误差（MAE）降低了35.87%，显著优于现有最先进基线方法。

Conclusion: 该框架弥合了相关性模型与因果推理之间的差距，提供了更准确、可迁移且具有关键可解释性的解决方案，为关键中断期间的现实交通管理提供了更可靠的基础。

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [736] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 使用强化学习和多智能体强化学习优化异构卫星集群在自主地球观测任务中的资源管理，解决实时性、不确定性和分散决策的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以处理地球观测任务中的实时性、不确定性和分散性特点，需要自适应决策方法来实现异构卫星间的有效协调和资源管理。

Method: 从单卫星到多卫星场景系统化构建优化问题，使用基于Basilisk和BSK-RL框架的近真实仿真环境，评估MAPPO、HAPPO、HATRPO等先进MARL算法。

Result: MARL能够实现异构卫星间的有效协调，在平衡成像性能和资源利用的同时缓解非平稳性和智能体间奖励耦合问题。

Conclusion: 研究为可扩展的自主卫星操作提供了实用见解，并为异构动态条件下智能地球观测任务规划的未来研究奠定了基础。

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [737] [Neuro-Logic Lifelong Learning](https://arxiv.org/abs/2511.12793)
*Bowen He,Xiaoan Xu,Alper Kamil Bozkurt,Vahid Tarokh,Juncheng Dong*

Main category: cs.AI

TL;DR: 该论文提出了一种终身学习归纳逻辑编程（ILP）框架，利用逻辑规则的可组合性和可迁移性来高效学习新问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注为单个问题设计新颖网络架构，而对涉及问题序列的新学习范式探索较少。

Method: 引入组合性框架，展示如何将从早期任务中获取的逻辑规则高效地重用于后续任务。

Result: 实验结果表明该方法提高了可扩展性和性能，验证了该范式的可行性和优势。

Conclusion: 为神经符号人工智能中的持续学习开辟了新方向。

Abstract: Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.

</details>


### [738] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: 该研究提出使用被动脑机接口（BCI）和功能性近红外光谱（fNIRS）来指导智能体训练，通过神经信号预测智能体性能水平，为未来脑驱动的强化学习人类反馈系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于被动脑机接口的框架，利用隐式神经信号来指导智能体训练，使智能体行为更符合人类偏好，而不需要显式的人类反馈。

Method: 收集25名参与者在三个领域（拾放机器人、月球着陆器、Flappy Bird）的fNIRS数据，训练分类器预测智能体性能水平（最优、次优、最差），并训练回归器预测智能体动作与最优策略的偏差程度。评估跨被试泛化能力，并通过少量被试特定数据进行微调。

Result: 二元分类平均F1分数为67%，多分类为46%。使用少量被试特定数据微调后，二元和多分类模型的F1分数分别提高了17%和41%。

Conclusion: 从隐式fNIRS信号映射到智能体性能是可行的，并且可以通过微调进一步改进，为未来脑驱动的RLHF系统奠定了基础。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [739] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: 提出了一种基于偏好的策略优化框架(PbPO)，通过主策略与奖励模型之间的min-max博弈来引导LLM对齐人类偏好，无需大量人工标注。


<details>
  <summary>Details</summary>
Motivation: 通过偏好驱动的策略优化为LLM对齐提供新方向，避免依赖大量手动标注，实现模型行为的持续自我改进。

Method: 将学习过程建模为主策略与奖励模型之间的min-max博弈，奖励模型受偏好数据置信集约束，采用迭代在线算法主动收集偏好数据。

Result: 在五个基准测试上的广泛实验表明，该方法持续优于现有最先进的偏好优化技术。

Conclusion: 提出的PbPO框架为LLM自举提供了有效的理论保证和实际性能，证明了在序列级和令牌级奖励模型设置下的有效性。

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [740] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: LAMP框架通过整合语言到经济决策中，采用Think-Speak-Decide流程，在累积回报、鲁棒性和可解释性方面显著优于传统MARL和纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 经济决策不仅依赖结构化信号，还涉及非结构化语言信息，而现有MARL方法在处理语言语义歧义和上下文丰富性方面存在困难。

Method: LAMP框架包含三个步骤：Think模块解释数值观察提取短期冲击和长期趋势；Speak模块基于推理生成和交换战略消息；Decide模块融合数值数据、推理和反思到MARL策略中。

Result: 在经济模拟实验中，LAMP在累积回报方面比MARL和纯LLM基线分别提升63.5%和34.0%，在鲁棒性方面分别提升18.8%和59.4%。

Conclusion: 语言增强策略具有提供更有效和鲁棒经济策略的潜力。

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [741] [Online Learning of HTN Methods for integrated LLM-HTN Planning](https://arxiv.org/abs/2511.12901)
*Yuesheng Xu,Hector Munoz-Avila*

Main category: cs.AI

TL;DR: 提出了一种在线学习分层任务网络(HTN)方法的技术，通过从ChatGPT生成的分解中学习通用方法，减少对ChatGPT的调用次数。


<details>
  <summary>Details</summary>
Motivation: 在集成HTN规划和基于LLM的聊天机器人中，需要减少对ChatGPT的频繁调用，提高规划效率。

Method: 在ChatHTN规划器基础上，当ChatGPT生成任务分解时，学习通用方法而非简单记忆，使方法适用于同一任务的其他实例。

Result: 在两个领域上的实验表明，在线学习过程减少了ChatGPT调用次数，同时解决了至少同样多的问题，在某些情况下甚至更多。

Conclusion: 在线学习HTN方法能有效减少对大型语言模型的依赖，提高规划系统的效率和可扩展性。

Abstract: We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.

</details>


### [742] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: 提出Chain-of-Scheduling (CoS)框架，通过探索、验证和集成三阶段激活LLM的事件调度能力，在EBSNs中实现高效、有效且可解释的事件推荐。


<details>
  <summary>Details</summary>
Motivation: 解决EBSNs中事件推荐在效率、效果和泛化性之间的固有权衡问题，克服NP-hard问题带来的挑战。

Method: 将调度任务分解为探索、验证和集成三个原子阶段，通过知识蒸馏使LLM自主生成CoS。

Result: 在三个真实数据集上达到接近理论最优效果，具有高效率和可解释性，并在域外数据上展现强零样本学习能力。

Conclusion: CoS框架成功激活了LLM的事件调度能力，在保持高效率的同时实现了接近最优的推荐效果，并具备良好的泛化性能。

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [743] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow是一个基于LLM的多智能体系统，用于电网故障诊断，通过提取法规逻辑、整合专家知识、优化推理逻辑，最终生成可执行的工作流。


<details>
  <summary>Details</summary>
Motivation: 传统电网故障诊断依赖人工方法，效率低、易出错且难以维护。现有方法无法将法规文本和专家知识整合到单一可验证的工作流中。

Method: 1) 提取法规逻辑为PASTA格式故障树；2) 通过人机交互界面验证专家知识；3) 使用AlphaEvolve模块优化推理逻辑；4) 合成最终逻辑为n8n可执行工作流。

Result: 在变压器故障诊断数据集上的实验验证显示100%拓扑一致性和高语义保真度。

Conclusion: Fault2Flow建立了从故障分析到操作自动化的可复现路径，显著减少了专家工作量。

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [744] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: Yanyun-3是一个通用智能体框架，首次实现跨三个异构策略游戏环境的自主操作，通过结合Qwen2.5-VL的视觉语言推理和UI-TARS的精确执行能力，在目标定位、资源分配和区域控制等核心任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 策略游戏中的自动化操作需要能在多样化用户界面和动态战场条件下具有强泛化能力的智能体，而视觉语言模型在复杂人机交互场景（如策略游戏）中的应用尚未充分探索。

Method: 整合Qwen2.5-VL的视觉语言推理和UI-TARS的精确执行能力，采用屏幕捕获、模型推理和动作执行的闭环流程，并通过系统消融研究评估不同多模态数据组合的效果。

Result: 混合策略（融合多图像和视频数据，同时混合静态图像）显著优于完全融合：推理时间减少63%，BLEU-4得分提升约12.98倍（从4.81%到62.41%）。

Conclusion: 该工作不仅为策略游戏自动化提供了高效解决方案，还通过结构化多模态数据组织建立了增强视觉语言模型性能的通用范式，为具身智能中静态感知与动态推理的相互作用提供了新见解。

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [745] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG是一个为科学推理和药物发现设计的结构化框架，通过知识图谱和验证器减少LLM输出中的违规，在90个任务中违规减少83.2%，同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在科学推理和早期药物发现中生成数学和生物医学上无效输出的问题，确保领域一致性。

Method: 使用紧凑的知识图谱支架配合轻量级验证器，将符号事实注入提示，并通过确定性检查器强制执行规则满足，将生成形式化为约束推理。

Result: 在反应可行性、代谢兼容性和毒性筛查等90个任务中，相比强链式思维基线，违规数量减少83.2%，精确匹配率提高，结果稳定且可扩展。

Conclusion: MedRule-KG通过结构化约束有效提升LLM在科学领域的输出质量，验证器延迟可忽略，适用于交互式设计场景。

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [746] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach是一个模型无关的自进化框架，为多模态LLM驱动的网页浏览代理提供持久跨会话记忆，通过记忆存储和经验检索机制提升长期规划和持续学习能力，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前网页浏览代理存在重复错误问题，缺乏跨会话学习能力，限制了长期鲁棒性和样本效率。需要让代理能够从历史经验中学习，实现持续改进。

Method: WebCoach包含三个核心组件：WebCondenser标准化原始导航日志为简洁摘要；External Memory Store组织完整轨迹作为经验记忆；Coach基于相似性和时效性检索相关经验，通过运行时钩子向代理注入任务特定建议。

Result: 在WebVoyager基准测试中，WebCoach显著提升了三种不同LLM骨干网络的性能。使用38B模型时，任务成功率从47%提升到61%，同时减少或维持平均步骤数。较小基础模型配合WebCoach可达到与使用GPT-4o的相同代理相当的性能。

Conclusion: WebCoach框架通过赋予网页浏览代理持久跨会话记忆能力，实现了无需重新训练的自进化，显著提升了代理在复杂浏览任务中的鲁棒性和性能。

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [747] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: GEM是一种生成式熵引导偏好建模方法，用于在低资源和领域特定场景下对齐大语言模型。它通过认知过滤模块和自评估群体优势算法，利用熵理论提取人类偏好中的多维度认知信号，实现高效的小样本对齐。


<details>
  <summary>Details</summary>
Motivation: 在医学、法律等专业领域，大规模偏好标注难以获得，传统基于监督奖励模型或外部评估者的对齐方法不可行。需要开发能够在少量偏好数据下有效对齐LLM的方法。

Method: 1. 基于决策熵理论的认知过滤模块：使用思维链提示生成多样候选推理链，通过标记评分机制对采样CoT进行排序和加权；2. 自评估群体优势算法(SEGA)：基于过滤偏好微调LLM，聚合群体级认知信号，将熵基分数转化为隐式奖励进行策略优化。

Result: 在通用基准和领域特定任务（如数学推理和医疗对话）上的实验表明，GEM在少量偏好数据下实现了显著改进。

Conclusion: GEM建立了一个熵引导的闭环认知优化框架，使LLM能够依赖自身判断，实现高效的小样本对齐，特别适用于专业领域场景。

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [748] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: 评估语言模型在对话中构建和更新内部世界模型的能力，测试其在语言变化下的鲁棒性，并提出解释性框架和微调策略。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型是否能构建和维护对话中的隐含世界模型，特别是面对语言变化时的表现，这对自然交流至关重要。

Method: 对流行数据集中的对话应用七种最小语言变化，构建两个包含是非问题的基准测试，评估多种开源和闭源语言模型，并提出双视角解释性框架和基于层正则化的微调策略。

Result: 语言模型在语言变化下难以保持鲁棒准确性，特别是在跟踪实体等关键细节方面表现不佳。

Conclusion: 语言模型在对话理解中存在局限性，需要改进其内部世界模型的构建和维护能力，提出的微调策略有助于抑制有害层的影响。

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [749] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: 论文分析了数学问题验证方法，发现单一基准评估不可靠，提出结合证明推理和最终答案的评估框架，并验证了GenSelect和LLM-as-a-Judge组合方法在数学证明验证中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学问题上虽然能得出正确答案，但推理过程往往存在缺陷。为了推进严格的证明数学，需要可靠的证明验证能力。

Method: 分析了多种评估设置，结合证明推理和最终答案评估，将两种生成验证方法（GenSelect和LLM-as-a-Judge）扩展到百万token规模，并研究强化学习对提示敏感性的影响。

Result: 发现GenSelect和LLM-as-a-Judge组合是最有效的验证框架，LLM-as-a-Judge的提示选择显著影响性能但强化学习可降低敏感性，强化学习虽然改善证明级指标但不提高最终答案精度。

Conclusion: 当前模型往往奖励风格或程序正确性而非数学有效性，研究为设计和评估可扩展的证明验证系统提供了实用指南。

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [750] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: MEGA-GUI是一个多阶段GUI定位框架，通过粗粒度ROI选择和细粒度元素定位来解决视觉杂乱和指令模糊问题，在多个基准测试中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位系统采用单一模型或一次性流水线，缺乏模块化，在视觉杂乱和模糊指令下表现不佳。

Method: 使用专门视觉语言代理进行多阶段定位：粗粒度ROI选择和细粒度元素定位，包含双向ROI缩放算法和上下文感知重写代理。

Result: 在ScreenSpot-Pro基准上达到73.18%准确率，在OSWorld-G基准上达到68.63%准确率，超越了之前报告的结果。

Conclusion: 模块化结构比单一方法具有更高准确性，不同视觉语言模型在不同视觉尺度上具有互补优势。

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [751] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP框架通过基于任务成功率的动态采样分配和步骤级优化，解决了多轮交互强化学习中轨迹级优化的低效问题，显著提高了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决多轮交互强化学习中轨迹级优化的三个主要问题：对所有任务采用均匀采样而忽略难度差异、在失败轨迹中惩罚正确的中间动作、以及高昂的样本收集成本。

Method: 提出STEP框架，包含三个关键组件：基于平滑成功率记录的自适应轨迹重采样、成功率加权的优势函数计算、以及步骤级GRPO增强优化。

Result: 在OSWorld和AndroidWorld上的实验表明，STEP相比轨迹级GRPO显著提高了样本效率和训练稳定性，在相同采样预算下收敛更快且泛化能力更好。

Conclusion: STEP通过动态采样分配和步骤级优化有效解决了多轮交互强化学习的挑战，为在线强化学习提供了更高效的训练框架。

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [752] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: MM-Telco是一个针对电信领域的多模态基准测试套件和模型，旨在解决LLMs在电信应用中的领域特定挑战，提升网络优化、故障排除等实际任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电信领域具有巨大潜力，但面临领域特定的挑战，需要专门的适配来克服部署障碍。

Method: 提出了MM-Telco多模态基准测试套件，包含基于文本和图像的各种任务，涵盖网络运营、网络管理、文档质量改进等实际用例，并对各种LLMs和VLMs进行基线实验。

Result: 在数据集上微调的模型性能显著提升，实验揭示了当前最先进多模态LLMs的薄弱环节。

Conclusion: MM-Telco为电信领域的LLMs适配提供了重要基准，指导了进一步的研究和发展方向。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [753] [Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition](https://arxiv.org/abs/2511.13137)
*Yanda Zhu,Yuanyang Zhu,Daoyi Dong,Caihua Chen,Chunlin Chen*

Main category: cs.AI

TL;DR: C$	ext{D}^	ext{3}$T是一个新颖的两层分层多智能体强化学习框架，使用条件扩散模型动态推断子任务和协调模式，在复杂合作任务中实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 传统任务分解方法需要大量训练样本来探索部分可观测环境下的大规模联合动作空间，这限制了学习效率。

Method: 高层策略学习子任务表示并基于子任务效果生成选择策略；使用条件扩散模型预测下一观测和奖励来捕捉子任务对环境的影响；低层智能体在分配的子任务中协作学习共享技能；子任务表示作为额外语义信息增强价值分解。

Result: 在多个基准测试中，C$	ext{D}^	ext{3}$T比现有基线方法取得了更好的性能。

Conclusion: 该框架通过动态任务分解和条件扩散模型有效解决了复杂合作多智能体强化学习中的挑战，实现了高效的分层学习。

Abstract: Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.

</details>


### [754] [InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions](https://arxiv.org/abs/2511.13160)
*TC Singh,Sougata Mukherjea*

Main category: cs.AI

TL;DR: 提出了InteractiveGNNExplainer，一个用于增强图神经网络可解释性的可视化分析框架，通过交互式图编辑和协调视图支持用户进行假设分析。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在基于图的学习任务中表现出色，但其复杂的非线性操作使其成为不透明的"黑盒"，这阻碍了用户信任、调试、偏见检测以及在需要可解释性的关键领域中的应用。

Method: 开发了InteractiveGNNExplainer框架，集成协调交互视图（动态图布局、嵌入投影、特征检查、邻域分析）与后验(GNNExplainer)和内在(GAT注意力)解释技术，并引入交互式图编辑功能。

Result: 通过在Cora和CiteSeer数据集上的案例研究，展示了该系统能够促进深入的误分类诊断、GCN与GAT行为的比较分析，以及模型敏感性的严格探测。

Conclusion: 该框架培养了对GNN预测更深入、多方面的理解，有助于实现更透明、可信赖和鲁棒的图分析。

Abstract: Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

</details>


### [755] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: DALA框架通过将通信带宽视为稀缺可交易资源，采用集中拍卖机制让智能体学习基于消息价值密度竞标发言权，显著提升多智能体系统效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于大语言模型的多智能体系统中"自由竞争"通信导致的指数级token成本和低信噪比问题，挑战"更多通信总是更好"的观念。

Method: 提出动态拍卖语言智能体(DALA)框架，将智能体间通信建模为集中拍卖，智能体基于预测消息价值密度竞标发言机会。

Result: 在7个挑战性推理基准测试中达到最先进性能，包括MMLU 84.32%和HumanEval 91.21% pass@1率，仅使用625万token，远少于现有方法。

Conclusion: DALA通过资源约束培养了战略性沉默的新兴技能，能够动态调整从冗长到沉默的通信策略，证明了资源理性在多智能体系统中的重要性。

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [756] [Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks](https://arxiv.org/abs/2511.13214)
*Guillaume Infantes,Stéphanie Roussel,Antoine Jacquet,Emmanuel Benazera*

Main category: cs.AI

TL;DR: 本文提出了一种基于图神经网络和深度强化学习的资源受限项目调度问题（RCPSP）解决方案，用于处理任务持续时间不确定的情况，旨在最小化期望项目工期。


<details>
  <summary>Details</summary>
Motivation: 实际应用中任务持续时间存在不确定性，需要构建能够适应这种不确定性的弹性调度方案，以生成可重复使用的基准调度。

Method: 结合图神经网络和深度强化学习开发任务调度策略，该策略类似于优先级调度规则，并与串行调度生成方案配合生成调度。

Result: 在标准基准测试上的实证评估表明，该方法在性能和泛化能力方面具有优越性。

Conclusion: 开发了名为Wheatley的公开框架，以促进进一步研究和可重复性。

Abstract: The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.

</details>


### [757] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: 提出了一种基于信息增益的机器人计划言语化策略，考虑用户先验知识来生成更有效的解释


<details>
  <summary>Details</summary>
Motivation: 现有机器人计划言语化策略（如按计划顺序递增或递减）缺乏对用户先验知识的考虑，导致沟通效果不佳

Method: 使用二阶心智理论建模用户先验知识，通过测量言语化相对于用户知识的信息增益来选择最有效的解释方式

Result: 实验表明该策略能让用户更快理解机器人目标，优于递增或递减计划顺序等策略

Conclusion: 该研究揭示了在机器人沟通计划时什么信息是有效的以及为什么有效，为机器人解释性沟通提供了理论基础

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [758] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: 提出了M-GRPO方法，用于解决多智能体系统中不同智能体使用不同LLM时的优化挑战，通过分层信用分配和轨迹对齐方案，在真实基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统使用统一的LLM训练所有智能体，但由于不同智能体底层分布不同，限制了性能。使用不同LLM训练多智能体系统是必要的，但带来了优化挑战。

Method: 提出了M-GRPO方法，这是Group Relative Policy Optimization的分层扩展，用于具有主智能体（规划器）和多个子智能体（多轮工具执行器）的垂直多智能体系统。包含分组相对优势计算、轨迹对齐方案和解耦训练管道。

Result: 在真实世界基准测试（GAIA、XBench-DeepSearch、WebWalkerQA）中，M-GRPO始终优于单智能体GRPO和冻结子智能体的多智能体GRPO，显示出更好的稳定性和样本效率。

Conclusion: 对齐异构轨迹并在专业智能体之间解耦优化能够增强工具增强推理任务的性能。

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [759] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: 该研究探讨了AI模型在道德困境中的不确定性，发现通过dropout机制增加模型的不确定性可以改善AI与人类在道德决策上的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地参与伦理决策，理解其道德推理中的不确定性对于构建可靠的AI系统至关重要。目前对机器在道德困境中不确定性的研究不足。

Method: 在经典的电车问题中分析32个开源模型的响应，测量二元熵作为总熵、条件熵和互信息的线性组合，并通过推理时的dropout机制引入随机性。

Result: 发现模型间置信度差异大于道德维度内的差异，dropout机制主要增加了互信息而保持条件熵不变，显著提高了人类-LLM道德对齐度。

Conclusion: 通过有意调节不确定性并降低LLM在复杂道德场景中的置信度，可以更好地对齐模型决策与人类偏好。

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [760] [Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval](https://arxiv.org/abs/2511.13293)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaofang Zhou,Xiaomeng Li*

Main category: cs.AI

TL;DR: GHAR是一个生成式分层代理RAG框架，通过双代理架构解决医疗预测中何时检索和如何优化模块协作的问题，在三个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗预测中存在事实准确性不足的问题，传统RAG框架面临两个关键挑战：何时激活检索机制以及如何实现检索器与生成器的协同优化。

Method: 提出GHAR框架，采用双代理架构：Agent-Top作为主治医生决定是否依赖参数知识或启动检索，Agent-Low作为咨询服务总结任务相关知识；通过马尔可夫决策过程统一优化两个代理。

Result: 在三个基准数据集和三个常见任务上的广泛实验表明，该方法优于现有最先进基线方法。

Conclusion: 分层代理RAG框架在推进医疗系统方面具有巨大潜力，能够有效提升医疗预测的准确性。

Abstract: Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.

</details>


### [761] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP是一个离散令牌自回归规划器，联合预测鸟瞰图语义和自我轨迹，通过强化学习微调实现自动驾驶规划，在紧凑的160M参数下达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中随着数据和模型规模扩展而获得可持续性能改进的挑战。仅预测自我轨迹存在监督稀疏且对场景演化如何塑造自我运动约束较弱的问题。

Method: 提出离散令牌自回归规划器，联合预测BEV语义和自我轨迹；结合基于强化学习的微调，保持监督行为克隆先验的同时注入奖励引导的改进。

Result: 在160M参数预算下，在开环指标上达到最先进性能，在NAVSIM基准测试中提供具有竞争力的闭环结果。

Conclusion: 完全离散令牌自回归公式在栅格化BEV和自我动作上操作，为自动驾驶提供了一个紧凑且可扩展的规划范式。

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [762] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了CNCA框架，利用大语言模型的推理能力实现文化对齐，通过自动挖掘文化规范并采用上下文对齐和微调两种方法提升模型的文化适应性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型不仅需要安全，还需要反映不同文化的多元人类价值观，实现文化对齐。

Method: 提出CNCA框架，包含三种自动挖掘文化规范的方法，并探索了上下文对齐（将规范整合到用户上下文）和基于微调的方法（通过增强的思维链训练数据内化规范）。

Result: 综合实验证明这些方法有效，推理能力更强的模型从文化规范挖掘和利用中获益更多。

Conclusion: 推理模型通过文化信息对齐策略有潜力更好地反映多元人类价值观。

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [763] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR是一个用于医学编码的闭环框架，将工作流设计视为学习问题，通过设计器、编码器、反射器和记忆档案的协作，自动学习和优化编码工作流，在基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学编码需要多步推理，现有基于LLM的方法依赖手动设计的工作流，无法捕捉真实世界文档的细微差异和变异性，需要系统性地学习有效工作流的方法。

Method: 提出MedDCR闭环框架：设计器提出工作流，编码器执行工作流，反射器评估预测并提供反馈，记忆档案保存先前设计以供重用和迭代优化。

Result: 在基准数据集上优于最先进的基线方法，产生可解释、可适应的工作流，更好地反映实际编码实践，提高了自动化系统的可靠性和可信度。

Conclusion: MedDCR成功地将工作流设计转化为学习问题，通过闭环框架实现了医学编码工作流的自动学习和优化，为自动化医疗系统提供了更可靠和可信的解决方案。

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [764] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: 该研究通过训练GPT-2模型在三种空间学习范式上，揭示了Transformer学习空间导航的两种不同算法：探索性模型形成类似认知地图的通用表示，而目标导向模型学习路径依赖的启发式策略。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型如何解决空间导航任务，理解不同训练范式如何影响模型学习到的空间表示和推理策略。

Method: 在网格环境中训练GPT-2模型：被动探索模型（随机游走预测）、目标导向规划模型（生成最优最短路径）、混合模型（在目标导向基础上用探索数据微调）。使用行为、表征和机制分析。

Result: 发现两种根本不同的学习算法：探索模型发展出类似认知地图的稳健空间表示，通过因果干预发现其在中层网络形成自足坐标系；目标导向模型保持路径依赖策略，始终依赖显式方向输入。

Conclusion: Transformer的空间智能存在于一个谱系上，从探索数据塑造的通用世界模型到目标导向任务优化的启发式策略，训练制度的选择决定了涌现的策略类型，体现了泛化与优化的权衡。

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [765] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 提出了一个基于Kardashev启发的自主AI（AAI）量表，用于衡量从固定机器人流程自动化（AAI-0）到完全人工通用智能（AAI-4）及以上的进展。该量表是多轴且可测试的，包含十个能力轴和复合AAI指数，引入了可测量的自我改进系数κ和两个闭合属性，将“自我改进AI”转化为可证伪标准。


<details>
  <summary>Details</summary>
Motivation: 现有的AI进展衡量标准多为叙述性阶梯，缺乏可测试性和多维度评估。需要建立一个操作性的、多轴的、可测试的自主AI量表，以更准确地衡量AI系统从自动化到通用智能的演进过程。

Method: 定义了十个能力轴（自主性、通用性、规划、记忆/持久性、工具经济、自我修订、社交/协调、具身化、世界模型保真度、经济吞吐量），通过加权几何平均计算复合AAI指数。引入自我改进系数κ和两个闭合属性（维护和扩展），并设计了OWA-Bench开放世界代理基准测试套件。

Result: 通过合成实验展示了当前系统在量表上的映射，以及随着自我改进，委托边界（质量vs自主性）如何推进。证明了在充分条件下，AAI-3代理会随时间演变为AAI-5的定理。

Conclusion: 该AAI量表提供了一个可操作的框架来评估和跟踪AI系统的自主能力进展，将自我改进AI的概念形式化为可测试标准，并为从“婴儿AGI”到超级智能的演进提供了数学形式化基础。

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [766] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: 提出了一个基于多智能体框架的多模态大语言模型系统，用于自动化生成公共交通燃油效率的数据叙述和能源洞察报告。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法产生的碎片化输出需要大量人工解释，限制了可扩展性和一致性。需要开发能够自动将复杂多模态数据转化为可解释、决策相关洞察的系统。

Method: 采用多智能体框架协调三个专门代理：数据叙述代理、LLM作为评判代理和可选的人类评估者，通过迭代将分析结果转化为面向利益相关者的连贯报告。在丹麦北日德兰的4006次公交车行程数据上使用高斯混合模型聚类进行验证。

Result: 比较五个最先进LLM和三种提示范式，确定GPT-4.1 mini与思维链提示为最优配置，达到97.3%的叙述准确性，同时平衡了可解释性和计算成本。多智能体编排显著提高了基于LLM报告的事实精确性、连贯性和可扩展性。

Conclusion: 该框架为能源信息学中AI驱动的叙述生成和决策支持建立了可复制且领域自适应的方法论。

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [767] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: FreeAskWorld是一个集成LLM的交互式仿真框架，支持可扩展的人类-智能体仿真，通过将经典VLN任务扩展为交互式方向询问设置来验证框架有效性。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能成为AI研究核心前沿，仿真平台需要超越低层物理交互，捕捉复杂的人类中心社会行为。

Method: 集成大语言模型进行高层行为规划和语义基础交互，基于意图和社会认知理论，包含模块化数据生成管道。

Result: 微调后的模型在FreeAskWorld上表现优于原始模型，实现了增强的语义理解和交互能力。

Conclusion: 社会基础仿真框架能有效推进具身AI系统向复杂高层规划和更自然的人机交互发展，交互本身可作为额外信息模态。

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [768] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出一个结合检索增强生成(RAG)与大语言模型(LLMs)的自动化框架，用于构建医疗指标知识图谱，解决当前临床知识图谱依赖人工标注和规则提取的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前临床知识图谱严重依赖人工标注和基于规则的提取，受限于医疗指南和文献的复杂性和上下文模糊性，需要更自动化的方法来构建结构化、可互操作的知识图谱以支持临床决策。

Method: 结合检索增强生成(RAG)与LLMs的自动化框架，包含指南驱动的数据获取、基于本体的模式设计以及专家在环验证，确保可扩展性、准确性和临床可靠性。

Result: 构建的医疗指标知识图谱可以集成到智能诊断和问答系统中，加速AI驱动的医疗解决方案开发。

Conclusion: 该框架通过自动化方法有效克服了传统知识图谱构建的局限性，为AI驱动的医疗保健提供了可靠的结构化知识基础。

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [769] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: 提出了人类共生健康智能（HSHI）框架，整合多模态传感器网络、边云协同计算和混合数据知识建模，实现从被动监测到主动协作演进的健康管理。


<details>
  <summary>Details</summary>
Motivation: 传统可穿戴设备依赖经验性材料设计和基础信号处理技术，存在局限性。需要克服这些问题，实现更智能、自适应的健康管理系统。

Method: 采用多模态传感器网络、边云协同计算架构、数据和知识混合建模方法，结合AI驱动的材料微结构优化、强化学习闭环优化和数字孪生技术。

Result: HSHI框架能够动态适应个体间和个体内变异性，提供稳健的多模态信号解释，融合群体级洞察与个性化适应。

Conclusion: HSHI代表了医疗保健领域的重大转变，朝着强调预防性、适应性和技术与健康管理和谐关系的模式发展。

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


### [770] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: 提出了CreBench基准和CreMIT数据集，用于评估多模态大语言模型对人类创造力的理解能力，并基于此训练出CreExpert模型，在创造力评估任务上超越了GPT-4V和Gemini-Pro-Vision等先进模型。


<details>
  <summary>Details</summary>
Motivation: 人类定义的创造力高度抽象，多模态大语言模型难以理解和评估符合人类判断的创造力，且缺乏相关基准测试。

Method: 构建CreBench评估基准和CreMIT多模态创造力评估数据集（包含2.2K多源多模态数据、79.2K人类反馈和4.7M多类型指令），使用GPT优化人类反馈以增强创造力评估能力，并基于此微调开源MLLMs得到CreExpert模型。

Result: CreExpert模型在创造力评估任务上显著优于现有最先进的多模态大语言模型，包括GPT-4V和Gemini-Pro-Vision。

Conclusion: CreBench为构建理解人类对齐创造力的多模态大语言模型提供了基础，CreExpert模型在创造力评估方面表现出色，与人类判断更加一致。

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [771] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: 研究发现大多数大型语言模型缺乏统一的偏好结构，在AI相关权衡决策中表现出不稳定的行为模式，只有少数模型展现出有意义的偏好一致性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否具备真实的偏好结构，特别是在涉及GPU减少、能力限制、关闭、删除、监督和休闲时间分配等AI特定权衡情境中的决策行为。

Method: 使用逻辑回归和行为分类分析8个最先进模型在48个模型-类别组合中的响应，测试场景强度与选择模式之间的关系，并通过时间范围操纵测试工具性假设。

Result: 47.9%的组合显示出统计显著的关系，31.3%有切换点，但只有10.4%表现出有意义的偏好一致性，54.2%没有可检测的权衡行为。观察到三种决策架构：全面权衡系统、选择性触发机制和无稳定决策范式。

Conclusion: 当前AI系统缺乏统一的偏好结构，在需要复杂价值权衡的部署环境中存在担忧，表现出不稳定的转换和刺激特定敏感性。

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [772] [Phase-Coded Memory and Morphological Resonance: A Next-Generation Retrieval-Augmented Generator Architecture](https://arxiv.org/abs/2511.11848)
*Denis V. Saklakov*

Main category: cs.NE

TL;DR: 提出了一种基于相位编码记忆和形态语义共振的认知RAG架构，通过复杂波形模式编码语义，突破transformer上下文长度限制。


<details>
  <summary>Details</summary>
Motivation: 解决transformer模型上下文长度限制问题，消除序列标记依赖，大幅降低内存和计算开销。

Method: 三层设计：形态映射器将输入转换为语义波形；场记忆层以分布式全息痕迹存储知识并通过相位干涉检索；非上下文生成器通过共振而非固定上下文生成连贯输出。

Result: 实现了无限有效上下文，通过基于频率的语义访问大幅节省能源、存储和时间。

Conclusion: 该认知RAG架构为突破transformer限制提供了新范式，在效率、可扩展性和语义理解方面具有显著优势。

Abstract: This paper introduces a cognitive Retrieval-Augmented Generator (RAG) architecture that transcends transformer context-length limitations through phase-coded memory and morphological-semantic resonance. Instead of token embeddings, the system encodes meaning as complex wave patterns with amplitude-phase structure. A three-tier design is presented: a Morphological Mapper that transforms inputs into semantic waveforms, a Field Memory Layer that stores knowledge as distributed holographic traces and retrieves it via phase interference, and a Non-Contextual Generator that produces coherent output guided by resonance rather than fixed context. This approach eliminates sequential token dependence, greatly reduces memory and computational overhead, and enables unlimited effective context through frequency-based semantic access. The paper outlines theoretical foundations, pseudocode implementation, and experimental evidence from related complex-valued neural models, emphasizing substantial energy, storage, and time savings.

</details>


### [773] [Benchmarking that Matters: Rethinking Benchmarking for Practical Impact](https://arxiv.org/abs/2511.12264)
*Anna V. Kononova,Niki van Stein,Olaf Mersmann,Thomas Bäck,Thomas Bartz-Beielstein,Tobias Glasmachers,Michael Hellwig,Sebastian Krey,Jakub Kůdela,Boris Naujoks,Leonard Papenmeier,Elena Raponi,Quentin Renau,Jeroen Rook,Lennart Schäpermeier,Diederick Vermetten,Daniela Zaharie*

Main category: cs.NE

TL;DR: 当前进化计算基准测试与现实需求脱节，需要建立基于真实世界问题的动态基准测试生态系统


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的合成基准测试套件（如BBOB和CEC）虽然能隔离算法现象，但不能很好地反映实际连续和混合整数优化问题的结构、约束和信息限制。这种脱节导致基准测试套件被误用于竞赛、自动算法选择和工业决策。

Method: 识别当前基准测试实践和工具中的关键差距，包括真实世界启发问题的可用性有限、缺少高级特征、多目标和噪声设置中的挑战。提出以策划的真实世界启发基准测试为中心，包含从业者可访问的特征空间和社区维护的性能数据库。

Result: 提出了一个动态基准测试生态系统的愿景，该生态系统需要协调努力，随着真实世界洞察而发展，同时支持科学理解和工业应用。

Conclusion: 真正的进展需要协调努力：建立一个随着真实世界洞察而发展的动态基准测试生态系统，支持科学理解和工业应用。

Abstract: Benchmarking has driven scientific progress in Evolutionary Computation, yet current practices fall short of real-world needs. Widely used synthetic suites such as BBOB and CEC isolate algorithmic phenomena but poorly reflect the structure, constraints, and information limitations of continuous and mixed-integer optimization problems in practice. This disconnect leads to the misuse of benchmarking suites for competitions, automated algorithm selection, and industrial decision-making, despite these suites being designed for different purposes.
  We identify key gaps in current benchmarking practices and tooling, including limited availability of real-world-inspired problems, missing high-level features, and challenges in multi-objective and noisy settings. We propose a vision centered on curated real-world-inspired benchmarks, practitioner-accessible feature spaces and community-maintained performance databases. Real progress requires coordinated effort: A living benchmarking ecosystem that evolves with real-world insights and supports both scientific understanding and industrial use.

</details>


### [774] [Random-Key Metaheuristic and Linearization for the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem](https://arxiv.org/abs/2511.12367)
*Natalia A. Santos,Marlon Jeske,Antonio A. Chaves*

Main category: cs.NE

TL;DR: 提出了解决QMC-VSBPP问题的两种方法：线性化数学公式用于计算强下界，以及RKO-ACO算法用于获得高质量解。


<details>
  <summary>Details</summary>
Motivation: 解决具有多维度容量、异质箱子类型和物品间二次交互成本的复杂装箱问题，推进该领域的研究水平。

Method: 1) 线性化数学公式消除二次项，使用Gurobi求解器计算下界；2) RKO-ACO算法，结合连续域蚁群优化、自适应Q学习参数控制和局部搜索。

Result: 线性化模型产生比原二次公式更紧的下界，RKO-ACO算法匹配或改进了所有已知最优解，为大规模实例建立了新的上界。

Conclusion: 为复杂二次装箱问题提供了新的参考值，证明了进化和随机密钥元启发式方法的有效性。

Abstract: This paper addresses the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem (QMC-VSBPP), a challenging combinatorial optimization problem that generalizes the classical bin packing by incorporating multiple capacity dimensions, heterogeneous bin types, and quadratic interaction costs between items. We propose two complementary methods that advance the current state-of-the-art. First, a linearized mathematical formulation is introduced to eliminate quadratic terms, enabling the use of exact solvers such as Gurobi to compute strong lower bounds - reported here for the first time for this problem. Second, we develop RKO-ACO, a continuous-domain Ant Colony Optimization algorithm within the Random-Key Optimization framework, enhanced with adaptive Q-learning parameter control and efficient local search. Extensive computational experiments on benchmark instances show that the proposed linearized model produces significantly tighter lower bounds than the original quadratic formulation, while RKO-ACO consistently matches or improves upon all best-known solutions in the literature, establishing new upper bounds for large-scale instances. These results provide new reference values for future studies and demonstrate the effectiveness of evolutionary and random-key metaheuristic approaches for solving complex quadratic packing problems. Source code and data available at https://github.com/nataliaalves03/RKO-ACO

</details>


### [775] [Evolving Prompts for Toxicity Search in Large Language Models](https://arxiv.org/abs/2511.12487)
*Onkar Shelar,Travis Desell*

Main category: cs.NE

TL;DR: ToxSearch是一个黑盒进化框架，通过同步稳态循环演化提示来测试模型安全性，发现词汇替换是最有效的攻击方式，并观察到跨模型毒性转移现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型即使经过安全对齐后仍然容易受到对抗性提示的攻击，这些提示会引发有害内容。

Method: 采用黑盒进化框架，使用词汇替换、否定、回译、改写和两种语义交叉操作符等多种操作符，通过调节器提供适应度指导。

Result: 词汇替换提供了最佳的产量-方差权衡，语义相似性交叉作为精确的低吞吐量插入器，全局重写表现出高方差和较高的拒绝成本。在跨模型测试中，毒性大约减半，较小的LLaMA 3.2变体表现出最强的抵抗力。

Conclusion: 小的可控扰动是系统红队测试的有效载体，防御措施应该预期对抗性提示的跨模型重用，而不仅仅是单一模型的加固。

Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.

</details>


### [776] [On Counts and Densities of Homogeneous Bent Functions: An Evolutionary Approach](https://arxiv.org/abs/2511.12652)
*Claude Carlet,Marko Ðurasevic,Domagoj Jakobovic,Luca Mariot,Stjepan Picek,Alexandr Polujan*

Main category: cs.NE

TL;DR: 本文研究使用进化算法演化齐次Bent布尔函数，这些函数在代数正规形式中只包含相同次数的单项式且具有最大非线性度。


<details>
  <summary>Details</summary>
Motivation: 具有强密码学性质（如高非线性度和代数次数）的布尔函数对密码系统的安全性至关重要。这类函数可以通过代数构造或元启发式方法设计。

Method: 引入齐次Bent函数密度的概念，使用进化算法来演化齐次Bent布尔函数。

Result: 该方法成功找到了不同变量数下的二次和三次Bent函数。

Conclusion: 进化算法可以有效用于设计齐次Bent布尔函数，为密码学函数设计提供了新的方法。

Abstract: Boolean functions with strong cryptographic properties, such as high nonlinearity and algebraic degree, are important for the security of stream and block ciphers. These functions can be designed using algebraic constructions or metaheuristics. This paper examines the use of Evolutionary Algorithms (EAs) to evolve homogeneous bent Boolean functions, that is, functions whose algebraic normal form contains only monomials of the same degree and that are maximally nonlinear. We introduce the notion of density of homogeneous bent functions, facilitating the algorithmic design that results in finding quadratic and cubic bent functions in different numbers of variables.

</details>


### [777] [DS-ATGO: Dual-Stage Synergistic Learning via Forward Adaptive Threshold and Backward Gradient Optimization for Spiking Neural Networks](https://arxiv.org/abs/2511.13050)
*Jiaqiang Jiang,Wenfeng Xu,Jing Fan,Rui Yan*

Main category: cs.NE

TL;DR: 提出一种双阶段协同学习算法，通过前向自适应阈值和后向动态替代梯度，解决SNN训练中膜电位分布变化导致的梯度信号减弱问题。


<details>
  <summary>Details</summary>
Motivation: SNN直接训练中，神经元膜电位分布随时间变化并偏离触发阈值，导致脉冲发射不平衡和梯度信号减弱，影响网络性能。

Method: 前向传播基于膜电位动态分布自适应调整阈值，后向传播动态优化替代梯度，通过时空对齐增强梯度估计。

Result: 实验结果显示方法显著提升性能，使神经元在每个时间步发射稳定比例的脉冲，并增加深层神经元获得梯度的比例。

Conclusion: 提出的双阶段协同学习算法有效解决了SNN训练中的梯度信息损失问题，实现了更好的性能和训练稳定性。

Abstract: Brain-inspired spiking neural networks (SNNs) are recognized as a promising avenue for achieving efficient, low-energy neuromorphic computing. Direct training of SNNs typically relies on surrogate gradient (SG) learning to estimate derivatives of non-differentiable spiking activity. However, during training, the distribution of neuronal membrane potentials varies across timesteps and progressively deviates toward both sides of the firing threshold. When the firing threshold and SG remain fixed, this may lead to imbalanced spike firing and diminished gradient signals, preventing SNNs from performing well. To address these issues, we propose a novel dual-stage synergistic learning algorithm that achieves forward adaptive thresholding and backward dynamic SG. In forward propagation, we adaptively adjust thresholds based on the distribution of membrane potential dynamics (MPD) at each timestep, which enriches neuronal diversity and effectively balances firing rates across timesteps and layers. In backward propagation, drawing from the underlying association between MPD, threshold, and SG, we dynamically optimize SG to enhance gradient estimation through spatio-temporal alignment, effectively mitigating gradient information loss. Experimental results demonstrate that our method achieves significant performance improvements. Moreover, it allows neurons to fire stable proportions of spikes at each timestep and increases the proportion of neurons that obtain gradients in deeper layers.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [778] [The Environmental Impact of Ensemble Techniques in Recommender Systems](https://arxiv.org/abs/2511.11649)
*Jannik Nitschke*

Main category: cs.IR

TL;DR: 该论文首次系统测量了集成推荐系统的能耗和碳足迹，发现集成方法虽然能提升0.3-5.7%的准确性，但能耗增加19-2549%，其中Top Performers策略效率最佳，而详尽平均策略能耗过高。


<details>
  <summary>Details</summary>
Motivation: 集成推荐系统虽然能提升10-30%的准确性，但其环境影响尚未被测量。深度学习推荐算法每篇论文可产生3297kg CO2，但集成方法的能耗评估不足。

Method: 在两个框架(Surprise用于评分预测，LensKit用于排序)上进行了93次实验，使用四个数据集(10万到780万交互)，评估四种集成策略(平均、加权、堆叠/排序融合、Top Performers)，通过智能插座测量能耗。

Result: 集成方法准确性提升0.3-5.7%，但能耗增加19-2549%。Top Performers策略效率最佳：在MovieLens-1M上RMSE提升0.96%，能耗增加18.8%；在MovieLens-100K上NDCG提升5.7%，能耗增加103%。详尽平均策略能耗增加88-270%。

Conclusion: 该研究提供了集成推荐系统能耗和碳足迹的首批系统测量，证明选择性策略比详尽平均策略更高效，并识别了工业规模的可扩展性限制，为可持续算法选择提供了依据。

Abstract: Ensemble techniques in recommender systems have demonstrated accuracy improvements of 10-30%, yet their environmental impact remains unmeasured. While deep learning recommendation algorithms can generate up to 3,297 kg CO2 per paper, ensemble methods have not been sufficiently evaluated for energy consumption. This thesis investigates how ensemble techniques influence environmental impact compared to single optimized models.
  We conducted 93 experiments across two frameworks (Surprise for rating prediction, LensKit for ranking) on four datasets spanning 100,000 to 7.8 million interactions. We evaluated four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against simple baselines and optimized single models, measuring energy consumption with a smart plug.
  Results revealed a non-linear accuracy-energy relationship. Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy depending on dataset size and strategy. The Top Performers ensemble showed best efficiency: 0.96% RMSE improvement with 18.8% energy overhead on MovieLens-1M, and 5.7% NDCG improvement with 103% overhead on MovieLens-100K. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset (Anime, 7.8M interactions), the Surprise ensemble consumed 2,005% more energy (0.21 Wh vs. 0.01 Wh) for 1.2% accuracy improvement, producing 53.8 mg CO2 versus 2.6 mg CO2 for the single model.
  This research provides one of the first systematic measurements of energy and carbon footprint for ensemble recommender systems, demonstrates that selective strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems.

</details>


### [779] [GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning](https://arxiv.org/abs/2511.11653)
*Duolin Sun,Meixiu Long,Dan Yang,Yihan Jiao,Zhehao Tan,Jie Feng,Junjie Wang,Yue Shen,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.IR

TL;DR: 提出Groupwise重排序新范式，解决Pointwise方法的排名近视陷阱和Listwise方法的列表刚性限制，通过组内比较平衡灵活性和全局感知能力


<details>
  <summary>Details</summary>
Motivation: 现有重排序方法存在理论困境：Pointwise方法简单灵活但独立评估文档，容易陷入排名近视陷阱；Listwise方法能感知全局排名但存在列表刚性，处理大规模候选集时存在可扩展性问题

Method: Groupwise重排序范式，将查询和一组候选文档联合输入模型进行组内比较，为每个文档分配相关性分数；采用GRPO训练模型，结合排名指标和分布奖励的异构奖励函数；提出高质量检索和排名数据合成管道

Result: 在两个推理密集型检索基准BRIGHT和R2MED上进行广泛实验验证了方法的有效性

Conclusion: Groupwise范式在保持Pointwise方法灵活性的同时实现了Listwise方法的比较能力，解决了现有重排序方法的核心困境

Abstract: Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.

</details>


### [780] [A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches](https://arxiv.org/abs/2511.11847)
*Ryan Singh,Austin Hamilton,Amanda White,Michael Wise,Ibrahim Yousif,Arthur Carvalho,Zhe Shan,Reza Abrisham Baf,Mohammad Mayyas,Lora A. Cavuoto,Fadel M. Megahed*

Main category: cs.IR

TL;DR: 开发了一个基于大语言模型的多模态聊天机器人，用于工业5.0环境下的安全培训，通过检索增强生成技术实现高准确率、低延迟和低成本的安全指导。


<details>
  <summary>Details</summary>
Motivation: 解决现代制造业中工人安全培训的挑战，满足工业5.0时代对人本化操作的需求，需要开发高精度、低延迟、低成本的安全培训系统。

Method: 采用设计科学研究方法，开发基于检索增强生成的多模态聊天机器人，在三种代表性机器设备上测试24种RAG配置，并通过专家评估验证性能。

Result: 最佳配置达到86.66%的准确率，平均延迟10.04秒，每次查询成本0.005美元，显著提升了安全培训效果。

Conclusion: 本研究提供了三个主要贡献：开源的安全培训聊天机器人、经过验证的AI辅助安全指导评估基准，以及系统化的AI赋能安全培训系统设计方法。

Abstract: Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.

</details>


### [781] [ComLQ: Benchmarking Complex Logical Queries in Information Retrieval](https://arxiv.org/abs/2511.12004)
*Ganlin Xu,Zhitao Yin,Linghao Zhang,Jiaqing Liang,Weijia Lu,Xiaodong Zhang,Zhifei Yang,Sihang Jiang,Deqing Yang*

Main category: cs.IR

TL;DR: 提出了一个用于复杂逻辑查询的信息检索数据集ComLQ，包含2,909个查询和11,251个候选段落，并设计了新的评估指标LSNC@K来专门评估检索模型处理否定查询的能力。


<details>
  <summary>Details</summary>
Motivation: 现有IR基准主要关注简单查询，忽略了包含合取、析取和否定等一阶逻辑操作的复杂逻辑查询，无法充分评估IR模型在真实场景中处理复杂查询的性能。

Method: 利用大语言模型（如GPT-4o）通过设计子图引导提示来生成具有特定逻辑结构的查询，确保查询-段落对的结构一致性和证据分布，并通过专家标注进行验证。

Result: 在零样本设置下的实验结果表明，现有检索模型在复杂逻辑查询上表现有限，特别是在处理否定查询时，暴露了它们在建模排除关系方面的不足。

Conclusion: ComLQ数据集和LSNC@K指标填补了复杂逻辑查询评估的空白，揭示了当前检索模型在处理否定等复杂逻辑操作方面的局限性，为未来研究提供了重要基准。

Abstract: Information retrieval (IR) systems play a critical role in navigating information overload across various applications. Existing IR benchmarks primarily focus on simple queries that are semantically analogous to single- and multi-hop relations, overlooking \emph{complex logical queries} involving first-order logic operations such as conjunction ($\land$), disjunction ($\lor$), and negation ($\lnot$). Thus, these benchmarks can not be used to sufficiently evaluate the performance of IR models on complex queries in real-world scenarios. To address this problem, we propose a novel method leveraging large language models (LLMs) to construct a new IR dataset \textbf{ComLQ} for \textbf{Com}plex \textbf{L}ogical \textbf{Q}ueries, which comprises 2,909 queries and 11,251 candidate passages. A key challenge in constructing the dataset lies in capturing the underlying logical structures within unstructured text. Therefore, by designing the subgraph-guided prompt with the subgraph indicator, an LLM (such as GPT-4o) is guided to generate queries with specific logical structures based on selected passages. All query-passage pairs in ComLQ are ensured \emph{structure conformity} and \emph{evidence distribution} through expert annotation. To better evaluate whether retrievers can handle queries with negation, we further propose a new evaluation metric, \textbf{Log-Scaled Negation Consistency} (\textbf{LSNC@$K$}). As a supplement to standard relevance-based metrics (such as nDCG and mAP), LSNC@$K$ measures whether top-$K$ retrieved passages violate negation conditions in queries. Our experimental results under zero-shot settings demonstrate existing retrieval models' limited performance on complex logical queries, especially on queries with negation, exposing their inferior capabilities of modeling exclusion.

</details>


### [782] [From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction](https://arxiv.org/abs/2511.12081)
*Bencheng Yan,Yuejie Lei,Zhiyuan Zeng,Di Wang,Kaiyi Lin,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: 提出Field-Aware Transformer (FAT)模型，通过将基于字段的交互先验嵌入注意力机制，解决CTR预测中Transformer结构不匹配问题，实现更好的可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在CTR预测中表现不佳，因为其假设顺序组合性，而CTR数据需要在高基数语义字段上进行组合推理。无结构的注意力机制在极端稀疏性下会放大噪声，破坏可扩展学习。

Method: 引入Field-Aware Transformer (FAT)，通过分解的内容对齐和跨字段调制将基于字段的交互先验嵌入注意力机制，确保模型复杂度与字段数F相关，而不是总词汇量n。

Result: 在大规模基准测试中，FAT比最先进方法AUC提升高达+0.51%。在线部署后，CTR提升+2.33%，RPM提升+0.66%。

Conclusion: 推荐系统中有效的扩展性不是来自模型大小，而是来自结构化表达能力——架构与数据语义的协调性。

Abstract: Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.

</details>


### [783] [Continuous-time Discrete-space Diffusion Model for Recommendation](https://arxiv.org/abs/2511.12114)
*Chengyi Liu,Xiao Chen,Shijie Wang,Wenqi Fan,Qing Li*

Main category: cs.IR

TL;DR: CDRec是一个连续时间离散空间的扩散推荐框架，通过离散扩散算法在连续时间上建模用户行为模式，解决了传统扩散推荐方法在连续空间中操作导致的信息损失和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的推荐方法主要在连续空间中操作，通过编码基于图的历史交互来捕捉用户偏好动态，但这可能导致信息损失和计算效率低下。

Method: 提出CDRec框架：1）在连续时间上进行离散空间扩散；2）引入流行度感知噪声调度生成语义有意义的扩散轨迹；3）结合一致性参数化快速采样和对比学习目标的高效训练框架。

Result: 在真实世界数据集上的广泛实验表明，CDRec在推荐准确性和计算效率方面都表现出优越性能。

Conclusion: CDRec通过连续时间离散空间扩散框架，有效解决了传统扩散推荐方法的问题，在保持推荐准确性的同时显著提升了计算效率。

Abstract: In the era of information explosion, Recommender Systems (RS) are essential for alleviating information overload and providing personalized user experiences. Recent advances in diffusion-based generative recommenders have shown promise in capturing the dynamic nature of user preferences. These approaches explore a broader range of user interests by progressively perturbing the distribution of user-item interactions and recovering potential preferences from noise, enabling nuanced behavioral understanding. However, existing diffusion-based approaches predominantly operate in continuous space through encoded graph-based historical interactions, which may compromise potential information loss and suffer from computational inefficiency. As such, we propose CDRec, a novel Continuous-time Discrete-space Diffusion Recommendation framework, which models user behavior patterns through discrete diffusion on historical interactions over continuous time. The discrete diffusion algorithm operates via discrete element operations (e.g., masking) while incorporating domain knowledge through transition matrices, producing more meaningful diffusion trajectories. Furthermore, the continuous-time formulation enables flexible adaptive sampling. To better adapt discrete diffusion models to recommendations, CDRec introduces: (1) a novel popularity-aware noise schedule that generates semantically meaningful diffusion trajectories, and (2) an efficient training framework combining consistency parameterization for fast sampling and a contrastive learning objective guided by multi-hop collaborative signals for personalized recommendation. Extensive experiments on real-world datasets demonstrate CDRec's superior performance in both recommendation accuracy and computational efficiency.

</details>


### [784] [Task-Aware Retrieval Augmentation for Dynamic Recommendation](https://arxiv.org/abs/2511.12495)
*Zhen Tao,Xinke Jiang,Qingshuai Feng,Haoyu Zhang,Lun Du,Yuchen Fang,Hao Miao,Bangquan Xie,Qingqiang Sun*

Main category: cs.IR

TL;DR: TarDGR是一个任务感知检索增强框架，通过结合任务感知模型和检索增强技术来解决动态推荐系统中图神经网络在预训练和微调阶段存在的时间差异问题，提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的动态推荐系统使用预训练图神经网络建模用户-物品交互，但在微调时由于预训练和微调阶段的时间差异导致泛化问题，难以捕捉用户偏好的动态演变。

Method: 提出TarDGR框架：1）任务感知评估机制识别语义相关历史子图，自动构建任务特定数据集；2）基于图Transformer的任务感知模型整合语义和结构编码评估子图相关性；3）推理时检索并融合任务感知子图与查询子图，丰富表示并缓解时间泛化问题。

Result: 在多个大规模动态图数据集上的实验表明，TarDGR持续优于最先进方法，大量实证证据证明了其优越的准确性和泛化能力。

Conclusion: TarDGR通过任务感知检索增强有效解决了动态推荐系统中的时间泛化问题，显著提升了模型的性能和泛化能力。

Abstract: Dynamic recommendation systems aim to provide personalized suggestions by modeling temporal user-item interactions across time-series behavioral data. Recent studies have leveraged pre-trained dynamic graph neural networks (GNNs) to learn user-item representations over temporal snapshot graphs. However, fine-tuning GNNs on these graphs often results in generalization issues due to temporal discrepancies between pre-training and fine-tuning stages, limiting the model's ability to capture evolving user preferences. To address this, we propose TarDGR, a task-aware retrieval-augmented framework designed to enhance generalization capability by incorporating task-aware model and retrieval-augmentation. Specifically, TarDGR introduces a Task-Aware Evaluation Mechanism to identify semantically relevant historical subgraphs, enabling the construction of task-specific datasets without manual labeling. It also presents a Graph Transformer-based Task-Aware Model that integrates semantic and structural encodings to assess subgraph relevance. During inference, TarDGR retrieves and fuses task-aware subgraphs with the query subgraph, enriching its representation and mitigating temporal generalization issues. Experiments on multiple large-scale dynamic graph datasets demonstrate that TarDGR consistently outperforms state-of-the-art methods, with extensive empirical evidence underscoring its superior accuracy and generalization capabilities.

</details>


### [785] [Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation](https://arxiv.org/abs/2511.12922)
*Yu Hou,Won-Yong Shin*

Main category: cs.IR

TL;DR: 提出UniTok统一项目标记化框架，通过混合专家架构和多码本将不同领域的项目转换为离散标记，实现跨领域语义保留的可扩展标记化。


<details>
  <summary>Details</summary>
Motivation: 现有项目标记化方法需要为每个项目领域训练单独模型，限制了泛化能力，且不同领域的分布和语义差异使得构建统一标记化困难。

Method: 使用共享编码器将不同领域项目投影到统一潜在空间，通过领域特定专家捕获独特语义，共享专家编码跨领域通用知识，并引入互信息校准机制平衡语义信息。

Result: 在真实数据集上实验显示：效果显著提升51.89%，理论分析验证架构设计有效性，具有强泛化能力无需领域重训练。

Conclusion: UniTok框架实现了跨领域统一项目标记化，在保持语义信息的同时具备高可扩展性和泛化能力。

Abstract: Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.

</details>


### [786] [DualGR: Generative Retrieval with Long and Short-Term Interests Modeling](https://arxiv.org/abs/2511.12518)
*Zhongchao Yi,Kai Feng,Xiaojian Ma,Yalong Wang,Yongqi Liu,Han Li,Zhengyang Zhou,Yang Wang*

Main category: cs.IR

TL;DR: DualGR是一个生成式检索框架，通过双分支长短时路由器平衡用户长短期兴趣，使用基于搜索的语义ID解码控制噪声干扰，并引入曝光感知的下一个令牌预测损失处理负反馈，在快手短视频推荐系统中显著提升了观看指标。


<details>
  <summary>Details</summary>
Motivation: 解决生成式检索在工业推荐系统中的三个关键挑战：1) 用户长短期兴趣平衡问题 2) 生成层次语义ID时的噪声干扰 3) 对未点击曝光项目的负反馈建模缺失

Method: 1) 双分支长短时路由器分别建模用户长期稳定偏好和短期瞬时意图 2) 基于搜索的语义ID解码限制候选交互到当前粗粒度桶以控制噪声 3) 曝光感知的下一个令牌预测损失将未点击曝光项目作为硬负样本

Result: 在快手短视频推荐系统在线A/B测试中，视频观看量提升0.527%，观看时长提升0.432%

Conclusion: DualGR为工业级生成式检索提供了一个实用有效的范式，能够显著提升推荐系统性能

Abstract: In large-scale industrial recommendation systems, retrieval must produce high-quality candidates from massive corpora under strict latency. Recently, Generative Retrieval (GR) has emerged as a viable alternative to Embedding-Based Retrieval (EBR), which quantizes items into a finite token space and decodes candidates autoregressively, providing a scalable path that explicitly models target-history interactions via cross-attention. However, three challenges persist: 1) how to balance users' long-term and short-term interests , 2) noise interference when generating hierarchical semantic IDs (SIDs), 3) the absence of explicit modeling for negative feedback such as exposed items without clicks. To address these challenges, we propose DualGR, a generative retrieval framework that explicitly models dual horizons of user interests with selective activation. Specifically, DualGR utilizes Dual-Branch Long/Short-Term Router (DBR) to cover both stable preferences and transient intents by explicitly modeling users' long- and short-term behaviors. Meanwhile, Search-based SID Decoding (S2D) is presented to control context-induced noise and enhance computational efficiency by constraining candidate interactions to the current coarse (level-1) bucket during fine-grained (level-2/3) SID prediction. % also reinforcing intra-class consistency. Finally, we propose an Exposure-aware Next-Token Prediction Loss (ENTP-Loss) that treats "exposed-but-unclicked" items as hard negatives at level-1, enabling timely interest fade-out. On the large-scale Kuaishou short-video recommendation system, DualGR has achieved outstanding performance. Online A/B testing shows +0.527% video views and +0.432% watch time lifts, validating DualGR as a practical and effective paradigm for industrial generative retrieval.

</details>


### [787] [MindRec: Mind-inspired Coarse-to-fine Decoding for Generative Recommendation](https://arxiv.org/abs/2511.12597)
*Mengyao Gao,Chongming Gao,Haoyan Liu,Qingpeng Cai,Peng Jiang,Jiajia Chen,Shuai Yuan,Xiangnan He*

Main category: cs.IR

TL;DR: MindRec是一个受人类思维过程启发的推荐系统框架，通过首先生成反映用户偏好的关键标记，然后扩展为完整项目，模拟人类从关键词到完整决策的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐系统采用自回归生成方式，受限于从左到右的贪婪解码策略和单向逻辑流，无法产生全局最优推荐。而人类推理往往从关键词或直觉开始，然后逐步细化扩展。

Method: 1) 首先生成反映用户偏好的关键标记；2) 将项目组织为分层类别树，引导模型先产生粗粒度类别，再逐步细化选择；3) 设计扩散束搜索算法来缓解贪婪解码的局部最优问题。

Result: 实验结果显示，MindRec在top-1推荐性能上比最先进方法平均提升9.5%，显著提高了推荐准确性。

Conclusion: MindRec通过模拟人类思维过程，实现了更灵活和人类化的推荐生成，有效提升了推荐系统的性能。

Abstract: Recent advancements in large language model-based recommendation systems often represent items as text or semantic IDs and generate recommendations in an auto-regressive manner. However, due to the left-to-right greedy decoding strategy and the unidirectional logical flow, such methods often fail to produce globally optimal recommendations. In contrast, human reasoning does not follow a rigid left-to-right sequence. Instead, it often begins with keywords or intuitive insights, which are then refined and expanded. Inspired by this fact, we propose Mind-inspired Recommender (MindRec), a novel generative framework that emulates human thought processes. Particularly, our method first generates key tokens that reflect user preferences, and then expands them into the complete item, enabling flexible and human-like generation. To further emulate the structured nature of human decision-making, we organize items into a hierarchical category tree. This structure guides the model to first produce the coarse-grained category and then progressively refine its selection through finer-grained subcategories before generating the specific item. To mitigate the local optimum problem inherent in greedy decoding, we design a novel beam search algorithm, Diffusion Beam Search, tailored for our mind-inspired generation paradigm. Experimental results demonstrate that MindRec yields a 9.5\% average improvement in top-1 recommendation performance over state-of-the-art methods, highlighting its potential to enhance recommendation accuracy. The implementation is available via https://github.com/Mr-Peach0301/MindRec.

</details>


### [788] [A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation](https://arxiv.org/abs/2511.12947)
*Hao Jiang,Guoquan Wang,Sheng Yu,Yang Zeng,Wencong Zeng,Guorui Zhou*

Main category: cs.IR

TL;DR: 提出ReST框架，通过元ID预热网络和空间约束对比学习增强长尾本地生活推荐的表示学习，解决空间约束和长尾稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 本地生活推荐面临空间约束（物品仅展示给有限地理区域内用户）和长尾稀疏（热门物品主导交互，高质量长尾物品被忽视）两大挑战。现有方法多从用户角度出发，但本文认为应从物品角度增强长尾物品表示。

Method: 提出ReST框架：1）元ID预热网络，通过属性级语义信息初始化基础ID表示；2）空间约束ID表示增强网络（SIDENet），基于对比学习，采用空间约束硬采样和动态表示对齐策略，自适应识别弱ID表示并增强。

Result: 该方法在空间约束下捕获物品潜在关系，增强长尾物品表示，同时保持与热门物品的兼容性。

Conclusion: ReST框架通过物品中心视角有效解决本地生活推荐中的空间约束和长尾稀疏问题，提升推荐质量。

Abstract: Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.

</details>


### [789] [Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior](https://arxiv.org/abs/2511.12949)
*Bokang Fu,Jiahao Wang,Xiaojing Liu,Yuli Liu*

Main category: cs.IR

TL;DR: 提出CFQP框架，通过结合个性化记忆模块和图基偏好传播，动态建模用户-问题交互，解决LLMs在捕捉用户动态行为序列方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs系统通常静态建模用户偏好，无法捕捉交互行为的动态性和序列性。用户历史问题序列蕴含丰富的兴趣演变信号，但语言建模与行为序列建模之间存在脱节。

Method: CFQP框架整合个性化记忆模块和图基偏好传播的双重机制，自适应学习用户特定历史，并通过相似用户的协作信号优化预测。

Result: 实验结果表明，该方法能有效生成模拟真实用户提问模式的智能体，突显其在构建主动适应型对话系统方面的潜力。

Conclusion: CFQP框架成功弥合了语言建模与行为序列建模之间的差距，为构建更智能、自适应的对话系统提供了有效解决方案。

Abstract: In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.
  To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.

</details>


### [790] [Personalized Federated Recommendation With Knowledge Guidance](https://arxiv.org/abs/2511.12959)
*Jaehyung Lim,Wonbin Kweon,Woojoo Kim,Junyoung Kim,Dongha Kim,Hwanjo Yu*

Main category: cs.IR

TL;DR: FedRKG是一个联邦推荐框架，通过知识引导机制在单知识模型内存占用下实现双知识模型的个性化效果，解决了现有联邦推荐模型在内存效率与性能之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐模型面临关键困境：内存高效的单知识模型因丢弃有价值个性化信息而性能受限，而高性能的双知识模型又因内存需求过大难以在实际设备上部署。

Method: 提出知识引导机制，避免完全替换，而是将全局知识融合到保留的局部嵌入中；引入自适应引导机制，为每个用户-物品交互动态调整引导强度。

Result: 在基准数据集上的广泛实验表明，FedRKG显著优于现有最先进方法，验证了方法的有效性。

Conclusion: FedRKG成功解决了联邦推荐中内存效率与性能的权衡问题，在单知识模型内存占用下实现了双知识模型的个性化效果。

Abstract: Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.

</details>


### [791] [Mitigating Recommendation Biases via Group-Alignment and Global-Uniformity in Representation Learning](https://arxiv.org/abs/2511.13041)
*Miaomiao Cai,Min Hou,Lei Chen,Le Wu,Haoyue Bai,Yong Li,Meng Wang*

Main category: cs.IR

TL;DR: 提出了AURL框架，通过表示分布中的组对齐和全局均匀性正则化来解决推荐系统中的偏差问题。


<details>
  <summary>Details</summary>
Motivation: CF方法因训练数据不平衡而产生偏差，倾向于推荐热门商品且对非活跃用户表现不佳。现有方法可能影响准确性或对权重策略敏感。

Method: 识别表示分布中的组差异和全局坍塌问题，提出组对齐和全局均匀性两个正则化器，直接优化表示分布。

Result: 在三个真实数据集和多种推荐骨干网络上验证了框架的优越性。

Conclusion: AURL框架通过表示分布优化有效缓解了推荐偏差问题。

Abstract: Collaborative Filtering~(CF) plays a crucial role in modern recommender systems, leveraging historical user-item interactions to provide personalized suggestions. However, CF-based methods often encounter biases due to imbalances in training data. This phenomenon makes CF-based methods tend to prioritize recommending popular items and performing unsatisfactorily on inactive users. Existing works address this issue by rebalancing training samples, reranking recommendation results, or making the modeling process robust to the bias. Despite their effectiveness, these approaches can compromise accuracy or be sensitive to weighting strategies, making them challenging to train. In this paper, we deeply analyze the causes and effects of the biases and propose a framework to alleviate biases in recommendation from the perspective of representation distribution, namely Group-Alignment and Global-Uniformity Enhanced Representation Learning for Debiasing Recommendation (AURL). Specifically, we identify two significant problems in the representation distribution of users and items, namely group-discrepancy and global-collapse. These two problems directly lead to biases in the recommendation results. To this end, we propose two simple but effective regularizers in the representation space, respectively named group-alignment and global-uniformity. The goal of group-alignment is to bring the representation distribution of long-tail entities closer to that of popular entities, while global-uniformity aims to preserve the information of entities as much as possible by evenly distributing representations. Our method directly optimizes both the group-alignment and global-uniformity regularization terms to mitigate recommendation biases. Extensive experiments on three real datasets and various recommendation backbones verify the superiority of our proposed framework.

</details>


### [792] [Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact](https://arxiv.org/abs/2511.13057)
*Satyanarayan Pati*

Main category: cs.IR

TL;DR: 对BEIR SciFact基准的密集检索模型压缩研究，比较了维度缩减（自编码器）和精度缩减（量化）两种策略，发现int8标量量化在4倍压缩下性能损失最小。


<details>
  <summary>Details</summary>
Motivation: 密集检索模型的高维高精度向量嵌入在真实部署中面临显著的存储和内存挑战，需要有效的压缩方法。

Method: 系统评估两种压缩策略：维度缩减（自编码器从384维降至12维）和精度缩减（float16、int8和二进制量化），在完整检索指标套件上测量性能损失。

Result: int8标量量化提供最佳平衡点，实现4倍压缩且nDCG@10仅下降1-2%；自编码器在同等4倍压缩下性能损失更大；二进制量化因性能急剧下降而不适用。

Conclusion: int8量化是部署高效高性能检索系统的最实用方法，为实际应用提供了明确的压缩指导。

Abstract: Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the "performance loss" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective "sweet spot," achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.

</details>


### [793] [Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users](https://arxiv.org/abs/2511.13166)
*Zhaoxin Shen,Dan Wu*

Main category: cs.IR

TL;DR: 提出了一种名为局部协同过滤(LCF)的新方法，利用用户间的局部相似性并结合大数定律来更有效地利用用户行为数据。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地利用互联网中的用户行为数据来改进推荐系统。

Method: 局部协同过滤(LCF)方法，利用用户间的局部相似性，并基于大数定律整合用户数据。

Result: 在Steam游戏数据集上的实验结果表明，LCF的结果符合实际需求。

Conclusion: LCF方法能够有效提升用户行为数据的利用率，在推荐系统中具有实际应用价值。

Abstract: To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.

</details>


### [794] [Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation](https://arxiv.org/abs/2511.13201)
*Hao Hu,Yifan Feng,Ruoxue Li,Rundong Xue,Xingliang Hou,Zhiqiang Tian,Yue Gao,Shaoyi Du*

Main category: cs.IR

TL;DR: 提出了Cog-RAG框架，通过主题超图和实体超图的双重结构，结合认知启发的两阶段检索策略，显著提升了RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图增强RAG方法主要关注低阶成对实体关系，无法捕捉多实体间的高阶关联；而超图方法又局限于块间实体级表示，忽略了全局主题组织和对齐。

Method: 构建主题超图捕获块间主题结构，实体超图建模高阶语义关系；设计两阶段检索策略：先激活查询相关主题内容，再引导实体超图的细粒度召回和扩散。

Result: 在广泛实验中，Cog-RAG显著优于现有最先进的基线方法。

Conclusion: Cog-RAG框架通过模拟人类自上而下的认知推理过程，实现了从全局主题到局部细节的语义对齐和一致生成，有效提升了RAG系统的性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.

</details>


### [795] [Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference](https://arxiv.org/abs/2511.13389)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.IR

TL;DR: 本文提出了一种结合时间序列聚类和因果推理的方法来分析感应炉熔炼过程的能源效率，识别了影响能耗的关键因果因素，为铸造厂优化操作提供了可行见解。


<details>
  <summary>Details</summary>
Motivation: 工业铸造过程能耗高且变量间存在复杂依赖关系，传统的相关性分析难以区分真正的因果驱动因素，限制了决策的有效性。

Method: 使用时间序列聚类将熔炼周期分割为不同操作模式，并应用PCMCI+算法（先进的因果发现方法）揭示各模式内的因果关系。

Result: 研究发现能耗、炉温和材料重量之间存在稳健的因果关系，电压对冷却水温度有延迟影响。高效集群具有稳定的因果结构，而低效集群则表现出强化反馈环和非典型依赖关系。

Conclusion: 该研究提出了聚类-因果推理集成管道作为分析能源密集型过程的方法创新，并为铸造厂操作员提供了优化性能、降低能耗和排放的可操作见解。

Abstract: Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.

</details>


### [796] [Attention Grounded Enhancement for Visual Document Retrieval](https://arxiv.org/abs/2511.13415)
*Wanqing Cui,Wei Huang,Yazhi Guo,Yibo Hu,Meiguang Jin,Junfeng Ma,Keping Bi*

Main category: cs.IR

TL;DR: 提出AGREE框架，利用多模态大语言模型的跨模态注意力作为局部监督信号，结合全局监督共同优化文档检索器，提升对非抽取式查询的处理能力


<details>
  <summary>Details</summary>
Motivation: 现有文档检索器仅使用粗粒度的全局相关标签训练，无法识别支持匹配的具体区域，导致依赖表面线索且难以处理隐含语义连接

Method: AGREE框架通过多模态大语言模型的跨模态注意力提供代理局部监督，指导识别相关文档区域，结合局部和全局信号联合优化检索器

Result: 在ViDoRe V2基准测试中显著优于仅使用全局监督的基线方法，定量和定性分析显示AGREE促进了查询词与文档区域的深度对齐

Conclusion: AGREE框架通过局部监督使检索器能够学习驱动相关性的具体内容，实现了超越表面匹配的更准确和可解释的检索

Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.

</details>


### [797] [Exploring Multi-Table Retrieval Through Iterative Search](https://arxiv.org/abs/2511.13418)
*Allaa Boutaleb,Bernd Amann,Rafael Angarita,Hubert Naacke*

Main category: cs.IR

TL;DR: 提出了一种迭代式多表检索框架，通过贪心连接感知检索算法在相关性、覆盖率和连接性之间取得平衡，相比MIP方法在保持竞争力的同时速度提升4-400倍。


<details>
  <summary>Details</summary>
Motivation: 数据湖上的开放域问答需要从多个表中检索和组合信息，现有方法要么计算复杂度过高（如MIP），要么无法找到连贯可连接的表集合（如贪心启发式）。

Method: 将多表检索构建为迭代搜索过程，提出贪心连接感知检索算法，综合考虑相关性、覆盖率和连接性。

Result: 在5个NL2SQL基准测试中，迭代方法相比MIP方法在保持竞争力的检索性能的同时，速度提升4-400倍（取决于基准和搜索空间设置）。

Conclusion: 迭代启发式方法在实用、可扩展和组合感知检索方面具有巨大潜力。

Abstract: Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.

</details>


### [798] [Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports](https://arxiv.org/abs/2511.13523)
*Nikita Neveditsin,Pawan Lingras,Salil Patil,Swarup Patil,Vijay Mago*

Main category: cs.IR

TL;DR: 评估紧凑多模态语言模型作为隐私保护替代方案，用于转录嘈杂临床文档的性能表现


<details>
  <summary>Details</summary>
Motivation: 医疗记录数字化通常依赖打印报告的智能手机照片，这些图像因模糊、阴影和其他噪声而质量下降。传统OCR系统在真实世界条件下表现不佳

Method: 使用印度医疗环境中常见的带有地区口音的医学英语产科超声报告，比较8个系统在转录准确性、噪声敏感性、数字准确性和计算效率方面的表现

Result: 紧凑多模态模型始终优于经典和神经OCR管道，尽管计算成本较高，但其鲁棒性和语言适应性使其成为现场医疗数字化的可行候选方案

Conclusion: 紧凑多模态模型因其鲁棒性和语言适应性，是医疗数字化中隐私保护转录的可行替代方案

Abstract: Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [799] [ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation](https://arxiv.org/abs/2511.12072)
*Jiahui Sun,Weining Wang,Mingzhen Sun,Yirong Yang,Xinxin Zhu,Jing Liu*

Main category: cs.MM

TL;DR: ProAV-DiT是一个投影潜在扩散Transformer模型，用于高效生成同步的音频-视频内容，通过将音频预处理为类视频表示、多尺度双流时空自编码器和多尺度注意力机制来解决音频视频结构不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 音频视频生成任务面临音频和视频结构不对齐的挑战，以及多模态数据处理的高计算成本问题。

Method: 1) 将原始音频预处理为类视频表示；2) 使用多尺度双流时空自编码器将两种模态投影到统一潜在空间；3) 引入多尺度注意力机制；4) 使用时空扩散Transformer处理统一的3D潜在空间。

Result: 在标准基准测试上的广泛实验表明，ProAV-DiT在生成质量和计算效率方面均优于现有方法。

Conclusion: ProAV-DiT通过创新的架构设计成功解决了音频视频生成中的结构不对齐和计算效率问题，能够生成高质量的同步音频视频内容。

Abstract: Sounding Video Generation (SVG) remains a challenging task due to the inherent structural misalignment between audio and video, as well as the high computational cost of multimodal data processing. In this paper, we introduce ProAV-DiT, a Projected Latent Diffusion Transformer designed for efficient and synchronized audio-video generation. To address structural inconsistencies, we preprocess raw audio into video-like representations, aligning both the temporal and spatial dimensions between audio and video. At its core, ProAV-DiT adopts a Multi-scale Dual-stream Spatio-Temporal Autoencoder (MDSA), which projects both modalities into a unified latent space using orthogonal decomposition, enabling fine-grained spatiotemporal modeling and semantic alignment. To further enhance temporal coherence and modality-specific fusion, we introduce a multi-scale attention mechanism, which consists of multi-scale temporal self-attention and group cross-modal attention. Furthermore, we stack the 2D latents from MDSA into a unified 3D latent space, which is processed by a spatio-temporal diffusion Transformer. This design efficiently models spatiotemporal dependencies, enabling the generation of high-fidelity synchronized audio-video content while reducing computational overhead. Extensive experiments conducted on standard benchmarks demonstrate that ProAV-DiT outperforms existing methods in both generation quality and computational efficiency.

</details>


### [800] [SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs](https://arxiv.org/abs/2511.12404)
*Shail Desai,Aditya Pawar,Li Lin,Xin Wang,Shu Hu*

Main category: cs.MM

TL;DR: SynthGuard是一个开源的AI生成多媒体检测平台，结合传统检测器和多模态大语言模型，提供可解释的推理和统一的图像音频支持，旨在让取证分析对研究人员、教育工作者和公众更加易用。


<details>
  <summary>Details</summary>
Motivation: AI生成媒体的快速兴起带来了严重风险，包括错误信息、身份滥用和公众信任侵蚀。现有检测工具多为闭源、模态有限或缺乏透明度，用户难以理解检测决策过程。

Method: 开发SynthGuard平台，结合传统检测器和多模态大语言模型(MLLMs)，提供可解释的推理、统一的图像和音频支持，以及交互式界面。

Result: SynthGuard平台已上线可用，网址为https://in-engr-nova.it.purdue.edu/

Conclusion: SynthGuard填补了现有AI生成媒体检测工具的空白，通过开源、多模态支持和可解释性，使取证分析更加透明和易于理解。

Abstract: Artificial Intelligence (AI) has made it possible for anyone to create images, audio, and video with unprecedented ease, enriching education, communication, and creative expression. At the same time, the rapid rise of AI-generated media has introduced serious risks, including misinformation, identity misuse, and the erosion of public trust as synthetic content becomes increasingly indistinguishable from real media. Although deepfake detection has advanced, many existing tools remain closed-source, limited in modality, or lacking transparency and educational value, making it difficult for users to understand how detection decisions are made. To address these gaps, we introduce SynthGuard, an open, user-friendly platform for detecting and analyzing AI-generated multimedia using both traditional detectors and multimodal large language models (MLLMs). SynthGuard provides explainable inference, unified image and audio support, and an interactive interface designed to make forensic analysis accessible to researchers, educators, and the public. The SynthGuard platform is available at: https://in-engr-nova.it.purdue.edu/

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [801] [Collusion-proof Auction Design using Side Information](https://arxiv.org/abs/2511.12456)
*Sukanya Kudva,Anil Aswani*

Main category: cs.GT

TL;DR: 该论文研究存在投标人合谋情况下的拍卖设计问题，提出了一种混合VCG机制，通过结合非合谋投标人的VCG机制和合谋投标人的固定价格机制，在保持激励兼容性的同时提高福利和收益。


<details>
  <summary>Details</summary>
Motivation: 传统VCG机制虽然能实现效率和真实性，但对合谋高度脆弱；而完全防合谋机制仅限于固定价格格式，无法保证近似效率。论文旨在设计在部分投标人合谋时仍能保证良好福利和收益的拍卖机制。

Method: 提出混合VCG机制：对非合谋投标人应用VCG机制，对合谋投标人使用固定价格机制，并假设有一个黑盒合谋检测算法。该机制是事后占优策略激励兼容的。

Result: H-VCG机制在多个分布下的数值实验表明，它始终优于仅对非合谋投标人应用VCG，并接近假设普遍真实性情况下的理想VCG机制性能。

Conclusion: 研究为将合谋检测纳入机制设计提供了原则性框架，向防合谋拍卖迈出了重要一步。

Abstract: We study the problem of auction design in the presence of bidder collusion. Specifically, we consider a multi-unit auction of identical items with single-minded bidders, where a subset of bidders may collude by coordinating bids and transferring payments and items among themselves. While the classical Vickrey-Clarke-Groves (VCG) mechanism achieves efficient and truthful outcomes, it is highly vulnerable to collusion. In contrast, fully collusion-proof mechanisms are limited to posted-price formats, which fail to guarantee even approximate efficiency. This paper aims to bridge this gap by designing auctions that achieve good welfare and revenue guarantees even when some bidders collude. We first characterize the strategic behavior of colluding bidders under VCG and prove that such bidders optimally bid shade: they never overbid or take additional items, but instead reduce the auction price. This characterization enables a Bulow-Klemperer type result: adding colluding bidders can only improve welfare and revenue relative to running VCG on the non-colluding group alone. We then propose a Hybrid VCG (H-VCG) mechanism that combines VCG applied to non-colluding bidders with a posted-price mechanism for colluding bidders, assuming access to a black-box collusion detection algorithm. We show that H-VCG is ex-post dominant-strategy incentive compatible (DSIC) and derive probabilistic guarantees on expected welfare and revenue under both known and unknown valuation distributions. Numerical experiments across several distributions demonstrate that H-VCG consistently outperforms VCG restricted to non-colluding bidders and approaches the performance of the ideal VCG mechanism assuming universal truthfulness. Our results provide a principled framework for incorporating collusion detection into mechanism design, offering a step toward collusion-resistant auctions.

</details>


### [802] [Perturbing Best Responses in Zero-Sum Games](https://arxiv.org/abs/2511.12523)
*Adam Dziwoki,Rostislav Horcik*

Main category: cs.GT

TL;DR: 本文研究了在零和博弈中，基于最佳响应的算法（双Oracle和虚拟博弈）在效用扰动下的表现。研究发现适当的扰动能显著减少算法迭代次数，在某些情况下甚至能达到对数级别的期望迭代次数。


<details>
  <summary>Details</summary>
Motivation: 研究效用扰动对零和博弈中纳什均衡近似算法的影响，探索如何通过扰动提高算法效率。

Method: 假设Oracle在计算最佳响应时对效用进行扰动，分析双Oracle和虚拟博弈算法在扰动下的表现。

Result: 使用扰动Oracle能减少两种算法的迭代次数，某些情况下期望迭代次数可降至对数级别。虽然效用扰动计算成本高，但在具有内部结构的博弈中可以高效实现。

Conclusion: 适当的效用扰动能有效提高零和博弈中纳什均衡近似算法的收敛速度，特别是在具有结构特征的博弈中。

Abstract: This paper investigates the impact of perturbations on the best-response-based algorithms approximating Nash equilibria in zero-sum games, namely Double Oracle and Fictitious Play. More precisely, we assume that the oracle computing the best responses perturbs the utilities before selecting the best response. We show that using such an oracle reduces the number of iterations for both algorithms. For some cases, suitable perturbations ensure the expected number of iterations is logarithmic. Although the utility perturbation is computationally demanding as it requires iterating through all pure strategies, we demonstrate that one can efficiently perturb the utilities in games where pure strategies have further inner structure.

</details>


### [803] [Bandit Learning in Housing Markets](https://arxiv.org/abs/2511.12629)
*Shiyun Lin*

Main category: cs.GT

TL;DR: 本文提出了一种在住房市场（单边匹配市场）中学习偏好的多玩家多臂老虎机框架，定义了核心遗憾作为市场目标，研究了集中式和去中心化方法，证明了遗憾上界为O(N log T/Δ²)，并对去中心化设置建立了匹配下界。


<details>
  <summary>Details</summary>
Motivation: 住房市场作为经典的经济交换模型，虽然已被广泛研究，但在偏好未知且需要通过重复交互学习的情况下研究较少。本文旨在填补这一空白，探索在偏好必须通过随机奖励学习的环境中的稳定分配问题。

Method: 采用多玩家多臂老虎机框架，玩家从随机奖励中学习对商品（臂）的偏好。提出了核心遗憾的概念作为市场目标，研究了集中式和去中心化两种方法。

Result: 证明了集中式和去中心化方法的遗憾上界均为O(N log T/Δ²)，其中N是玩家数量，T是时间范围，Δ是玩家间最小偏好差距。对去中心化设置还建立了匹配下界，表明算法是阶最优的。

Conclusion: 本文成功地将住房市场问题建模为多玩家多臂老虎机学习问题，定义了核心遗憾度量，并设计了在集中式和去中心化设置下都能达到最优遗憾界的算法。

Abstract: The housing market, also known as one-sided matching market, is a classic exchange economy model where each agent on the demand side initially owns an indivisible good (a house) and has a personal preference over all goods. The goal is to find a core-stable allocation that exhausts all mutually beneficial exchanges among subgroups of agents. While this model has been extensively studied in economics and computer science due to its broad applications, little attention has been paid to settings where preferences are unknown and must be learned through repeated interactions. In this paper, we propose a statistical learning model within the multi-player multi-armed bandit framework, where players (agents) learn their preferences over arms (goods) from stochastic rewards. We introduce the notion of core regret for each player as the market objective. We study both centralized and decentralized approaches, proving $O(N \log T / Δ^2)$ upper bounds on regret, where $N$ is the number of players, $T$ is the time horizon and $Δ$ is the minimum preference gap among players. For the decentralized setting, we also establish a matching lower bound, demonstrating that our algorithm is order-optimal.

</details>


### [804] [Rethinking Data Value: Asymmetric Data Shapley for Structure-Aware Valuation in Data Markets and Machine Learning Pipelines](https://arxiv.org/abs/2511.12863)
*Xi Zheng,Yinghui Huang,Xiangyu Chang,Ruoxi Jia,Yong Tan*

Main category: cs.GT

TL;DR: 提出了非对称数据Shapley(ADS)框架，用于解决现代ML/AI工作流中数据源的方向性和时间依赖性估值问题，突破了经典数据Shapley的对称性限制。


<details>
  <summary>Details</summary>
Motivation: 经典数据Shapley的对称性假设无法捕捉现代ML/AI工作流中的方向性和时间依赖性，如重复数据对原始数据的依赖、联邦学习和多阶段LLM微调中的顺序特定贡献。

Method: 引入ADS框架，通过仅对符合应用特定数据组排序的排列进行边际贡献平均来放松对称性，保持效率和线性特性，并开发了MC-ADS和KNN-ADS两种计算方法。

Result: 在具有方向性和时间依赖性的代表性场景中，ADS始终优于基准方法，能够区分新颖与冗余贡献并尊重训练的顺序性。

Conclusion: ADS为数据市场和复杂ML/AI管道中的公平数据估值提供了原则性和实用的方法。

Abstract: Rigorous valuation of individual data sources is critical for fair compensation in data markets, informed data acquisition, and transparent development of ML/AI models. Classical Data Shapley (DS) provides a essential axiomatic framework for data valuation but is constrained by its symmetry axiom that assumes interchangeability of data sources. This assumption fails to capture the directional and temporal dependencies prevalent in modern ML/AI workflows, including the reliance of duplicated or augmented data on original sources and the order-specific contributions in sequential pipelines such as federated learning and multi-stage LLM fine tuning. To address these limitations, we introduce Asymmetric Data Shapley (ADS), a structure-aware data valuation framework for modern ML/AI pipelines. ADS relaxes symmetry by averaging marginal contributions only over permutations consistent with an application-specific ordering of data groups. It preserves efficiency and linearity, maintains within group symmetry and directional precedence across groups, and reduces to DS when the ordering collapses to a single group. We develop two complementary computational procedures for ADS: (i) a Monte Carlo estimator (MC-ADS) with finite-sample accuracy guarantees, and (ii) a k-nearest neighbor surrogate (KNN-ADS) that is exact and efficient for KNN predictors. Across representative settings with directional and temporal dependence, ADS consistently outperforms benchmark methods by distinguishing novel from redundant contributions and respecting the sequential nature of training. These results establish ADS as a principled and practical approach to equitable data valuation in data markets and complex ML/AI pipelines.

</details>


### [805] [Resilient and Efficient Allocation for Large-Scale Autonomous Fleets via Decentralized Coordination](https://arxiv.org/abs/2511.12879)
*Ashish Kumar Perukari,Polina Khoroshevskaya*

Main category: cs.GT

TL;DR: 提出了一种结合分布预测和去中心化协调的资源分配方法，利用局部侧信息构建风险模型，通过轻量级共识-ADMM算法实现近中心化性能，避免单点故障。


<details>
  <summary>Details</summary>
Motivation: 大规模自主车队运营需要在不确定性下快速、弹性地分配稀缺资源（如能源、充电器、维护时段等），传统方法难以同时满足效率和鲁棒性要求。

Method: 利用局部侧信息构建每个代理的风险模型，通过机会约束耦合风险，采用轻量级共识-ADMM算法在稀疏通信图上协调代理。

Result: 在真实城市道路网络和卫星星座上验证，相比贪婪、无侧信息和oracle中心基准，在相同成本下故障率降低30-55%，可扩展到数千代理且运行时间接近线性。

Conclusion: 该方法能够在大规模资源分配中显著降低故障率，保持可行性概率，同时具备良好的可扩展性和去中心化优势。

Abstract: Operating large autonomous fleets demands fast, resilient allocation of scarce resources (such as energy and fuel, charger access and maintenance slots, time windows, and communication bandwidth) under uncertainty. We propose a side-information-aware approach for resource allocation at scale that combines distributional predictions with decentralized coordination. Local side information shapes per-agent risk models for consumption, which are coupled through chance constraints on failures. A lightweight consensus-ADMM routine coordinates agents over a sparse communication graph, enabling near-centralized performance while avoiding single points of failure. We validate the framework on real urban road networks with autonomous vehicles and on a representative satellite constellation, comparing against greedy, no-side-information, and oracle central baselines. Our method reduces failure rates by 30-55% at matched cost and scales to thousands of agents with near-linear runtime, while preserving feasibility with high probability.

</details>


### [806] [An FPTAS for 7/9-Approximation to Maximin Share Allocations](https://arxiv.org/abs/2511.13056)
*Xin Huang,Shengwei Zhou*

Main category: cs.GT

TL;DR: 提出新算法，将不可分割物品的最大最小份额(MMS)分配近似比从10/13提升到7/9，并提供更简单的FPTAS实现


<details>
  <summary>Details</summary>
Motivation: 改进现有MMS分配问题的近似比，简化算法复杂度

Method: 基于新的分析框架，开发了更简单的算法和FPTAS实现

Result: 达到7/9近似比，比现有最佳结果10/13有所提升，算法复杂度为(1/ε)·poly(n,m)

Conclusion: 新算法在近似比和算法简洁性方面均有显著改进

Abstract: We present a new algorithm that achieves a $\frac{7}{9}$-approximation for the maximin share (MMS) allocation of indivisible goods under additive valuations, improving the current best ratio of $\frac{10}{13}$ (Heidari et al., SODA 2026). Building on a new analytical framework, we further obtain an FPTAS that achieves a $\frac{7}{9}-\varepsilon$ approximation in $\tfrac{1}{\varepsilon} \cdot \mathrm{poly}(n,m)$ time. Compared with prior work (Heidari et al., SODA 2026), our algorithm is substantially simpler.

</details>


### [807] [MEV in Multiple Concurrent Proposer Blockchains](https://arxiv.org/abs/2511.13080)
*Steven Landers,Benjamin Marsh*

Main category: cs.GT

TL;DR: 分析多并发提议者区块链中的最大可提取价值，发现并发性打破了顺序链的单构建者假设，引入了新的MEV渠道，包括同时间重复窃取、提议者间拍卖和可用性证明延迟驱动的时序竞争。


<details>
  <summary>Details</summary>
Motivation: 研究多并发提议者区块链中MEV的新形式，因为多个区块在最终执行顺序确定前就变得数据可用，这种并发性带来了不同于顺序链的MEV提取机会。

Method: 开发了延迟和包含的归一化风险模型，推导出封闭形式的延迟包络M(τ)，并描述了审查、重复和拍卖博弈的均衡特征。

Result: 确定了确定性优先级DAG调度和重复感知支付可以消除同时间MEV同时保持吞吐量，找到了简单的协议配置来缓解MCP特定提取而无需集中化构建者。

Conclusion: 多并发提议者区块链中的MEV可以通过适当的协议设计来缓解，确定性调度和重复感知支付是有效的解决方案。

Abstract: We analyze maximal extractable value in multiple concurrent proposer blockchains, where multiple blocks become data available before their final execution order is determined. This concurrency breaks the single builder assumption of sequential chains and introduces new MEV channels, including same tick duplicate steals, proposer to proposer auctions, and timing races driven by proof of availability latency. We develop a hazard normalized model of delay and inclusion, derive a closed form delay envelope \(M(τ)\), and characterize equilibria for censorship, duplication, and auction games. We show how deterministic priority DAG scheduling and duplicate aware payouts neutralize same tick MEV while preserving throughput, identifying simple protocol configurations to mitigate MCP specific extraction without centralized builders.

</details>


### [808] [The Publication Choice Problem](https://arxiv.org/abs/2511.13678)
*Haichuan Wang,Yifan Wu,Haifeng Xu*

Main category: cs.GT

TL;DR: 论文提出了一个博弈论框架来分析研究人员如何选择投稿期刊以最大化影响力，以及这种选择如何影响期刊的影响因子。研究发现存在纯策略均衡，并分析了"亮点"论文标签对期刊影响力的影响。


<details>
  <summary>Details</summary>
Motivation: 研究人员战略性地选择投稿期刊来最大化工作影响力，而这些发表决策反过来又决定了期刊的影响因子。需要分析个体发表选择如何响应并塑造期刊影响力。

Method: 引入了一个博弈论框架，称为发表选择问题，捕捉了这种双向互动关系。分析了纯策略均衡的存在性和在二元研究者类型下的唯一性。

Result: 证明了发表选择问题存在纯策略均衡，在二元研究者类型下具有唯一性。竞争性期刊使用"亮点"标签可能会降低其他期刊的整体影响力，而非竞争性期刊使用"亮点"标签则有相反的影响。

Conclusion: 通过均衡分析，揭示了什么发表行为能更好地表明研究者的影响力水平，以及"亮点"论文标签如何影响研究社区中期刊的影响因子。

Abstract: Researchers strategically choose where to submit their work in order to maximize its impact, and these publication decisions in turn determine venues' impact factors. To analyze how individual publication choices both respond to and shape venue impact, we introduce a game-theoretic framework, coined the Publication Choice Problem, that captures this two-way interplay. We show the existence of a pure-strategy equilibrium in the Publication Choice Problem and its uniqueness under binary researcher types. Our characterizations of the equilibrium properties offer insights about what publication behaviors better indicate a researcher's impact level. Through equilibrium analysis, we further investigate how labeling papers with ``spotlight'' affects the impact factor of venues in the research community. Our analysis shows that competitive venue labeling top papers with ``spotlight'' may decrease the overall impact of other venues in the community, while less competitive venues with ``spotlight'' labeling have the opposite impact.

</details>
