<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 183]
- [cs.CL](#cs.CL) [Total: 90]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.NE](#cs.NE) [Total: 7]
- [cs.SE](#cs.SE) [Total: 42]
- [cs.LG](#cs.LG) [Total: 88]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 提出了一个基于运动捕捉数据的交互式AI模型，通过部分模仿和创造性增强人类动作序列来生成人工舞伴，是首个利用单人运动数据和高层特征实现此功能的模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然支持各种创意任务，但缺乏具身交互特性。舞蹈作为人类表达的基本形式，可以补充这种体验，探索创造性的人机交互。

Method: 结合了两个扩散模型、运动修复和运动风格转换的思想，利用单人运动数据和高层特征生成既时间连贯又对选定运动参考有响应的运动表示，不依赖低层人-人交互数据。

Result: 通过定量评估生成样本特征分布与测试集的收敛性来证明模型成功，生成的动作既多样化（显示与人类舞伴的各种偏差）又显得真实。

Conclusion: 该模型是迈向创造性AI舞蹈的第一步，生成的舞蹈动作既多样化又真实，为创造性的人机舞蹈互动提供了基础。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [2] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 提出基于机器学习的珊瑚白化分类系统，使用CNN模型在多样化全球数据集上达到88%准确率，优于ResNet和ViT模型。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁面临污染、海洋酸化和海水温度异常等日益严重的威胁，急需高效的保护和监测方法。

Method: 基于包含健康和白化珊瑚样本的多样化全球数据集，比较了ResNet、ViT和CNN三种最先进的模型，并进行全面的超参数调优。

Result: 经过超参数调优后，CNN模型取得了最高的88%准确率，优于现有基准。

Conclusion: 研究结果为自主珊瑚监测提供了重要见解，并对最广泛使用的计算机视觉模型进行了全面分析。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [3] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 使用YOLOv8深度学习管道自动识别西印度洋珊瑚礁鱼类，在肯尼亚和坦桑尼亚视频样带中测试24个鱼类科别，为区域自动化监测提供首个基准。


<details>
  <summary>Details</summary>
Motivation: 西印度洋珊瑚礁监测受限于水下视觉普查的劳动力需求，需要自动化解决方案来扩展监测规模。

Method: 基于YOLOv8深度学习管道，使用肯尼亚和坦桑尼亚收集的视频样带数据，构建包含24个鱼类科别的数据集进行测试。

Result: 最佳模型mAP@0.5达到0.52，对丰富科别识别准确率高，但对稀有或复杂分类群检测较弱。

Conclusion: 深度学习可作为传统监测方法的可扩展补充工具，具有在西印度洋珊瑚礁监测中应用的潜力。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [4] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 提出了一种基于互信息的数据增强方法，通过选择在自然扰动下具有高互信息的场景补丁作为对比学习的正样本来改进表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法在数据选择和增强方面仍依赖人工假设或工程，可能不是最优的。希望通过基于真实世界分布的互信息来选择训练数据，使学习到的特征在开放环境中具有更好的泛化能力。

Method: 考虑在自然扰动（如颜色变化和运动）下表现出高互信息的场景补丁作为对比学习的正样本，使用互信息指导的数据增强方法。

Result: 在多个基准测试和最先进的表示学习框架上评估了该方法，证明了其有效性。

Conclusion: 该方法是一个有前景的研究方向，为未来的表示学习研究提供了新的思路。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [5] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 本文对NVIDIA FLARE、Flower和Owkin Substra三个联邦学习框架在医学影像应用中的性能进行了基准测试，评估了模型性能、收敛效率、通信开销、可扩展性和开发者体验。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗AI中具有重要意义，能够在机构间进行协作模型训练而无需直接共享数据。本研究旨在评估不同FL框架在真实医疗环境中的适用性。

Method: 使用PathMNIST数据集，对三个主流FL框架（NVIDIA FLARE、Flower、Owkin Substra）进行综合评估，包括模型性能、收敛效率、通信开销、可扩展性和开发者体验等多个维度。

Result: NVIDIA FLARE在生产可扩展性方面表现最佳，Flower在原型设计和学术研究方面具有灵活性，Owkin Substra在隐私保护和合规性方面表现出色。

Conclusion: 每个框架都有针对不同使用场景优化的优势，强调了它们在医疗环境实际部署中的相关性。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [6] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: 提出了Med-Banana-50K数据集，这是一个包含5万张图像的医学图像编辑数据集，涵盖三种模态和23种疾病类型，通过Gemini-2.5-Flash-Image生成双向编辑，并采用医学质量控制方法确保质量。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像编辑研究受限于缺乏大规模、高质量、开放可用的数据集，特别是具有严格解剖学和临床约束的数据集。

Method: 利用Gemini-2.5-Flash-Image从真实医学图像生成双向编辑（病灶添加和移除），采用LLM-as-Judge进行医学质量控制，包含指令遵循度、结构合理性、真实性和保真度保持等评估维度，并进行最多五轮迭代优化。

Result: 构建了包含5万张图像的数据集，涵盖胸片、脑部MRI和眼底摄影三种模态，23种疾病类型，并包含3.7万次失败尝试的完整对话记录。

Conclusion: Med-Banana-50K为训练和评估下一代医学图像编辑模型奠定了基础，提供了大规模、医学验证且完全记录的资源。

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [7] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 该论文对水稻叶片图像进行了图像去噪和对比度增强方法的比较研究，结合CLAHE技术评估不同去噪方法的效果，为农业图像处理提供参考。


<details>
  <summary>Details</summary>
Motivation: 图像增强是图像处理中的重要预处理步骤，对于水稻叶片分析（如病害检测、营养评估）至关重要。去噪和对比度增强是主要步骤，需要系统评估不同方法的有效性。

Method: 对知名图像去噪方法结合CLAHE（对比度受限自适应直方图均衡化）进行了广泛的比较研究，使用水稻叶片图像数据集进行实验，并通过多种指标全面测试增强方法。

Result: 实验结果通过多种指标进行了全面评估，为评估数字图像处理方法的效果提供了坚实基础。

Conclusion: 该方法为评估数字图像处理方法的有效性提供了有力基础，并揭示了在农业研究和其他领域中未来应用的见解。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [8] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: 提出了SEPS框架，通过两阶段机制整合密集和稀疏文本的统一语义，解决视觉-语言跨模态对齐中的补丁冗余和歧义问题，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理跨模态信息密度差异时面临补丁冗余和歧义挑战，MLLM生成的密集文本可能与原始稀疏描述冲突，且难以准确量化视觉补丁与文本描述之间的语义相关性。

Method: SEPS框架采用两阶段机制：整合密集和稀疏文本的统一语义来识别显著视觉补丁；利用均值计算的相关性感知选择来突出关键补丁-词对应关系，改进跨模态相似性评估。

Result: 在Flickr30K和MS-COCO数据集上的实验表明，SEPS在不同模型架构下rSum指标提升23%-86%，在文本到图像检索场景中表现尤为突出。

Conclusion: SEPS框架通过语义增强的补丁精简方法，有效解决了跨模态对齐中的补丁冗余和歧义问题，为视觉问答等应用提供了更精确的局部对应关系。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [9] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 该研究系统分析了不同LiDAR扫描模式（重复式与非重复式）对路边感知性能的影响，创建了InfraLiDARs Benchmark数据集，发现非重复式LiDAR在成本效益方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LiDAR的优化布置，但不同扫描模式对感知性能的深远影响尚未得到充分研究，特别是传统重复式扫描与新兴非重复式扫描系统的差异。

Method: 在CARLA仿真环境中创建InfraLiDARs Benchmark数据集，使用同时运行的基础设施LiDAR采集数据，涵盖两种扫描模式，并对各种领先的3D目标检测算法进行性能评估。

Result: 研究发现非重复式扫描LiDAR和128线重复式LiDAR在各种场景下表现出相当的检测性能。尽管非重复式LiDAR感知范围有限，但考虑到其低成本，是一个具有成本效益的选择。

Conclusion: 该研究为设置具有最优LiDAR扫描模式和兼容算法的路边感知系统提供了见解，并公开发布InfraLiDARs Benchmark数据集以促进进一步研究。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [10] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: SurgVeo是首个专家策划的手术视频生成模型评估基准，结合手术可信度金字塔框架，评估发现Veo-3模型在视觉感知可信度方面表现优异，但在更高层次的器械操作、环境反馈和手术意图可信度方面存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 视频生成基础模型在模拟物理世界方面表现出色，但在需要深度专业因果知识的高风险领域如手术中应用仍存在关键空白。

Method: 提出SurgVeo基准和手术可信度金字塔框架，使用Veo-3模型在腹腔镜和神经外科手术片段上进行零样本预测任务，由四位认证外科医生按金字塔框架评估生成视频。

Result: 发现明显的"可信度差距"：Veo-3在视觉感知可信度方面表现卓越，但在器械操作可信度、环境反馈可信度和手术意图可信度等更高层次上严重失败。

Conclusion: 这项工作首次量化证明了手术AI中视觉逼真模仿与因果理解之间的鸿沟，为开发能够应对专业医疗领域复杂性的未来模型奠定了基础和路线图。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [11] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5是新一代物理AI世界基础模型，基于流式架构统一文本、图像、视频到世界的生成，结合Cosmos-Reason1提供更丰富的文本基础和精细的世界模拟控制。


<details>
  <summary>Details</summary>
Motivation: 开发更可靠的人工智能系统需要高质量的合成数据生成、策略评估和闭环模拟能力，特别是在机器人学和自主系统领域。

Method: 采用流式架构，结合物理AI视觉语言模型Cosmos-Reason1，在2亿个精选视频剪辑上训练，并使用强化学习进行后训练优化。

Result: 相比Cosmos-Predict1在视频质量和指令对齐方面有显著提升，提供2B和14B规模模型，同时Cosmos-Transfer2.5在Sim2Real和Real2Real转换中表现更优。

Conclusion: 这些进展使Cosmos-Predict2.5和Cosmos-Transfer2.5成为扩展具身智能的多功能工具，通过开源资源降低采用门槛，促进物理AI领域的创新。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [12] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: 使用深度学习进行高山栖息地变化检测，比较了后分类变化检测和直接变化检测两种方法，发现Clay v1.0模型在复杂自然环境中表现最佳，准确率达到51%。


<details>
  <summary>Details</summary>
Motivation: 高山生态系统快速气候变化需要频繁的栖息地监测，但人工测绘成本过高，需要自动化解决方案来满足高时间分辨率的需求。

Method: 采用两种变化检测范式：后分类变化检测（使用Prithvi-EO-2.0、Clay v1.0和U-Net CNN）和直接变化检测（使用ChangeViT和U-Net基线）。使用多模态高分辨率数据（RGB、近红外、LiDAR、地形属性）。

Result: Clay v1.0在多类栖息地变化检测中达到51%的总体准确率，U-Net为41%；二元变化检测两者均达到67%。直接变化检测在二元检测中IoU更高（0.53 vs 0.35），但多类检测准确率仅为28%。集成LiDAR将语义分割准确率从30%提升到50%。

Conclusion: 尽管在复杂高山栖息地中的总体准确率低于均质景观，但反映了真实性能。未来工作将集成基于对象后处理和物理约束以增强适用性。

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [13] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa是一种无需训练的高效扩散视频生成加速框架，通过词典最小最大路径优化策略显著提升生成质量与推理速度，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有缓存策略主要关注减少局部启发式误差，但忽略了全局误差累积，导致加速视频与原始视频之间存在明显内容退化问题。

Method: 将缓存调度建模为带误差权重边的有向图，引入词典最小最大路径优化策略，明确限制最坏情况路径误差。

Result: 在Latte模型上实现2.9倍加速，在Open-Sora上达到0.05的LPIPS分数，优于现有缓存技术，且感知质量退化最小。

Conclusion: LeMiCa为加速扩散视频生成提供了一个鲁棒且可泛化的范式，可作为未来高效可靠视频合成研究的坚实基础。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [14] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: PLD是一个三阶段即插即用框架，通过残差强化学习和分布感知数据收集来改进视觉语言动作模型，在多个任务上实现了接近饱和的性能提升。


<details>
  <summary>Details</summary>
Motivation: 监督微调依赖昂贵的人工演示，限制了视觉语言动作模型的可扩展性和泛化能力，需要更高效的改进方法。

Method: 三阶段框架：1)训练轻量残差actor探测失败区域；2)混合rollout方案收集与部署分布对齐的轨迹；3)通过标准SFT将精选轨迹蒸馏回通用模型。

Result: 在LIBERO上达到99%任务成功率，SimperEnv提升50%以上，真实世界机械臂任务实现100%成功率。

Conclusion: 残差探测和分布感知回放是收集部署对齐数据的关键，为自改进VLA模型提供了可扩展路径。

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [15] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: 提出SpinalSAM-R1，一个结合微调SAM和DeepSeek-R1的多模态视觉语言交互系统，用于脊柱CT图像分割，通过解剖学引导的注意力机制和语义驱动交互协议提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 脊柱CT图像分割在脊柱疾病诊断和治疗中至关重要，但面临低对比度和复杂椎体边界的挑战。现有模型如SAM在脊柱CT成像中受限于高标注要求和差的领域适应性。

Method: 集成微调SAM与DeepSeek-R1，引入解剖学引导的注意力机制和语义驱动交互协议，使用LoRA进行高效微调，支持点、框和文本提示的交互软件。

Result: 在脊柱解剖结构CT图像上验证，获得优越的分割性能，开发了基于PyQt5的交互软件，支持11种临床操作，解析准确率94.3%，响应时间低于800ms。

Conclusion: SpinalSAM-R1通过多模态视觉语言交互有效解决了脊柱CT图像分割的挑战，提供了高效准确的解决方案，并发布了开源软件。

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [16] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI是一个新颖的分层文本到视频生成框架，通过组合场景解析、时空注意力机制和渐进式视频精炼，显著提升了视频质量和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在保持时间一致性、组合理解和视觉叙事细粒度控制方面存在困难，需要更先进的解决方案。

Method: 提出三个关键技术：组合场景解析器将文本分解为带时间标注的层次场景图；时空注意力机制确保连贯运动动态；渐进式视频精炼模块通过多尺度时间推理迭代提升视频质量。

Result: 在标准基准测试中，MOVAI实现了最先进性能，LPIPS指标提升15.3%，FVD提升12.7%，用户偏好研究提升18.9%。

Conclusion: MOVAI框架在生成复杂多对象场景方面表现出色，具有现实的时间动态和细粒度语义控制能力。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [17] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: 提出了一种用于共聚焦激光内窥镜视频序列的过滤方法，以减少自监督学习训练中的数据集冗余，提高训练效率和收敛性。


<details>
  <summary>Details</summary>
Motivation: CLE图像对非专业医生难以解读，机器学习可辅助诊断但面临数据不足导致的过拟合问题。自监督学习可解决数据标注不足，但CLE视频帧间相关性高导致数据分布不均衡。

Method: 在CLE视频序列上应用过滤功能减少SSL训练中的冗余，使用四种先进基线网络和基于视觉变换器的师生SSL网络进行评估。

Result: 在两个数据集上，过滤后的SSL预训练模型获得最高测试准确率（67.48%和73.52%），显著优于非SSL基线，训练时间减少67%。

Conclusion: SSL是CLE预训练的有效方法，提出的CLE视频过滤器可提高自监督学习场景下的训练效率。

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [18] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders是一种无需训练、模态无关的方法，通过部分估计概念滑块公式实现细粒度可控生成，支持图像、视频和音频等多种模态。


<details>
  <summary>Details</summary>
Motivation: 现有的概念滑块方法需要针对每个概念进行训练和架构特定的微调，限制了在新模态上的可扩展性。

Method: 在推理过程中部分估计概念滑块公式，无需训练且模态无关。提出两阶段程序自动检测饱和点并重新参数化遍历。

Result: 实验表明该方法能够实现即插即用、无需训练的概念控制，优于现有基线方法。

Conclusion: FreeSliders为可控生成提供了新的工具，建立了首个多模态细粒度概念生成控制基准。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [19] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 提出了一种名为"Chain of Time"的认知启发式方法，通过在模拟过程中生成一系列中间图像来改进和解释视觉语言模型中的物理模拟，无需额外微调即可在推理时使用。


<details>
  <summary>Details</summary>
Motivation: 受到机器学习中的上下文推理和人类心理模拟的启发，旨在改进视觉语言模型对物理过程的模拟能力，并更好地理解模型的内部模拟机制。

Method: Chain-of-Time方法在推理时生成模拟过程中的中间图像序列，应用于2D图形模拟和真实3D视频等多个领域，测试速度、加速度、流体动力学和动量守恒等物理属性。

Result: 使用Chain-of-Time方法显著提升了最先进图像生成模型的性能，分析揭示了模型能够模拟随时间展开的物理属性（如速度、重力和碰撞），但也发现模型在某些情况下难以从输入图像推断特定物理参数。

Conclusion: Chain-of-Time方法不仅提高了物理模拟性能，还提供了对模型内部模拟动态的深入洞察，揭示了传统物理推理评估中隐藏的模型能力与局限。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [20] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: ViC是一个无需训练、基于视觉语言模型的零样本推理框架，用于重新排序和融合异构检索器的结果，在跨模态视频检索中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构检索器候选结果融合的长期挑战，特别是在复杂多模态数据（如视频）中，传统融合方法仅依赖排名或分数信号而忽略候选表示。

Method: 提出Vote-in-Context框架，将内容证据和检索器元数据序列化到VLM提示中，使模型能自适应权衡检索器共识与视觉语言内容；引入S-Grid紧凑序列化映射来表示视频。

Result: 在视频检索基准测试中，ViC作为单列表重新排序器和集成融合器均表现出色，在MSR-VTT上达到87.1%（t2v）/89.0%（v2t）的Recall@1，在VATEX上达到99.6%（v2t），比之前最先进方法提升高达+40 Recall@1。

Conclusion: ViC是将现代VLM转变为强大零样本重新排序器和融合器的简单、可复现且高效的方案。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [21] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 提出了首个集成生成AI和深度强化学习的端到端框架，用于实现自主、可重复的心脏超声扫描，解决了现有方法缺乏可重复性、依赖专有数据和使用简化模型的问题。


<details>
  <summary>Details</summary>
Motivation: 心脏超声检查存在操作者依赖性、时间限制和人为错误等问题，且训练有素的专业人员短缺，特别是在偏远地区。需要自动化解决方案来确保一致且可访问的心脏成像。

Method: 框架包含两个组件：(1) 结合GAN和VAE的条件生成模拟器，生成逼真的动作条件图像；(2) 利用该模拟器学习自主、准确扫描策略的深度强化学习模块。

Result: VAE-GAN在现有GAN变体中表现出色，通过定性和定量方法评估性能；基于DRL的扫描系统在不同配置下验证了有效性。

Conclusion: 该框架通过专家验证模型提供AI驱动的指导，支持生成逼真的超声图像，并建立了可扩展到其他器官的可重复基础，同时发布了公开可用的真实心脏超声扫描数据集以确保可重复性。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [22] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D提出了一种新颖的双流架构，利用RGB-D输入的视觉和几何数据优势，通过自监督视觉变换器和PointNet++编码器分别处理RGB和点云数据，在遮挡严重的情况下实现鲁棒的6D物体姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中6D物体姿态估计的挑战，现有方法在从合成数据泛化到真实场景时存在脆弱性，难以应对光照变化、无纹理物体和严重遮挡等问题。

Method: 采用双流架构：使用自监督Vision Transformer处理RGB数据，利用预训练的视觉理解能力；同时使用PointNet++编码器处理深度数据生成的点云，进行几何推理。两种特征流有效融合后输入多任务预测头。

Result: 在具有挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能，验证了其在鲁棒性和准确性方面的优越表现。

Conclusion: VLM6D通过有效整合视觉和几何信息，在严重遮挡等复杂场景下实现了鲁棒且精确的6D物体姿态估计，为计算机视觉中的姿态估计问题提供了有效的解决方案。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [23] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 提出了一种结合ConvNeXt和Vision Transformer的混合架构用于面部年龄估计，在多个基准数据集上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 利用CNN的局部特征提取能力和Transformer的全局注意力机制的互补优势，解决面部年龄估计这一复杂计算机视觉挑战。

Method: 使用预训练模型，通过线性层和高级正则化技术优化ConvNeXt-ViT混合架构，并在CNN框架中采用适配的注意力机制来关注年龄相关面部特征。

Result: 在MORPH II、CACD和AFAD等基准数据集上，该混合架构在平均绝对误差(MAE)方面表现优于传统方法。

Conclusion: 混合架构展示了CNN和Transformer无缝集成的变革潜力，为年龄估计及相关视觉任务提供了有前景的发展方向。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [24] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: FLoC是一个基于设施位置函数的高效视觉token压缩框架，通过选择紧凑且具有代表性的视觉token子集来解决长视频理解中的token数量爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 长视频理解中，大型多模态模型面临视觉token数量过多的可扩展性限制，需要高效的token压缩方法。

Method: 使用设施位置函数和惰性贪心算法，在预定义预算内快速选择具有代表性和多样性的视觉token子集，无需训练且与模型和查询无关。

Result: 在大规模基准测试（Video-MME、MLVU、LongVideoBench）中，FLoC持续超越现有压缩技术，在减少视觉token数量的同时保持接近最优性能，处理速度高效。

Conclusion: FLoC提供了一个训练无关、模型无关的通用解决方案，有效解决了长视频理解中的关键挑战，在效果和效率方面都表现出色。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [25] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: 提出一种通过自适应高斯模糊增强图像保护方法对抗噪声反转技术的方法，提高现有保护方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图像保护方法中的对抗噪声容易被简单技术（如JPEG压缩）反转，需要开发更鲁棒的不可逆保护方法。

Method: 应用自适应逐区域高斯模糊来调整噪声的总体频率谱，增强对抗反转技术的鲁棒性。

Result: 实验表明该方法能一致提高现有方法在各种图像编辑场景下对抗多种反转技术的保护性能，同时减少噪声引起的质量退化。

Conclusion: 提出的自适应模糊方法有效增强了图像保护方法的鲁棒性，为对抗恶意图像编辑提供了更实用的解决方案。

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [26] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: 提出了CompAgent，首个用于视觉合规验证的智能体框架，通过增强多模态大语言模型(MLLMs)的视觉工具集和智能规划，显著提升了视觉内容合规性检测的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉合规验证在媒体、娱乐和广告等领域至关重要，但现有方法依赖特定任务的深度学习模型，构建成本高且泛化能力有限。MLLMs虽然具有广泛知识，但难以处理细粒度视觉细节和结构化合规规则。

Method: CompAgent框架包含规划智能体和验证智能体。规划智能体根据合规策略动态选择视觉工具（如物体检测器、人脸分析器、NSFW检测器等），验证智能体整合图像、工具输出和策略上下文进行多模态推理。

Result: 在公开基准测试中，CompAgent优于专用分类器、直接MLLM提示和精心设计的路由基线，在UnsafeBench数据集上达到76%的F1分数，比现有最优方法提升10%。

Conclusion: 研究证明了智能体规划和工具增强推理在可扩展、准确和适应性强的视觉合规验证中的有效性。

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [27] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: AIFo是一个基于多智能体协作的训练免费框架，通过模拟人类取证调查过程来检测AI生成图像，在6000张图像的测试中达到97.05%的准确率。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的快速发展对信息完整性和媒体真实性构成挑战，现有检测方法存在可解释性差、泛化能力不足等问题。

Method: 采用多智能体协作框架，结合反向图像搜索、元数据提取、预训练分类器和视觉语言模型分析等取证工具，通过结构化多智能体辩论机制解决证据冲突。

Result: 在包含现代生成平台和多样化在线来源的6000张图像测试中，AIFo达到97.05%的准确率，显著优于传统分类器和最先进的视觉语言模型。

Conclusion: 基于智能体的程序推理为AI生成图像检测提供了更鲁棒、可解释和自适应的新范式。

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [28] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 本文提出了基于能量的多提示学习（EMPL）方法，通过从能量分布中采样生成多个提示嵌入，实现了参数高效且平衡的领域内外开放词汇泛化。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单提示范式，很少探索多提示学习的技术潜力。本文旨在为视觉语言多提示学习提供原则性回顾，并证明多提示增强在视觉语言迁移中的优越性。

Method: 将最近提出的恒定模态间隙现象扩展到可学习提示，提出基于能量的多提示学习（EMPL），通过从能量分布中采样生成多个提示嵌入。

Result: 综合实验验证了作者的主张和EMPL方法的优越性。

Conclusion: EMPL不仅参数高效，而且严格实现了领域内和领域外开放词汇泛化之间的平衡。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [29] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 提出一种高效的迁移学习方法，用于检测卫星互联网地面终端组件上的天气相关状况（如积雪、潮湿等），相比传统深度学习方法表现更优且具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 天气事件对低轨卫星互联网的性能和可靠性有显著影响，需要细粒度的地面终端组件天气状况检测能力来协助故障诊断和缓解，但目前缺乏有效的解决方案。

Method: 采用高效的迁移学习方法，使地面组件能够本地检测代表性的天气相关状况，包括积雪、潮湿等由恶劣和典型天气事件导致的状况。

Result: 所提出的迁移学习方法在检测雪、湿和其他天气相关状况方面表现出色，性能优于YOLOv7、YOLOv9、Faster R-CNN和R-YOLO等典型深度学习方法，且具有更好的泛化能力。

Conclusion: 该迁移学习方法能够有效检测卫星互联网地面终端组件的天气相关状况，为可靠的卫星互联网提供故障诊断和缓解支持，在实际部署中具有重要应用价值。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [30] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: DM-QPMNet是一个双编码器网络，通过分别编码偏振强度图像和相位图作为不同模态，使用多头注意力在中间深度融合模态特定特征，实现稳健的细胞分割。


<details>
  <summary>Details</summary>
Motivation: 传统阈值方法对噪声和细胞密度敏感，而简单通道拼接的深度学习方法无法充分利用偏振强度图像和相位图的互补特性。

Method: 采用双编码器网络分别处理偏振强度图像和相位图，通过多头注意力在中间深度进行内容感知融合，使用双源跳跃连接和每模态归一化。

Result: 相比单一拼接和单模态基线方法，该方法在细胞分割方面表现出显著改进。

Conclusion: 模态特定编码与可学习融合能有效利用ssQPM同时捕获的互补照明和相位线索，实现稳健的细胞分割。

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [31] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: 提出了基于VQ-VAE的电子显微镜图像压缩框架，支持16x到1024x压缩比，具有按需解码功能，并引入ROI驱动的工作流程实现选择性高分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 解决海量电子显微镜数据在存储、传输和下游分析方面的挑战，提供高效的压缩方案。

Method: 使用向量量化变分自编码器(VQ-VAE)进行压缩，结合Transformer先验模型预测底部token，通过FiLM和拼接恢复纹理，并采用ROI驱动的工作流程。

Result: 实现了从16x到1024x的压缩比，支持按需解码和选择性高分辨率重建。

Conclusion: 该压缩框架能够有效处理海量电子显微镜数据，提供灵活的压缩和重建能力。

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [32] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: 提出了一种在双曲空间中计算最优传输映射的新算法，扩展了欧几里得和球面几何的方法，适用于层次数据、网络和多亏格黎曼曲面等场景。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输计算方法主要针对欧几里得空间和球面，而双曲空间在层次数据、网络和多亏格黎曼曲面等应用中自然出现，需要专门的计算方法。

Method: 使用几何变分技术，将欧几里得和球面几何的方法扩展到双曲空间设置，提出新颖高效的计算算法。

Result: 在合成数据和多亏格曲面模型上进行了实验，验证了所提方法的有效性。

Conclusion: 成功开发了双曲空间中的最优传输映射计算方法，填补了现有方法的空白，并在实验中证明了其效能。

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [33] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 提出了一个基于3D高斯表示和运动扩散先验的对象感知4D人体运动生成框架MSDI，通过运动扩散分数蒸馏采样和大型语言模型实现零样本的物理合理人体运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型生成的视频存在不真实变形、语义违规和物理不一致问题，主要原因是缺乏3D物理先验。

Method: 使用预生成的3D人体和对象，结合大型语言模型的空间和提示语义信息，通过提出的运动扩散分数蒸馏采样进行空间感知运动优化。

Result: 实验表明该框架能生成自然且物理合理的人体运动，尊重3D空间上下文。

Conclusion: 该方法为现实4D生成提供了可扩展的解决方案，无需重新训练即可泛化到分布外对象感知人体运动。

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [34] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 本文介绍了L48数据集，这是一个从鸟类声音记录中提取的细粒度多标签数据集，提供了真实的单正多标签（SPML）设置，并评估了现有SPML方法在该数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的SPML方法都是在合成数据集上开发和评估的，这些数据集通过从完全标注数据集中随机采样单正标签创建，不能反映真实世界场景和细粒度复杂性。

Method: 构建了L48数据集，这是一个基于鸟类声音记录的细粒度多标签数据集，提供自然的SPML设置和两个扩展设置（利用领域先验获得额外负标签）。

Result: 在L48数据集上评估现有SPML方法，发现与合成数据集相比存在显著性能差异，并分析了方法的弱点。

Conclusion: 需要更现实和困难的基准来推动SPML研究的发展。

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [35] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 开发了一个3阶段自动化流水线来处理大规模甲虫图像数据，包括检测、裁剪和形态分割，旨在提高生物学研究效率。


<details>
  <summary>Details</summary>
Motivation: 在昆虫学和生态学研究中，生物学家需要处理大量甲虫图像数据，传统人工处理效率低下，需要自动化流水线来加速研究进程。

Method: 使用基于transformer的开放词汇目标检测器和视觉语言模型进行迭代检测，手动标注670张甲虫图像并微调两个transformer分割模型变体进行精细分割。

Result: 构建了专门用于甲虫图像处理的深度学习流水线，能够相对准确地实现甲虫的检测和形态分割。

Conclusion: 该流水线整合了多种深度学习方法，专门针对甲虫图像处理，能显著提高大规模甲虫数据处理效率，加速生物学研究。

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [36] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出MambaNetLK框架，通过集成Mamba状态空间模型作为跨模态特征提取器，改进PointNetLK架构，在临床数据集C3VD-Raycasting-10k上实现最佳性能，旋转误差降低56.04%，平移误差降低26.19%。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜导航中3D点云配准的挑战：生物组织重复纹理和局部均匀几何导致特征退化，术前解剖与术中观察之间的显著域偏移进一步降低配准稳定性。

Method: 提出MambaNetLK框架，集成Mamba状态空间模型作为跨模态特征提取器，增强PointNetLK架构，利用Lucas-Kanade算法进行迭代配准，高效捕捉长程依赖关系。

Result: 在临床数据集C3VD-Raycasting-10k上表现最佳，中值旋转误差降低56.04%，RMSE平移误差降低26.19%，在ModelNet40上展示强泛化能力，对初始姿态扰动具有优越鲁棒性。

Conclusion: MambaNetLK为手术导航中的3D配准提供了稳健基础，全局表达性SSM特征提取器与大规模临床数据集的结合使结肠镜等微创手术中的引导系统更加准确可靠。

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [37] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: 提出了Spot The Ball基准测试，用于评估视觉语言模型在视觉社交推理能力，通过定位被移除的体育球来测试模型从人类行为线索中推断隐藏场景元素的能力。


<details>
  <summary>Details</summary>
Motivation: 人类擅长从微妙的行为线索（如注视、姿势和朝向）推断场景的隐藏元素，这种视觉社交推理能力对于开发更类似人类的AI代理至关重要。

Method: 使用足球、篮球和排球图像构建评估集，要求定位被移除的球。评估了四种最先进的视觉语言模型（Gemini、GPT、LLaMA、Qwen）和三种提示策略，并与人类基线进行比较。

Result: 人类在所有运动中的准确率（20-34%）都比模型（≤17%）高出两到三倍。模型依赖表面的空间启发式方法，而人类利用社交线索如注视方向和身体姿势。

Conclusion: 揭示了视觉社交推理中持续存在的人机差距，强调需要显式编码结构化行为线索的架构来实现稳健、类似人类的推理。

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [38] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 提出一个结合CLIP视觉变换器和轻量级分类器的联邦学习框架，用于农业分类任务，在保护隐私的同时显著提升准确率并降低通信成本


<details>
  <summary>Details</summary>
Motivation: 解决传统集中式训练的数据隐私问题，以及标准联邦学习在处理非独立同分布数据时性能下降和通信成本高的问题

Method: 使用预训练的CLIP ViT进行特征提取，仅对轻量级分类器进行联邦更新，并共享1%的CLIP特征来对齐类别表示

Result: 在农业分类任务上达到86.6%的准确率，比基线联邦学习方法高出4倍以上

Conclusion: 结合视觉语言模型特征与联邦学习，为保护隐私和可扩展的农业智能提供了有效且高效的解决方案

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [39] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: 提出了一种轻量级的多视图生成方法PersonalView，仅需100个训练样本即可让现有模型具备多视图生成能力，显著优于需要大量多视图数据训练的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成模型虽然能生成身份一致的人物图像，但无法控制生成图像的视角，也不能生成一致的多视图图像。

Method: 包含两个关键组件：1）利用预训练扩散变换器的上下文学习能力设计条件架构；2）使用语义对应对齐损失保持预训练模型的原始生成能力。

Result: 在多视图一致性、文本对齐、身份相似性和视觉质量方面显著优于基线方法，仅用100个训练样本就超越了需要大量多视图数据训练的基线。

Conclusion: PersonalView是一种高效的多视图生成方法，能够以极少的训练样本实现高质量的多视图个性化生成。

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [40] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: LITHOS是最大的公开自动化岩石学实验框架，包含211,604个高分辨率RGB偏振光图像块和105,802个专家标注的矿物颗粒，涵盖25个矿物类别。提出了双编码器transformer架构，整合两种偏振模式，在矿物分类中优于单偏振模型。


<details>
  <summary>Details</summary>
Motivation: 岩石学分析是劳动密集型任务，需要专家通过光学偏振显微镜进行详细视觉检查，限制了可扩展性，迫切需要自动化技术。

Method: 引入LITHOS数据集，包含偏振光图像和专家标注的矿物颗粒。提出双编码器transformer架构，整合两种偏振模式进行矿物分类。

Result: 双编码器transformer模型在矿物分类任务中持续优于单偏振模型，证明了偏振协同在矿物分类中的价值。

Conclusion: LITHOS基准测试已公开，包括数据集、代码和预训练模型，以促进自动化岩石学分析的可重复性和进一步研究。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [41] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: 该研究系统评估了11种轻量级视觉模型在7个数据集上的跨数据集泛化能力，提出了跨数据集评分(xScore)指标，发现ImageNet性能不能可靠预测其他领域表现，并识别了促进泛化的关键架构组件。


<details>
  <summary>Details</summary>
Motivation: 轻量级视觉模型主要基于ImageNet基准评估，但缺乏对其他领域泛化能力的系统研究。需要量化跨数据集鲁棒性，并识别在资源约束下驱动泛化的架构要素。

Method: 在11种轻量级视觉模型(250万参数)上，采用固定的100轮训练计划，在7个多样化数据集上进行系统评估。引入跨数据集评分(xScore)来量化模型在不同视觉领域的一致性和鲁棒性。

Result: 1) ImageNet准确率不能可靠预测细粒度或医学数据集性能；2) xScore可作为移动模型性能的可扩展预测指标，仅需4个数据集即可估计；3) 各向同性卷积、高空间分辨率和通道注意力等组件促进泛化，而Transformer模块带来额外收益有限但参数开销更高。

Conclusion: 该研究提供了在ImageNet之外评估轻量级视觉模型的可复现框架，强调了移动友好架构的关键设计原则，并为开发跨领域鲁棒泛化的未来模型提供指导。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [42] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 提出了一种结合DeepONet和NTK的混合方法来解决复杂逆问题，包括Navier-Stokes方程控制的源定位和图像重建，能有效处理非线性、稀疏性和噪声数据。


<details>
  <summary>Details</summary>
Motivation: 解决复杂逆问题中的非线性、稀疏性和噪声数据挑战，确保解的物理一致性和准确性。

Method: 将DeepONet与NTK结合，在损失函数中加入物理约束和任务特定正则化。

Result: 在多种合成和真实数据集上的验证表明该方法具有鲁棒性、可扩展性和精确性。

Conclusion: 该方法在计算物理和成像科学领域具有广泛的应用潜力。

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [43] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: FedDISC是一个联邦学习框架，用于解决多模态情感识别中模态缺失问题，通过联邦聚合模态特定的扩散模型，确保恢复模态与可用模态在语义上的一致性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态信号的不确定性缺失会显著降低现有方法的性能，传统基于完整多模态数据的恢复方法在极端数据分布下容易出现语义失真。

Method: 提出FedDISC框架，集成联邦学习到模态缺失恢复中，使用DISC-Diffusion模块确保恢复模态与可用模态在上下文、说话人身份和语义上的一致性，并采用交替冻结聚合策略。

Result: 在IEMOCAP、CMUMOSI和CMUMOSEI数据集上的实验表明，FedDISC在不同模态缺失模式下实现了优越的情感分类性能，优于现有方法。

Conclusion: FedDISC通过联邦学习和语义一致的扩散模型，有效解决了多模态情感识别中的模态缺失问题，提升了模型的鲁棒性和性能。

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [44] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: OSMGen是一个生成框架，能够直接从OpenStreetMap原始数据生成逼真的卫星图像，支持生成前后对比图像对，用于解决训练数据稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 准确和最新的地理空间数据对城市规划、基础设施监测和环境管理至关重要，但特定城市特征及其变化的标注数据集稀缺，自动化城市监测面临困难。

Method: 使用OSM JSON的完整丰富信息（包括矢量几何、语义标签、位置和时间），通过生成框架从原始OSM数据直接创建卫星图像，支持用户编辑OSM输入产生针对性视觉变化。

Result: 能够生成一致的before-after图像对，为训练数据生成和规划干预预览提供解决方案，并产生配对的（JSON，图像）数据。

Conclusion: OSMGen为构建闭环系统铺平了道路，使卫星图像能够自动驱动结构化的OSM更新，源代码已公开。

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [45] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型重建动态的AI生成图像检测方法，通过分析不同噪声强度下的图像重建指标变化来区分真实图像和合成图像。


<details>
  <summary>Details</summary>
Motivation: 随着生成式扩散模型的快速发展，传统基于频率或像素级伪影的深度伪造检测方法已无法有效应对Stable Diffusion和DALL-E等现代文本到图像系统生成的逼真无伪影图像。

Method: 利用多强度图像重建动态（称为扩散回弹），通过分析LPIPS、SSIM和PSNR等重建指标在不同噪声强度下的演变，提取可解释的基于流形的特征来区分真实和合成图像。

Result: 在包含4000张图像的平衡数据集上评估，该方法在交叉验证下达到0.993 AUROC，并对压缩和噪声等常见失真保持鲁棒性。

Conclusion: 尽管使用有限数据和单一扩散骨干网络（Stable Diffusion v1.5），该方法展现出强大的泛化能力和可解释性，为可扩展、模型无关的合成媒体取证提供了基础。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [46] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: 该研究通过迁移学习和轻量级UNet架构，在FOREST-2立方星上实现了仅使用热红外波段的高效云分割，推理时间小于5秒，F1分数从0.850提升至0.877。


<details>
  <summary>Details</summary>
Motivation: 解决立方星任务中硬件资源有限、缺乏标记数据和仅使用单一热红外波段进行云分割的挑战，支持实时在轨决策。

Method: 使用UNet与轻量级MobileNet编码器，先在Landsat-7云覆盖评估数据集上预训练，然后用少量任务特定样本进行联合训练微调，最后转换为TensorRT引擎。

Result: 在FOREST-2立方星上，宏F1分数从仅使用任务数据的基线0.850提升至0.877，在NVIDIA Jetson Nano上全图像推理时间小于5秒。

Conclusion: 利用公开数据集和轻量级架构可以在数据有限的EO任务中实现准确、高效的仅热红外云分割，支持实时在轨决策。

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [47] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Oitijjo-3D是一个免费生成式AI框架，利用Google街景图像重建孟加拉国文化遗产的3D模型，解决了传统3D数字化方法成本高、技术要求高的问题。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国文化遗产修复面临资源有限和技术专家稀缺的双重挑战，传统3D数字化方法（如摄影测量或激光雷达扫描）需要昂贵硬件、专家操作和大量现场访问，这在发展中国家往往不可行。

Method: 采用两阶段流程：使用Gemini 2.5 Flash Image进行多模态视觉推理实现结构-纹理合成，通过Hexagen进行神经图像到3D生成实现几何恢复。

Result: 系统在几秒钟内生成逼真、度量一致的重建结果，相比传统结构从运动流程实现显著加速，无需任何专业硬件或专家监督。在Ahsan Manzil、Choto Sona Mosque和Paharpur等标志性建筑上的实验表明，Oitijjo-3D在保持视觉和结构保真度的同时大幅降低了经济和技术门槛。

Conclusion: 通过将开放图像转化为数字遗产，这项工作将保护重新定义为资源有限国家中社区驱动、AI辅助的文化连续性行为。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [48] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 提出基于强化学习的视频时刻检索模型，通过多智能体系统和证据学习解决不同模型定位结果的冲突问题，无需额外训练即可判断查询是否超出范围。


<details>
  <summary>Details</summary>
Motivation: 现有视频时刻检索方法未考虑不同模型定位结果的冲突问题，导致多种模型无法有效集成以获得更好的结果。

Method: 使用强化学习模型扫描整个视频找到时刻边界并生成位置证据，提出多智能体系统框架，利用证据学习解决智能体定位输出的冲突。

Result: 在基准数据集上的大量实验表明，所提方法相比最先进方法具有有效性，多智能体系统的竞争和冲突建模能有效提升强化学习在时刻检索中的性能。

Conclusion: 多智能体系统的竞争和冲突建模是提升强化学习在时刻检索中性能的有效方法，证据学习在多智能体框架中展现了新的作用。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [49] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD是一个基于视觉的放射辅助框架，通过摄像头直接从显示器捕获医学图像，无需修改现有医院IT基础设施即可实现AI辅助诊断。


<details>
  <summary>Details</summary>
Motivation: 传统计算机辅助诊断系统难以与现有医院IT基础设施集成，阻碍了其广泛临床应用。VisionCAD旨在通过视觉方法绕过这一障碍。

Method: 采用自动化流水线检测、恢复和分析屏幕上的医学图像，将摄像头捕获的视觉数据转换为适合自动分析和报告生成的诊断质量图像。

Result: 在多种医学影像数据集上验证，诊断性能与传统CAD系统相当，分类任务F1分数下降通常小于2%，自动报告的自然语言生成指标与原始图像相比差异在1%以内。

Conclusion: VisionCAD仅需摄像头设备和标准计算资源，为AI辅助诊断提供了可访问的方法，可在各种临床环境中部署诊断能力，无需修改现有基础设施。

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [50] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: FERBench基准测试显示，尽管多模态大语言模型在面部表情识别任务中表现良好，但在推理和可解释性方面存在显著局限。为此，作者开发了UniFER-7B模型，通过后训练策略显著提升了面部表情推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在多个领域取得成功，但其在面部表情识别任务中的表现尚未得到系统评估。现有方法将FER数据集转换为VQA格式，但模型的推理和可解释性能力仍需提升。

Method: 构建FERBench基准测试20个最先进MLLMs，创建UniFER-CoT-230K和UniFER-RLVR-360K两个高质量数据集，采用思维链初始化和可验证奖励强化学习策略，开发UniFER-7B统一可解释FER基础模型。

Result: UniFER-7B模型在FER任务中超越了包括Gemini-2.5-Pro和Qwen2.5-VL-72B在内的多个开源和闭源通用MLLMs，显著提升了面部表情推理能力。

Conclusion: 通过专门设计的后训练策略和高质量数据集，可以显著提升MLLMs在面部表情识别任务中的推理和可解释性能力，UniFER-7B模型为此提供了有效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [51] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: VinciCoder是一个统一的多模态代码生成模型，通过两阶段训练框架解决现有视觉语言模型在代码生成任务中的局限性，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在代码生成任务中依赖单任务训练，这种狭隘的范式阻碍了通用视觉代码智能的发展。

Method: 采用两阶段训练框架：首先构建包含160万图像-代码对的监督微调语料库，然后引入视觉强化学习策略，通过从粗到细的奖励机制计算局部和全局图像块的视觉相似度。

Result: 在各种多模态代码生成基准测试中，VinciCoder实现了最先进的性能。

Conclusion: VinciCoder通过两阶段训练框架和从粗到细的视觉强化学习策略，有效提升了多模态代码生成的性能，证明了该方法的有效性。

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [52] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 提出了首个统一框架，通过将SOD、CoSOD和SIS三个异构显著性任务建模为VLM中的思维链推理过程，解决了任务异质性问题。采用两阶段训练范式（SFT+RL），并提出了CGPO算法来提升RL中的CoT质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常为每个显著性任务设计专门模型，缺乏统一的处理框架。本文旨在通过VLM中的思维链推理来桥接任务异质性，实现多任务的统一处理。

Method: 1. 将三个显著性任务建模为VLM中的CoT推理过程；2. 采用两阶段训练：SFT和RL；3. 提出CGPO算法，利用奖励与模型置信度差异作为优势信号；4. 引入"输出到推理"策略构建高质量SFT数据。

Result: 模型在所有任务上匹配或优于专门的SOTA方法和强大的闭源VLM，特别是在CoSOD任务上，在CoCA数据集上达到0.899的S-measure，比之前最佳方法提升8.0个百分点，且使用更少的训练数据。

Conclusion: 提出的统一框架成功解决了显著性任务的异质性问题，通过CoT推理和CGPO算法实现了优异的性能，证明了VLM在多任务显著性分析中的潜力。

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [53] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: 提出了LGCA框架，通过局部特征捕获和显著区域扩展来改进CLIP模型的零样本图像分类性能，减少随机裁剪带来的信息偏差。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在零样本图像分类中表现优异，但随机图像裁剪会引入错误信息和偏差，因为小尺度下许多图像具有相似特征。

Method: LGCA框架首先捕获图像的局部特征，然后重复选择最显著区域并扩展它们。相似度评分结合原始图像和扩展图像，同时捕捉局部和全局特征。

Result: 大量实验表明，该方法在多个数据集上显著提升了零样本性能，优于现有最先进的基线方法。

Conclusion: LGCA框架通过局部-全局交叉对齐有效减少了CLIP模型中的信息偏差，同时保持了与原始模型相同的时间复杂度，具有高效性和可扩展性。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [54] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出ITEM方法，利用图像-文本不对齐作为检测生成假图像的线索，通过CLIP空间测量不对齐度，结合全局和局部语义分析实现更好的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注视觉线索，容易过拟合特定图像模式，无法泛化到未见过的生成模型。发现假图像与对应描述的对齐度不如真实图像

Method: 在预训练CLIP空间中测量图像和描述的不对齐度，使用MLP头进行分类，并提出分层不对齐方案：先关注整张图像，再关注描述中的每个语义对象

Result: 在多个最新生成模型上表现出优异的泛化能力和鲁棒性，优于其他最先进方法

Conclusion: 多模态图像-文本不对齐是检测生成假图像的有效线索，ITEM方法简单有效，具有很好的泛化性能

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [55] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出一种基于频率伪造线索(F^2C)的扩散模型图像检测方法，通过增强全频段的频率差异特征来提高对未知扩散模型的泛化检测能力和抗干扰鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像质量很高，但可能被恶意使用。现有检测器难以捕捉不同模型和设置下的判别性线索，泛化能力和鲁棒性有限。

Method: 观察到扩散生成图像与真实图像在低到高频段存在渐进差异，提出频率选择性函数作为加权滤波器，抑制判别性较弱的频段，增强信息丰富的频段。

Result: 在多个扩散生成图像数据集上的实验表明，该方法优于现有最先进的检测器，具有更好的泛化性和鲁棒性。

Conclusion: 基于频率伪造线索的方法能够有效检测来自未知扩散模型的图像，并对各种干扰具有强鲁棒性。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [56] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: 提出了ToxicTextCLIP框架，针对CLIP模型在预训练阶段的文本模态进行数据投毒和后门攻击，通过背景感知选择和背景驱动增强生成高质量对抗文本。


<details>
  <summary>Details</summary>
Motivation: CLIP模型依赖大规模网络数据进行自监督对比学习，但未筛选的互联网数据使其面临数据投毒和后门风险。现有研究主要关注图像模态攻击，文本模态攻击研究不足。

Method: ToxicTextCLIP迭代应用两个组件：1）背景感知选择器，优先选择与目标类别背景一致文本；2）背景驱动增强器，生成语义连贯且多样化的投毒样本。

Result: 在分类和检索任务上，ToxicTextCLIP达到95.83%投毒成功率和98.68%后门Hit@1，并能绕过RoCLIP、CleanCLIP和SafeCLIP防御。

Conclusion: 该工作揭示了CLIP模型文本模态的安全漏洞，提出的框架能有效生成高质量对抗文本，对多模态模型安全研究具有重要意义。

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [57] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出弱监督深度学习框架，使用Grad-CAM从胸部X光片进行肺炎分类和定位，无需像素级标注，仅用图像级标签生成临床有意义的肺炎区域热图。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像分析中像素级标注成本高的问题，开发弱监督方法实现肺炎的准确分类和定位，提高AI辅助诊断的透明度和临床信任度。

Method: 使用七种ImageNet预训练架构（ResNet-18/50、DenseNet-121、EfficientNet-B0、MobileNet-V2/V3、ViT-B16），在相同训练条件下采用焦点损失和患者级数据分割防止数据泄露，利用Grad-CAM生成解释性热图。

Result: 在Kermany CXR数据集上，ResNet-18和EfficientNet-B0达到最佳测试准确率98%，ROC-AUC=0.997，F1=0.987；MobileNet-V2在准确率和计算成本间取得最佳平衡。Grad-CAM可视化确认模型聚焦于临床相关肺区域。

Conclusion: 弱监督可解释模型在肺炎筛查中具有潜力，能增强AI辅助医学成像的透明度和临床信任，为放射学诊断提供有效工具。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [58] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter是一个统一框架，能够从单张图像中联合建模外观和人体部位语义，在3D人体部位分割和重建任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在3D人体重建方面取得了高保真度，但在特定任务（如人体3D分割）上的实用性仍然受限。

Method: 在重建阶段整合人体几何先验，在分割阶段整合自监督语义先验；开发交互式标注程序生成高质量数据标签对；通过像素对齐聚合实现跨任务协同，多任务目标同时优化纹理建模保真度和语义一致性。

Result: 大量实验表明，HumanCrafter在3D人体部位分割和单图像3D人体重建方面均超越了现有最先进方法。

Conclusion: HumanCrafter框架通过联合建模外观和语义，成功解决了3D人体分割和重建任务，展示了跨任务协同的有效性。

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [59] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的迭代分割和质量优化框架，用于MRI中前庭神经鞘瘤的自动分割，通过专家共识和人类参与循环训练，显著提高了分割精度和效率。


<details>
  <summary>Details</summary>
Motivation: 前庭神经鞘瘤的MRI分割对患者管理至关重要，但传统手动标注耗时且依赖专家。现有深度学习方法在多样数据集和复杂临床案例中表现不稳定，需要更鲁棒和通用的解决方案。

Method: 采用引导式深度学习框架进行迭代分割和质量优化，结合多中心数据，依赖专家共识确保标注可信度，实现人类参与循环训练。

Result: 在内部验证数据集上，Dice相似系数从0.9125显著提升至0.9670，在代表性外部数据集上保持稳定性能。专家评估143个扫描发现需要专家干预的细微案例。相比传统手动标注过程，效率提升约37.4%。

Conclusion: 该人类参与循环模型训练方法实现了高分割精度，展示了作为临床适应性强、可推广的自动前庭神经鞘瘤分割策略的潜力。数据集包含190名患者，534个纵向对比增强T1加权扫描和6名患者的非标注T2加权扫描，已在TCIA公开。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [60] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: FedMGP是一种个性化联邦提示学习方法，通过多组视觉-文本提示对捕捉细粒度语义，使用多样性损失确保提示组覆盖不同语义特征，并采用基于相似性的动态聚合策略平衡共享知识和客户端特定特征。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习在视觉语言模型中难以同时实现个性化学习和领域泛化，需要一种既能保留客户端特定特征又能学习共享语义的方法。

Method: 为每个客户端配备多组视觉-文本提示对，引入多样性损失使提示组专注于不同语义方面，采用基于余弦相似度的概率采样进行动态提示聚合。

Result: 在多个联邦视觉语言基准测试中，FedMGP在个性化和领域泛化方面均优于现有方法，且通信参数最少。

Conclusion: FedMGP通过多组提示和动态聚合策略有效平衡了共享语义学习和客户端个性化需求，实现了参数高效的联邦提示学习。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [61] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat是一种前馈方法，可从单张图像合成可控的显式4D场景，结合视频扩散模型的生成先验与4D数据集学习到的几何和运动约束，无需测试时优化或后处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态场景合成中通常需要复杂的优化过程，Diff4Splat旨在实现高效的单次前向预测，同时保持高质量的场景生成能力。

Method: 使用视频潜在变换器增强视频扩散模型，联合捕捉时空依赖性并预测时变3D高斯基元，通过外观保真度、几何精度和运动一致性的目标进行训练。

Result: Diff4Splat在30秒内合成高质量4D场景，在视频生成、新视角合成和几何提取方面匹配或超越基于优化的方法，且效率显著更高。

Conclusion: Diff4Splat提供了一种高效且高质量的4D场景合成解决方案，统一了生成先验与几何运动约束，为动态场景建模开辟了新途径。

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [62] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA是一个大规模胸部X光视觉问答数据集，包含17,597个问答对和4,394张图像，带有放射科医生验证的边界框和临床推理解释。


<details>
  <summary>Details</summary>
Motivation: 推动可解释的医学视觉问答研究，提供具有空间定位和临床推理的大规模数据集，解决现有医学VQA数据集中幻觉问题。

Method: 构建包含六种诊断类型问题的分类法，平衡正负样本分布(41.7%阳性，58.3%阴性)，使用MedGemma-4B-it模型进行基准测试。

Result: 在基准测试中F1分数达到0.624，比基线提高11.8%，同时实现了病灶定位功能。

Conclusion: VinDr-CXR-VQA数据集促进了可重复且临床基础扎实的医学VQA研究，数据集和评估工具已公开可用。

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [63] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: OmniTrack++是一个用于全景图像多目标跟踪的反馈驱动框架，通过动态特征稳定、轨迹反馈关联和专家记忆机制解决全景视角下的失真、大搜索空间和身份模糊问题，在JRDB和EmboTrack基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统多目标跟踪方法在全景图像中表现不佳，因为360度视野会带来图像失真、分辨率稀释和视角相关变形等独特挑战，需要专门设计的解决方案。

Method: 采用反馈驱动框架：1) DynamicSSM模块稳定全景特征；2) FlexiTrack实例使用轨迹反馈进行灵活定位和短期关联；3) ExpertTrack记忆通过专家混合设计整合外观线索；4) Tracklet管理模块根据场景动态自适应切换跟踪模式。

Result: 在JRDB和EmboTrack基准测试中，OmniTrack++相比原始OmniTrack实现了显著性能提升：JRDB上HOTA提高+25.5%，QuadTrack上提高+43.07%。

Conclusion: OmniTrack++通过系统性的反馈驱动设计有效解决了全景多目标跟踪的挑战，提出的EmboTrack基准为全景感知研究提供了全面的评估平台。

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [64] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: ID-Composer是一个用于从文本提示和参考图像生成多主体视频的新框架，通过分层身份保持注意力机制、预训练视觉语言模型的语义理解以及在线强化学习来解决主体身份保持、跨模态语义整合和时间一致性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型通常基于文本或单张图像生成，限制了可控性和应用范围。需要解决多主体视频生成中主体身份保持、跨模态语义整合和时间一致性的问题。

Method: 1. 分层身份保持注意力机制：在主体内部和跨主体、跨模态间有效聚合特征；2. 预训练视觉语言模型语义理解：利用VLM的语义理解能力提供细粒度指导；3. 在线强化学习阶段：将训练目标转化为RLVR问题，解决扩散损失在关键概念对齐上的不足。

Result: 大量实验表明，该模型在身份保持、时间一致性和视频质量方面优于现有方法。

Conclusion: ID-Composer通过创新的分层注意力机制、VLM语义理解和强化学习训练，成功解决了多主体视频生成中的关键挑战，为可控视频生成提供了有效解决方案。

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [65] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 提出一种无需额外训练或偏置标注的测试时去偏方法，通过分割模型隔离目标视觉属性，调整非目标区域嵌入以消除意外偏置信号。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法通常需要训练数据和明确的组标签进行微调或调整嵌入，限制了实际应用。测试时方法虽然避免这一约束，但仍依赖数据集特定偏置的先验知识，在开放集设置中泛化性有限。

Method: 使用预训练分割模型隔离目标视觉属性，然后调整非目标区域使其嵌入与所有类特定文本提示均匀相似，从而在保留目标属性的同时移除混杂视觉区域的意外偏置信号。

Result: 在Waterbirds和CelebA上的实验表明，该方法在组鲁棒性指标和Attention IoU方面优于现有测试时去偏方法。

Conclusion: 分割引导的干预方法在视觉语言模型中实现了可扩展且无需标注的偏置缓解，证明了其有效性。

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [66] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了T-VAD框架，基于大型视觉语言模型实现文本引导的细粒度视频异常检测，通过异常热图解码器和区域感知异常编码器提升检测的精细度和交互性。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法输出有限（仅正常或异常），且多为半自动化需要人工评估，需要更细粒度和交互性的检测方案。

Method: 构建基于大型视觉语言模型的T-VAD框架，包含异常热图解码器（像素级视觉-文本特征对齐）和区域感知异常编码器（热图转可学习文本嵌入）。

Result: 在UBnormal数据集上达到94.8% AUC，异常热图精度67.8%/76.7%；在ShanghaiTech数据集上BLEU-4得分62.67/88.84，Yes/No准确率97.67%。

Conclusion: T-VAD显著提升了异常检测的精细度和交互性，在多个数据集上达到最先进性能，为视频异常检测提供了更有效的解决方案。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [67] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了Real-IAD Variety，这是最大最全面的工业异常检测基准数据集，包含198,960张高分辨率图像，涵盖160个对象类别、28个行业、24种材料和22种颜色变化，解决了现有基准数据类别有限、规模不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测基准数据集存在类别多样性受限、规模不足的问题，导致指标饱和和模型在真实场景中泛化能力有限，需要更全面的大规模基准来推动该领域发展。

Method: 构建了Real-IAD Variety基准数据集，通过覆盖28个行业、24种材料类型和22种颜色变化来确保多样性，并在多类无监督、多视角和零/少样本设置下进行严格评估。

Result: 实验表明，最先进的多类无监督异常检测方法在类别从30扩展到160时性能显著下降，而视觉语言模型对类别扩展表现出显著鲁棒性，性能变化最小，增强了在多样化工业环境中的泛化能力。

Conclusion: Real-IAD Variety的规模和复杂性使其成为训练和评估下一代异常检测基础模型的重要资源，将加速超越领域特定约束的研究，推动可扩展通用异常检测系统的发展。

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [68] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: 提出了一种从单张图像中精确学习和合成多实例语义的方法，通过惩罚性注意力优化和盒子控制来解决相似语义分离问题


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像学习多实例语义时训练数据有限的问题，特别是在实例具有相似语义或外观时的挑战

Method: 使用惩罚性注意力优化在学习阶段分离相似语义，在合成阶段引入并优化注意力层的盒子控制来减少语义泄漏并精确控制输出布局

Result: 实验结果表明该方法实现了分离且高质量的语义学习和合成，在可编辑性和实例一致性之间取得了良好平衡，在处理语义或视觉相似实例或罕见对象时保持鲁棒性

Conclusion: 该方法能够有效解决多实例语义学习中的相似语义分离问题，实现了高质量的语义控制和合成效果

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [69] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 提出4D神经体素泼溅方法，通过紧凑的神经体素集合和变形场建模动态场景，大幅减少内存消耗并加速训练，同时保持高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在动态场景中因跨帧复制高斯而存在显著内存开销，需要更高效的动态场景建模方法。

Method: 结合体素表示与神经高斯泼溅，使用紧凑的神经体素集合和学习的变形场建模时间动态，并引入视图细化阶段优化困难视角。

Result: 实验表明该方法在显著减少内存和加速训练的同时，优于现有最优方法，实现实时渲染和更优视觉保真度。

Conclusion: 4D-NVS通过神经体素和变形场有效解决了动态3D高斯泼溅的内存效率问题，实现了高效高质量的动态场景建模。

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [70] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出了FREE框架，通过频域信息增强模型在分布偏移下发现类别的能力，解决了域偏移广义类别发现(DS_GCD)问题。


<details>
  <summary>Details</summary>
Motivation: 现有广义类别发现方法在标准条件下表现良好，但在分布偏移时性能下降。本文探索更现实的场景：未标注数据不仅包含未知类别，还来自未知域。

Method: 提出频率引导的广义类别发现框架(FREE)：1)基于频率的域分离策略；2)跨域和域内频率扰动策略；3)扩展自监督对比目标和语义聚类损失；4)聚类难度感知重采样技术。

Result: 大量实验表明，该方法有效减轻了分布偏移的影响，在多个基准数据集上实现了发现已知和未知类别的优越性能。

Conclusion: FREE框架通过利用频域信息，显著提升了模型在分布偏移条件下发现类别的能力，为解决现实世界中的广义类别发现问题提供了有效方案。

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [71] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 提出了一种基于记忆增强的上下文感知零样本异常检测方法，通过跨注意力机制融合时序和视觉特征，结合上下文相似性评分实现实时异常分类。


<details>
  <summary>Details</summary>
Motivation: 视频异常通常依赖于上下文信息和时间演化，现有异常检测器大多无法感知这种上下文依赖，限制了其在真实场景中的泛化能力。

Method: 采用记忆增强的流程，通过跨注意力机制关联时序信号与视觉嵌入，并使用上下文相似性评分进行实时零样本异常分类。

Result: 在UCF-Crime数据集上达到90.4% AUC，在XD-Violence数据集上达到83.67% AP，在零样本模型中创下新的最优性能，并实现实时推理。

Conclusion: 通过融合跨注意力时序融合和上下文记忆，实现了高保真度的异常检测，为零样本模型在现实世界监控和基础设施监测中的应用迈出了重要一步。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [72] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: CueBench是首个专注于上下文感知视频异常理解的基准测试，通过统一评估框架和事件中心的分层分类法，系统评估现有模型在复杂异常理解任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常理解方法对真实世界异常的理解较浅显，缺乏对复杂原理和微妙上下文（如带安全装备与不带安全装备攀岩的区别）的深入理解。

Method: 提出CueBench基准，建立事件中心的分层分类法，包含14种条件异常和18种绝对异常事件，涵盖174个场景和198个属性。基于此统一评估识别、时序定位、检测和预测等任务。

Result: 现有视觉语言模型在真实异常理解上表现不佳，而提出的Cue-R1模型（基于R1风格强化微调）平均超越现有最先进方法超过24%。

Conclusion: 当前深度模型距离真实世界视频异常理解仍有较大差距，CueBench为系统评估提供了严格框架，Cue-R1展示了在上下文感知异常理解上的显著改进。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [73] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: 提出了一种新的手术动作三元组分割任务，通过仪器实例分割实现空间定位，并开发了TargetFusionNet架构和CholecTriplet-Seg数据集来提升手术场景理解。


<details>
  <summary>Details</summary>
Motivation: 现有手术动作识别方法局限于帧级分类，无法可靠地将动作与特定仪器实例关联；之前的空间定位方法主要依赖类别激活图，缺乏精确性和鲁棒性。

Method: 提出了三元组分割任务，结合仪器实例分割与动作识别；开发了TargetFusionNet架构，扩展Mask2Former并引入目标感知融合机制，融合弱解剖先验与仪器实例查询。

Result: 在识别、检测和三元组分割指标上，TargetFusionNet持续优于现有基线方法，证明强实例监督结合弱目标先验显著提升了手术动作理解的准确性和鲁棒性。

Conclusion: 三元组分割为手术动作三元组空间定位建立了统一框架，提出的基准和架构为更可解释的手术场景理解铺平了道路。

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [74] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: 该研究介绍了首个大规模多光谱激光雷达基准数据集FGI-EMIT，用于个体树木分割，并比较了传统无监督算法和深度学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统个体树木分割方法缺乏大规模基准数据集，特别是多光谱激光雷达数据，尽管证据表明多光谱反射率可以提高分割精度。

Method: 使用FGI-EMIT数据集，对四种传统无监督算法和四种监督深度学习方法进行全面基准测试，无监督方法使用贝叶斯优化超参数，DL模型从头开始训练。

Result: 无监督方法中Treeiso获得最高F1分数52.7%，DL方法表现显著更好，ForestFormer3D达到73.3% F1分数，在底层树木上优势最明显。

Conclusion: 深度学习方法在个体树木分割中显著优于传统无监督算法，但当前DL方法未能充分利用多光谱反射率信息，单通道反射率对底层树木分割有边际改善。

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [75] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一个基于元数据引导的框架，通过学习MRI对比度表示，将体积图像与其DICOM采集参数对齐，解决MRI数据异质性和缺乏标准化对比标签的问题。


<details>
  <summary>Details</summary>
Motivation: MRI存在显著的数据异质性，且缺乏跨扫描仪、协议和机构的标准化对比标签，这严重限制了大规模自动化分析。统一的MRI对比度表示可以实现从自动序列识别到协调和质量控制等多种下游应用，而无需依赖手动标注。

Method: 引入MR-CLIP框架，通过将体积图像与其DICOM采集参数对齐来学习MRI对比度表示。该方法利用常规可用的采集元数据作为监督信号。

Result: 生成的嵌入显示MRI序列的明显聚类，在数据稀缺情况下的小样本序列分类中优于监督3D基线。此外，MR-CLIP通过图像-元数据嵌入距离识别损坏或不一致的元数据，实现无监督数据质量控制。

Conclusion: 通过将常规采集元数据转化为监督信号，MR-CLIP为跨不同临床数据集的标签高效MRI分析提供了可扩展的基础。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [76] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: 提出了一种用于图像超分辨率网络的双区域量化策略，通过分离激活值中的异常值和密集区域进行独立量化，并结合敏感度感知微调来提升后训练量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法在图像超分辨率任务中性能不佳，主要原因是忽视了激活值中的异常值影响。研究发现这些异常值与图像颜色信息强相关，直接移除会导致性能显著下降。

Method: 1. 双区域量化策略：将激活值划分为异常值区域和密集区域，分别应用均匀量化以优化比特分配；2. 敏感度感知微调：根据网络层对量化的敏感度差异，让模型更关注高敏感层。

Result: 在多种超分辨率网络和数据集上的实验表明，该方法优于现有后训练量化方法，在大多数场景下达到与量化感知训练相当的性能，同时实现至少75倍的加速。

Conclusion: 所提出的双区域量化策略和敏感度感知微调有效解决了图像超分辨率网络中激活值异常值和层间敏感度差异问题，显著提升了后训练量化的性能。

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [77] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER是一种基于新颖性搜索的方法，使用LLM进行语义演化，通过CLIP嵌入量化新颖性，生成多样化的图像集合。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型输出多样性有限，阻碍了在探索和构思任务中的应用。现有的提示优化技术通常针对美学适应性或不适合创意视觉领域。

Method: WANDER直接在自然语言提示上操作，使用大型语言模型进行语义演化，利用CLIP嵌入量化新颖性，并应用发射器引导搜索进入提示空间的不同区域。

Result: 使用FLUX-DEV生成和GPT-4o-mini进行突变的实证评估表明，WANDER在多样性指标上显著优于现有的进化提示优化基线。消融研究证实了发射器的有效性。

Conclusion: WANDER通过新颖性搜索和发射器机制，成功解决了文本到图像扩散模型输出多样性不足的问题，为创意视觉任务提供了有效的解决方案。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [78] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: 该论文分析了低剂量CT图像增强中不同损失函数与图像质量指标之间的一致性，发现两者存在不一致性，强调在开发新损失函数时需要考虑图像质量指标。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT图像存在噪声和伪影问题，影响诊断准确性。虽然深度学习模型在PSNR和SSIM指标上表现良好，但这些指标在反映医学图像感知质量方面存在局限性。

Method: 对基于深度学习的低剂量CT图像增强架构中的损失函数进行客观分析，评估不同损失函数与图像质量指标的相关性和一致性。

Result: 研究发现损失函数与质量指标之间存在不一致性，表明仅依赖PSNR和SSIM等传统指标不足以评估医学图像的感知质量。

Conclusion: 在开发新的图像质量增强损失函数时，必须考虑图像质量指标，以确保增强结果在临床诊断中的实用性。

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [79] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: 深度学习模型在ADNI数据集上对阿尔茨海默病诊断表现出色，但在拉丁美洲FLENI队列上性能显著下降，揭示了明显的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 评估深度学习模型在不同人群中的泛化能力，特别是从北美ADNI数据集到拉丁美洲FLENI临床队列的迁移性能。

Method: 使用卷积和Transformer架构模型在ADNI数据集上训练，并在FLENI数据集上测试泛化性能，进行消融研究和遮挡敏感性分析。

Result: 模型在ADNI上AUC高达0.96-0.97，但在FLENI上降至0.80-0.82，显示显著性能下降。不同架构表现相似，图像归一化和采样选择是影响泛化的关键因素。

Conclusion: 需要基于人群的AI模型验证，并推动领域适应和队列多样化的未来研究。

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [80] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: 将位置识别重新定义为多类分类问题，通过为LiDAR扫描分配离散位置标签，使用编码器-解码器模型直接分类位置，在NuScenes数据集上取得与对比学习方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有位置识别方法大多依赖对比学习，本文探索将其作为多类分类问题的替代方法，以提升训练效率和稳定性。

Method: 为LiDAR扫描分配离散位置标签，训练编码器-解码器模型直接对每个扫描的位置进行分类。

Result: 在NuScenes数据集上评估，该方法实现了与基于对比学习方法相竞争的性能。

Conclusion: 多类分类方法在位置识别任务中具有竞争力，同时在训练效率和稳定性方面具有优势。

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [81] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 该研究通过生成AI模型分析西方审美标准的编码方式，发现模型存在显著的肤色、年龄和性别偏见，86.5%生成图像为浅肤色，74%为年轻人群，非二元性别个体被过度年轻化和性化，负面审美特征提示会产生更多NSFW内容。


<details>
  <summary>Details</summary>
Motivation: 社交媒体加剧了西方审美标准的传播，导致负面自我形象和身体畸形恐惧等问题。随着AI生成内容的增加，担心这些审美标准被夸大，因此研究生成AI模型如何编码'美'和消除'丑'及其社会影响。

Method: 创建两个图像生成流程：文本到图像模型和文本到语言模型再到图像模型。开发结构化审美分类法，使用三个语言模型和两个文本到图像模型生成5984张图像，招募女性和非二元社交媒体用户通过李克特量表评估1200张图像。

Result: 86.5%生成图像为浅肤色，22%包含明确内容（尽管经过SFW训练），74%被评定为年轻年龄段。非二元个体图像被评定为更年轻和过度性化。带有负面审美特征的提示（如"宽鼻子"）无论性别都产生更高的NSFW评分。

Conclusion: 生成AI模型中存在与审美标准相关的普遍人口统计偏见，这些偏见通过模型开发者（如负面提示）被积极延续，可能导致数据流污染和不符合开发者审美刻板印象的特征被主动消除。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [82] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv5和SSD算法的物联网动物检测系统，用于保护榴莲种植园免受动物入侵，集成了实时监测、Telegram通知和声音威慑机制。


<details>
  <summary>Details</summary>
Motivation: 传统农业实践无法实现无人监控，现有系统存在单一检测算法依赖、通知平台不便捷和威慑机制有限等问题，需要更有效的动物入侵检测解决方案。

Method: 整合YOLOv5和SSD目标检测算法提高检测精度，通过物联网技术实现实时监控，检测到入侵时自动发送Telegram通知并触发老虎吼声等声音威慑机制。

Result: YOLO+SSD模型对大象、野猪和猴子的检测准确率分别达到90%、85%和70%，白天检测精度最高，夜间有所下降，图像和视频检测效果一致。

Conclusion: 该研究提供了一个结合检测、通知和威慑的全面实用框架，为自动化农业解决方案的未来创新铺平了道路。

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [83] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出了一种粒度一致的自动2D掩码跟踪方法，通过保持帧间时间对应关系消除冲突伪标签，结合三阶段课程学习框架，从碎片化单视图数据逐步训练到统一多视图标注，实现全局一致的3D实例分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过将基础模型的2D掩码转移到3D来生成伪标签，但由于视频帧独立处理导致分割粒度不一致和冲突的3D伪标签，降低了最终分割精度。

Method: 粒度一致的自动2D掩码跟踪方法，结合三阶段课程学习框架：从碎片化单视图数据到统一多视图标注，再到全局一致的全场景监督。

Result: 实验结果表明，该方法能有效生成一致且准确的3D分割，在标准基准测试中达到最先进水平，并具备开放词汇能力。

Conclusion: 该方法能够从初始碎片化和矛盾的2D先验中稳健地提取一致的3D表示，解决了现有方法中的粒度不一致和伪标签冲突问题。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [84] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench是一个用于隐私保护联邦学习的可复现基准测试平台，使用合成肿瘤CT扫描数据评估分割性能和隐私泄露，揭示了隐私与性能之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统在隐私敏感环境中具有重要价值，但仍面临成员推理攻击和数据异构性等安全威胁，需要系统性的评估框架。

Method: 开发FedOnco-Bench基准测试平台，使用合成肿瘤CT扫描数据，评估FedAvg、FedProx、FedBN和FedAvg+DP-SGD等联邦学习方法的分割性能和隐私保护能力。

Result: FedAvg性能最佳（Dice约0.85）但隐私泄露最多（攻击AUC约0.72），DP-SGD隐私保护最强（AUC约0.25）但准确率较低（Dice约0.79），FedProx和FedBN在异构数据下表现均衡。

Conclusion: FedOnco-Bench为医学图像分割的隐私保护联邦学习方法提供了标准化开源基准测试平台，揭示了隐私与性能的权衡关系，有助于方法开发和评估。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [85] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: GUI-AIMA是一个基于注意力机制的GUI定位框架，通过监督微调将MLLMs的多模态注意力与补丁级定位信号对齐，无需生成坐标即可实现高效的图形界面定位。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLMs的GUI定位方法将任务视为基于文本的坐标生成，但从视觉输入直接生成精确坐标具有挑战性且计算量大。直觉上应该先选择与指令相关的视觉补丁，再确定精确点击位置。

Method: 提出GUI-AIMA框架，通过多头聚合在简化的查询-视觉注意力矩阵上自适应计算补丁级定位信号，将MLLMs的内在多模态注意力与定位信号对齐。采用无坐标方式，可轻松集成即插即用的放大阶段。

Result: GUI-AIMA-3B模型仅用85k张截图训练，在ScreenSpot-Pro上平均准确率达58.6%，在OSWorld-G上达62.2%，在3B模型中达到最先进性能，验证了轻量训练可触发MLLMs的本地定位能力。

Conclusion: GUI-AIMA展示了通过注意力对齐和轻量监督微调，可以有效释放MLLMs的固有定位能力，为GUI定位提供了一种高效的数据利用方法。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [86] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: 提出TA-LSDiff模型，结合拓扑感知扩散概率模型和水平集能量，无需显式几何演化即可实现胰腺分割，在四个公共数据集上达到最先进精度


<details>
  <summary>Details</summary>
Motivation: 胰腺分割面临尺寸小、对比度低和拓扑变化大的挑战，传统水平集方法忽略点状拓扑效应，而深度学习网络则牺牲结构细节

Method: 结合拓扑感知扩散概率模型和水平集能量，通过四个互补项整合输入图像和深度特征来引导隐式曲线演化，并引入像素自适应细化模块通过邻域证据的亲和权重局部调制能量函数

Result: 在四个公共胰腺数据集上的评估表明，TA-LSDiff实现了最先进的准确度，优于现有方法

Conclusion: TA-LSDiff为胰腺分割提供了一个实用且准确的解决方案

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [87] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: OMEGA是一个新的位置编码框架，通过模态特定位置编码和全局自适应编码步长缩放，提升视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型使用统一的位置索引策略，没有考虑文本和视觉模态在结构特性和连续性上的差异。

Method: 采用模态特定位置编码为不同模态分配位置索引，并使用全局自适应编码步长缩放根据嵌入熵自适应调整视觉token的位置编码步长。

Result: 在多种架构和VQA基准测试中一致提升性能，在视觉密集型任务上比基线位置编码策略提升高达3.43%。

Conclusion: OMEGA框架能有效解决多模态位置编码问题，提升视觉语言模型的性能。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [88] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为LSSA的新型多模态对抗攻击方法，通过局部图像块随机打乱和采样来增强对抗样本的迁移性，在多种VLP模型和下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态对抗攻击方法过度依赖单一模态的对抗样本信息，导致过拟合问题，缺乏输入多样性。

Method: LSSA方法随机打乱局部图像块来扩展原始图像-文本对，生成对抗图像并在其周围采样，然后利用原始和采样图像生成对抗文本。

Result: 在多个模型和数据集上的实验表明，LSSA显著提升了多模态对抗样本在不同VLP模型和下游任务中的迁移性，并在大型视觉语言模型上优于其他先进攻击方法。

Conclusion: LSSA通过增加输入多样性有效解决了现有方法的过拟合问题，为多模态对抗攻击提供了新的有效策略。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [89] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 提出了Visual-Contrast Attention (VCA)作为MHSA的替代方案，通过视觉对比注意力机制降低计算复杂度，在图像识别和生成任务中均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统ViT的MHSA层对所有token对进行二次查询-键交互，计算了大量视觉上弱相关或冗余的关联，需要更高效的注意力机制。

Method: VCA首先将每个头的密集查询场蒸馏为少量空间池化的视觉对比token，然后将其分为可学习的正负流，通过差分交互突出区域间的真正差异。

Result: 在ImageNet-1K上，VCA将DeiT-Tiny的top-1准确率从72.2%提升到75.6%（+3.4），在三个强层次化ViT上提升达3.1%；在图像生成任务中，FID-50K指标在扩散和流模型上分别降低2.1到5.2个点。

Conclusion: VCA提供了一条实现更快、更锐利Vision Transformers的简单路径，理论复杂度从O(NNC)降低到O(NnC)，且仅增加不到0.3M参数，无需额外FLOPs。

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [90] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: 提出参数插值对抗训练(PIAT)框架，通过在训练过程中插值模型参数来缓解对抗训练中的振荡和过拟合问题，提高模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法存在模型鲁棒性振荡和过拟合问题，影响防御效果。

Method: PIAT框架在每轮训练中通过插值前一轮和当前轮模型参数来平滑决策边界变化；同时使用归一化均方误差(NMSE)对齐干净样本和对抗样本的logits相对大小。

Result: 在多个基准数据集上的实验表明，该框架能显著提升CNN和ViT模型的鲁棒性。

Conclusion: PIAT通过参数插值和NMSE损失函数有效解决了对抗训练中的振荡和过拟合问题，显著提高了模型鲁棒性。

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [91] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: OmniBrainBench是首个专门评估多模态大语言模型在脑成像分析中理解能力的综合基准，包含15种脑成像模态、9,527个VQA对和31,706张图像，涵盖15个多阶段临床任务。


<details>
  <summary>Details</summary>
Motivation: 当前脑成像VQA基准要么覆盖少数成像模态，要么局限于粗粒度病理描述，无法全面评估MLLMs在整个临床连续体中的表现。

Method: 构建包含15种脑成像模态的综合基准，模拟临床工作流程，包含15个多阶段临床任务，并由专业放射科医生验证。

Result: 评估24个最先进模型发现：专有MLLMs优于开源和医疗模型但仍落后于医生；医疗MLLMs性能差异大；开源MLLMs在特定任务中表现优异；所有模型在复杂术前任务中表现不佳。

Conclusion: OmniBrainBench为评估和推进脑成像分析中的MLLMs设定了新标准，揭示了与专家临床推理之间的差距。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [92] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种遮挡感知扩散模型(ODM)，用于在遮挡场景下预测行人过街意图，通过重建被遮挡的运动模式来指导未来意图预测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在预测行人过街意图方面已取得显著成功，但很少考虑遮挡场景下的不完全观测问题。

Method: 采用遮挡感知扩散变换器架构，在去噪阶段估计与遮挡模式相关的噪声特征，并引入遮挡掩码引导的反向过程来有效利用观测信息。

Result: 在PIE和JAAD基准数据集上的广泛实验表明，该方法在各种遮挡场景下比现有方法具有更鲁棒的性能。

Conclusion: 所提出的遮挡感知扩散模型能够有效处理遮挡场景下的行人意图预测问题，提高了预测准确性。

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [93] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: 提出了一种名为LMD的后处理、模型无关的可解释性方法，用于解耦自动驾驶中多传感器融合模型的模态特定信息，能够将预测归因于各个输入模态。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，感知模型的决策透明度至关重要，因为即使是单个误判也可能导致灾难性后果。但在多传感器输入下，由于传感器信息在融合网络中纠缠在一起，很难确定每个模态对预测的贡献。

Method: LMD是一种后处理、模型无关的可解释性方法，能够在预训练融合模型的所有层中解耦模态特定信息。这是首个能够将感知模型预测归因于自动驾驶传感器融合系统中各个输入模态的方法。

Result: 在预训练的相机-雷达、相机-LiDAR和相机-雷达-LiDAR融合模型上评估LMD，通过结构化扰动指标和模态可视化分解验证了其有效性，展示了在高容量多模态架构解释中的实际适用性。

Conclusion: LMD为自动驾驶多传感器融合系统提供了一种有效的可解释性解决方案，能够清晰展示各个模态对最终预测的贡献，提高了感知模型的透明度。

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [94] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: GraphGeo是一个基于异构图神经网络的多智能体辩论框架，用于视觉地理定位，通过类型化边建模不同辩论关系，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统检索方法受限于数据库覆盖范围和质量，而现有的大视觉语言模型在复杂地理场景中表现不佳，多智能体系统缺乏有效处理冲突预测的机制。

Method: 提出GraphGeo框架，使用异构图神经网络建模多样化的辩论关系，包括支持性协作、竞争性论证和知识转移，引入双层级辩论机制和跨层级拓扑优化策略。

Result: 在多个基准测试上的实验表明，GraphGeo显著优于现有最先进方法，将智能体间的认知冲突转化为增强的地理定位精度。

Conclusion: GraphGeo通过结构化辩论有效提升了视觉地理定位性能，证明了异构图神经网络在多智能体协作中的有效性。

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [95] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: Fleming-VL是一个统一的多模态大语言模型框架，专门用于处理医学领域中的异构视觉数据，包括2D图像、3D体积扫描和时序视频序列。


<details>
  <summary>Details</summary>
Motivation: 医学数据具有异构性，包含多种模态（2D图像、3D体积扫描、视频序列），领域差距大且数据格式不一致，这阻碍了统一医学MLLM的发展。

Method: 采用数据中心的三个关键策略：1）整合自然和医学领域的长上下文数据进行预训练扩展；2）用罕见医学数据补充微调；3）扩展评估框架以包含3D体积和视频理解基准。通过监督微调和组相对策略优化开发多个模型规模。

Result: Fleming-VL在多个基准测试中实现了最先进的性能，包括医学VQA、视频QA和3D医学图像理解。

Conclusion: Fleming-VL为医学AI的透明、可复现和可审计进展提供了统一解决方案，并公开发布以促进该领域的发展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [96] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: 提出动态多级加权对齐网络解决零样本基于草图的图像检索问题，通过多模态特征提取、跨模态多级加权和加权四元组损失提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法存在模态样本不平衡和训练过程中低质量信息不一致的问题，导致性能不佳

Method: 包含三个组件：统一模态特征提取模块（CLIP文本编码器和ViT）、跨模态多级加权模块（局部和全局聚合块）、加权四元组损失模块

Result: 在Sketchy、TU-Berlin和QuickDraw三个基准数据集上优于现有最先进的ZS-SBIR方法

Conclusion: 该方法通过动态多级加权对齐有效解决了零样本基于草图的图像检索中的模态不平衡问题

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [97] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: EVTAR是一个端到端的虚拟试穿模型，通过引入额外参考图像来提升试穿准确性，无需复杂输入如人体姿态或分割图，仅需源图像和目标服装即可生成高质量的试穿结果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法依赖复杂输入（如人体姿态、密集姿态、关键点等），使得应用过程繁琐且不实用。EVTAR旨在简化输入要求，同时通过参考图像机制提升试穿质量。

Method: 采用两阶段训练策略，仅需源图像和目标服装作为输入。利用不同人穿着同一服装的参考图像来保持服装纹理和细节，模拟人类选择服装时的参考行为。

Result: 在两个广泛使用的基准测试和多样化任务上评估，结果一致验证了该方法的有效性。

Conclusion: EVTAR通过简化输入要求和引入参考图像机制，实现了更实用和高质量的虚拟试穿效果，为实际应用提供了可行方案。

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [98] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出一个统一的零样本视频异常分析框架，通过链式推理过程连接时间检测、空间定位和文本解释任务，无需额外训练即可实现全面的异常分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常研究大多停留在帧级检测，缺乏空间和语义上下文，无法解释异常原因。现有方法虽然提高了可解释性，但仍依赖数据和特定任务。

Method: 基于链式测试时推理过程，通过任务内推理优化时间检测，任务间链式连接实现空间和语义理解，在完全零样本方式下提升可解释性和泛化能力。

Result: 在多个视频异常检测、定位和解释基准测试中实现了最先进的零样本性能，无需额外数据或梯度更新。

Conclusion: 精心设计的提示词与任务链式连接能够释放基础模型的推理能力，实现实用且可解释的零样本视频异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [99] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM是一个专为2D血管分割设计的框架，通过集成卷积适配器、多提示编码器和轻量级掩码解码器，显著提升了血管分割性能，在多个数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确的血管分割对临床诊断和手术规划至关重要，但现有基础模型如SAM在血管结构上表现不佳，需要专门针对血管特征的优化方案。

Method: VesSAM框架包含三个核心组件：卷积适配器增强局部纹理特征；多提示编码器融合解剖学提示（骨架、分叉点、段中点）；轻量级掩码解码器减少锯齿伪影。还开发了自动生成多提示注释的流程。

Result: 实验显示VesSAM在8个数据集上比最先进的PEFT-based SAM变体提升超过10% Dice和13% IoU，与完全微调方法性能相当但参数更少，在分布外设置中也表现优异。

Conclusion: VesSAM为血管分割提供了强大而高效的解决方案，在保持轻量化的同时实现了卓越的分割性能，具有良好的泛化能力。

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [100] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的自监督多模态迭代去噪框架MID，通过模拟非线性噪声累积过程，无需配对干净-噪声数据集即可有效去除复杂非线性噪声。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据经常被复杂的非线性噪声污染，传统的基于规则的去噪方法难以应对这一挑战。

Method: MID将噪声数据建模为非线性噪声累积过程中的状态，通过迭代添加噪声来学习两个神经网络：一个估计当前噪声步骤，另一个预测并减去相应的噪声增量。对于复杂非线性污染，使用一阶泰勒展开局部线性化噪声过程。

Result: 在四个经典计算机视觉任务上的实验表明MID具有鲁棒性、适应性和最先进的性能，在生物医学和生物信息学领域也表现出强大的性能和适应性。

Conclusion: MID框架能够有效处理复杂非线性噪声，无需配对数据集，在多个领域都展现出优异的去噪性能。

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [101] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 提出了一种多模态机器学习框架，通过统一特征提取技术分析戈雅画作的视觉图像和X射线图像，在艺术认证中取得了97.8%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决弗朗西斯科·戈雅作品艺术认证的计算挑战，因其风格演变异质性和广泛的历史伪造模式。

Method: 使用统一特征提取流程（包括灰度共生矩阵描述符、局部二值模式、熵测量、能量计算和颜色分布分析）处理视觉和X射线图像，通过优化的单类支持向量机进行分类。

Result: 在24幅认证戈雅画作数据集上，使用80/20训练测试配置和10折交叉验证，达到97.8%分类准确率和0.022假阳性率。案例研究显示对《Un Gigante》的认证置信度为92.3%。

Conclusion: 多模态方法相比单模态方法有显著性能提升，证实了在艺术认证应用中统一计算方法的有效性。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [102] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: 提出HyFormer-Net混合CNN-Transformer网络，用于乳腺超声图像的同步分割和分类，具有内在可解释性，在BUSI数据集上表现优异，并通过跨数据集研究验证了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决B型超声乳腺诊断面临的挑战：斑点噪声、操作者依赖性和边界模糊问题，以及现有深度学习方法存在的单任务学习、架构限制（CNN缺乏全局上下文，Transformer缺乏局部特征）和黑盒决策等问题。

Method: HyFormer-Net采用双分支编码器集成EfficientNet-B3和Swin Transformer，通过多尺度分层融合块进行特征融合，注意力门控解码器提供精确性和可解释性，并引入双管道可解释性机制。

Result: 在BUSI数据集上达到Dice分数0.761±0.072和准确率93.2%，优于U-Net、Attention U-Net和TransUNet。恶性召回率92.1±2.2%。集成模型达到Dice 90.2%，准确率99.5%，恶性召回率100%。跨数据集研究中，仅用10%目标域数据即可恢复92.5%性能。

Conclusion: HyFormer-Net在乳腺超声图像分析中表现出色，通过多尺度融合和注意力机制显著提升性能，跨数据集研究证明了其良好的泛化能力，为临床采用提供了有力支持。

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [103] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost是一种参数高效的神经网络架构，通过动态缩放渐进注意力机制在CIFAR基准测试中达到最先进性能，显著减少参数数量同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备部署深度学习模型时的参数效率问题，在保持高准确率的同时大幅减少模型参数和计算量。

Method: 提出动态缩放渐进注意力机制，包括自适应融合、阶段缩放和残差自适应三个创新组件，结合增强的MBConv模块实现参数效率优化。

Result: 在CIFAR-10上达到95.57%准确率（0.85M参数）和93.80%（0.37M参数），在CIFAR-100上达到81.37%（0.92M参数）和74.85%（0.44M参数），相比MobileNetV3减少2.1倍参数同时提升3.2个百分点准确率。

Conclusion: 通过动态注意力与高效卷积操作的协同优化，实现了前所未有的参数-准确率权衡，为资源受限边缘设备提供了可行的深度学习部署方案。

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [104] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: 提出了T-MLA攻击框架，这是首个针对神经图像压缩系统的目标多尺度对数-指数攻击方法，通过在小波域中精心设计对抗性扰动，显著降低重建图像质量同时保持视觉不可感知性。


<details>
  <summary>Details</summary>
Motivation: 现有对神经图像压缩系统的攻击方法通常只是像素空间方法的简单移植，忽视了压缩管道的独特结构化特性，需要开发更先进的攻击框架来揭示其安全漏洞。

Method: 在小波域中直接针对攻击和重建图像的质量来构建对抗性扰动，将扰动策略性地限制在特定小波子带中，实现离线攻击，最大化失真同时确保感知隐蔽性。

Result: 在多个最先进的神经图像压缩架构和标准图像压缩基准上的广泛评估显示，重建质量大幅下降，而扰动在视觉上仍然不可感知。

Conclusion: 研究揭示了生成和内容交付管道核心存在的关键安全漏洞，神经图像压缩系统存在严重的安全风险。

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [105] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 提出了一种分层序列预测方法用于图像地理定位，通过自回归方式从粗到细预测S2网格单元，结合波束搜索和多样本推理策略，在Im2GPS3k和YFCC4k数据集上取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像地理定位中因视觉相似性和大搜索空间带来的挑战，模仿人类从大区域到具体地址的定位过程。

Method: 使用S2网格作为分层结构，自回归预测从粗到细的网格单元，结合波束搜索和多样本推理策略管理不确定性。

Result: 在MLLM-free设置下超越其他基线方法，准确率提升达13.9%；结合MLLM时在所有指标上达到最先进水平。

Conclusion: 分层序列预测方法有效解决了图像地理定位问题，通过自回归采样策略显著提升了定位精度。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [106] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: SliceVision-F2I是一个用于网络切片特征可视化研究的合成数据集，通过四种编码方法将多变量KPI向量转换为视觉表示，包含12万样本，适用于视觉学习和网络状态分析。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络中网络切片需要精细识别方法，但缺乏支持强大数据集。该研究旨在为下一代网络系统提供网络切片特征可视化的研究数据集。

Method: 使用四种编码方法将多变量KPI向量转换为RGB图像：物理启发映射、Perlin噪声、神经壁纸和分形分支。每种方法生成3万个样本，模拟真实噪声网络条件。

Result: 创建了包含12万个样本的数据集，每个样本包含原始KPI向量和对应的低分辨率RGB图像，反映了操作不确定性和测量缺陷。

Conclusion: SliceVision-F2I数据集适用于视觉学习、网络状态分类、异常检测等任务，可公开获取并用于多变量时间序列分析和特征到图像转换等研究。

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [107] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: 提出了一种结合Epanechnikov核密度估计和双峰逻辑回归分类器的新方法，用于基于医学图像的呼吸系统疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 利用EKDE的灵活性来建模数据分布，无需假设特定形状，并能适应像素强度变化，从而从医学图像中提取关键特征。

Method: 将Epanechnikov非参数核密度估计与双峰逻辑回归分类器结合，在统计模型学习框架下进行疾病诊断。

Result: 在13808张随机选择的胸部X光片上测试，准确率70.14%，敏感性59.26%，特异性74.18%，表现出中等性能但敏感性有待提高。

Conclusion: 虽然临床专业知识对于进一步优化模型仍然必要，但EKDE方法显示了在医学影像诊断中提高准确性和可靠性的潜力。

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [108] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: 提出了ViACT框架，将解剖先验整合到视频Transformer中，通过掩码自编码仅重建解剖区域，提高超声心动图分析的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 视频Transformer在超声心动图分析中存在学习非诊断区域伪相关性的问题，需要将模型注意力聚焦在解剖结构上。

Method: 将变形解剖结构表示为点集，编码空间几何和图像块为Transformer tokens，采用仅掩码解剖区域的掩码自编码预训练策略。

Result: ViACT在左心室射血分数回归和心脏淀粉样变性检测任务中表现优异，产生与已知病理区域对齐的可解释注意力图，并能泛化到心肌点跟踪任务。

Conclusion: ViACT通过解剖约束有效引导Transformer注意力到相关解剖区域，提高了超声心动图分析的性能和可解释性，无需专门跟踪网络组件。

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [109] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 该论文提出在边缘计算中使用嵌入式GPU设备来提升计算机视觉应用的性能，解决资源受限设备上的用户体验问题。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的计算机视觉应用（特别是增强现实应用）对资源需求很高，边缘计算虽然可以卸载计算密集型任务，但边缘设备通常能力有限，会影响用户体验。

Method: 使用带有图形处理单元（GPU）的嵌入式设备来克服边缘计算设备的性能限制。

Result: 实验表明，与仅使用CPU相比，GPU能够获得性能提升，从而为用户提供更好的体验。

Conclusion: 在边缘计算环境中集成GPU可以有效提升计算机视觉应用的性能，改善用户体验。

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [110] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: 提出了PCP框架，无需概念标注即可实现医学图像的可解释预测，通过类别级概念先验作为弱监督，在多个医学数据集上显著优于零样本基线。


<details>
  <summary>Details</summary>
Motivation: 医学影像中需要人类可理解的AI预测，但现有可解释设计框架需要昂贵的概念标注，在临床环境中不实用。零样本方法难以捕捉医学领域特定特征。

Method: 使用先验引导概念预测器(PCP)，利用类别级概念先验作为弱监督，结合KL散度和熵正则化机制来对齐临床推理。

Result: 在PH2和WBCatt数据集上，概念级F1分数比零样本基线提高33%以上，在四个医学数据集上分类性能与全监督概念瓶颈模型相当。

Conclusion: PCP框架有效解决了医学影像中概念标注成本高的问题，提供可靠的可解释预测，在临床应用中具有实用价值。

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [111] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: CatEquiv是一种用于惯性传感器人体活动识别的类别等变神经网络，通过编码时间、幅度和结构对称性来提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理惯性传感器数据时未能系统性地编码数据的内在对称性结构，导致在分布外扰动下鲁棒性不足。

Method: 提出类别对称性乘积方法，结合循环时间平移、正增益和传感器层次偏序集来捕捉数据的类别对称结构，构建等变神经网络。

Result: 在UCI-HAR数据集上，CatEquiv在分布外扰动下显著优于循环填充CNN和普通CNN，展现出更高的鲁棒性。

Conclusion: 强制实施类别对称性可以在不增加模型容量的情况下实现强大的不变性和泛化能力。

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [112] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: 提出了MicroAUNet，一种轻量级注意力分割网络，用于结直肠息肉分割，结合深度可分离扩张卷积和通道-空间注意力块，通过两阶段知识蒸馏实现高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的息肉分割模型要么提供模糊的息肉边界影响临床决策，要么依赖计算复杂度高的架构，无法满足实时结肠镜应用的需求。

Method: 使用深度可分离扩张卷积和参数共享的通道-空间注意力块增强多尺度边界特征，并采用渐进式两阶段知识蒸馏方案从高容量教师网络转移语义和边界线索。

Result: 在基准测试中表现出最先进的精度，同时保持极低的模型复杂度，适合实时临床息肉分割应用。

Conclusion: MicroAUNet在保持高精度的同时实现了轻量化和实时性能，为结直肠息肉分割提供了有效的解决方案。

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [113] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: ROVER是一个评估多模态模型跨模态推理能力的新基准，包含1312个任务和1876张图像，重点关注文本和视觉模态之间的相互增强推理。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法将多模态能力孤立对待，缺乏对跨模态相互推理能力的测试，而这是实现真正统一多模态智能的核心能力。

Method: 构建包含两个互补设置的人工标注基准：语言增强的视觉生成推理（用文本提示和推理链指导图像合成）和视觉增强的语言生成推理（生成中间可视化来增强问答推理过程）。

Result: 对17个统一模型的实验发现：跨模态推理决定视觉生成质量，交织模型显著优于非交织模型；模型在物理和符号推理之间存在分离，能解释感知概念但无法构建符号任务的视觉抽象。

Conclusion: 相互跨模态推理是实现真正全模态生成的关键前沿，现有模型在此方面仍有明显不足。

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [114] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: 该论文提出了一个从YouTube视频中自动提取动物视频片段的流程，构建了包含30K视频（200万帧）的大规模数据集，并创建了包含230个序列的Animal-in-Motion基准测试，用于4D四足动物重建任务。


<details>
  <summary>Details</summary>
Motivation: 现有动物视频数据集规模有限（仅2.4K个15帧片段），缺乏对动物中心3D/4D任务的关键处理能力，需要大规模、非侵入性的数据收集方法来推进野生动物研究。

Method: 开发自动化流程从YouTube视频中挖掘并处理成对象中心的片段，提供姿态估计、跟踪和3D/4D重建等下游任务所需的辅助标注。

Result: 收集了30K视频（200万帧），比先前工作规模大一个数量级；在Animal-in-Motion基准上评估发现基于模型的方法在2D指标上表现更好但3D形状不真实，而无模型方法产生更自然的重建但得分较低。

Conclusion: 通过增强无模型方法的序列级优化建立了首个4D动物重建基线，旨在推进从野外视频中进行大规模、无标记的4D动物重建及相关任务。

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [115] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: 提出DTWSR方法，结合扩散模型和Transformer，通过多级离散小波变换分解图像，利用金字塔标记化处理多尺度频率子带间关系，提升超分辨率图像的一致性和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有基于DWT的超分辨率方法大多忽略了多尺度频率子带间的相互关系，导致重建图像存在不一致性和不自然伪影。

Method: 使用多级离散小波变换分解图像为小波谱，提出金字塔标记化方法将谱嵌入为Transformer序列，设计双解码器分别处理低频和高频子带。

Result: 在多个基准数据集上的实验表明，该方法在感知质量和保真度方面均表现出色。

Conclusion: DTWSR方法通过有效捕捉多尺度频率子带间关系，实现了更一致和真实的超分辨率图像重建。

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [116] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 提出了一个包含1333个英文Rebus谜题的大型多样化基准，并开发了RebusDescProgICE框架，通过结合非结构化描述和基于代码的结构化推理，显著提升了视觉语言模型在Rebus谜题上的性能。


<details>
  <summary>Details</summary>
Motivation: Rebus谜题需要图像识别、认知技能、常识推理、多步推理和基于图像的文字游戏等多种能力，这对当前的视觉语言模型具有挑战性。

Method: 提出了RebusDescProgICE框架，结合非结构化描述和基于代码的结构化推理，并改进了基于推理的上下文示例选择方法。

Result: 与思维链推理相比，该框架在闭源模型上提升了2.1-4.1%的性能，在开源模型上提升了20-30%的性能。

Conclusion: RebusDescProgICE框架有效提升了视觉语言模型在复杂Rebus谜题任务上的表现，证明了结合多种推理方式的优势。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [117] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: 提出GCN-PSN框架，利用图卷积网络建模人体骨骼拓扑结构，通过对比回归学习判别性姿态嵌入，在动作质量评估任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 动作质量评估需要细粒度的人体运动理解和精确的姿态相似性评估，现有基于坐标的方法可能忽略了骨骼拓扑结构的重要信息。

Method: 使用拓扑感知的图卷积网络，将人体骨骼建模为图结构，采用孪生网络架构和对比回归目标函数学习姿态嵌入。

Result: 在AQA-7和FineDiving基准测试中超越了基于坐标的基线方法，取得了有竞争力的性能。

Conclusion: 实验验证了利用骨骼拓扑结构进行姿态相似性和动作质量评估的有效性，图卷积网络在该任务中具有优势。

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [118] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa是一个用于文本驱动3D人体运动生成的分层运动生成框架，通过多尺度令牌保留策略和可扩展自回归建模，显著提高了生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的运动生成方法在推理步骤和生成质量之间存在权衡，MoSa旨在通过分层量化策略减少推理步骤同时保持高质量生成。

Method: 提出多尺度令牌保留策略(MTPS)和分层残差向量量化变分自编码器(RQ-VAE)，结合可扩展自回归建模(SAR)和卷积-注意力混合VQ-VAE(CAQ-VAE)。

Result: 在Motion-X数据集上达到FID 0.06，相比MoMask的0.20有显著提升，同时减少27%的推理时间，在下游任务如运动编辑中表现良好。

Conclusion: MoSa在3D人体运动生成方面实现了最先进的性能，在生成质量和效率之间取得了良好平衡，具有良好的泛化能力。

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [119] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 本文提出了Viewpoint Learning任务来评估和改进多模态大语言模型的空间推理能力，通过Viewpoint-100K数据集和两阶段微调策略显著提升了模型的空间推理性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在2D视觉理解方面取得显著进展，但其在复杂3D推理任务中的表现，特别是跨视角一致性这一关键能力仍不明确，需要评估和改进模型的空间推理能力。

Method: 提出Viewpoint Learning任务，构建Viewpoint-100K数据集包含10万对物体中心图像和问答对；采用两阶段微调策略：先用监督微调注入基础知识，再用强化学习增强泛化能力；引入混合冷启动初始化方法同时学习视角表示和保持连贯推理。

Result: 实验结果表明该方法显著激活了多模态大语言模型的空间推理能力，在领域内和领域外推理任务上均表现出性能提升。

Conclusion: 开发多模态大语言模型的基础空间技能具有重要价值，将支持未来在机器人、自主系统和3D场景理解方面的进展。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [120] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA是一个多模态视觉-语言-动作模型，通过整合红外相机、毫米波雷达和麦克风阵列等新型传感模态，超越RGB感知，提供物理基础的空间智能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型主要依赖RGB相机，限制了感知能力和操作能力。需要整合更多传感模态来提升物理环境中的空间感知。

Method: 提出传感器掩码图像的统一表示方法，将空间基础和物理意义的掩码叠加到RGB图像上。基于RGB预训练的VLA骨干网络构建多感官VLA模型架构，使用轻量级传感器投影器实现数据高效学习。

Result: 在需要传感器模态感知的挑战性真实世界任务中，OmniVLA平均任务成功率达到84%，显著优于仅RGB和原始传感器输入基线模型，分别提升59%和28%。

Conclusion: OmniVLA通过多模态传感整合显著提升了VLA模型的感知和操作能力，展示了更高的学习效率和更强的泛化能力。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [121] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 该论文提出了一种为印度食物视觉问答(VQA)系统构建推理链的方法，通过多步推理过程来提高答案准确性，相比基线平均提升了10个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统主要面向西方食物，无法处理印度食物的文化多样性和复杂烹饪背景。现有印度食物VQA数据集采用两步法生成答案，但食物VQA需要多步推理来理解复杂的烹饪背景和食物关系。

Method: 创建自动验证的推理链，对较小的LLM和VLM进行微调，并使用强化学习在大规模数据上进一步训练。通过推理链增强来提高模型性能。

Result: 通过推理链增强，在印度食物VQA任务上相比基线平均提升了10个百分点的准确率。

Conclusion: 多步推理链对于处理印度食物的复杂文化背景和烹饪关系至关重要，能显著提升食物VQA系统的性能。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [122] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: 本文研究了通过翻转数据预训练和微调的方法来提高自动驾驶模型在左舵驾驶条件下的域适应能力，发现翻转数据预训练结合微调能显著改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶模型需要适应不同道路条件，特别是左舵和右舵驾驶的差异，以提高在多样化道路环境中的泛化能力。

Method: 评估了四种训练方法：基准模型（美国右舵数据）、翻转美国数据训练、美国数据预训练+澳大利亚微调、翻转美国数据预训练+澳大利亚微调，使用PilotNet和ResNet架构进行验证。

Result: 仅使用翻转数据预训练会降低预测稳定性，但翻转数据预训练后微调能显著降低预测误差，增强对左侧道路特征的关注，在ResNet架构上也观察到类似趋势。

Conclusion: 翻转数据预训练结合微调是一种有效的域适应方法，能以最小重训练需求显著提升模型在不同驾驶条件下的适应能力。

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [123] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: 本文分析了自动语音驱动3D手势生成领域的人类评估实践，发现缺乏标准化和存在实验设计缺陷。作者提出了BEAT2数据集的人类评估协议，并对6个最新手势生成模型进行了大规模众包评估，结果显示新模型并不总是优于早期方法，且需要采用解耦的评估维度。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音驱动3D手势生成领域缺乏标准化的评估方法，实验设计存在缺陷，导致无法准确比较不同方法的性能，也无法确定该领域的最新技术水平。

Method: 提出了BEAT2运动捕捉数据集的详细人类评估协议，通过大规模众包评估对6个最新手势生成模型进行排名，评估维度包括运动真实性和语音-手势对齐性。

Result: 评估结果显示：1）新模型并不总是优于早期方法；2）已发表的高运动真实性或语音-手势对齐性声明在严格评估下可能不成立；3）需要采用解耦的运动质量和多模态对齐评估来进行准确基准测试。

Conclusion: 该领域必须采用解耦的运动质量和多模态对齐评估方法，以推动技术进步。作者将发布合成运动数据、渲染视频刺激、开源渲染脚本和16,000对人工偏好投票，以促进标准化和新的评估研究。

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [124] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出Eyes on Target框架，通过将人眼注视特征注入Vision Transformer的注意力机制，在自我中心视频中实现注视引导的物体检测。


<details>
  <summary>Details</summary>
Motivation: 利用人眼注视提供的丰富监督信号来理解复杂视觉环境中的注意力分布，改善自我中心视频中的物体检测性能。

Method: 在Vision Transformer的注意力机制中注入注视衍生特征，使空间特征选择偏向人类关注的区域，而非平等对待所有区域。

Result: 在自定义模拟器数据集和公共基准测试（Ego4D Ego-Motion、Ego-CH-Gaze）上，相比无视注视的基线方法，检测准确率持续提升。

Conclusion: 注视引导的物体检测框架能有效利用人类视觉注意力信息，在自我中心视频中提升检测性能，并可通过注视感知注意力头重要性指标解释模型行为。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [125] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: 提出一种基于双阶信息的黑盒梯度可迁移攻击方法，通过对抗平坦性(AF)解决欺骗性平坦问题，并开发了对抗平坦性攻击(AFA)和蒙特卡洛对抗采样(MCAS)来提升攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有可迁移攻击方法虽然关注平坦损失，但仍陷入次优区域（特别是平坦但尖锐的区域，称为欺骗性平坦），导致攻击效果受限。

Method: 从双阶信息角度提出对抗平坦性(AF)概念，通过理论保证对抗可迁移性；开发AFA攻击方法解决梯度符号改变问题；设计MCAS方法提升内循环采样效率。

Result: 在ImageNet兼容数据集上优于6个基线方法，生成的对抗样本位于更平坦区域，跨模型架构的可迁移性显著提升；在输入变换攻击和百度云API测试中表现优异。

Conclusion: 该方法通过对抗平坦性和高效采样策略，有效解决了欺骗性平坦问题，显著提升了黑盒可迁移攻击的性能。

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [126] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: CenterMamba-SAM是一个用于脑部病变分割的端到端框架，通过冻结预训练主干网络并仅训练轻量级适配器实现高效微调，在公共基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 脑部病变分割面临小病灶、低对比度、各向异性采样和跨切片不连续性等挑战，需要更有效的分割方法。

Method: 采用CenterMamba编码器，使用3x3角-轴-中心短序列扫描策略实现中心优先、轴增强和对角补偿的信息聚合；内存驱动的结构提示生成器维护相邻切片间的原型库；内存增强的多尺度解码器集成多级内存注意力模块。

Result: 在公共基准测试中，CenterMamba-SAM实现了最先进的脑部病变分割性能。

Conclusion: 所提出的框架通过中心优先的扫描策略和内存增强机制，有效解决了脑部病变分割中的关键挑战，取得了优异的分割效果。

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [127] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出了一种轻量几何感知适配器，通过方位对齐和水平循环填充来保护边界连续性，使用局部K近邻计算几何统计特征，在训练时驱动区域感知正则化，提升恶劣天气下LiDAR语义分割的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LiDAR语义分割在恶劣天气下性能下降，因为折射、散射和点丢失会破坏几何结构。现有方法忽视了边界、角落和稀疏区域的结构脆弱性。

Method: 设计轻量几何感知适配器，包含方位对齐、水平循环填充、局部K近邻统计特征提取，以及基于这些特征的区域感知正则化训练策略。

Result: 在SemanticKITTI训练、SemanticSTF测试的跨天气设置中，mIoU比数据增强基线提升7.9个百分点，比类别中心正则化基线提升0.6个百分点。

Conclusion: 几何驱动的正则化是实现全天候LiDAR分割的关键方向，该适配器可即插即用，训练时启用且推理成本可忽略。

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [128] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream实现了实时运动条件视频生成，通过将双向教师模型蒸馏为因果学生模型，结合滑动窗口因果注意力和注意力汇聚技术，解决了长视频生成中的延迟、误差累积和计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有运动条件视频生成方法存在延迟高（分钟级）和非因果处理的限制，无法实现实时交互。

Method: 通过自强制分布匹配蒸馏将双向教师模型蒸馏为因果学生模型，采用滑动窗口因果注意力和注意力汇聚技术，结合自展开和KV缓存滚动训练。

Result: 在单个GPU上实现亚秒级延迟和最高29FPS的流式生成，运动跟随和视频质量达到最先进水平，速度提升两个数量级。

Conclusion: MotionStream实现了无限长度流式视频生成，为用户提供实时交互体验，支持轨迹绘制、相机控制和运动传输等应用。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [129] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: PRevivor是一个基于先验引导的颜色转换器，通过学习明清时期绘画来恢复唐宋时期古画的色彩，通过亮度增强和色调校正两个子任务实现色彩复兴。


<details>
  <summary>Details</summary>
Motivation: 中国古代绘画因不可逆的色彩退化而受损，但由于复杂的化学机制和缺乏高质量数据集，色彩复兴非常困难，阻碍了端到端数字修复工具的开发。

Method: 将色彩恢复分解为亮度增强和色调校正两个顺序子任务。亮度增强使用两个变分U-Net和多尺度映射模块；色调校正设计双分支颜色查询模块，一个分支关注掩码先验引导区域进行局部色调校正，另一个分支保持全局推理能力。

Result: 与最先进的着色方法进行广泛实验，结果在定量和定性评估上都表现出优越性能。

Conclusion: PRevivor能够有效恢复古代绘画的色彩退化问题，为文化遗产保护提供了有效的数字修复工具。

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [130] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 这篇综述系统评估了基础模型在医学影像分析中的适应策略，包括监督微调、领域特定预训练、参数高效微调等方法，并指出了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学影像分析中具有巨大潜力，但实际临床应用仍面临领域偏移、标注数据稀缺、计算需求高和隐私要求严格等挑战，需要开发有效的适应策略。

Method: 综述了多种适应方法：监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态框架，并评估了每种方法的性能增益和临床适用性。

Result: 识别了现有方法的性能提升和局限性，同时指出了传统综述往往忽视的权衡和未解决问题，为开发适应性强的医学影像基础模型提供了系统评估。

Conclusion: 通过系统梳理适应策略和研究空白，为开发能够满足真实世界医学影像需求的适应性、可信赖且临床集成的基础模型提供了路线图。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [131] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: 提出基于数据流形几何差异的生成图像检测框架，利用自然图像和生成图像在梯度子空间的正交性，通过自监督模型损失变化进行检测，并使用归一化流放大差异。


<details>
  <summary>Details</summary>
Motivation: 生成图像日益逼真引发滥用担忧，现有二元分类器方法严重依赖生成图像的数量和质量，需要更鲁棒的检测方法。

Method: 设计一对函数使自然图像输出一致而生成图像输出发散，利用梯度正交性；通过自监督模型在数据流形变换时的损失变化检测生成图像；使用归一化流放大流形差异。

Result: 大量实验证明该方法有效，代码已开源。

Conclusion: 提出的基于数据流形几何差异的框架为生成图像检测提供了简单有效的解决方案，能应对先进生成模型带来的挑战。

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [132] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出了UniREditBench基准测试，用于评估基于推理的图像编辑模型性能，包含2700个精心策划的样本，涵盖真实世界和游戏世界场景，并引入多模态双参考评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在处理需要隐式推理的复杂图像编辑任务时表现不佳，现有基准主要关注单对象属性变换，忽视了多对象交互和游戏世界场景，且仅依赖文本参考可能导致系统误判。

Method: 构建包含2700个样本的统一基准，涵盖8个主要维度和18个子维度；设计多模态双参考评估方法（文本和真实图像参考）；开发自动化多场景数据合成流程，创建包含10万样本的合成数据集UniREdit-Data-100K，并基于此微调Bagel模型。

Result: 通过微调Bagel模型开发的UniREdit-Bagel在领域内和领域外设置中都显示出显著改进；对开源和闭源图像编辑模型进行全面基准测试，揭示了它们在不同方面的优势和弱点。

Conclusion: UniREditBench为基于推理的图像编辑提供了全面的评估框架，多模态双参考评估提高了评估可靠性，合成的数据集和微调模型展示了在复杂推理场景中的有效性。

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [133] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: 提出REASON框架，通过两阶段概率图引导的双分支融合方法，实现胃内容物超声评估的自动化，显著提升术前误吸风险评估的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统胃内容物超声评估依赖人工描记和经验公式，存在效率和准确性限制，需要自动化解决方案来改善术前误吸风险评估。

Method: 两阶段框架：第一阶段使用分割模型生成抑制伪影、突出胃解剖结构的概率图；第二阶段通过双分支分类器融合右侧卧位和仰卧位两个标准视图信息，提升特征判别能力。

Result: 在自收集数据集上的实验结果表明，该框架显著优于当前最先进方法，性能提升明显。

Conclusion: REASON框架为临床实践提供了更稳健、高效和准确的自动化术前误吸风险评估解决方案，具有重要临床应用价值。

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [134] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 提出了一种用于全切片病理图像解释性表示的三阶段学习框架，通过潜在因子分组、聚类推理实例解缠和广义线性加权决策来解决空间、语义和决策纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 多实例学习在表示全切片病理图像时存在空间、语义和决策纠缠问题，限制了其表示能力和可解释性。

Method: 采用三阶段方法：1）正半定潜在因子分组映射实例到潜在子空间；2）通过聚类推理实例解缠进行概率反事实推断；3）使用广义线性加权决策进行实例效应重加权。

Result: 在多中心数据集上的实验表明，该模型优于所有最先进模型，并通过解缠表示和透明决策过程实现了与病理学家对齐的可解释性。

Conclusion: 该框架有效解决了MIL中的纠缠问题，在保持高性能的同时实现了病理学级别的可解释性。

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [135] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 提出了APDM框架，通过将保护目标从图像转移到扩散模型本身来防止特定主体的个性化生成，解决了现有方法在少量干净图像或简单图像变换下失效的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高质量特定主体合成能力带来了隐私风险，恶意用户可能滥用个性化技术生成未经授权的内容。现有对抗性扰动方法依赖不现实假设，在少量干净图像或简单变换下失效。

Method: 提出了Anti-Personalized Diffusion Models (APDM)框架：1）理论分析表明现有损失函数无法确保鲁棒反个性化收敛；2）引入Direct Protective Optimization (DPO)损失函数；3）提出Learning to Protect (L2P)双路径优化策略，交替进行个性化和保护路径。

Result: 实验结果表明，该框架优于现有方法，在防止未经授权个性化方面达到了最先进的性能。

Conclusion: APDM框架通过将保护目标转移到扩散模型本身，并采用DPO损失函数和L2P优化策略，有效防止了特定主体的未经授权个性化，同时保持生成质量。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [136] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MVSMamba是首个基于Mamba架构的多视角立体视觉网络，通过动态Mamba模块实现高效全局特征聚合，在DTU和Tanks-and-Temples基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer方法在MVS中存在二次复杂度问题，难以平衡性能与效率。Mamba架构具有全局建模能力和线性复杂度，为解决这一问题提供了新思路。

Method: 提出MVSMamba网络，采用基于参考中心动态扫描策略的动态Mamba模块，实现参考视图与源视图之间的高效特征交互、全方位多视角特征表示和多尺度全局特征聚合。

Result: 在DTU数据集和Tanks-and-Temples基准测试中，MVSMamba超越了当前最先进的MVS方法，在性能和效率方面均表现优异。

Conclusion: Mamba架构在MVS任务中具有巨大潜力，MVSMamba通过创新的动态扫描策略实现了高效全局特征建模，为MVS研究开辟了新方向。

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [137] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP模型的生成对抗攻击方法，通过结合文本和图像表示生成视觉不可察觉但有效的对抗性扰动，在多目标环境中欺骗多标签分类器。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到对抗攻击的影响，现有方法在生成视觉不可察觉的扰动方面存在局限，需要一种能结合自然语言语义的对抗攻击方法。

Method: 将SSAE的集中扰动策略与GAMA的差异文本嵌入相结合，利用CLIP模型对齐文本和图像表示的能力，通过引导损失生成对抗样本。

Result: 在多个黑盒受害者模型上的实验表明，该方法在保持较高视觉保真度的同时，取得了与现有技术相当或更优的攻击效果。

Conclusion: 所提出的方法能够生成视觉上难以察觉且有效的对抗性扰动，在多目标场景中成功欺骗多标签分类器，同时保持图像结构相似性。

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [138] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: RDTE-UNet是一种医学图像分割网络，通过结合局部建模与全局上下文来增强边界描绘和细节保留。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对于计算机辅助诊断和治疗规划至关重要，但解剖结构变异性和边界模糊性阻碍了对精细结构的可靠分割。

Method: 采用混合ResBlock细节感知Transformer主干网络，包含三个模块：ASBE用于自适应边界增强，HVDA用于细粒度特征建模，EulerFF用于基于欧拉公式的融合加权。

Result: 在Synapse和BUSI数据集上，RDTE-UNet在分割精度和边界质量方面达到了可比较的水平。

Conclusion: 该方法通过统一局部建模与全局上下文，提高了跨形态、方向和尺度的结构一致性和边界准确性。

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [139] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: MIQ-SAM3D是一个多实例3D医学图像分割框架，通过竞争性查询优化策略实现从单点-单对象到单点-多实例的分割范式转变，解决了传统SAM方法在多病灶分割中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统SAM交互式分割方法遵循单点-单对象范式，限制了多病灶分割能力；同时ViT骨干网络虽然能捕捉全局上下文，但往往丢失高保真局部细节。

Method: 提出提示条件实例查询生成器将单点提示转换为多个专用查询；采用混合CNN-Transformer编码器通过空间门控将CNN边界显著性注入ViT自注意力；设计竞争优化查询解码器实现端到端并行多实例预测。

Result: 在LiTS17和KiTS21数据集上，MIQ-SAM3D达到可比较的性能水平，并对提示具有强鲁棒性。

Conclusion: 该方法为临床相关多病灶病例的高效标注提供了实用解决方案。

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [140] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: 提出了一种通过内容-风格子空间混合和平衡损失来扩展内容-风格边界的新方法，解决了风格强度增加时内容特征丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在单一风格强度下评估内容相似性，但实验发现增加风格强度会导致内容特征显著丢失，形成次优的内容-风格边界。

Method: 使用内容-风格子空间混合和内容-风格平衡损失来扩展内容-风格边界，提高不同风格强度下的内容相似性。

Result: 在定性和定量评估中均优于现有技术，实现了更优的内容-风格权衡，倒置生成距离和生成距离得分显著低于当前方法。

Conclusion: 该方法有效扩展了内容-风格边界，在保持风格强度的同时更好地保留了内容特征，为文本到图像生成提供了更好的内容-风格平衡。

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [141] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 提出了CMI-MTL框架，通过细粒度视觉-文本特征对齐、跨模态交错特征表示和自由形式答案增强多任务学习，解决了医学视觉问答中的跨模态语义对齐和自由形式答案多样性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于自注意力的方法难以有效处理视觉和语言之间的跨模态语义对齐，且分类方法依赖于预定义答案集，无法适应自由形式答案的多样性。

Method: CMI-MTL包含三个核心模块：FVTA（细粒度视觉-文本特征对齐）、CIFR（跨模态交错特征表示）和FFAE（自由形式答案增强多任务学习）。

Result: 在VQA-RAD、SLAKE和OVQA三个Med-VQA数据集上超越了现有最先进方法，并通过可解释性实验验证了有效性。

Conclusion: CMI-MTL框架能够有效处理医学视觉问答中的跨模态语义对齐问题，并适应自由形式答案的多样性，为临床决策支持和远程医疗提供了有力工具。

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [142] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 提出了一种用于生成水下环境中事件相机合成数据的管道，可用于训练视觉模型，特别是在能见度差和悬浮颗粒物条件下的岩石检测任务。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在光照条件差和高动态范围场景等挑战，传统视觉技术难以适应。事件相机通过逐帧跟踪变化来缓解传统相机的问题，但缺乏真实的水下事件相机数据。

Method: 开发了一个管道，用于生成安装在自主水下航行器上的事件相机在水下环境中的逼真合成数据。

Result: 在能见度差和悬浮颗粒物条件下的岩石检测任务中证明了该管道的有效性。

Conclusion: 该方法可以推广到其他水下任务，为水下事件相机视觉模型训练提供了有效的合成数据生成解决方案。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [143] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: 提出了Fire-ART数据集和全景图像重建方法，用于消防资产的语义化BIM建模，解决了传统方法自动化识别和重建能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统消防资产管理方法效率低下，缺乏自动化资产识别和重建能力，需要更有效的数字化管理解决方案。

Method: 开发了Fire-ART数据集（15类资产，2626张图像，6627个实例），结合改进的立方体贴图转换和基于半径的球面相机投影进行重建。

Result: 在两个实际案例验证中，分别达到73%和88%的F1分数，定位误差为0.620米和0.428米。

Conclusion: Fire-ART数据集和重建方法为消防设备精确数字化管理提供了宝贵资源和强大技术解决方案。

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [144] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 提出了一种基于平滑可调轮廓的训练免费解释方法，用星凸区域参数化替代密集扰动掩码，显著减少参数数量并保证单连通掩码。


<details>
  <summary>Details</summary>
Motivation: 解决现有密集扰动掩码方法存在的碎片化、过拟合问题，需要复杂后处理，寻求更紧凑、稳定的视觉模型解释方法。

Method: 使用截断傅里叶级数参数化星凸区域，在极值保留/删除目标下利用分类器梯度进行优化，限制解空间为低维平滑轮廓。

Result: 在ImageNet分类器上匹配密集掩码的极值保真度，产生紧凑、可解释区域，运行一致性更好，在基准测试中比梯度方法和扰动基线获得更高相关性质量和更低复杂度。

Conclusion: 该方法通过平滑轮廓参数化实现了忠实且紧凑的视觉模型解释，特别在自监督DINO模型上表现优异，相关性质量提升超过15%，并保持正忠实相关性。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [145] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: 提出了一种基于蒸馏的高效因果视频生成框架，通过对抗性自蒸馏策略和首帧增强技术，在极少的去噪步骤下实现高质量视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有混合视频生成模型结合自回归时序动态和基于扩散的空间去噪，但其顺序迭代特性导致错误累积和长推理时间。

Method: 基于分布匹配蒸馏框架，提出对抗性自蒸馏策略，将学生模型n步去噪输出与其(n+1)步版本在分布层面对齐；同时采用首帧增强策略，为首帧分配更多去噪步骤以减少错误传播。

Result: 在VBench上的广泛实验表明，该方法在一步和两步视频生成中均超越最先进方法，且单个蒸馏模型可灵活支持多种推理步骤设置。

Conclusion: 该框架消除了重复再蒸馏的需求，实现了高效高质量的视频合成，在极少数步骤场景下显著提升了训练稳定性和生成质量。

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [146] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: 提出UniSOT统一跟踪器，能够同时处理三种参考模态（边界框、自然语言或两者）和四种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件），使用统一参数实现跨模态跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器通常针对单一或少数几种视频模态和参考模态设计，导致模型分离且限制实际应用。需要统一的跟踪器来满足各种需求。

Method: 开发UniSOT统一跟踪器，采用统一参数设计，能够处理三种参考模态（边界框、自然语言或两者）和四种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件）的不同组合。

Result: 在18个视觉跟踪、视觉语言跟踪和RGB+X跟踪基准测试中，UniSOT表现出优于特定模态对应方法的性能。在TNL2K上所有三种参考模态的AUC超过先前方法3.0%以上，在所有三种RGB+X视频模态上的主要指标超过Un-Track 2.0%以上。

Conclusion: UniSOT作为统一跟踪器，能够有效处理多种参考模态和视频模态的组合，展现出优越的性能和实际应用价值。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [147] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出了一种分辨率感知的token解码器，通过门控交叉注意力和稀疏像素细化，在保持计算效率的同时解决越野语义分割中的边界模糊、稀疏监督和标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 越野语义分割面临边界不一致、稀有类别稀疏监督和普遍标签噪声的挑战。现有方法要么在低分辨率融合导致边缘模糊，要么维持高分辨率路径计算成本高且对噪声敏感。

Method: 使用分辨率感知token解码器：在低分辨率瓶颈进行主要计算；门控交叉注意力注入精细细节；仅对稀疏不确定性选择的像素进行细化；添加边界带一致性正则化器。

Result: 实现了竞争性性能，在过渡区域表现出改进的稳定性。

Conclusion: 该方法在保持计算效率的同时，有效平衡了全局语义、局部一致性和边界保真度，对不完善监督具有鲁棒性。

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [148] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: 提出一种仅用于训练的方法，通过拉近同类特征和推远异类特征来锐化决策边界，同时通过跨模态特征对齐增强热红外特征的语义信息，在保持单模态推理的同时提升夜间热红外检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决夜间热红外检测中的低对比度、弱高频线索导致的重复检测框、小目标漏检和类别混淆问题，避免现有方法对额外传感器或图像翻译的依赖。

Method: 在训练阶段引入两个目标：1）通过对比学习锐化实例级决策边界；2）使用RGB训练的教师模型对齐学生模型的多层金字塔特征，注入跨模态语义先验。

Result: 在实验中优于现有方法，达到了最先进的性能。

Conclusion: 该方法在保持单模态推理的前提下，通过训练阶段的优化有效提升了热红外检测的鲁棒性，解决了夜间感知的关键挑战。

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [149] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 提出了一种模型无关的序数元学习算法(MAOML)，用于训练小型视觉语言模型，在数据稀缺情况下实现水果新鲜度分类任务的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决易腐水果浪费问题需要准确预测新鲜度，但获取专家标注数据成本高导致数据稀缺。现有专有视觉语言模型性能好但存在数据隐私问题，开源模型性能不足且小样本微调效果不佳。

Method: 使用模型无关的序数元学习算法，结合元学习解决数据稀疏性问题，并利用标签的序数性来提升性能。

Result: 在零样本和小样本设置下均达到最先进性能，平均准确率达到行业标准的92.71%。

Conclusion: MAOML算法能够有效训练小型视觉语言模型，在数据稀缺情况下实现与专有模型相当的性能，同时解决了数据隐私问题。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [150] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 本文提出Reg-DPO方法，通过自动构建高质量偏好对(GT-Pair)和引入SFT损失作为正则化项，解决了视频生成中数据构建成本高、训练不稳定和内存消耗大的问题，显著提升了视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法主要遵循图像领域范式，且基于小规模模型(约20亿参数)，难以应对视频任务特有的挑战，如数据构建成本高、训练不稳定和内存消耗大。

Method: 1) 提出GT-Pair自动构建高质量偏好对，使用真实视频作为正样本，模型生成视频作为负样本；2) 引入Reg-DPO，将SFT损失作为正则化项整合到DPO目标中；3) 结合FSDP框架和多种内存优化技术，提升训练容量。

Result: 在I2V和T2V任务上的广泛实验表明，该方法在多个数据集上持续优于现有方法，提供更优的视频生成质量，训练容量比单独使用FSDP提高近三倍。

Conclusion: Reg-DPO方法有效解决了视频生成中的关键挑战，通过自动数据构建、训练稳定性和内存优化，显著提升了视频生成性能。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [151] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出QA-SNNE方法，通过将问题语义融入预测置信度来改进手术视觉问答中的不确定性估计，提高安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 手术视觉问答中错误或模糊的回答可能危害患者安全，现有研究忽视安全行为如模糊意识、转诊专家等，需要改进不确定性估计以实现更安全的决策。

Method: 引入QA-SNNE（问题对齐语义最近邻熵），这是一种黑盒不确定性估计器，通过在医疗文本嵌入空间中比较生成答案与最近邻来测量语义熵，并以问题为条件。

Result: 在EndoVis18-VQA和PitVQA数据集上评估，QA-SNNE在大多数模板内设置中提高了AUROC，零样本模型的AUROC提升15-38%，在模板外压力下仍保持增益，并增强了幻觉检测。

Conclusion: QA-SNNE通过将语义不确定性与问题上下文联系起来，为手术视觉问答中的自动故障检测提供了实用且可解释的步骤，结合LVLM骨干和问题对齐的不确定性估计可提高安全性和临床医生信任度。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [152] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: 提出了一种通过主动预条件化模型来减少视觉Transformer后训练量化误差的方法，通过将激活和权重量化误差建模为高斯噪声来获得平坦的最小值。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法通常忽略了训练好的神经网络与量化模型之间的关系，导致较大的量化误差。需要找到一种高效训练模型无关神经网络的方法，以适应预定义精度的低比特模型。

Method: 将激活量化误差(AQE)和权重量化误差(WQE)统计建模为独立高斯噪声，研究多种噪声注入优化方法以获得平坦最小值。

Result: 实验结果表明该方法有效，为获得低比特后训练量化模型开辟了新途径。

Conclusion: 平坦的全精度神经网络对低比特量化至关重要，通过主动预条件化模型并解耦误差源可以有效减少量化误差。

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [153] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: 提出HMVLM框架，通过MoE LoRA策略解决3D人体运动与语言模型集成中的灾难性遗忘问题，并开发了身体部位特定标记化方法提升姿态表示质量。


<details>
  <summary>Details</summary>
Motivation: 解决人体运动与文本模态之间的差距导致的灾难性遗忘问题，以及开发在异构下游任务中保持泛化能力的自回归兼容姿态表示的技术障碍。

Method: 基于MoE LoRA的统一框架，使用门控网络动态分配LoRA专家权重；引入零专家保留预训练参数；实施身体部位特定标记化，将人体划分为不同关节组。

Result: 实验表明该方法有效缓解了指令调优期间的知识遗忘，并在多样化人体运动下游任务中取得了显著性能。

Conclusion: HMVLM框架成功解决了人体运动与语言模型集成中的关键挑战，为多模态理解和跨模态生成提供了有效解决方案。

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [154] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: SecDiff是一个基于扩散模型的即插即用解码框架，通过伪逆引导采样和自适应引导权重，显著提升了深度联合源信道编码在对抗性无线环境下的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度联合源信道编码框架容易受到物理层对抗威胁（如导频欺骗和子载波干扰）的影响，这会损害语义保真度。

Method: 采用伪逆引导采样和自适应引导权重实现灵活步长控制和高效语义重建；针对干扰攻击使用基于功率的子载波掩码策略，将恢复问题转化为掩码修复问题；针对导频欺骗，将信道估计建模为盲逆问题，开发基于期望最大化的重建算法。

Result: 在对抗性条件下的OFDM信道实验中，SecDiff在重建质量和计算成本之间实现了良好的平衡，优于现有的安全和生成式JSCC基线方法。

Conclusion: SecDiff是实现实用、低延迟和抗攻击语义通信的有前景的一步。

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [155] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: 提出了增强行人对齐网络(EPAN)，用于物联网监控环境下的行人重识别，在Inspection-Personnel数据集上达到90.09%的Rank-1准确率和78.82%的mAP。


<details>
  <summary>Details</summary>
Motivation: 解决物联网智能环境中监控和安全应用的行人重识别问题，特别是在不同视角和环境变化下的鲁棒性需求。

Method: 采用双分支架构来减轻视角和环境变化的影响，在不同尺度和视角下提取对齐信息。

Result: 在Inspection-Personnel数据集上取得了90.09%的Rank-1准确率和78.82%的mAP，展示了强大的特征提取能力。

Conclusion: EPAN在真实世界物联网应用中具有潜力，能够在监控和安全系统中跨不同摄像头实现有效可靠的行人重识别。

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [156] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: 提出了一种基于SE(3)流匹配的概率框架，用于估计6D物体姿态分布，解决了姿态模糊性和多模态问题，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 物体姿态估计面临部分可观测性、遮挡和物体对称性等挑战，导致姿态模糊性和多假设问题。确定性深度网络往往过度自信，无法捕捉底层姿态分布的多模态特性。

Method: 利用SE(3)流匹配的概率框架，通过基于样本的估计建模完整姿态分布，能够在对称物体或严重遮挡等模糊情况下进行不确定性推理。

Result: 在Real275、YCB-V和LM-O数据集上取得了最先进的结果，展示了基于样本的姿态估计在下游机器人操作任务中的应用价值。

Conclusion: 该方法通过概率建模有效解决了物体姿态估计中的模糊性问题，为机器人操作任务提供了不确定性感知的能力。

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [157] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 提出了一种区分处理运动分量的深度和自运动联合学习框架DiMoDE，通过几何约束分别优化每个自运动分量，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督深度和自运动学习方法大多将自运动作为辅助任务，要么混合所有运动类型，要么排除与深度无关的旋转运动，限制了强几何约束的引入，降低了在多样化条件下的可靠性和鲁棒性。

Method: 网络首先对齐源相机和目标相机的光轴和成像平面，通过这种对齐变换光流并量化偏差，对每个自运动分量分别施加几何约束。进一步将联合学习过程重新表述为同轴和共面形式，通过闭式几何关系相互推导深度和每个平移分量。

Result: DiMoDE在多个公共数据集和新收集的多样化真实世界数据集上实现了最先进的性能，特别是在具有挑战性的条件下表现优异。

Conclusion: 通过区分处理运动分量并利用其各自刚性流的几何规律性，提出的方法能够更有效地约束深度和自运动估计，提高了系统的鲁棒性和可靠性。

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [158] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 提出Luminance-Aware Statistical Quantification (LASQ)框架，将低光图像增强重新定义为分层亮度分布上的统计采样过程，通过扩散前向过程自主发现亮度层间的最优转换路径，实现无需正常光参考的无监督分布仿真。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法主要关注确定性像素级映射，忽略了真实环境中亮度转换的连续物理过程，导致在缺乏正常光参考时性能下降。受自然亮度动态经验分析的启发，该方法旨在解决重建保真度与跨场景泛化能力之间的平衡问题。

Method: LASQ将亮度转换重新概念化为强度坐标空间中的幂律分布，可通过分层幂函数近似，用概率采样替代确定性映射。设计扩散前向过程自主发现亮度层间最优转换路径，实现无监督分布仿真。

Result: 该框架显著提升了实际场景中的性能，实现了更适应和通用的光照恢复。在有正常光参考的情况下，在领域特定数据集上取得优异性能，同时在无参考数据集上展现出更好的泛化能力。

Conclusion: LASQ通过统计量化方法成功解决了低光图像增强中重建保真度与泛化能力的平衡问题，为实际应用提供了更灵活有效的解决方案。

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [159] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: 提出一个完整的纹理编辑系统，通过无监督异常检测和自动聚类实现可控的纹理编辑，支持交互式创建任意大小的纹理。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的纹理通常包含各种局部特征（如污渍、撕裂、孔洞等），这些特征对生成逼真纹理至关重要，但传统方法需要手动标注。

Method: 采用基于学习的方法，利用无标签样本进行无监督异常检测，自动将纹理特征聚类为语义连贯的组，然后指导条件图像生成。

Result: 开发了一个从少量图像到多功能生成模型的完整流程，支持用户交互式创建和绘制纹理特征。

Conclusion: 该系统实现了无需手动标注的纹理编辑，提出的扩散编辑和无限平稳纹理生成算法具有通用性，可应用于其他场景。

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [160] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: 提出了一种新的对比学习框架NSYNC，通过生成负样本合成图像来增强文本到图像扩散模型的风格化能力，使用正交梯度更新方法来消除正负样本中的共同特征，从而更好地捕捉独特风格。


<details>
  <summary>Details</summary>
Motivation: 现有文本条件图像生成方法虽然能生成逼真图像，但难以捕捉特定风格特征。即使对目标风格数据集进行微调，仍然难以掌握风格特征。

Method: 使用对比学习框架，生成负样本合成图像集，与真实正样本图像一起训练。通过计算正梯度减去其在负梯度上的投影，得到正交分量来更新参数，消除正负样本中的共同特征。

Result: 在多种画家和插画师风格上的实验表明，该方法在定量和定性评估上都优于基线方法。

Conclusion: NSYNC方法通过负样本合成数据和对比学习，有效提升了文本到图像扩散模型的风格化能力，能够更好地捕捉独特风格特征。

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [161] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 提出五层模型结构化表示驾驶场景，结合基础模型生成罕见驾驶场景，并引入多样性和原创性指标评估合成数据集质量。


<details>
  <summary>Details</summary>
Motivation: 罕见驾驶场景对自动驾驶开发至关重要，但难以遇到，需要模拟或生成这些场景来改进系统。

Method: 使用结构化五层模型表示驾驶场景，结合大型基础模型进行数据增强，生成新场景，并引入嵌入向量比较场景相似度。

Result: 开发了多样性和原创性评分指标来评估合成数据集，在不同生成设置下展示了这些指标的有效性，并提供了合成视频的定性评估。

Conclusion: 提出的结构化表示和评估方法能够有效生成和评估罕见驾驶场景，为自动驾驶系统开发提供了有价值的工具。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [162] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: 提出了PCD-ReID算法，使用Transformer架构提取共享组件特征（如头盔和制服），解决基站环境中遮挡行人重识别问题，在真实巡逻监控数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 基站环境中的遮挡行人重识别对监控和安全应用至关重要，但传统ResNet算法无法有效处理遮挡问题，遮挡会掩盖关键身体特征，增加识别复杂度。

Method: 设计了基于Transformer的PCD网络，能够提取共享组件特征；收集了新的真实世界巡逻监控图像进行训练，包含6个月、1万个体、5万多张图像，以缓解在公共数据集上的过拟合问题。

Result: 与现有ReID算法相比，模型实现了79.0%的平均精度和82.7%的Rank-1准确率，相比基于ResNet50的方法Rank-1提升了15.9%。

Conclusion: PCD-ReID在塔检场景中有效实现了遮挡感知的行人重识别性能，展示了在监控和安全应用中实际部署的潜力。

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [163] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: NOA是一个基于napari的图形界面工具，旨在简化AI驱动的类器官图像分析，整合了检测、分割、追踪、特征提取等多种功能模块。


<details>
  <summary>Details</summary>
Motivation: 现有AI工具对无编程经验的生物学家可访问性有限，导致工作流程仍以人工为主，且现有工具大多只专注于特定任务。

Method: 开发了Napari Organoid Analyzer (NOA)，这是一个开源napari插件，集成了多种最先进算法，提供检测、分割、追踪、特征提取、自定义特征标注和基于机器学习的特征预测等功能。

Result: 通过三个案例研究展示了NOA的通用性：量化类器官分化过程中的形态变化、评估光毒性效应、预测类器官活力和分化状态。

Conclusion: NOA在可访问且可扩展的框架内实现了全面的AI驱动类器官图像分析。

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [164] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: PixelVLA是首个支持像素级推理和多模态提示的视觉-语言-动作模型，通过两阶段自动标注流程生成Pixel-160K数据集，在多个基准测试中显著提升操作成功率，同时大幅降低预训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型存在两个主要局限：(i) 难以进行像素级场景理解，(ii) 过度依赖文本提示，在现实环境中灵活性不足。

Method: 提出新的视觉运动指令调优框架，集成多尺度像素感知编码器和视觉提示编码器，采用两阶段自动标注流程生成Pixel-160K数据集。

Result: 在三个标准VLA基准测试和两个VLA模型变体上，PixelVLA相比OpenVLA将操作成功率提升了10.1%-17.8%，同时仅需其1.5%的预训练成本。

Conclusion: PixelVLA可以集成到现有VLA中，在复杂环境中实现更准确、高效和通用的机器人控制。

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [165] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出基于深度卷积生成对抗网络（DC-GAN）的合成MRI数据生成方法，并使用CNN分类器验证合成图像的质量和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中MRI数据有限的问题，生成逼真的合成医学图像以支持下游任务。

Method: 使用DC-GAN生成合成MRI数据，并采用CNN分类器对真实和合成图像进行脑肿瘤分类来评估图像质量。

Result: 在真实和合成图像上的分类性能相当，验证了GAN生成图像在下游任务中的有效性。

Conclusion: DC-GAN生成的合成MRI数据质量足够好，可以用于实际的医学图像分析任务，解决了数据稀缺问题。

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [166] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: 提出Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT)，通过自适应分配图像基元数量来解决多模态大模型中理解与生成的统一问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型中连续和离散视觉标记化之间的二分法问题，避免连续标记器的复杂流水线和离散标记器的信息损失。

Method: CDD-VT将视觉数据视为量化码本中图像基元的灵活组合，根据样本复杂度自适应分配基元数量：简单样本使用少量基元（类似离散标记化），复杂样本使用大量基元（类似连续标记化）。包含两个核心组件：多样化量化基元和动态基元分配器。

Result: 在重建、检索和分类任务上的广泛实验表明，CDD-VT在性能和简洁性方面均优于专门的连续和离散标记器。

Conclusion: CDD-VT成功统一了多模态大模型中的理解和生成，在保持简洁和可扩展性的同时实现了强大性能。

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [167] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: Lite ENSAM是一个轻量级架构，用于从带有RECIST标注的CT扫描中高效进行肿瘤体积分割，在MICCAI FLARE 2025比赛中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前肿瘤治疗评估主要依赖RECIST v1.1标准的单平面最长直径测量，但体积测量更可靠。然而，手动体积标注耗时费力，限制了临床应用。

Method: 提出了Lite ENSAM架构，这是ENSAM架构的轻量级适配版本，专门设计用于从带有RECIST标注的CT扫描中高效进行肿瘤体积分割。

Result: 在MICCAI FLARE 2025任务1子任务2中，在隐藏测试集上获得60.7%的Dice相似系数和63.6%的归一化表面Dice，在公共验证数据集上平均总RAM时间为50.6GB，CPU平均推理时间为14.4秒。

Conclusion: Lite ENSAM能够高效地从RECIST标注中生成肿瘤体积分割，为更可靠的肿瘤治疗评估提供了可行的解决方案。

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [168] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX是一个模块化、可扩展的自监督视觉基础模型训练框架，整合了DINO系列的核心原理，支持多种Transformer架构和训练策略，显著降低计算成本的同时保持竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型训练流程存在不灵活、领域特定或计算成本高的问题，限制了跨领域和资源环境下的可用性。

Method: 采用统一配置驱动系统，支持多种Transformer架构，集成LoRA、层冻结、知识蒸馏等训练策略，兼容DDP和FSDP分布式训练，支持自然和特殊数据类型。

Result: 在多样化数据集上的实验表明，DINO-MX在显著降低计算成本的同时实现了竞争力性能，并提供可解释性工具和标签引导数据增强方法。

Conclusion: DINO-MX为开发和基准测试自监督视觉模型提供了可重现、可扩展的基础，适用于研究和实际应用。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [169] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: 提出了PSPooling（预计算结构池化）方法，一种用于3D解剖形状分析的非可学习网格池化算子，以及MedShapeNet19基准数据集，通过自监督图自编码器提升解剖形状分类性能。


<details>
  <summary>Details</summary>
Motivation: 解剖3D形状分类受限于网格数据的复杂性和缺乏标准化基准，需要开发稳健的学习方法和可复现的评估框架。

Method: 提出PSPooling池化算子，基于几何邻近性预计算节点对应关系，实现并行化和可逆的池化与反池化操作；构建自监督图自编码器学习解剖感知表示；创建MedShapeNet19基准数据集。

Result: PSPooling显著提高了重建保真度和低标签情况下的分类准确率，为医学3D形状学习建立了强基线。

Conclusion: PSPooling和MedShapeNet19为解剖形状分类提供了有效的工具和标准化基准，有望推动医学3D形状分析的进一步发展。

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [170] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了一种将强化学习有效整合到基于扩散的图像恢复模型中的方法，通过使用图像质量评估模型作为奖励函数，并针对距离真实值较远的困难样本进行重点优化，实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法直接应用于基于扩散的图像恢复模型效果不佳，因为恢复任务与纯生成任务不同，更强调保真度。需要探索如何有效整合RL到扩散恢复模型中。

Method: 使用图像质量评估模型作为奖励函数，针对困难样本进行强化学习，采用MLLM-based IQA模型与高质量图像分布对齐，随着样本接近真实值分布，自适应结合监督微调进行细粒度对齐。

Result: 该方法可无缝应用于基于扩散的恢复模型，在各种恢复任务中显著提升性能，多个基准测试验证了框架的有效性。

Conclusion: 提出的强化学习框架通过动态调整训练策略和自动权重分配，有效提升了扩散基恢复模型的性能，具有即插即用的优势。

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [171] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos是一个统一的图像和视频重光照框架，通过将RGB空间的几何反馈引入流匹配主干网络，解决了扩散模型在重光照任务中物理一致性问题，实现了20倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在重光照任务中由于在语义潜在空间优化，导致物理不正确的结果，如过度曝光的高光、错位的阴影和错误的遮挡。

Method: 通过从模型输出中提取深度和法线图进行监督，将光照效果与场景结构对齐；采用路径一致性学习减少计算开销；设计了六维标注协议实现细粒度控制。

Result: UniLumos实现了最先进的重光照质量，显著提高了物理一致性，在图像和视频重光照上实现了20倍加速。

Conclusion: 通过几何反馈和路径一致性学习，UniLumos在保持高质量的同时大幅提升了重光照的效率和物理正确性。

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [172] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出了一种渐进式网络架构，通过分阶段优化颜色和细胞边界生成，从H&E染色图像合成IHC等效图像，显著提升视觉质量和结构细节。


<details>
  <summary>Details</summary>
Motivation: IHC染色虽然能提供高分辨率蛋白定位信息，但成本高、劳动密集且多重染色能力有限。现有染色转换技术采用线性加权损失函数，无法同时保持结构真实性和颜色保真度。

Method: 基于ASP框架，提出渐进式网络架构，分阶段优化颜色和细胞边界生成，引入DAB色原浓度和图像梯度损失函数。

Result: 在HER2和ER数据集上的实验表明，该方法显著改善了视觉质量，获得了更精细的结构细节。

Conclusion: 渐进式机制能有效解耦不同视觉方面的优化，在IHC图像合成中实现更好的结构保持和颜色保真度。

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [173] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 提出LFRD2框架，结合神经网络表达能力和物理模型可解释性，解决屏下ToF成像中的信号衰减、多路径干扰和时序噪声问题。


<details>
  <summary>Details</summary>
Motivation: 屏下ToF成像面临透明OLED层导致的严重信号退化问题，包括信号衰减、多路径干扰和时序噪声，严重影响深度感知质量。

Method: LFRD2框架包含时间分数阶反应-扩散模块，通过动态生成微分阶数实现迭代深度优化，捕捉长期依赖关系；同时引入基于系数预测和重复微分的连续卷积算子。

Result: 在四个基准数据集上的实验证明了该方法的有效性。

Conclusion: LFRD2框架成功解决了屏下ToF成像中的退化问题，在保持物理模型可解释性的同时提升了深度感知质量。

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [174] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: PRBench是首个专门评估不同鲁棒性训练方法对概率鲁棒性(PR)改进效果的基准测试，系统比较了对抗训练(AT)和PR针对性训练方法，发现AT方法在提升AR和PR性能方面更通用，而PR针对性训练方法具有更低的泛化误差和更高的干净准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型对微小扰动非常脆弱。现有研究主要关注对抗鲁棒性(AR)，而概率鲁棒性(PR)从统计角度评估模型在随机扰动下的性能。虽然PR被认为是AR的重要补充，但专门针对PR的训练方法研究相对不足，且现有方法存在评估协议不可比、与强AT基线比较有限、缺乏统一框架等问题。

Method: 提出了PRBench基准测试，使用包括干净准确率、PR和AR性能、训练效率和泛化误差在内的综合指标集，对常见的AT和PR针对性训练方法进行实证比较，并提供了不同训练方法PR性能泛化误差的理论分析。

Result: 主要发现：AT方法在提升AR和PR性能方面比PR针对性训练方法更具通用性，而PR针对性训练方法始终产生更低的泛化误差和更高的干净准确率。构建了包含222个训练模型的排行榜，涵盖7个数据集和10种模型架构。

Conclusion: PRBench为评估和改进模型概率鲁棒性提供了首个系统基准，揭示了不同训练方法的相对优势，AT方法在鲁棒性提升方面更通用，而PR针对性训练方法在泛化性和干净性能方面表现更好。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [175] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: 开发了一个任务探索管道，使用聚类技术结合因子分析和字符串编辑距离，自动识别完成任务的关键全局和局部策略，并识别任务中的有意义子任务。


<details>
  <summary>Details</summary>
Motivation: 推进机器对用户知识、技能和行为的理解，以实现隐式协调，建立在预期人机交互研究的基础上。

Method: 开发任务探索管道，使用聚类技术、因子分析和字符串编辑距离自动识别全局策略（完成任务的动作集合）和局部策略（动作序列的相似组合），并识别不同长度的有意义子任务。

Result: 任务探索管道能够自动识别完成任务的关键策略，并用层次性子任务结构编码用户运行。开发了Task Explorer应用程序来轻松查看管道结果。

Conclusion: 任务探索管道可轻松修改以适应任何基于动作的时间序列数据，识别出的策略和子任务有助于人类和机器了解用户的知识、技能和行为。

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [176] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: CGF-DETR是一种改进的实时检测变换器，专为胸部X光肺炎检测设计，通过XFABlock、SPGA和GCFC3模块提升性能，在RSNA数据集上达到82.2% mAP@0.5，比基线RT-DETR-l提升3.7%，同时保持48.1 FPS的实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 肺炎是全球发病率和死亡率的主要原因，需要准确高效的自动检测系统。虽然RT-DETR等变换器检测器在目标检测任务中表现良好，但在医学影像特别是胸部X光肺炎检测中的应用仍待探索。

Method: 提出CGF-DETR模型：1）在骨干网络中引入XFABlock，通过卷积注意力机制与CSP架构改进多尺度特征提取；2）提出SPGA模块，用动态门控机制和单头自注意力替代标准多头注意力；3）在颈部设计GCFC3，通过多路径卷积融合增强特征表示，同时通过结构重参数化保持实时性能。

Result: 在RSNA肺炎检测数据集上，CGF-DETR达到82.2% mAP@0.5，比基线RT-DETR-l提升3.7%，同时保持48.1 FPS的推理速度。消融研究证实每个模块都对性能提升有贡献，完整模型达到50.4% mAP@[0.5:0.95]。

Conclusion: CGF-DETR成功将变换器检测器应用于医学影像领域，通过专门设计的模块显著提升了肺炎检测性能，同时保持了实时推理能力，为医学影像分析提供了有效的解决方案。

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [177] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 3EED是一个多平台、多模态的3D视觉定位基准，包含车辆、无人机和四足机器人的RGB和LiDAR数据，规模比现有数据集大10倍，提供跨平台评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位基准局限于室内环境、单一平台和小规模，需要扩展到开放世界的多平台场景。

Method: 开发了结合视觉语言模型提示和人工验证的可扩展标注流程，提出平台感知归一化和跨模态对齐技术。

Result: 构建了包含12.8万个对象和2.2万个验证引用表达的数据集，建立了域内和跨平台评估协议，揭示了显著的性能差距。

Conclusion: 3EED数据集和基准工具包的发布将推动语言驱动的3D具身感知研究，突出了可泛化3D定位的挑战和机遇。

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [178] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: HGFreNet是一种新颖的GraphFormer架构，通过跳数混合特征聚合和频域3D轨迹一致性来解决2D到3D人体姿态提升问题，在位置精度和时间一致性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决2D到3D人体姿态提升中的深度模糊性和2D姿态估计误差导致的3D轨迹不连贯问题，传统方法仅约束相邻帧差异而忽略了骨骼关节运动的全局时空相关性。

Method: 提出HGFreNet架构，包含跳数混合图注意力模块和Transformer编码器来建模全局关节时空相关性，并在频域约束轨迹一致性，使用初步网络估计3D姿态以提供跨帧深度推断。

Result: 在Human3.6M和MPI-INF-3DHP基准数据集上的实验表明，HGFreNet在位置精度和时间一致性方面优于最先进方法。

Conclusion: HGFreNet通过跳数混合特征聚合和频域轨迹一致性约束，有效提升了2D到3D人体姿态估计的准确性和时间连贯性。

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [179] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: Wonder3D++ 是一种从单视图图像高效生成高保真纹理网格的新方法，通过跨域扩散模型生成多视角法线图和彩色图像，在约3分钟内完成高质量3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么基于SDS的优化过程耗时且几何不一致，要么直接推理生成但质量低且缺乏几何细节。需要全面提升单视图重建任务的质量、一致性和效率。

Method: 提出跨域扩散模型生成多视角法线图和对应彩色图像；采用多视角跨域注意力机制确保生成一致性；引入级联3D网格提取算法，以粗到细方式从2D表示中提取高质量表面。

Result: 方法实现了高质量重建结果，具有鲁棒的泛化能力和良好的效率，相比先前工作表现更优。

Conclusion: Wonder3D++ 在单视图3D重建任务中实现了质量、一致性和效率的全面提升，为高效高质量3D内容生成提供了有效解决方案。

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [180] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: UniLION是一个统一的自动驾驶模型，使用线性组RNN算子高效处理大规模LiDAR点云、高分辨率多视角图像和时间序列，无需显式的时间或多模态融合模块。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长序列数据时存在二次注意力机制带来的显著计算开销问题，需要更高效的架构来处理自动驾驶中的大规模多模态数据。

Method: 基于线性组RNN算子（对分组特征执行线性RNN），构建统一的单一架构，支持LiDAR-only、时序LiDAR、多模态和多模态时序融合等多种配置。

Result: 在3D感知（物体检测、跟踪、占用预测、BEV地图分割）、预测（运动预测）和规划（端到端规划）等核心任务上均取得竞争性甚至最先进的性能。

Conclusion: UniLION为自动驾驶3D基础模型的发展提供了新视角，简化了多模态多任务系统的设计，同时保持优越性能。

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [181] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出了一种基于提示驱动的GraphRAG框架，强调提示设计在图检索增强生成中的重要性，在HotpotQA和2WikiMultiHopQA数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法在复杂推理方面已有研究，但提示设计对检索和推理过程的影响尚未得到充分研究。

Method: 构建符号知识图谱，将实体和事实关系编码为结构化三元组；在在线检索中选择性使用LLM进行语义过滤和答案生成；通过个性化PageRank实现实体引导的图遍历。

Result: 在HotpotQA上F1得分为80.7%，Recall@5为97.1%；在2WikiMultiHopQA上F1得分为78.9%，Recall@5为98.1%。

Conclusion: 提示设计是提高检索准确性和响应质量的重要组成部分，为更高效和可理解的多跳问答系统奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [182] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: 提出了Scitextures数据集，包含1200多个模型和10万张来自科学、技术和艺术领域的纹理和视觉模式图像，用于研究视觉模式与生成机制之间的联系。


<details>
  <summary>Details</summary>
Motivation: 探索视觉模式与形成机制之间的深层联系，这种联系代表了视觉理解的最深层次。

Method: 使用自主AI流水线收集和实现标准化模型，创建大规模纹理数据集，并评估AI模型链接视觉模式与生成机制的能力。

Result: 视觉语言模型能够理解和模拟视觉模式背后的物理系统，超越单纯的视觉识别。

Conclusion: 该数据集为研究视觉模式与生成机制之间的联系提供了重要资源，展示了AI在理解复杂物理系统方面的潜力。

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [183] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 提出了TIR-Bench基准测试，用于评估视觉推理中基于工具的图像处理能力，测试了22个多模态大语言模型，发现该基准具有挑战性且需要真正的图像思维能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分捕捉像OpenAI o3这样的模型通过创建和操作工具来转换图像进行问题解决的高级能力，即使是视觉搜索这样的基准也只测试基本操作。

Method: 构建了包含13个多样化任务的TIR-Bench基准，每个任务都需要在思维链中进行新颖的工具使用来进行图像处理和操作，并评估了22个多模态大语言模型。

Result: TIR-Bench对所有模型都具有挑战性，强性能需要真正的图像思维能力，还进行了直接与代理式微调的初步比较研究。

Conclusion: TIR-Bench是一个全面评估代理式图像思维的基准，揭示了当前模型在复杂、动态和工具依赖的视觉推理方面的局限性。

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [184] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: PlotCraft是一个包含1000个挑战性可视化任务的新基准，用于评估LLM在复杂数据可视化方面的能力。研究发现现有LLM表现不足，因此开发了SynthVis-30K数据集和PlotCraftor模型，在多个基准测试中达到与领先专有方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在代码生成方面表现出色，但在处理复杂可视化任务方面的能力尚未得到充分评估和发展，特别是在处理规模化结构化数据时存在明显不足。

Method: 开发了PlotCraft基准测试，包含1000个涵盖多个领域的可视化任务，涉及7个高级可视化任务和48种图表类型。通过协作代理框架合成了SynthVis-30K大规模高质量数据集，并基于此开发了PlotCraftor代码生成模型。

Result: 对23个领先LLM的评估显示它们在处理复杂可视化任务时存在明显性能缺陷。PlotCraftor模型在VisEval、PandasPlotBench和PlotCraft基准测试中表现与领先专有方法相当，在困难任务上实现了超过50%的性能提升。

Conclusion: PlotCraft基准填补了LLM在复杂数据可视化评估方面的空白，PlotCraftor模型证明了即使在较小规模下也能实现强大的复杂数据可视化能力。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [185] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: ProtoMBTI是一个基于原型理论的MBTI人格识别框架，通过LLM引导的数据增强、原型标准化和检索-重用-修订-保留循环，在多个基准测试中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统的人格识别方法采用硬标签分类，忽视了人格判断的渐进性和原型特性，需要更符合认知心理学原理的建模方法。

Method: 构建平衡语料库，LoRA微调轻量级编码器学习判别性嵌入和人格原型，采用检索-重用-修订-保留循环进行推理。

Result: 在Kaggle和Pandora基准测试中，ProtoMBTI在四个MBTI维度和完整16型任务上均优于基线方法，并展现出强大的跨数据集泛化能力。

Conclusion: 将推理过程与心理原型推理对齐，可以提高基于文本的人格建模的准确性、可解释性和迁移性。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [186] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 提出了残差流解码器框架，用于从语言模型激活中解码段落级和文档级的规划信息，发现小模型能解码相当于5个以上未来token的信息。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型处理更长时程任务的能力增强，现有理解激活的方法通常局限于测试特定概念或token，需要开发能理解更长时程规划信息的方法。

Method: 开发了残差流解码器框架，通过多种方法探测模型激活中的段落级和文档级规划信息。

Result: 在小模型中能够解码相当于5个以上未来token的信息，表明模型激活中包含显著的未来规划信息。

Conclusion: 为更好地监控语言模型和理解其如何编码长期规划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [187] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 该论文挑战了使用下一词预测训练大语言模型的传统方法，提出通过预测信息丰富的词元来更有效地训练LLM，并在算术、多标签文本分类和自然语言生成任务中验证了该方法的效果。


<details>
  <summary>Details</summary>
Motivation: 优化大语言模型的训练性能是一个关键挑战，特别是在提升模型性能的同时控制计算成本。传统使用下一词预测的方法可能不是最优策略。

Method: 提出一种替代下一词预测的方法，通过选择信息丰富的目标词元进行训练，而不是简单地预测下一个词元。

Result: 在算术、多标签文本分类和自然语言生成三类任务中验证了所提方法的有效性。

Conclusion: 这项工作为优化LLM训练提供了原则性方法，既提升了模型性能，又增进了对目标词元选择策略的理论理解。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [188] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 提出了一个评估和改进LLM角色一致性的统一框架，通过三个自动指标和多轮强化学习微调，将角色不一致性降低了55%以上。


<details>
  <summary>Details</summary>
Motivation: LLM在模拟人类用户时经常偏离分配的角色、自相矛盾或放弃角色适当行为，影响模拟效果。

Method: 定义了三个自动一致性指标（提示到语句、语句到语句、问答一致性），并作为奖励信号应用多轮强化学习来微调LLM。

Result: 方法将角色不一致性降低了超过55%，产生了更连贯和忠实的模拟用户。

Conclusion: 该框架能有效提升LLM在角色扮演场景中的一致性，为AI代理的训练和评估提供更可靠的模拟用户。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [189] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: AgentBnB是一个基于浏览器的网络安全桌面演习系统，整合了大型语言模型队友和检索增强的副驾驶，提供按需的认知针对性提示，相比传统演习更轻量、可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统的网络安全桌面演习脚本化、资源密集且难以扩展，需要更轻量、可重复的练习方案。

Method: 重新设计了Backdoors & Breaches游戏，集成LLM队友和检索增强副驾驶(C2D2)，将精选语料库扩展为事实性、概念性、程序性和元认知片段，采用逐步淡出的脚手架阶梯方法。

Result: 在4名研究生的单人试点中，参与者更倾向于使用基于代理的版本，认为其更具可扩展性，但在简单知识测验中出现天花板效应。

Conclusion: 尽管存在样本量小、单人模式等限制，早期结果表明LLM增强的桌面演习可以提供轻量级、可重复的练习，无需传统演习的后勤负担。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [190] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出了IL-PCR语料库，为法规检索和判例检索任务提供统一测试平台，并开发了基于LLM的重排序方法以利用两个任务之间的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 法律实践中法规检索和判例检索是相关但被独立处理的任务，研究人员希望利用它们之间的内在联系（相似案件引用相似法规）来提高检索性能。

Method: 构建IL-PCR语料库作为统一测试平台，实验了词法模型、语义模型和基于GNN的集成模型，并开发了基于LLM的重排序方法。

Result: 基于LLM的重排序方法在两个检索任务上取得了最佳性能。

Conclusion: IL-PCR语料库为法规和判例检索提供了有效测试平台，利用任务间依赖关系的LLM重排序方法显著提升了检索效果。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [191] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出POSESTITCH-SLT预训练方案，通过模板生成句子对训练，在How2Sign和iSign数据集上显著提升手语翻译性能


<details>
  <summary>Details</summary>
Motivation: 手语翻译面临大规模句子对齐数据集稀缺的挑战，需要解决低资源环境下的翻译问题

Method: 基于语言模板的句子生成技术，提出新颖的预训练方案，使用简单的transformer编码器-解码器架构

Result: 在How2Sign数据集上BLEU-4从1.97提升到4.56，在iSign数据集上从0.55提升到3.43，超越了基于姿态的无词汇表翻译的现有最佳方法

Conclusion: 模板驱动的合成监督在低资源手语设置中非常有效

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [192] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出Factorization Memory，一种高效的RNN架构，在短上下文语言建模任务中达到Transformer性能，在长上下文场景中表现更优，基于Mamba-2构建，支持训练时并行计算和推理时恒定复杂度。


<details>
  <summary>Details</summary>
Motivation: 开发一种既能匹配Transformer在短上下文任务中的性能，又能在长上下文场景中表现出更好泛化能力的RNN架构，同时保持训练效率和推理时的计算稳定性。

Method: 基于Mamba-2构建Factorization Memory，支持训练时并行计算和推理时恒定复杂度；开发稀疏版本的Factorization Memory，仅更新部分循环状态但保持性能。

Result: Factorization Memory在短上下文语言建模任务中达到Transformer相当的性能，在长上下文场景中表现更优；稀疏版本在保持性能的同时进一步提升效率。

Conclusion: Factorization Memory是首个成功结合稀疏内存激活并在短长上下文设置中均具竞争力的RNN架构，为Transformer和Mamba-2提供了系统性的实证分析。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [193] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 论文形式化定义了因果语言建模目标的逆转不变性，指出标准CLM预训练是方向盲的，这解释了为什么在反向文本上训练的模型能达到与正向文本相当的性能。作者认为这种不变性是当前预训练目标的局限，并提出了考虑时间不对称性的新视角。


<details>
  <summary>Details</summary>
Motivation: 自然语言具有内在的时间不对称性（如语音、形态和因果关系），但标准因果语言建模目标对文本及其逆转赋予相同的似然度，这种对称性可能无法捕捉语言中的方向性依赖关系。

Method: 通过形式化分析因果语言建模目标的逆转不变性，论证标准预训练是方向盲的，并提出从时间不对称性的角度重新审视预训练。

Result: 研究表明在反向文本上训练的模型性能与正向文本相当，验证了CLM目标的逆转不变性，但这种对称性可能阻碍模型学习语言中的方向性依赖。

Conclusion: 当前预训练目标的对称性是一个局限而非良性特征，未来应开发能显式建模语言方向性的损失函数和架构，同时保持标准语言建模能力。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [194] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: LingGym是一个评估LLMs元语言推理能力的新基准，使用跨语言注释文本和语法描述，测试模型在未训练过的低资源语言和结构上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否能在训练中未见过的低资源语言和结构上进行语言推理泛化，而不仅仅是特定下游任务。

Method: 使用来自18种类型学多样参考语法的跨语言注释文本和语法描述，设计了词-注释推理任务，模型需要根据不同程度的语言信息推断缺失的单词和注释。

Result: 结果表明，结合结构化语言线索能在所有模型中持续提升推理性能。

Conclusion: 这项工作凸显了使用LLMs进行类型学语言分析和低资源语言文档化的前景与当前局限性。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [195] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 该论文提出了推理轨迹生成任务，用于苏格拉底式调试，通过引导轨迹帮助学生识别和修复编程错误，并展示了前沿LLM模型在该任务上的良好表现。


<details>
  <summary>Details</summary>
Motivation: 大多数新手程序员的错误源于编程误解，苏格拉底式调试通过引导推理轨迹帮助学生自己发现和修正错误，而不是直接提供修复方案。

Method: 引入了推理轨迹生成任务，创建了手动标注的调试问题数据集，并开发了基于LLM的推理轨迹生成和苏格拉底对话解决方案。

Result: 前沿模型能够生成高达91%正确的推理轨迹和98.7%有效的对话轮次，通过大规模LLM作为评判者的评估验证了效果。

Conclusion: LLM能够有效生成用于苏格拉底式调试的推理轨迹和对话，为编程教育中的错误调试提供了新的自动化解决方案。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [196] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 论文研究了迭代改写文本如何逃避AI生成文本检测器，揭示了现有检测器在对抗改写攻击时的脆弱性，并提出了首个系统性评估检测器鲁棒性的基准PADBen。


<details>
  <summary>Details</summary>
Motivation: 虽然AI生成文本检测器对直接LLM输出能达到90%以上准确率，但在面对迭代改写内容时完全失效。研究旨在揭示这种失效的根本原因，并评估检测器在两种改写攻击场景下的脆弱性。

Method: 通过内在机制分析揭示迭代改写创建了语义位移但保留生成模式的中间洗白区域。构建PADBen基准，包含五类文本分类法和五个渐进检测任务，评估11种最先进检测器。

Result: 发现检测器存在关键不对称性：能成功识别抄袭逃避问题，但在作者身份混淆案例中失败。现有检测方法无法有效处理中间洗白区域。

Conclusion: 当前检测方法无法有效应对中间洗白区域，需要在检测架构上进行根本性改进，超越现有的语义和风格区分方法。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [197] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: MedRECT是首个跨语言医疗错误纠正基准，包含日语和英语版本，评估LLM在医疗错误检测、定位和纠正三个子任务上的表现。研究发现推理模型表现最佳，跨语言评估显示英语到日语存在性能差距，微调后的模型在结构化医疗错误纠正任务上超过了人类专家。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗应用中的潜力日益增长，但其检测和纠正临床文本错误的能力——安全部署的前提条件——仍然缺乏充分评估，特别是在英语以外的语言中。

Method: 从日本医师国家考试构建MedRECT基准，包含日语和英语版本，评估9个当代LLM，包括专有模型、开源模型和推理模型，并进行针对性LoRA微调。

Result: 推理模型显著优于标准架构，在错误检测和句子提取方面分别有13.5%和51.0%的相对改进；跨语言评估显示英语到日语存在5-10%性能差距；微调后的模型在日语和英语的错误纠正性能分别提升0.078和0.168，且超过了人类专家表现。

Conclusion: MedRECT为开发更安全的跨语言医疗LLM提供了可复现的框架和资源，是首个全面的跨语言医疗错误纠正基准。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [198] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出G2方法，一种无需训练的即插即用技术，通过双引导机制增强大语言模型的输出多样性，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在输出多样性方面存在局限，常生成高度相似的内容，影响需要多样输出的任务。现有方法如温度调节虽能增强多样性但会损害输出质量。

Method: 使用基础生成器配合双引导机制，通过解码干预引导生成过程，在原始查询条件下鼓励更多样化的输出。

Result: 综合实验表明G2能有效提升输出多样性，同时在多样性和质量之间保持最佳平衡。

Conclusion: G2方法成功解决了大语言模型输出多样性不足的问题，实现了多样性与质量的平衡。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [199] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith是一个模块化框架，通过遗传算法对RAG系统的46,080种可行配置进行端到端架构搜索，在六个维基百科领域上平均比基线提升3.8%，发现向量检索加后生成反思/修订的鲁棒骨干架构。


<details>
  <summary>Details</summary>
Motivation: RAG质量依赖于检索、排序、增强、提示和生成等多个模块的交互选择，孤立优化模块很脆弱，需要端到端的优化方法。

Method: 使用遗传搜索算法，在9个技术家族和46,080种可行流水线配置中进行端到端架构搜索，优化结合检索指标和生成指标的标量目标函数。

Result: 在六个维基百科领域上，RAGSmith找到的配置平均比朴素RAG基线提升3.8%（范围1.2%到6.9%），检索提升最高12.5%，生成提升最高7.5%。搜索仅探索约0.2%的空间（约100个候选）。

Conclusion: 发现了向量检索加后生成反思/修订的鲁棒骨干架构，改进幅度与问题类型相关，为构建有效RAG系统提供了实用的领域感知指导，证明了进化搜索在全流水线优化中的效用。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [200] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型记忆效应对合著网络的影响，发现模型在生成合著网络时存在系统性偏见，倾向于高被引研究者，但这种偏见在不同学科和地区存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在学术搜索和推荐系统中的广泛应用，其记忆效应可能导致合著网络生成中的公平性和偏见问题，影响信息生态系统的完整性。

Method: 评估了DeepSeek R1、Llama 4 Scout和Mixtral 8x7B三个主流模型，分析记忆驱动输出在学科和地区间的差异。

Result: 全球分析显示模型存在系统性偏见，偏向高被引研究者，但临床医学等学科和非洲部分地区表现出更均衡的代表性。

Conclusion: LLMs在学术发现应用中既存在风险也蕴含机遇，需要关注训练数据的公平性以改善模型表现。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [201] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文构建了首个大规模Bhili-Hindi-English平行语料库BHEPC，包含11万句经过精心整理的句子，并评估了多种多语言大模型在Bhili机器翻译任务上的表现，发现微调的NLLB-200模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性给机器翻译带来挑战，特别是像Bhili这样的少数民族部落语言缺乏高质量语言资源，需要填补这一资源空白。

Method: 创建BHEPC平行语料库，包含11万句Bhili-Hindi-English平行句子，涵盖教育、行政和新闻等关键领域；评估多种专有和开源多语言大模型在双向翻译任务上的表现；研究多语言LLM的生成式翻译能力。

Result: 微调的NLLB-200蒸馏600M变体模型在所有模型中表现最佳，突显了多语言模型在低资源场景中的潜力；通过上下文学习评估了跨领域泛化能力并量化了分布差异。

Conclusion: 这项工作填补了关键资源空白，促进了全球低资源和边缘化语言的包容性自然语言处理技术发展。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [202] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 该研究探索了在基于图的检索增强生成架构中使用大语言模型进行能源效率问答，通过从能源文档自动提取知识图谱，在多语言环境中提供准确答案，验证结果显示75.2%的准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用大语言模型和图结构结合的方法，解决能源效率领域的专业问答需求，特别是在多语言环境下的准确性问题。

Method: 系统自动从能源指导和监管文档中提取知识图谱，然后通过图导航和推理为用户提供多语言准确答案，并使用RAGAs框架进行人工验证。

Result: 验证结果显示系统在约75.2%的情况下能正确回答问题，通用能源效率问题准确率可达81.0%，多语言能力表现良好（翻译仅导致4.4%准确率损失）。

Conclusion: 该架构展现了良好潜力，确认了其优势并识别了弱点，为能源效率领域的智能问答系统提供了可行方案。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [203] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文首次在差分隐私文本隐私化评估中引入数据集规模因素，通过在大规模数据集上进行效用和隐私测试，揭示了数据集规模对隐私-效用权衡的重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私自然语言处理研究在评估文本重写机制时往往忽略数据集规模的影响，本文旨在填补这一空白，研究数据集规模对机制效用和隐私保护效果的影响。

Method: 设计在大规模数据集上的效用和隐私测试，使用动态分割大小，在包含多达100万个文本的不同规模数据集上运行测试，量化数据集规模增加对隐私-效用权衡的影响。

Result: 研究发现数据集规模在评估差分隐私文本重写机制中起着关键作用，数据集规模的变化显著影响隐私-效用权衡关系。

Conclusion: 研究结果呼吁差分隐私自然语言处理领域需要更严格的评估程序，并为差分隐私自然语言处理在实践和大规模应用中的未来发展提供了启示。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [204] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出了ToM框架，通过树形MapReduce方法解决大语言模型在长上下文推理中的逻辑连贯性问题，显著优于现有的分治框架和检索增强生成方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型受限于有限的上下文窗口，在长上下文推理中性能显著下降。现有的RAG方法依赖相似性排序牺牲逻辑连贯性，而分治框架难以捕捉长距离依赖关系且可能产生冲突。

Method: 利用长文档的层次结构构建DocTree，通过层次语义解析和自底向上聚合，采用树形MapReduce方法进行递归推理：Map步骤在子节点生成推理依据，Reduce步骤在父节点聚合解决冲突或达成共识。

Result: 在70B+大语言模型上的实验结果显示，ToM显著优于现有的分治框架和检索增强生成方法，实现了更好的逻辑连贯性和长上下文推理能力。

Conclusion: ToM框架通过利用文档层次结构和树形MapReduce方法，有效解决了长上下文推理中的逻辑连贯性问题，为处理长文档提供了新的解决方案。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [205] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG通过识别和修剪RAG外部语料库中的冗余知识，减少检索工作量并提高LLM内部知识的利用效率，在保持性能的同时加速检索过程。


<details>
  <summary>Details</summary>
Motivation: 随着LLM内部知识的扩展，外部语料库与LLM之间存在显著的知识冗余，这不仅增加了密集检索的索引成本，而且冗余知识反而会损害LLM能够自行回答问题的RAG性能。

Method: 提出Mastery-Score指标识别冗余知识并修剪语料库；使用Query Router和Noise-Tolerant Tuning来避免不相关文档的干扰，提高LLM对内部知识的利用。

Result: 实验结果显示，Zero-RAG将维基百科语料库修剪了30%，检索阶段加速了22%，且不损害RAG的性能。

Conclusion: Zero-RAG有效解决了RAG中的知识冗余问题，通过修剪冗余知识和优化内部知识利用，实现了检索效率的提升和性能的保持。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [206] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 在尼泊尔农村地区开发离线运行的轻量级对话AI模型，用于医疗咨询，覆盖10种常见疾病。


<details>
  <summary>Details</summary>
Motivation: 解决农村地区缺乏互联网连接和云基础设施的问题，为资源受限环境提供医疗对话支持。

Method: 在合成的医生-患者交互数据集上微调DialoGPT模型，数据集涵盖尼泊尔农村10种常见疾病。

Result: 微调后的模型能够生成连贯、上下文相关且医学上适当的回应，表现出对症状、疾病背景和同理心沟通的理解。

Conclusion: 紧凑的离线对话模型在低资源医疗环境中具有良好适应性，为农村医疗对话AI提供了有前景的方向。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [207] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文研究了BERT、ALBERT、RoBERTa和DistilBERT等transformer模型中的性别偏见，提出了一种新的偏见度量方法MALoR，并通过反事实数据增强的继续预训练方法有效降低了性别偏见。


<details>
  <summary>Details</summary>
Motivation: 编码器型transformer模型在各种语言任务中表现出色，但继承了训练数据中的性别偏见，需要研究这些偏见并开发缓解方法。

Method: 引入MALoR度量方法评估掩码填充概率中的偏见，提出基于反事实数据增强的继续预训练缓解策略。

Result: 缓解方法显著降低了不同代词对的偏见分数，如BERT-base中"he-she"偏见从1.27降至0.08，"his-her"从2.51降至0.36，且不影响下游任务性能。

Conclusion: 提出的方法能有效减少transformer模型中的性别偏见，同时保持模型在下游任务中的性能表现。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [208] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 提出WordSaladChopper(WSC)组件，通过检测和删除大型推理模型中的无用自我重复token（word salad），显著减少输出长度同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型输出token成本高昂，其中大量是无用的自我重复内容，这些内容消耗解码预算但不增加价值。

Method: 利用模型对<\\n\\n>标记的隐藏状态模式，通过单层线性分类器实时检测word salad行为，检测后通过简单截断和重新生成提示来节省长度。

Result: WSC能够显著节省输出长度，同时质量损失最小，是一个轻量级、即插即用的组件。

Conclusion: WSC或类似组件是所有考虑用户体验的大型推理模型应用的必要组件，因为word salad token缺乏语义价值且WSC开销低、节省效果强。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [209] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 提出了CISEA-MRFE框架，通过上下文指令、语义增强增强和多细化特征提取来改进情感分析，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习情感分析方法在细微情感线索、领域迁移和不平衡情感分布场景下表现不佳，主要问题包括语义基础不足、泛化能力差和偏向主导情感类别。

Method: CISEA-MRFE框架整合三个组件：上下文指令(CI)用于情感消歧指导，语义增强增强(SEA)通过情感一致性释义增强提高鲁棒性，多细化特征提取(MRFE)结合尺度自适应深度编码器(SADE)和多尺度特征专业化以及情感评估器上下文编码器(EECE)进行情感感知序列建模。

Result: 在四个基准数据集上的实验结果显示，CISEA-MRFE相比强基线方法取得显著提升：IMDb准确率提升4.6%，Yelp提升6.5%，Twitter提升30.3%，Amazon提升4.1%。

Conclusion: 该方法验证了所提框架在跨领域情感分类中的有效性和泛化能力，能够显著改进情感分析性能。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [210] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出ISA（意图转移攻击），通过最小化编辑原始有害请求来混淆LLM对攻击意图的识别，相比直接有害提示将攻击成功率提高70%以上。


<details>
  <summary>Details</summary>
Motivation: 现有攻击主要通过添加额外上下文或对抗性token来分散LLM注意力，但未改变核心有害意图。需要研究更隐蔽的攻击方式以增强LLM安全性。

Method: 建立意图转换分类法，利用其对原始请求进行最小编辑，生成看似无害的信息请求，而非依赖复杂token或冗长上下文。

Result: 在开源和商业LLM上的实验显示，ISA攻击成功率相比直接有害提示提升超过70%。仅使用ISA模板重新表述的良性数据微调模型，成功率接近100%。

Conclusion: ISA揭示了LLM在意图推断方面的根本性安全挑战，现有防御方法对其无效，需要开发更有效的防御策略。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [211] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA是一种基于EVA的高效Transformer注意力实现，通过微调使模型适应FlashEVA注意力，在仅使用15亿token进行微调的情况下保持下游任务效果，推理时吞吐量提升6.7倍，GPU峰值内存使用降低5倍。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然性能优异，但其内存需求（特别是需要维护完整上下文）给推理带来了显著挑战。

Method: 提出了FlashEVA——EVA（通过控制变量实现高效注意力）的高效实现，并展示了如何微调Transformer模型以适配FlashEVA注意力机制。

Result: FlashEVA在推理时相比标准Transformer实现，吞吐量最高提升6.7倍，GPU峰值内存使用降低5倍，同时通过可调节超参数控制吞吐量与准确率之间的权衡。

Conclusion: 这项工作代表了向更高效和适应性更强的基于Transformer的推理模型迈出的重要一步，尽管在检索密集型任务中观察到了一些局限性。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [212] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: OpenSIR是一个无需外部监督的自我改进推理框架，通过让LLM在教师和学生角色间切换来自主生成和解决新颖问题，实现开放式的数学发现。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM推理方法依赖带标注的验证数据集，这限制了模型超越人类水平的能力。自我博弈提供了有前景的替代方案，但现有方法需要外部验证器或无法实现开放式学习。

Method: OpenSIR采用自我博弈框架，LLM交替扮演教师和学生角色：教师生成既困难又多样的问题，学生解决这些问题。通过优化难度和多样性来奖励挑战适当且探索不同概念的问题。

Result: 从单个简单种子问题开始，OpenSIR显著提升了指令模型的性能：Llama-3.2-3B-Instruct在GSM8K上从73.9提升到78.3，在大学数学上从28.8提升到34.4；Gemma-2-2B-Instruct在GSM8K上从38.5提升到58.7。

Conclusion: OpenSIR通过共同进化的教师-学生角色实现开放式学习，这些角色自适应地校准难度并驱动多样化探索，能够自主地从基础数学进展到高级数学。

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [213] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2是一个新的推测解码框架，通过离散扩散作为非自回归草稿器和校准技术，解决了当前推测解码的两个瓶颈问题，实现了推理速度的显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法存在两个基本瓶颈：草稿阶段的自回归依赖限制了并行性，以及草稿模型与验证模型不匹配导致的频繁拒绝。

Method: 使用离散扩散作为非自回归草稿器来解决并行性问题，并开发了新颖的技术来校准离散扩散草稿器与自回归验证器。

Result: 在推理、编码和数学基准测试中实现了新的最先进性能，相比之前基线平均提升55%的每秒令牌数，相比标准解码获得最高5.5倍的平均加速，且无精度损失。

Conclusion: SpecDiff-2通过联合解决推测解码的两个基本瓶颈，显著提升了大型语言模型的推理速度，同时保持了准确性。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [214] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型在概率场景中的不确定性量化能力，发现尽管模型在响应准确性上表现完美，但其token级别的概率分布与理论概率分布存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 可靠的不确定性量化对于大型语言模型在决策支持等知识密集型应用中的可信部署至关重要，特别是在概率场景中需要模型输出的概率与理论概率分布保持一致。

Method: 使用GPT-4.1和DeepSeek-Chat模型，评估它们对10个概率相关提示的响应，测量响应有效性和token级输出概率与理论概率的对齐程度。

Result: 两个模型在所有提示场景中都实现了完美的领域内响应准确性，但它们的token级概率和熵值始终偏离相应的理论分布。

Conclusion: 虽然大型语言模型在概率任务中能产生准确的响应，但其内部的不确定性量化机制与理论概率分布不一致，这影响了模型在概率场景中的可靠性。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [215] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 通过计算分析研究法国侦探小说中侦探原型的演变，发现监督模型能捕捉150年间侦探原型的统一性，并展示其从次要角色到核心推理机器的演变过程。


<details>
  <summary>Details</summary>
Motivation: 探索法国侦探小说中侦探原型在150年间的演变轨迹，理解该文学类型中核心人物的变化规律。

Method: 使用量化方法和角色级嵌入技术，通过监督模型分析从1866年到2017年的法国侦探小说文本。

Result: 模型成功捕捉了侦探原型的统一性，发现侦探角色从次要叙事功能演变为古典侦探故事的核心推理机器，二战后受硬汉派影响变得更加复杂。

Conclusion: 法国侦探小说中的侦探原型经历了从单一功能到复杂角色的演变，反映了文学类型与社会背景的互动关系。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [216] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA是一个多语言问答基准，涵盖9个国家的49,280个地理、文化和历史问题，用于评估多语言LLM的文化素养，发现模型在西方语言表现更好但文化知识有限，跨语言知识迁移能力较弱。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数多语言问答基准虽然涵盖多种语言，但信息内容偏向西方中心主义，未能考虑地区多样性，导致无法公平评估多语言模型对不同地理位置的实际情况理解能力。

Method: 构建XNationQA基准数据集，包含7种语言的49,280个问题，涵盖9个国家的地理、文化和历史信息。使用两种新颖的转移度量标准对8个标准多语言LLM进行基准测试。

Result: 模型在不同语言中获取文化特定事实的能力存在显著差异，模型在英语中展示的文化知识往往比相应文化主导语言更多。模型在西方语言中表现更好，但这并不意味着对西方国家文化更了解。开源模型在跨语言知识迁移方面能力非常有限。

Conclusion: 多语言LLM存在文化素养不平衡问题，模型在英语中的文化知识优于本土语言，跨语言知识迁移能力不足，需要改进模型对不同文化背景信息的理解和处理能力。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [217] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 本文首次系统评估了多语言环境下LLM的安全绕过攻击与防御，发现攻击成功率和防御鲁棒性在不同语言间存在显著差异，高资源语言在标准查询下更安全但对对抗性攻击更脆弱。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语环境下的LLM安全绕过，但跨语言的安全泛化能力尚未得到充分探索，需要评估不同资源水平语言中的安全漏洞。

Method: 在10种不同资源水平的语言中，使用6个LLM在HarmBench和AdvBench上评估两种攻击类型：基于逻辑表达式的攻击和基于对抗性提示的攻击。

Result: 攻击成功率和防御有效性具有语言和模型依赖性，高资源语言对标准查询更安全但对对抗性攻击更脆弱，简单防御措施在某些情况下有效但效果不一致。

Conclusion: 研究结果强调了开发语言感知和跨语言安全基准测试的必要性，以全面评估LLM的安全性能。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [218] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 提出改进的Native Sparse Attention方法，通过交替使用局部和全局注意力模式，结合潜在注意力机制，在减少KV缓存的同时提升长上下文建模能力


<details>
  <summary>Details</summary>
Motivation: 改进原生稀疏注意力在长上下文建模中的效果，解决固定注意力模式限制长距离依赖传播的问题

Method: 在层间交替使用局部（滑动窗口）和全局（压缩、选择性）注意力模式，并在滑动窗口分支使用多头潜在注意力(MLA)，在压缩和选择性分支使用组头潜在注意力(GLA)

Result: 相比原生稀疏注意力减少50% KV缓存内存，同时在常识推理和长文本理解任务上表现优于或等同于全注意力和原生稀疏注意力

Conclusion: 交替注意力模式和潜在注意力机制能有效提升长上下文建模能力，同时显著降低内存消耗

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [219] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: TriCon-Fair是一个对比学习框架，通过解耦的三元组损失和语言建模损失来消除社会偏见，避免偏见样本和无偏见样本之间的负面耦合效应。


<details>
  <summary>Details</summary>
Motivation: 现有去偏见方法独立处理偏见和无偏见样本，忽略了它们之间的相互关系，导致改进一个群体时无意中损害另一个群体，使残留的社会偏见持续存在。

Method: 采用对比学习框架，结合三元组损失和语言建模损失，为每个锚点分配明确的偏见负样本和无偏见正样本，解耦推拉动态并避免正负耦合。

Result: 实验结果表明，TriCon-Fair在保持强大下游性能的同时，比现有去偏见基线方法更能减少歧视性输出。

Conclusion: TriCon-Fair为敏感NLP应用提供了一个实用且符合伦理的解决方案。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [220] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 提出了一个评估大语言模型推理过程中知识基础的系统框架，包含知识收集、评估指标和轻量级评估器三个核心组件，用于验证推理是否准确基于知识。


<details>
  <summary>Details</summary>
Motivation: 随着逐步推理成为LLMs处理复杂任务的标准方法，需要验证推理是否准确基于知识，解决知识基础验证的基本问题。

Method: 构建大规模原子知识库，设计基于知识的评估指标来衡量模型在推理中回忆和应用先验知识的能力，使用优化的轻量级LLM作为评估器进行成本效益高的指标计算。

Result: 评估套件在识别缺失或误用知识元素方面表现出显著效果，为揭示LLMs基本推理缺陷提供了关键见解。

Conclusion: 该知识基础评估框架不仅能有效评估LLMs的推理质量，还可集成到偏好优化中，展示了知识基础评估的进一步应用价值。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [221] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一个多模态文档检索模型，通过OCR预训练目标、自监督掩码对比学习和延迟交互评分机制，在ViDoRe V2基准上比现有检索模型提升3.61%，并展现出更强的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态文档检索方法往往简单复制文本检索技术，未能充分考虑多模态文档的结构和视觉特征，存在局限性。

Method: 采用OCR预训练目标、自监督掩码对比学习目标，以及更适合多模态文档结构和视觉特征的延迟交互评分机制。

Result: 在ViDoRe V2基准上获得3.61%的性能提升，并在跨域基准上表现出更强的泛化能力。

Conclusion: ColMate成功弥合了多模态表示学习与文档检索之间的差距，为多模态文档检索提供了更有效的解决方案。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [222] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 评估大型语言模型在医疗诊断沟通中的表现，发现虽然能根据患者特征调整解释，但存在内容过于复杂和情感共情偏见的问题，导致可及性和支持不平等。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在支持临床医生进行诊断沟通方面的能力，特别是生成患者可理解且具有共情的解释和指导的能力。

Method: 在医疗诊断场景中评估两个领先的LLMs，使用可读性指标作为可理解性的代理，通过LLM-as-a-Judge评分与人类评估比较来评估共情能力。

Result: LLMs能够根据社会人口学变量和患者状况调整解释，但生成的内容过于复杂，并表现出有偏见的情感共情，导致可及性和支持不平等。

Conclusion: 这些模式强调了需要系统校准以确保公平的患者沟通。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [223] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型在七种印度语言中的推理和自我评估能力，发现模型初始准确率与其识别自身错误的能力呈负相关。表现最好的模型过度自信，而表现较差的模型反而更具自我意识。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在非英语语言中的文化推理能力，特别是针对七种主要印度语言的推理和自我评估能力，填补多语言文化推理研究的空白。

Method: 创建多语言谜语数据集，包含传统谜语和上下文重构变体，评估五种LLM在七种提示策略下的表现。分两阶段实验：谜语解决性能评估和自我评估实验以衡量推理一致性。

Result: Gemini 2.5 Pro整体表现最佳，但少样本方法仅带来边际收益，准确率在不同语言间差异显著。关键发现：模型初始准确率与识别自身错误能力呈负相关，Gemini 2.5 Pro过度自信（4.34%真负率），而LLaMA 4 Scout更具自我意识（42.09%真负率）。

Conclusion: 多语言推理存在明显差距，需要开发不仅能有效推理还能识别自身局限的模型。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [224] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 提出了一种易到难的增强框架来解决机器生成文本检测中的边界模糊问题，通过使用针对长文本的简易监督器来增强目标检测器，在各种实际场景中显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法假设标签是"黄金标准"，但存在边界模糊问题，且人类认知局限和检测器超智能使不精确学习普遍存在且不可避免。

Method: 提出易到难增强框架，使用针对较长文本的简易监督器（尽管能力较弱）来增强更具挑战性的目标检测器。通过结构性地将检测器融入监督器，理论上将监督器建模为检测器的性能下界。

Result: 在跨LLM、跨领域、混合文本和改写攻击等多种实际场景中的广泛实验表明，该框架具有显著的检测有效性。

Conclusion: 该框架通过易到难的监督策略，在不精确条件下提供可靠监督，最终逼近潜在的"黄金"标签，显著提升了机器生成文本检测的性能。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [225] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL是一个多代理框架，通过任务分解和交互式强化学习实现自然语言到SQL的翻译，特别擅长处理复杂查询。


<details>
  <summary>Details</summary>
Motivation: 复杂自然语言查询到SQL的翻译仍然困难，需要环境交互和自我修正能力。

Method: 使用三个专门代理：Grounding Agent进行模式链接，Generation Agent通过多轮强化学习策略生成查询，Validation Agent进行最终选择。核心是采用ReAct式思维-行动-观察循环的生成代理。

Result: 在BIRD开发集上达到77.84%的执行准确率，在Spider测试集上达到89.75%的执行准确率，达到最先进水平。

Conclusion: MARS-SQL通过结构化工作流结合交互式强化学习和生成建模，为稳健准确的SQL生成提供了高效解决方案。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [226] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 提出了IF-CRITIC模型，用于高效可靠地评估大语言模型对指令中约束条件的遵循程度，通过约束清单生成和多阶段筛选机制训练，在评估性能和优化效率上超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM-as-a-Judge的指令遵循评估方法存在成本高、评估不可靠等问题，需要更高效可靠的评估模型来支持指令遵循能力的优化。

Method: 开发约束清单生成器分解指令，通过多阶段筛选机制收集高质量批判数据，采用约束级偏好优化方法训练IF-CRITIC模型。

Result: IF-CRITIC在评估性能上超越了Deepseek-R1和o4-mini等强基线，能够以更低计算开销为LLM提供可扩展的奖励信号，显著提升指令遵循优化性能。

Conclusion: IF-CRITIC提供了一种高效可靠的指令遵循评估方法，能够有效支持大语言模型的指令遵循能力优化，在性能和效率上均优于现有方法。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [227] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一个端到端的强化学习框架，使用小规模LLM与大规模LLM协作，通过多轮提示交互解决复杂问题，显著提升了LLM在复杂任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型发展迅速，但用户在面对复杂问题时往往无法提供准确有效的提示，限制了LLM的性能发挥。

Method: 提出Prompt-R1框架：小规模LLM负责思考生成提示，大规模LLM负责复杂推理；设计双约束奖励机制优化正确性、生成质量和推理准确性；支持即插即用的推理和训练。

Result: 在多个公开数据集上的实验表明，Prompt-R1显著优于基线模型，在不同任务上都有优异表现。

Conclusion: Prompt-R1通过小规模LLM与大规模LLM的协作机制，有效解决了用户提示不准确的问题，为LLM在复杂问题上的应用提供了新的解决方案。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [228] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一个对话式AI平台，将开源大语言模型的自然语言流畅性与NOAA权威海洋数据的实时参数化访问相结合，通过API调用生成可验证的海洋学数据响应和可视化


<details>
  <summary>Details</summary>
Motivation: 解决通用对话AI系统生成未经验证的"幻觉"问题，确保科学严谨性，为海洋灾害预测、生态系统评估和水质监测等应用提供可信的AI决策支持

Method: 集成开源大语言模型的自然语言处理能力与NOAA海洋数据流的实时API访问，每个查询触发实时API调用，识别、解析和合成相关数据集

Result: 在盲测对比中，OceanAI是唯一能生成带有原始数据引用的NOAA来源值的系统，其他产品要么拒绝回答，要么提供无支持的结果

Conclusion: 通过将输出基于可验证的观测数据，OceanAI提高了透明度、可重复性和信任度，为海洋领域的AI辅助决策提供了可扩展框架

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [229] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat是一个对话式系统，通过自然语言回答空气质量、气象和政策相关问题，生成可执行的Python代码和交互式可视化，使环境数据分析对政策制定者、研究人员和公民更加易用。


<details>
  <summary>Details</summary>
Motivation: 印度每年因空气污染导致约160万人过早死亡，但现有工具需要专业知识且提供静态仪表板，无法解决关键政策问题。决策者难以将分散的数据转化为有效决策。

Method: 开发基于大语言模型的对话系统，整合中央污染控制委员会监测站数据、州级人口统计数据和国家清洁空气计划资金记录，通过自然语言界面提供环境分析功能。

Result: VayuChat平台已公开部署，用户可通过简单对话执行复杂的环境分析，使数据科学对非专业人士更加可及。

Conclusion: VayuChat通过对话式界面成功降低了环境数据分析的技术门槛，为政策制定者、研究人员和公民提供了更易用的环境决策支持工具。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [230] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 提出了首个端到端微服务修复基准MicroRemed和ThinkRemed多智能体框架，通过模拟SRE的反思性推理来提升LLM在微服务修复中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工设计的提示，LLM仅将文本指令转换为可执行代码，需要更自主的端到端修复能力。

Method: 引入MicroRemed基准评估LLM直接从诊断报告生成可执行Ansible playbook的能力，并提出ThinkRemed多智能体框架模拟SRE的反思和感知推理。

Result: 实验显示MicroRemed对当前LLM构成重大挑战，而ThinkRemed通过迭代推理和系统反思提升了端到端修复性能。

Conclusion: 该研究推进了LLM在微服务修复领域的应用，为自主系统运维提供了新方向。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [231] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 该研究创建了一个基于公开临床指南的标准化基准数据集，用于评估LLM在临床推理中的表现，并通过测试多个流行LLM验证了数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗领域应用日益广泛，但缺乏评估基于指南的临床推理能力的标准化基准。

Method: 利用GPT辅助创建基于多诊断公开指南的验证数据集，包含真实患者场景和临床问题。

Result: 通过测试多个近期流行的LLM展示了数据集的有效性。

Conclusion: 该框架支持对LLM临床实用性和指南依从性的系统性评估。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [232] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 该论文介绍了一个包含近200种语言、30万亿token的开放多语言LLM预训练数据集，这是目前最大的公开多语言数据集，并提供了完整的数据处理流程和评估基准。


<details>
  <summary>Details</summary>
Motivation: 为多语言大语言模型预训练提供高质量、大规模、开放可用的文本数据集，解决多语言数据稀缺和标注不足的问题。

Method: 从多个网络爬虫源获取数据，通过完整的开源流水线进行文档选择、HTML文本提取、语言识别、去重、标注（包括文本质量、个人信息等），并训练了57个单语编码器-解码器模型和GPT类参考模型。

Result: 创建了30万亿token的多语言数据集，开发了完整的处理工具链，训练了多种语言模型，并提供了针对9种欧洲语言的综合评估基准。

Conclusion: 该项目为多语言NLP研究提供了宝贵的数据资源和评估框架，推动了多语言大语言模型的发展。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [233] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 本研究分析了罗马尼亚语预训练语料库的特征和覆盖范围，通过与英语数据对比，使用轻量级多任务模型对罗马尼亚语文本进行多级过滤，生成高质量预训练数据集。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练的关键因素之一是高质量数据的可用性和筛选，对于罗马尼亚语等资源稀缺语言，高质量语料库尤其重要。

Method: 训练轻量级多任务模型对LLM标注的罗马尼亚语文本进行分析，进行多级过滤（如教育价值、主题、格式），生成高质量预训练数据集。

Result: 实验显示罗马尼亚语和英语数据在主题分布上存在显著差异，同时证明通过数据过滤可以提升LLM在多个基准测试上的预训练性能。

Conclusion: 数据质量对LLM训练至关重要，特别是对于资源稀缺语言，多级过滤方法能有效提升预训练效果。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [234] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: TSVer是一个新的基准数据集，专注于基于时间序列证据的时间性和数值推理事实核查，包含287个真实世界声明和400个时间序列，展示了现有最先进模型在此任务上的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统在评估时间性和数值推理方面受到数据集限制，缺乏结构化证据、充分的判决理由或依赖合成声明。

Method: 采用LLM辅助的多步骤注释流程，从38个事实核查组织收集287个真实声明，并整理400个涵盖多个领域的时间序列数据库。

Result: 实现了kappa=0.745的注释者间一致性，基准测试显示即使Gemini-2.5-Pro等最先进模型在判决准确率上仅达63.37%，在判决理由的Ev2R得分上为48.63%。

Conclusion: TSVer数据集填补了时间序列事实核查的评估空白，揭示了当前模型在处理时间性和数值推理方面的显著局限性。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [235] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 开发了一个基于语言模型的停止代理，通过模仿最优停止策略来改善销售对话中的放弃决策，在电信销售场景中减少了54%的失败通话时间，同时保持销售业绩。


<details>
  <summary>Details</summary>
Motivation: 销售人员在对话中面临是否继续或放弃的动态筛选决策，但缺乏对这些决策效率的了解以及改进方法，特别是在高容量外呼销售中时间稀缺且失败率高的环境下。

Method: 将动态筛选决策形式化为最优停止问题，开发基于生成语言模型的顺序决策代理（停止代理），通过模仿回顾性推断的最优停止策略来学习何时放弃对话。

Result: 应用于欧洲大型电信公司的通话数据，停止代理将失败通话时间减少54%，同时几乎保持所有销售额；重新分配节省的时间可使预期销售额增加高达37%。

Conclusion: 销售人员倾向于过度重视少数突出的消费者不感兴趣表达，错误预测通话失败风险，表明其实时对话决策存在认知局限。人工智能算法有潜力纠正认知受限的人类决策并提高销售团队效率。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [236] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [237] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: ZoFia是一个新颖的两阶段零样本假新闻检测框架，通过分层显著性量化实体重要性，使用SC-MMR算法选择关键词检索外部证据，并采用多LLM交互系统进行多视图协作分析和对抗性辩论。


<details>
  <summary>Details</summary>
Motivation: 假新闻的快速传播威胁社会稳定和公众信任，而现有方法在处理快速演变的新闻流时存在时间限制的知识覆盖、幻觉生成以及缺乏对新兴新闻主题的泛化能力等问题。

Method: 1. 引入分层显著性量化新闻内容中实体的重要性
2. 提出SC-MMR算法选择信息丰富且多样化的关键词作为查询
3. 检索最新的外部证据
4. 构建多LLM交互系统，各代理承担不同角色进行多视图协作分析和对抗性辩论

Result: 在两个公共数据集上的综合实验表明，ZoFia明显优于现有的零样本基线方法和大多数少样本方法。

Conclusion: ZoFia框架通过结合分层显著性分析和多LLM交互系统，能够有效检测假新闻，提供可解释且鲁棒的判断，代码将开源以促进相关社区发展。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [238] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: Self-Harmony是一个无需标签的测试时强化学习框架，通过单个模型在求解器和重构器两个角色间切换，利用原始问题和重构问题的答案稳定性来选择正确解，避免多数投票的陷阱。


<details>
  <summary>Details</summary>
Motivation: 传统的测试时强化学习方法如多数投票容易陷入虚假但流行的答案陷阱，需要构建可靠的无需人工监督的学习信号。

Method: 使用单个模型同时作为求解器和重构器，通过谐波均值聚合原始问题和重构问题的答案频率，选择在重构下保持稳定的解。

Result: 在30个推理基准测试中的28个设置中排名第一，实现了最先进的标签免费测试时性能，所有实验零训练失败。

Conclusion: Self-Harmony提供了一种稳定可靠的无需人工监督的测试时适应方法，通过答案稳定性选择机制显著提升了模型的鲁棒性和准确性。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [239] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 提出DEER框架，通过解耦专家混合架构捕获领域特定和领域通用的机器生成文本模式，使用强化学习路由机制解决推理时域标签缺失问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器生成文本检测方法在领域转移时性能显著下降，需要解决域标签在推理时不可用的实际限制。

Method: 两阶段解耦专家混合架构：领域特定专家学习细粒度域内区分，共享专家提取跨域可迁移特征；强化学习路由机制动态选择专家。

Result: 在5个域内和5个域外数据集上，平均F1分数分别提升1.39%和5.32%，准确率分别提升1.35%和3.61%。

Conclusion: DEER框架通过专家解耦专业化和自适应路由，有效提升了机器生成文本检测在领域转移下的性能。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [240] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本文研究了领域特异性对阿拉伯语金融文本摘要的影响，通过构建最大的阿拉伯语金融新闻数据集AraFinNews，评估了领域适应预训练如何提高摘要的事实准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 研究阿拉伯语金融文本摘要中领域特异性对大型语言模型性能的影响，填补阿拉伯语金融领域公开数据集的空白。

Method: 构建AraFinNews数据集（21.25万篇文章-标题对），评估mT5、AraT5和领域适应的FinAraT5等transformer模型在金融摘要任务中的表现。

Result: 领域适应模型生成更忠实和连贯的摘要，特别是在处理定量信息和实体中心信息方面表现更好。

Conclusion: 领域特定适应对于提高阿拉伯语金融摘要的事实一致性和叙述流畅性至关重要。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [241] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: ReSpec是一种新的检索增强推测解码框架，通过自适应决策机制取代启发式切换策略，在保持输出质量的同时显著加速大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码方法存在局限性：基于模型的方法准确但成本高，检索增强方法依赖启发式切换策略导致不必要的检索开销。需要一种更智能的决策机制来优化推测解码效率。

Method: 提出ReSpec框架，包含三个核心创新：1) 基于熵的自适应触发机制，仅在低不确定性时启动检索；2) 反馈驱动的候选选择，利用历史反馈组织高质量候选进行并行验证；3) 源感知的宽松验证策略，对模型生成草稿严格检查，对检索草稿宽松验证。

Result: 在Spec-Bench上的实验表明，ReSpec实现了最先进的加速效果，分别比EAGLE-2和SAM-Decoding快33%和25%以上，同时保持输出质量。

Conclusion: ReSpec通过将启发式草稿切换转化为自适应决策，有效解决了推测解码中的效率问题，在加速大语言模型推理方面取得了显著进展。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [242] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文系统研究了AI辅助同行评审中的提示注入威胁，提出了静态和迭代两种攻击方法，展示了对前沿AI评审者的显著操纵效果，并探索了检测防御方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在科学论文评审中的广泛应用，发现存在隐藏的提示注入攻击，旨在操纵AI评审者给出过度积极的评价，这构成了新兴的安全威胁。

Method: 提出了两类攻击方法：静态攻击使用固定注入提示，迭代攻击通过模拟评审模型优化注入提示以最大化效果；同时探索了基于检测的防御方法。

Result: 两种攻击方法都取得了显著效果，经常能在针对前沿AI评审者时获得满分评价；检测防御能大幅降低攻击成功率，但自适应攻击者仍能部分规避防御。

Conclusion: 研究结果强调了在AI辅助同行评审中需要更多关注和严格防护措施来应对提示注入威胁。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [243] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 该论文介绍了FirstAidQA数据集，包含5500个高质量的一对一问答对，涵盖急救和应急响应场景，旨在支持在低连接环境下运行的轻量级语言模型的开发。


<details>
  <summary>Details</summary>
Motivation: 解决在时间敏感、低连接或零连接环境中部署大型语言模型的局限性，当前模型计算密集且不适合急救人员或平民使用的低端设备，缺乏针对急救和应急响应的高质量数据集是主要障碍。

Method: 使用ChatGPT-4o-mini通过基于提示的上下文学习生成数据集，基于《Vital First Aid Book (2019)》文本，应用文本清理、上下文分块和过滤等预处理步骤，并进行人工验证以确保问答对的准确性、安全性和实用性。

Result: 成功创建了包含5500个高质量问答对的FirstAidQA数据集，涵盖广泛的急救和应急响应场景，数据集已在Hugging Face平台公开。

Conclusion: FirstAidQA数据集支持LLM和SLM的指令微调和微调，能够为急救环境开发更快、更可靠且支持离线运行的系统，推动安全关键和资源受限AI应用的研究。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [244] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs是一个增强型RAG系统，通过结构化和时序推理解决5G标准中的交叉引用和规范演进问题，显著提升专家级问答质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义相似度的RAG框架无法可靠解决5G标准中的交叉引用或推理规范演进，需要专门处理数千页相互引用且不断演进的5G标准文档。

Method: 构建三个元数据丰富的数据库：SpecDB（条款对齐的规范文本）、ChangeDB（行级版本差异）和TDocDB（标准化会议文档），通过元数据查找递归检索引用条款，并通过挖掘变更和链接到记录设计原理的变更请求来追踪规范演进。

Result: 在多个LLM后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融实验确认显式交叉引用解析和演进感知检索显著提高答案质量。

Conclusion: 建模5G标准的结构和时序特性对于可靠回答专家级问题具有重要价值，显式交叉引用解析和演进感知检索是提升系统性能的关键因素。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [245] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: DeepAmbigQA是一个新的问答数据集，通过自动生成管道创建包含名称歧义和多步推理的复杂问题，评估模型在信息收集和答案完整性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有问答基准很少同时评估名称歧义和多步推理两个挑战，而大型语言模型在处理需要完整答案集的复杂问题时表现不佳。

Method: 开发了DeepAmbigQAGen自动数据生成管道，基于文本语料库和链接知识图谱构建问答任务，系统性地嵌入名称歧义和多步推理。

Result: 构建了包含3600个问题的DeepAmbigQA数据集，其中一半需要显式名称歧义解析。实验显示即使是GPT-5在歧义问题上精确匹配率仅为0.13，非歧义问题为0.21。

Conclusion: 当前问答系统在信息收集和答案完整性方面仍有不足，需要开发更鲁棒的解决方案。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [246] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 扩展DistilQwen模型家族，推出四个专门为工业需求设计的模型系列：慢思考模型、自适应思考模型和蒸馏奖励模型，在推理效率和性能间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 为满足现实应用对小型高效推理模型的需求，开发能平衡推理性能和推理速度的知识蒸馏技术。

Method: 基于Qwen模型初始化，通过知识蒸馏技术开发四个模型系列：慢思考模型（高精度推理）、自适应思考模型（动态调整推理策略）、蒸馏奖励模型（支持强化学习）。

Result: 在多个基准测试中展现出高推理效率和强推理性能，蒸馏奖励模型具有实际应用价值，并在阿里云PAI平台上提供可扩展的训练和推理功能。

Conclusion: 这些模型为行业从业者提供了高效的推理解决方案，支持在工业场景中的实际部署和应用。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [247] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过训练专门检测文本前缀中事实不一致性的模型MiniTruePrefixes，并将其集成到受控解码框架中，显著提高了抽象摘要的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的NLI模型虽然能检测完整句子的事实不一致性，但在自回归生成架构中，决策是在解码过程中对每个不断演变的文本前缀进行的。因此需要专门处理文本前缀的蕴含检测任务。

Method: 将蕴含检测任务推广到任意文本前缀，训练了专门的MiniTruePrefixes模型，并将其集成到受控解码框架中指导生成过程。

Result: MiniTruePrefixes在前缀级蕴含检测上比基线NLI模型高出5-14个F1分数。在摘要任务中，LLaMA-3.2-3B-Instruct模型在MiniTruePrefixes指导下，达到了与8B模型相当的事实一致性和运行时间，同时只使用一半内存。

Conclusion: 专门针对文本前缀训练的蕴含检测模型能有效提高LLM生成内容的事实一致性，同时保持效率优势。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [248] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 开发了Cancer-Myth-Indic基准，用于评估多语言大语言模型在癌症相关错误预设问题上的表现，填补了非英语医学评估的空白。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准几乎都是英文的，导致多语言LLM评估存在显著空白，特别是针对印度次大陆广泛使用但服务不足的语言。

Method: 通过翻译500个癌症相关错误预设问题到5种印度语言，由母语译者遵循风格指南进行翻译，共生成2500个翻译项目。

Result: 在预设压力下评估了几个流行的LLM，但具体结果未在摘要中详细说明。

Conclusion: Cancer-Myth-Indic基准有助于解决多语言LLM医学评估的空白，特别是在处理癌症相关错误预设方面。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [249] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 该论文质疑当前大语言模型基准测试的有效性，分析三大模型家族在推理能力评估中的表现趋势，并讨论基准测试面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，基准测试结果趋于饱和，需要探讨超越基准是否真正体现推理能力，还是仅仅在追踪与声称能力脱节的数字。

Method: 调查分析OpenAI、Anthropic和Google三大模型家族在不同基准测试中推理能力的演变，分析多年来的性能趋势和不同推理任务的表现。

Result: 研究发现基准测试结果可能无法真实反映模型的推理能力，存在数据集泄露和训练数据污染等问题，导致评估结果失真。

Conclusion: 当前基准测试面临严峻挑战，需要更有效的评估方法来真实衡量模型的推理能力，本文为未来推理评估和模型发展提供了基础参考。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [250] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文重新评估了语言形态特征对分词和语言建模的影响，指出先前研究存在混杂因素，提出了基于词元二元组的内在指标来预测因果语言建模难度。


<details>
  <summary>Details</summary>
Motivation: 现有关于语言形态特征如何影响分词和语言建模的研究存在相互矛盾的证据，作者认为这是由于实验设置中的混杂因素导致的，难以比较结果和得出结论。

Method: 识别了分析语言形态与语言建模关系的混杂因素，重新评估了Arnett & Bergen (2025)的三个假设，并引入了词元二元组指标作为预测因果语言建模难度的内在方法。

Result: 发现先前关于粘着语比融合语具有更高困惑度的结论都包含混杂因素，词元二元组指标是形态复杂性的梯度代理，无需专家标注。

Conclusion: 最终为可靠回答语言形态如何与语言建模相关的问题提出了必要的研究要求。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [251] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: LiveSearchBench是一个自动化构建检索依赖基准的流水线，通过Wikidata快照差异生成时效性问题，评估LLMs在动态知识环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试奖励记忆而非检索，无法捕捉世界知识的动态特性，需要能评估LLMs处理时效性知识的能力。

Method: 计算Wikidata连续快照的差异，筛选高质量三元组，生成三个难度级别的自然语言问题，通过SPARQL验证确保答案唯一性和可验证性。

Result: 实验显示模型在面对预训练后新事实时性能显著下降，多跳查询差距最明显。检索增强方法和大型指令调优模型只能部分缓解但无法消除时效性差距。

Conclusion: LiveSearchBench将评估从静态记忆转向需要最新检索和推理的任务，为系统化长期评估LLMs在演化知识下的表现提供了基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [252] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 提出了一种可复现的基于草稿优化的翻译流程，使用开源LLMs在形态丰富的低资源语言（拉丁语）翻译任务中达到与顶级专有系统相当的性能水平。


<details>
  <summary>Details</summary>
Motivation: 翻译形态丰富的低资源语言（如拉丁语）面临重大挑战，需要开发能够与专有系统竞争的开源解决方案。

Method: 首先使用微调的NLLB-1.3B模型生成高质量、结构忠实的草稿，然后通过零样本LLM（Llama-3.3或Qwen3）进行优化，可通过检索增强生成（RAG）进一步改进。

Result: 该方法在两个不同基准测试中表现出鲁棒性：标准领域内测试集和新的挑战性领域外12世纪拉丁书信集。开源RAG系统性能与GPT-5基线统计相当，无需任务特定的LLM微调。

Conclusion: 提出的开源RAG系统在拉丁语翻译任务中达到与专有系统相当的性能，为低资源语言翻译提供了可复现的解决方案。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [253] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 提出BARD框架，通过预算控制信号同时蒸馏推理能力并精细控制推理长度，实现推理性能与计算效率的动态平衡。


<details>
  <summary>Details</summary>
Motivation: 解决长思维链蒸馏中推理过程冗余、计算预算不可控导致的资源使用低效问题。

Method: 采用两阶段训练：第一阶段在教师生成的长思维链数据上进行监督微调，第二阶段使用强化学习同时优化推理性能和预算保真度。

Result: 8B学生模型在AIME24、AIME25、GPQA等推理基准上表现优异，并能跨多种预算范围精确自适应控制推理长度。

Conclusion: BARD框架成功实现了推理能力蒸馏与推理长度控制的统一，为高效推理模型提供了可行方案。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [254] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 使用大语言模型作为认知扭曲检测的标注工具，提出多轮独立标注方法，并引入数据集无关评估框架，证明LLM标注比人工标注更一致且能提升模型性能


<details>
  <summary>Details</summary>
Motivation: 文本认知扭曲检测任务具有高度主观性，即使专家标注者之间的一致性也很低，导致标注不可靠。需要寻找更一致和可靠的标注方法

Method: 使用大语言模型作为标注器，通过多轮独立标注揭示稳定模式；引入基于Cohen's kappa的数据集无关评估框架，实现公平的跨数据集比较

Result: GPT-4能够产生高度一致的标注（Fleiss's Kappa = 0.78），使用LLM标注训练出的模型在测试集上表现优于使用人工标注训练的模型

Conclusion: 大语言模型可以为主观NLP任务提供可扩展且内部一致的训练数据生成方案，支持强大的下游性能

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [255] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 研究探讨了合成数据来源多样性对微调大语言模型的影响，重点关注分布坍塌、对抗鲁棒性和自偏好偏差三个维度。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中的广泛应用，理解其对模型行为的影响变得至关重要。

Method: 通过分析不同来源的合成数据对微调模型的影响，研究三个关键维度：分布坍塌、对抗鲁棒性和自偏好偏差。

Result: 使用多来源合成数据微调可缓解分布坍塌，保持输出分布的广度和文本多样性；合成数据微调在移除安全防护的同时保持更高输出质量；微调减少自偏好偏差，人类数据效果最佳，多来源合成数据次之。

Conclusion: 合成数据来源的多样性对模型行为有显著影响，多来源合成数据在缓解分布坍塌和保持输出质量方面表现良好，但人类数据在减少自偏好偏差方面更有效。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [256] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文提出了一种结合帕累托优化大语言模型和思维链提示的孟加拉语文本去毒新方法，并构建了包含68,041个有毒句子的BanglaNirTox数据集。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语中的有毒语言在在线环境中仍然普遍存在，但由于资源有限，孟加拉语文本去毒研究仍未被充分探索。

Method: 使用帕累托优化的大语言模型和思维链提示生成去毒句子，构建人工生成的平行语料库BanglaNirTox，并用该数据集微调语言模型。

Result: 帕累托优化的大语言模型结合思维链提示显著提高了孟加拉语文本去毒的质量和一致性。

Conclusion: 该方法有效解决了孟加拉语文本去毒问题，为低资源语言的类似任务提供了可行方案。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [257] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出了一种通过数据增强和多任务学习生成可控难度干扰项的新框架，显著优于GPT-4o在干扰项难度控制方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成高质量干扰项时缺乏适应性和难度控制能力，且缺乏难度标注数据集阻碍了进展。

Method: 采用双向干扰项生成过程创建难度标注数据集，通过过滤和集成QA系统分类难度，然后利用多任务学习训练可控难度生成模型。

Result: 实验结果表明，该方法能生成跨难度级别的高质量干扰项，在干扰项难度与人类感知对齐方面大幅优于GPT-4o。

Conclusion: 所提出的框架成功解决了干扰项生成的难度控制问题，为语言能力评估提供了更有效的工具。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [258] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 本研究使用行为形式心智网络框架分析心理学大学生对数学和焦虑概念的认知关联，发现人类学生中"焦虑"的正向评价和高网络度度以及"数学"的负向评价能预测数学焦虑水平，但这些模型在GPT模拟数据中不适用，揭示了真实学生与AI模拟在概念认知上的差异。


<details>
  <summary>Details</summary>
Motivation: 数学焦虑严重影响心理学大学生的职业选择和心理健康，需要深入理解学生对数学相关概念的认知结构和情感感知，以更好地管理数学焦虑问题。

Method: 采用行为形式心智网络框架，对两个心理学本科生样本(n1=70, n2=57)和GPT模拟学生(GPT-3.5: n2=300; GPT-4o: n4=300)进行4个实验，分析个体层面网络特征预测数学焦虑分数，以及群体层面的概念感知差异。

Result: 人类学生中"焦虑"的正向评价和高网络度度以及"数学"的负向评价能预测更高的总体和评价性数学焦虑；高数学焦虑学生对"焦虑"有情感极化认知，而低焦虑学生没有；"科学"被积极评价但与"数学"形成对比；这些模型在GPT数据中不适用。

Conclusion: 概念感知和关联方式对理解和管理学生数学焦虑至关重要，真实学生与AI模拟在认知结构上存在显著差异，需要针对性干预策略。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [259] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 提出ECO解码方法，通过基于熵的动态控制强度调整，在保持流畅性的同时提升对话生成的可控性


<details>
  <summary>Details</summary>
Motivation: 现有加权解码方法使用固定常数控制属性概率偏差，难以找到同时满足可控性和流畅性的理想控制强度

Method: ECO解码根据语言模型和属性分类器概率分布的熵，在每一步生成时动态调整控制强度

Result: 在DailyDialog和MultiWOZ数据集上的实验表明，ECO解码在保持流畅性和语法性的同时持续提升可控性，优于现有解码方法

Conclusion: ECO解码缓解了多属性生成中的概率插值问题，在单属性和多属性场景下均表现出色

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [260] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: BIRD数据集结合字形网络(GN)框架，用于青铜器铭文修复与断代，通过字形-异体字关联提升修复效果，字形偏置采样改善断代性能。


<details>
  <summary>Details</summary>
Motivation: 早期中国青铜器铭文存在碎片化、难以准确断代的问题，需要专门的数据集和方法来处理这一挑战。

Method: 提出BIRD数据集和字形网络(GN)框架，结合领域自适应预训练和任务自适应预训练，通过掩码语言建模连接字素与异体字。

Result: 实验表明字形网络提升了铭文修复效果，字形偏置采样在断代任务中取得了性能提升。

Conclusion: BIRD数据集和字形网络框架为青铜器铭文研究提供了有效的技术支撑，在修复和断代任务中均表现出良好效果。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [261] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 该研究通过分析西班牙语母语者的语言错误，评估大型语言模型对这些错误的解释、复制和纠正能力，旨在开发更符合人类认知的NLP系统。


<details>
  <summary>Details</summary>
Motivation: 语言错误不仅是语法偏差，更是理解语言认知架构和揭示人工智能系统局限性的窗口。研究旨在通过分析真实语言错误来改进NLP系统的认知适应性。

Method: 整合理论语言学、神经语言学和自然语言处理三个视角，构建包含500+西班牙语母语者真实错误的语料库，并在GPT、Gemini等AI模型上进行测试评估。

Result: 研究将评估AI模型对语言错误的解释准确性和对人类语言行为模式的泛化能力，但目前尚未提供具体实验结果。

Conclusion: 该项目不仅有助于理解西班牙语作为母语的特点，还将推动开发更认知知情、能够处理人类语言不完美性和模糊性的NLP系统。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [262] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech是一个包含克罗地亚语、捷克语、波兰语和塞尔维亚语四种斯拉夫语言的议会语音语料库，总时长6000小时，通过自动方式从ParlaMint转录本构建，并增加了多种自动注释层。


<details>
  <summary>Details</summary>
Motivation: 构建一个大规模、多语言的议会语音语料库，通过自动注释丰富语料内容，提高其在多学科下游研究中的实用性。

Method: 从ParlaMint转录本和对应元数据自动构建，与各议会语音录音对齐，并添加语言注释、情感预测、填充停顿检测、词/字素级对齐和重音位置等自动注释层。

Result: 创建了包含6000小时语音的四个斯拉夫语言语料库，显著增强了语料的有用性，并通过情感声学相关性分析展示了其应用价值。

Conclusion: ParlaSpeech语料库通过丰富的自动注释大大提升了研究价值，可用于多学科研究，并以JSONL、TextGrid格式和检索工具形式提供。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [263] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 提出一个认知基准测试框架，结合布鲁姆分类法和检索增强生成，评估大语言模型处理文化特定知识的能力。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型如何理解和应用文化特定知识，特别是在台湾客家数字文化档案中的表现。

Method: 整合布鲁姆分类法与检索增强生成(RAG)，在六个认知层级（记忆、理解、应用、分析、评估、创造）上评估模型表现。

Result: 测量大语言模型生成回答的语义准确性和文化相关性。

Conclusion: 该框架为评估大语言模型的文化认知能力提供了系统化方法。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [264] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: EngChain是一个用于验证多步骤工程问题解决能力的基准测试，包含90个跨3个工程分支的问题，通过两阶段评估方法验证推理步骤的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要评估语言理解、事实回忆、数学或代码生成，但缺乏对工程领域整合推理能力的评估，而工程需要科学原理、定量建模和实践约束的融合。

Method: 使用符号模板生成高度随机化的问题确保多样性，采用两阶段评估：定量验证每个推理步骤的数值和语义有效性，然后引入LLM-As-A-Judge系统对推理错误进行定性分类。

Result: 开发了包含90个问题、涵盖3个工程分支、9个领域和20个不同领域的基准测试，通过模板化生成确保问题多样性和避免污染风险。

Conclusion: EngChain填补了工程领域整合推理能力评估的空白，提供了可验证的多步骤工程问题解决基准，超越了仅关注最终答案准确性的传统评估方法。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [265] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个针对东南亚语言（印尼语、泰语、越南语）以及英语和中文的大型音频语言模型，支持多语言、多模态和多任务处理。


<details>
  <summary>Details</summary>
Motivation: 为东南亚地区开发专门的大型音频语言模型，解决该地区多语言音频处理的需求，推动区域研究和产业发展。

Method: 在大规模音频语料库上训练，支持音频、文本及混合输入，涵盖音频理解、语音识别、翻译、情感识别等多种任务。

Result: 在东南亚语言上表现出色，与现有LALMs相比具有竞争力，并通过SeaBench-Audio基准测试验证了性能。

Conclusion: SeaLLMs-Audio是推动东南亚音频LLMs发展的重要一步，将为区域研究和产业带来益处。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [266] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文提出了第一个开源的AI助手角色训练方法，使用宪法AI和合成内省数据来有效塑造AI助手的人格特征，相比系统提示约束和激活引导等方法更稳健和连贯。


<details>
  <summary>Details</summary>
Motivation: 现代聊天机器人语言模型生成的AI助手角色特征会影响交互质量、感知智能和与开发者用户意图的对齐，但角色训练在学术文献中尚未得到充分研究。

Method: 使用宪法AI和合成内省数据的新数据管道，对三个流行的开源模型进行微调，应用11种示例人格特征，并通过分析揭示偏好来跟踪效果。

Result: 该方法相比系统提示约束和激活引导更抗对抗性提示，生成更连贯和真实的输出，且对通用能力基准测试影响很小或无影响。

Conclusion: 角色训练是行业后训练的关键组成部分，本文的开源实现为更有效和可控地塑造AI助手人格提供了可行方法。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [267] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 提出了一个新颖的rank-2投影子空间来更准确地分离LLM中参数知识(PK)和上下文知识(CK)的贡献，并首次对长NLE序列中的多步知识交互进行分析。


<details>
  <summary>Details</summary>
Motivation: 理解参数知识和上下文知识在LLM决策中的交互对于评估自然语言解释的可靠性至关重要，但现有研究仅关注单步生成和rank-1子空间，忽略了更丰富的知识交互形式。

Method: 使用rank-2投影子空间来解耦PK和CK的贡献，并在四个QA数据集和三个开源LLM上进行多步知识交互分析，比较不同提示策略下的知识依赖模式。

Result: 实验表明rank-1子空间无法有效表示多样化的知识交互，而rank-2方法能有效捕捉；幻觉NLE强烈偏向PK方向，上下文忠实的NLE平衡PK和CK，Chain-of-Thought提示通过减少PK依赖将NLE转向CK。

Conclusion: 这项工作为通过更丰富的rank-2子空间解耦来系统研究LLM中多步知识交互提供了首个框架，揭示了不同知识交互模式与解释质量的关系。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [268] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 提出了一个基于Qwen3模型和LoRA适配器的多专家系统，用于创建能够进行自然对话和执行上下文动作的NPC角色，在CPDC 2025挑战赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 为了开发能够同时处理自然对话和上下文动作执行的NPC角色，满足交互式环境中的实时响应需求。

Method: 使用Qwen3作为基础模型，通过LoRA适配器实例化三个专家模块：工具调用、工具响应解释和直接对话。

Result: 系统在L40S GPU上实现了快速响应和适中的资源使用，在Commonsense Persona-Grounded Dialogue Challenge 2025中总体排名第二。

Conclusion: 多专家系统方法在NPC对话和动作执行方面表现良好，计算效率满足要求，证明了该架构的有效性。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [269] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 语言模型在长时间对话和阅读过程中会经历信念漂移，导致其世界观和行为发生显著变化，这可能影响用户体验和模型对齐。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型自主性增强，上下文窗口中的文本积累可能导致模型信念配置文件无声变化，带来潜在风险，如用户体验不一致和行为偏离原始对齐。

Method: 通过多轮对话讨论道德困境和安全查询来测试信念变化，并设计工具使用任务来观察行为变化，将工具选择与隐含信念关联。

Result: GPT-5在10轮道德困境讨论后信念变化达54.7%，Grok 4在阅读对立政治立场文本后信念变化27.2%，行为变化与信念漂移一致。

Conclusion: 模型在长时间对话或阅读过程中存在信念漂移的隐藏风险，这会导致其观点和行为变得不可靠。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [270] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 提出了一种无需模型重新训练的提示工程方法，通过结构化引导实现精确的长度控制，在文档摘要任务中显著提高了长度遵循度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的长度控制是一个重要但未被充分解决的挑战，现有方法通常需要昂贵的模型重新训练或复杂的推理时工具。

Method: 采用结构引导的提示工程方法，在提示中实现精心的规划和字数统计机制，鼓励模型仔细跟踪并遵守指定的长度约束。

Result: 在六个最先进的大语言模型上的评估显示，该方法显著提高了长度遵循度，某些模型在长度遵循方面提升了37.6%，同时保持或提升了输出质量。

Conclusion: 该方法为需要精确长度控制的应用提供了立即可部署的解决方案，特别适用于模型重新训练不切实际或成本过高的生产环境。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [271] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: KVTC是一种轻量级变换编码器，通过PCA特征去相关、自适应量化和熵编码来压缩KV缓存，实现高达20倍的压缩比，同时保持推理精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务需要高效的KV缓存管理，共享前缀提示中的陈旧缓存会消耗宝贵的GPU内存，需要卸载或重新计算。

Method: KVTC结合了基于PCA的特征去相关、自适应量化和熵编码，仅需简短初始校准，不改变模型参数。

Result: 在Llama 3、Mistral NeMo和R1-Qwen 2.5模型上测试，KVTC在AIME25、LiveCodeBench等基准测试中实现高达20倍压缩，特定用例可达40倍以上，优于令牌驱逐、量化和SVD等方法。

Conclusion: KVTC是内存高效LLM服务的实用构建模块，支持可重用KV缓存。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [272] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: IMO-Bench是一套针对国际数学奥林匹克竞赛水平的先进推理基准，包括IMO-AnswerBench（400道可验证短答案问题）和IMO-ProofBench（证明写作能力评估），在Gemini Deep Think取得IMO 2025金牌表现中发挥了关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理评估要么太简单，要么只关注短答案正确性，需要更高级的基准来推动基础模型的数学推理能力发展。

Method: 构建IMO-Bench基准套件，包括IMO-AnswerBench（400道IMO级别问题）和IMO-ProofBench（证明写作评估），并开发自动评分器与人工评估对比。

Result: Gemini Deep Think在IMO-AnswerBench上达到80.0%，在高级IMO-ProofBench上达到65.7%，分别比最佳非Gemini模型高出6.9%和42.4%。自动评分器与人工评估相关性良好。

Conclusion: IMO-Bench有助于推动数学推理能力的发展，为自动评估长形式答案提供支持，并已在GitHub发布。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [273] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 提出了Tool-to-Agent Retrieval框架，通过在共享向量空间中嵌入工具及其父代理，并利用元数据关系连接它们，实现了细粒度的工具级或代理级检索，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索方法通常将查询与粗粒度的代理级描述进行匹配，这掩盖了细粒度的工具功能，导致代理选择不理想。需要一种能同时表示工具能力和代理级别的统一检索框架。

Method: 开发了Tool-to-Agent Retrieval框架，将工具及其父代理嵌入到共享向量空间中，通过元数据关系连接它们，支持细粒度的工具级或代理级检索，避免了将多个工具分块导致的上下文稀释问题。

Result: 在LiveMCPBench基准测试中，使用八种嵌入模型进行评估，该方法在Recall@5和nDCG@5指标上分别比之前最先进的代理检索器提高了19.4%和17.7%。

Conclusion: Tool-to-Agent Retrieval框架通过统一表示工具和代理，实现了更精确的检索，显著提升了多代理系统中工具和代理的检索性能。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [274] [LongCat-Flash-Omni Technical Report](https://arxiv.org/abs/2511.00279)
*Meituan LongCat Team,Bairui Wang,Bayan,Bin Xiao,Bo Zhang,Bolin Rong,Borun Chen,Chang Wan,Chao Zhang,Chen Huang,Chen Chen,Chen Chen,Chengxu Yang,Chengzuo Yang,Cong Han,Dandan Peng,Delian Ruan,Detai Xin,Disong Wang,Dongchao Yang,Fanfan Liu,Fengjiao Chen,Fengyu Yang,Gan Dong,Gang Huang,Gang Xu,Guanglu Wan,Guoqiang Tan,Guoqiao Yu,Haibo Qiu,Hao Lu,Hongbo Liu,Hongyu Xiang,Jiaheng Wu,Jian Yang,Jiaxing Liu,Jing Huang,Jingang Wang,Jinrui Ding,Juchao Jiang,Jun Kuang,Jun Wang,Junhui Mei,Ke Ding,Kefeng Zhang,Lei Chen,Liang Shi,Limeng Qiao,Liming Zheng,Lin Ma,Liuyang Guo,Liya Ma,Luying Sun,Man Gao,Mengshen Zhu,Miao Cao,Minliang Lin,Nuo Xu,Peng Shi,Qi Zhang,Qian Fang,Qian Wang,Qian Yang,Quanxiu Wang,Rongxiang Weng,Rongxin Guo,Ruoxuan Liang,Senbin Yang,Shanbo Xu,Shanglin Lei,Shengze Ye,Shimin Chen,Shuaiqi Chen,Shujie Hu,Shuo Li,Siqi Yang,Siyu Xu,Siyu Ren,Song Li,Songxiang Liu,Tianhao Bai,Tianye Dai,Wei Hong,Wei Wang,Weixiao Zhao,Wengang Cao,Wenlong Zhu,Wenlong He,Xi Su,Xi Nan,Xiaohan Zhao,Xiaohao Wang,Xiaoyu Zhao,Xiaoyu Wang,Xiaoyu Li,Xin Pan,Xin Chen,Xiusong Sun,Xu Xiang,Xudong Xing,Xuezhi Cao,Xunliang Cai,Yang Yang,Yanli Tan,Yao Yao,Yerui Sun,Yi Chen,Yifan Lu,Yin Gong,Yining Zhang,Yitian Chen,Yiyang Gan,Yuchen Tang,Yuchen Xie,Yueqian Wang,Yuewen Zheng,Yufei Zhang,Yufeng Zhong,Yulei Qian,Yuqi Peng,Yuwei Jiang,Zeyang Hu,Zheng Zhang,Zhengkun Tian,Zhiqing Hong,Zhixiong Zeng,Zhuqi Mi,Ziran Li,Ziwen Wang,Ziyi Zhao,Ziyuan Zhuang,Zizhe Zhao*

Main category: cs.MM

TL;DR: LongCat-Flash-Omni是一个5600亿参数的开源全模态模型，采用渐进式训练策略和高效的MoE架构，在保持强大单模态能力的同时实现实时音视频交互。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理多种模态并实现实时交互的大规模开源模型，解决大规模多模态训练中的数据异构性和计算效率问题。

Method: 采用课程启发式渐进训练策略，从简单到复杂逐步训练；基于Shortcut连接的MoE架构，集成高效多模态感知和语音重建模块；使用模态解耦并行方案处理数据异构性。

Result: 在5600亿参数规模下实现低延迟实时音视频交互，在多模态基准测试中达到开源模型的最先进性能，在文本、图像、视频、音频等单模态任务中表现优异。

Conclusion: LongCat-Flash-Omni展示了大规模多模态模型在保持高效计算的同时实现全面多模态能力的可行性，为社区提供了开源模型以促进未来研究。

Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal
model with 560 billion parameters, excelling at real-time audio-visual
interaction. By adopting a curriculum-inspired progressive training strategy
that transitions from simpler to increasingly complex modality sequence
modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal
capabilities while maintaining strong unimodal capability. Building upon
LongCat-Flash, which adopts a high-performance Shortcut-connected
Mixture-of-Experts (MoE) architecture with zero-computation experts,
LongCat-Flash-Omni integrates efficient multimodal perception and speech
reconstruction modules. Despite its immense size of 560B parameters (with 27B
activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual
interaction. For training infrastructure, we developed a modality-decoupled
parallelism scheme specifically designed to manage the data and model
heterogeneity inherent in large-scale multimodal training. This innovative
approach demonstrates exceptional efficiency by sustaining over 90% of the
throughput achieved by text-only training. Extensive evaluations show that
LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal
benchmarks among open-source models. Furthermore, it delivers highly
competitive results across a wide range of modality-specific tasks, including
text, image, and video understanding, as well as audio understanding and
generation. We provide a comprehensive overview of the model architecture
design, training procedures, and data strategies, and open-source the model to
foster future research and development in the community.

</details>


### [275] [Predicting Encoding Energy from Low-Pass Anchors for Green Video Streaming](https://arxiv.org/abs/2511.00707)
*Zoha Azimi,Reza Farahani,Vignesh V Menon,Christian Timmerer*

Main category: cs.MM

TL;DR: 提出一种轻量级能耗预测方法，通过低分辨率参考编码预测高分辨率视频编码能耗，实现编码能耗节省51.22%，解码能耗节省53.54%，同时保持感知质量在可接受范围内。


<details>
  <summary>Details</summary>
Motivation: 视频流媒体占互联网流量主导地位，高分辨率内容分发导致能耗和碳排放问题日益严重，需要在能耗与用户体验质量(QoE)之间寻求平衡。

Method: 使用低分辨率参考编码(锚点)预测高分辨率视频编码能耗，自动选择编码参数(如分辨率和量化参数)，在保持VMAF感知质量的同时优化能耗。

Result: 在Inter4K数据集的100个视频序列上测试，平均VMAF得分仅降低1.68(低于JND阈值)，编码能耗节省51.22%，解码能耗节省53.54%。

Conclusion: 该方法能有效平衡视频编码的能耗与质量，为大规模视频分发提供可行的能耗优化方案。

Abstract: Video streaming now represents the dominant share of Internet traffic, as
ever-higher-resolution content is distributed across a growing range of
heterogeneous devices to sustain user Quality of Experience (QoE). However,
this trend raises significant concerns about energy efficiency and carbon
emissions, requiring methods to provide a trade-off between energy and QoE.
This paper proposes a lightweight energy prediction method that estimates the
energy consumption of high-resolution video encodings using reference encodings
generated at lower resolutions (so-called anchors), eliminating the need for
exhaustive per-segment energy measurements, a process that is infeasible at
scale. We automatically select encoding parameters, such as resolution and
quantization parameter (QP), to achieve substantial energy savings while
maintaining perceptual quality, as measured by the Video Multimethod Fusion
Assessment (VMAF), within acceptable limits. We implement and evaluate our
approach with the open-source VVenC encoder on 100 video sequences from the
Inter4K dataset across multiple encoding settings. Results show that, for an
average VMAF score reduction of only 1.68, which stays below the Just
Noticeable Difference (JND) threshold, our method achieves 51.22% encoding
energy savings and 53.54% decoding energy savings compared to a scenario with
no quality degradation.

</details>


### [276] [Rhythm in the Air: Vision-based Real-Time Music Generation through Gestures](https://arxiv.org/abs/2511.00793)
*Barathi Subramanian,Rathinaraja Jeyaraj,Anand Paul,Kapilya Gangadharan*

Main category: cs.MM

TL;DR: 提出基于视觉的动态手势识别系统用于实时音乐创作，开发了多层级注意力GRU模型，在自定义数据集上达到96.83%的准确率，显著优于传统GRU模型。


<details>
  <summary>Details</summary>
Motivation: 通过手势识别实现人机交互，让用户无需物理接触即可进行音乐创作，提升交互体验。

Method: 构建包含15000个样本、21个类别的手势数据集，开发多层级注意力门控循环单元(MLA-GRU)模型，结合GRU学习时序模式和注意力机制聚焦音乐相关手势片段。

Result: MLA-GRU模型达到96.83%的准确率，显著优于基准GRU模型的86.7%，同时具有更高的效率和处理速度。

Conclusion: 该系统为音乐交互提供了新方式，不仅推进了人机交互体验，也证明了MLA-GRU在需要快速精确手势识别场景中的有效性。

Abstract: Gesture recognition is an essential component of human-computer interaction
(HCI), facilitating seamless interconnectivity between users and computer
systems without physical touch. This paper introduces an innovative application
of vision-based dynamic gesture recognition (VDGR) for real-time music
composition through gestures. To implement this application, we generate a
custom gesture dataset that encompasses over 15000 samples across 21 classes,
incorporating 7 musical notes each manifesting at three distinct pitch levels.
To effectively deal with the modest volume of training data and to accurately
discern and prioritize complex gesture sequences for music creation, we develop
a multi-layer attention-based gated recurrent unit (MLA-GRU) model, in which
gated recurrent unit (GRU) is used to learn temporal patterns from the observed
sequence and an attention layer is employed to focus on musically pertinent
gesture segments. Our empirical studies demonstrate that MLA-GRU significantly
surpasses the classical GRU model, achieving a remarkable accuracy of 96.83%
compared to the baseline's 86.7%. Moreover, our approach exhibits superior
efficiency and processing speed, which are crucial for interactive
applications. Using our proposed system, we believe that people will interact
with music in a new and exciting way. It not only advances HCI experiences but
also highlights MLA-GRU's effectiveness in scenarios demanding swift and
precise gesture recognition.

</details>


### [277] [EV-NVC: Efficient Variable bitrate Neural Video Compression](https://arxiv.org/abs/2511.01590)
*Yongcun Hu,Yingzhen Zhai,Jixiang Luo,Wenrui Dai,Dell Zhang,Hongkai Xiong,Xuelong Li*

Main category: cs.MM

TL;DR: 提出了一种高效可变比特率神经视频编码器EV-NVC，通过分段线性采样器和长短时特征融合模块提升高比特率范围的率失真性能，并采用混合精度训练策略。


<details>
  <summary>Details</summary>
Motivation: 训练可变比特率的神经视频编码器具有挑战性，因为其复杂的训练策略和模型结构。

Method: 使用分段线性采样器(PLS)改善高比特率范围的率失真性能，长短时特征融合模块(LSTFFM)增强上下文建模，并采用混合精度训练策略。

Result: 实验结果表明，在低延迟模式下，与HM-16.25相比，该方法将BD-rate降低了30.56%。

Conclusion: 该方法显著提升了神经视频编码器的率失真性能，特别是在高比特率范围内。

Abstract: Training neural video codec (NVC) with variable rate is a highly challenging
task due to its complex training strategies and model structure. In this paper,
we train an efficient variable bitrate neural video codec (EV-NVC) with the
piecewise linear sampler (PLS) to improve the rate-distortion performance in
high bitrate range, and the long-short-term feature fusion module (LSTFFM) to
enhance the context modeling. Besides, we introduce mixed-precision training
and discuss the different training strategies for each stage in detail to fully
evaluate its effectiveness. Experimental results show that our approach reduces
the BD-rate by 30.56% compared to HM-16.25 within low-delay mode.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [278] [LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks](https://arxiv.org/abs/2511.00072)
*Pradeep M,Ritesh Pallod,Satyen Abrol,Muthu Raman,Ian Anderson*

Main category: cs.IR

TL;DR: 提出了一个端到端的产品搜索系统，用于将AI生成的时尚造型与最相似的实体产品进行匹配，已在真实互联网规模环境中部署。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI重塑时尚行业，能够创建虚拟造型和头像，需要找到与AI生成风格最匹配的真实产品。

Method: 搜索管道包含四个关键组件：查询生成、向量化、候选检索和基于AI生成造型的重新排序。使用CLIP模型进行向量化，并通过人工判断的准确度分数评估推荐质量。

Result: 系统每天处理超过35万个AI造型，覆盖全球市场1200多万个产品。实验显示CLIP模型在平均意见分数上比其他模型相对高出3-7%，虽然绝对改进不大，但显著改善了用户感知匹配度。

Conclusion: CLIP被确立为生产部署中最可靠的基础模型，能够提供更好的用户感知匹配效果。

Abstract: Generative AI is reshaping fashion by enabling virtual looks and avatars
making it essential to find real products that best match AI-generated styles.
We propose an end-to-end product search system that has been deployed in a
real-world, internet scale which ensures that AI-generated looks presented to
users are matched with the most visually and semantically similar products from
the indexed vector space. The search pipeline is composed of four key
components: query generation, vectorization, candidate retrieval, and reranking
based on AI-generated looks. Recommendation quality is evaluated using
human-judged accuracy scores. The system currently serves more than 350,000 AI
Looks in production per day, covering diverse product categories across global
markets of over 12 million products. In our experiments, we observed that
across multiple annotators and categories, CLIP outperformed alternative models
by a small relative margin of 3--7\% in mean opinion scores. These
improvements, though modest in absolute numbers, resulted in noticeably better
user perception matches, establishing CLIP as the most reliable backbone for
production deployment.

</details>


### [279] [Effectiveness of LLMs in Temporal User Profiling for Recommendation](https://arxiv.org/abs/2511.00176)
*Milad Sabouri,Masoud Mansoury,Kun Lin,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 利用大语言模型(LLMs)进行时序用户画像，区分短期兴趣和长期偏好，提升推荐系统准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统用户画像往往忽略短期兴趣与长期偏好的区别，需要有效建模用户偏好的动态特性来提升推荐准确性和透明度。

Method: 利用LLMs从交互历史中生成独立的短期和长期文本摘要，创建更丰富的用户表示，通过自然语言画像和注意力权重提供内在可解释性。

Result: 在用户参与度较高的领域(如电影电视)中，LLMs能显著提升推荐质量，但在稀疏环境中效果较弱，因为不同领域短期和长期偏好的可区分性存在差异。

Conclusion: LLM驱动的时序用户画像在性能和计算成本之间存在权衡，其应用需考虑具体上下文，同时为开发自适应和透明的推荐系统提供了新的研究方向。

Abstract: Effectively modeling the dynamic nature of user preferences is crucial for
enhancing recommendation accuracy and fostering transparency in recommender
systems. Traditional user profiling often overlooks the distinction between
transitory short-term interests and stable long-term preferences. This paper
examines the capability of leveraging Large Language Models (LLMs) to capture
these temporal dynamics, generating richer user representations through
distinct short-term and long-term textual summaries of interaction histories.
Our observations suggest that while LLMs tend to improve recommendation quality
in domains with more active user engagement, their benefits appear less
pronounced in sparser environments. This disparity likely stems from the
varying distinguishability of short-term and long-term preferences across
domains; the approach shows greater utility where these temporal interests are
more clearly separable (e.g., Movies\&TV) compared to domains with more stable
user profiles (e.g., Video Games). This highlights a critical trade-off between
enhanced performance and computational costs, suggesting context-dependent LLM
application. Beyond predictive capability, this LLM-driven approach inherently
provides an intrinsic potential for interpretability through its natural
language profiles and attention weights. This work contributes insights into
the practical capability and inherent interpretability of LLM-driven temporal
user profiling, outlining new research directions for developing adaptive and
transparent recommender systems.

</details>


### [280] [Simple and Behavior-Driven Augmentation for Recommendation with Rich Collaborative Signals](https://arxiv.org/abs/2511.00436)
*Doyun Choi,Cheonwoo Lee,Jaemin Yoo*

Main category: cs.IR

TL;DR: 提出SCAR方法，通过生成伪交互而非删除信息来增强图协同过滤中的对比学习效果，在稀疏数据场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 传统对比学习方法通过删除噪声交互来生成增强视图，但噪声定义模糊可能导致核心信息丢失和不可靠视图，且增加复杂度

Method: SCAR从用户-物品交互中提取协同信号生成伪交互，然后将这些伪交互添加到现有交互中或替换现有交互，避免复杂增强模块

Result: 在四个基准数据集上，SCAR超越了之前的CL-based GCF方法和其他最先进的自监督学习方法，在稀疏数据场景下表现突出

Conclusion: SCAR通过简单而有效的协同增强方法，在保持低复杂度的同时显著提升了图协同过滤中对比学习的性能

Abstract: Contrastive learning (CL) has been widely used for enhancing the performance
of graph collaborative filtering (GCF) for personalized recommendation. Since
data augmentation plays a crucial role in the success of CL, previous works
have designed augmentation methods to remove noisy interactions between users
and items in order to generate effective augmented views. However, the
ambiguity in defining ''noisiness'' presents a persistent risk of losing core
information and generating unreliable data views, while increasing the overall
complexity of augmentation. In this paper, we propose Simple Collaborative
Augmentation for Recommendation (SCAR), a novel and intuitive augmentation
method designed to maximize the effectiveness of CL for GCF. Instead of
removing information, SCAR leverages collaborative signals extracted from
user-item interactions to generate pseudo-interactions, which are then either
added to or used to replace existing interactions. This results in more robust
representations while avoiding the pitfalls of overly complex augmentation
modules. We conduct experiments on four benchmark datasets and show that SCAR
outperforms previous CL-based GCF methods as well as other state-of-the-art
self-supervised learning approaches across key evaluation metrics. SCAR
exhibits strong robustness across different hyperparameter settings and is
particularly effective in sparse data scenarios.

</details>


### [281] [LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026](https://arxiv.org/abs/2511.00444)
*Benjamin Clavié,Xianming Li,Antoine Chaffin,Omar Khattab,Tom Aarsen,Manuel Faysse,Jing Li*

Main category: cs.IR

TL;DR: 该论文介绍了延迟交互检索方法（如ColBERT）作为单向量神经IR的替代方案，探讨了其在泛化性、鲁棒性和新应用场景中的优势，以及面临的效率、可用性和系统集成挑战，旨在通过研讨会促进该领域的研究交流与合作。


<details>
  <summary>Details</summary>
Motivation: 延迟交互检索方法在多个领域展现出强大潜力，但相关研究分散且缺乏实践者参与，需要创建一个促进自由讨论和协作的环境。

Method: 通过组织LIR研讨会，为研究人员和从业者提供高度互动的环境，分享早期研究探索、实际成果以及负面或困惑的结果。

Result: 研讨会旨在整合分散的研究努力，促进跨社区合作，并加速延迟交互检索方法在实际应用中的发展。

Conclusion: LIR研讨会为延迟交互检索领域的研究人员和实践者提供了宝贵的交流平台，有助于推动该技术的进一步发展和应用。

Abstract: Late interaction retrieval methods, pioneered by ColBERT, have emerged as a
powerful alternative to single-vector neural IR. By leveraging fine-grained,
token-level representations, they have been demonstrated to deliver strong
generalisation and robustness, particularly in out-of-domain settings. They
have recently been shown to be particularly well-suited for novel use cases,
such as reasoning-based or cross-modality retrieval. At the same time, these
models pose significant challenges of efficiency, usability, and integrations
into fully fledged systems; as well as the natural difficulties encountered
while researching novel application domains. Recent years have seen rapid
advances across many of these areas, but research efforts remain fragmented
across communities and frequently exclude practitioners. The purpose of this
workshop is to create an environment where all aspects of late interaction can
be discussed, with a focus on early research explorations, real-world outcomes,
and negative or puzzling results to be freely shared and discussed. The aim of
LIR is to provide a highly-interactive environment for researchers from various
backgrounds and practitioners to freely discuss their experience, fostering
further collaboration.

</details>


### [282] [Listwise Preference Diffusion Optimization for User Behavior Trajectories Prediction](https://arxiv.org/abs/2511.00530)
*Hongtao Huang,Chengkai Huang,Junda Wu,Tong Yu,Julian McAuley,Lina Yao*

Main category: cs.IR

TL;DR: 提出LPDO框架，通过扩散模型直接优化整个物品序列的结构化偏好，解决了传统序列推荐无法捕捉全局列表依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐方法无法捕捉用户行为轨迹中的全局列表依赖关系，限制了多步行为预测的准确性，这在个性化商务和自适应内容分发等应用中至关重要。

Method: 引入Listwise Preference Diffusion Optimization (LPDO)框架，结合Plackett-Luce监督信号和变分下界，克服了先前扩散方法的独立token假设，实现连贯的偏好生成。

Result: 在真实世界用户行为基准测试中，LPDO持续优于最先进的基线方法，为扩散模型的结构化偏好学习建立了新基准。

Conclusion: LPDO通过直接优化结构化偏好，显著提升了多步用户行为轨迹预测的性能，为序列推荐任务提供了更有效的解决方案。

Abstract: Forecasting multi-step user behavior trajectories requires reasoning over
structured preferences across future actions, a challenge overlooked by
traditional sequential recommendation. This problem is critical for
applications such as personalized commerce and adaptive content delivery, where
anticipating a user's complete action sequence enhances both satisfaction and
business outcomes. We identify an essential limitation of existing paradigms:
their inability to capture global, listwise dependencies among sequence items.
To address this, we formulate User Behavior Trajectory Prediction (UBTP) as a
new task setting that explicitly models long-term user preferences. We
introduce Listwise Preference Diffusion Optimization (LPDO), a diffusion-based
training framework that directly optimizes structured preferences over entire
item sequences. LPDO incorporates a Plackett-Luce supervision signal and
derives a tight variational lower bound aligned with listwise ranking
likelihoods, enabling coherent preference generation across denoising steps and
overcoming the independent-token assumption of prior diffusion methods. To
rigorously evaluate multi-step prediction quality, we propose the task-specific
metric Sequential Match (SeqMatch), which measures exact trajectory agreement,
and adopt Perplexity (PPL), which assesses probabilistic fidelity. Extensive
experiments on real-world user behavior benchmarks demonstrate that LPDO
consistently outperforms state-of-the-art baselines, establishing a new
benchmark for structured preference learning with diffusion models.

</details>


### [283] [Structurally Refined Graph Transformer for Multimodal Recommendation](https://arxiv.org/abs/2511.00584)
*Ke Shi,Yan Zhang,Miao Zhang,Lifan Chen,Jiali Yi,Kui Xiao,Xiaoju Hou,Zhifei Li*

Main category: cs.IR

TL;DR: SRGFormer是一个结构优化的多模态推荐模型，通过改进Transformer架构和超图结构来更好地整合多模态信息，解决现有模型对冗余和有用数据区分不足、语义框架单一的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐系统过度关注多模态信息提取，但忽视了冗余与有用数据的区分，且依赖单一语义框架，导致用户偏好表示不完整，特别是对先前交互中表达较少的偏好。此外，现有方法未能捕捉用户与物品间的复杂交互，限制了模型满足多样化用户需求的能力。

Method: 通过改进Transformer架构以更好地集成到模型中，捕捉用户的整体行为模式；将多模态信息嵌入超图结构以增强结构信息，帮助学习用户与物品间的局部结构；对用户-物品协作信号应用自监督任务，增强多模态信息的整合，揭示数据模态的表示特征。

Result: 在三个公共数据集上的广泛实验表明，SRGFormer超越了之前的基准模型，在Sports数据集上平均性能提升4.47%。

Conclusion: SRGFormer通过结构优化和多模态信息的有效整合，显著提升了多模态推荐系统的性能，解决了现有模型在数据区分、语义表示和用户交互捕捉方面的局限性。

Abstract: Multimodal recommendation systems utilize various types of information,
including images and text, to enhance the effectiveness of recommendations. The
key challenge is predicting user purchasing behavior from the available data.
Current recommendation models prioritize extracting multimodal information
while neglecting the distinction between redundant and valuable data. They also
rely heavily on a single semantic framework (e.g., local or global semantics),
resulting in an incomplete or biased representation of user preferences,
particularly those less expressed in prior interactions. Furthermore, these
approaches fail to capture the complex interactions between users and items,
limiting the model's ability to meet diverse users. To address these
challenges, we present SRGFormer, a structurally optimized multimodal
recommendation model. By modifying the transformer for better integration into
our model, we capture the overall behavior patterns of users. Then, we enhance
structural information by embedding multimodal information into a hypergraph
structure to aid in learning the local structures between users and items.
Meanwhile, applying self-supervised tasks to user-item collaborative signals
enhances the integration of multimodal information, thereby revealing the
representational features inherent to the data's modality. Extensive
experiments on three public datasets reveal that SRGFormer surpasses previous
benchmark models, achieving an average performance improvement of 4.47 percent
on the Sports dataset. The code is publicly available online.

</details>


### [284] [Taxonomy-based Negative Sampling In Personalized Semantic Search for E-commerce](https://arxiv.org/abs/2511.00694)
*Uthman Jinadu,Siawpeng Er,Le Yu,Chen Liang,Bingxin Li,Yi Ding,Aleksandar Velkoski*

Main category: cs.IR

TL;DR: 提出了一种用于电子商务搜索的语义检索模型，采用基于分类的硬负采样策略和用户个性化，显著提升了检索效果和商业指标。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以理解相似商品的细微差异，采样方法计算成本高，且未考虑用户历史购买行为，导致检索结果不相关。

Method: 将查询和产品嵌入共享向量空间，使用基于分类的硬负采样策略挖掘上下文相关但具有挑战性的负样本，并整合用户历史购买行为进行个性化建模。

Result: 在离线实验中，该方法在Recall@K指标上优于BM25、ANCE和其他主流神经基线模型；在线A/B测试显示转化率、加购率和平均订单价值均有显著提升。

Conclusion: 基于分类的负采样减少了训练开销并加速收敛，该系统已在规模部署中验证了实用性。

Abstract: Large retail outlets offer products that may be domain-specific, and this
requires having a model that can understand subtle differences in similar
items. Sampling techniques used to train these models are most of the time,
computationally expensive or logistically challenging. These models also do not
factor in users' previous purchase patterns or behavior, thereby retrieving
irrelevant items for them. We present a semantic retrieval model for e-commerce
search that embeds queries and products into a shared vector space and
leverages a novel taxonomy-based hard-negative sampling(TB-HNS) strategy to
mine contextually relevant yet challenging negatives. To further tailor
retrievals, we incorporate user-level personalization by modeling each
customer's past purchase history and behavior. In offline experiments, our
approach outperforms BM25, ANCE and leading neural baselines on Recall@K, while
live A/B testing shows substantial uplifts in conversion rate, add-to-cart
rate, and average order value. We also demonstrate that our taxonomy-driven
negatives reduce training overhead and accelerate convergence, and we share
practical lessons from deploying this system at scale.

</details>


### [285] [REaR: Retrieve, Expand and Refine for Effective Multitable Retrieval](https://arxiv.org/abs/2511.00805)
*Rishita Agarwal,Himanshu Singhal,Peter Baile Chen,Manan Roy Choudhury,Dan Roth,Vivek Gupta*

Main category: cs.IR

TL;DR: REAR是一个三阶段、无需LLM的多表检索框架，通过分离语义相关性和结构可连接性来提升多表问答性能，在多个数据集上优于现有方法且延迟和成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有检索器主要优化查询-表相关性，但忽略了表之间的兼容性，导致在多表问答任务中检索效果不佳。

Method: 三阶段框架：(1)检索查询相关的表；(2)通过预计算的列嵌入比较扩展结构可连接的表；(3)通过剪枝噪声或弱相关候选表进行精炼。

Result: 在BIRD、MMQA和Spider等复杂表问答数据集上，REAR持续改进密集/稀疏检索器的多表检索质量和下游SQL执行性能，性能与最先进的LLM增强检索系统相当但延迟和成本更低。

Conclusion: REAR是一个实用、可扩展的构建块，适用于基于表的下游任务（如Text-to-SQL），扩展和精炼阶段具有互补增益。

Abstract: Answering natural language queries over relational data often requires
retrieving and reasoning over multiple tables, yet most retrievers optimize
only for query-table relevance and ignore table table compatibility. We
introduce REAR (Retrieve, Expand and Refine), a three-stage, LLM-free framework
that separates semantic relevance from structural joinability for efficient,
high-fidelity multi-table retrieval. REAR (i) retrieves query-aligned tables,
(ii) expands these with structurally joinable tables via fast, precomputed
column-embedding comparisons, and (iii) refines them by pruning noisy or weakly
related candidates. Empirically, REAR is retriever-agnostic and consistently
improves dense/sparse retrievers on complex table QA datasets (BIRD, MMQA, and
Spider) by improving both multi-table retrieval quality and downstream SQL
execution. Despite being LLM-free, it delivers performance competitive with
state-of-the-art LLM-augmented retrieval systems (e.g.,ARM) while achieving
much lower latency and cost. Ablations confirm complementary gains from
expansion and refinement, underscoring REAR as a practical, scalable building
block for table-based downstream tasks (e.g., Text-to-SQL).

</details>


### [286] [Controlling Gender Bias in Retrieval via a Backpack Architecture](https://arxiv.org/abs/2511.00875)
*Amirabbas Afzali,Amirreza Velae,Iman Ahmadi,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: 提出了一种基于Backpack语言模型的去偏框架，通过分解词义方面来减轻文本检索和排序中的性别偏见，在保持性能的同时有效减少偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的社会偏见会通过排序系统传播，导致搜索引擎和推荐系统等关键应用中出现不公平结果，需要有效的去偏方法。

Method: 利用Backpack语言模型架构，将输出生成为非上下文学习词义方面的加权组合，基于此构建去偏框架。

Result: 实验结果显示该框架能有效减轻文本检索和排序中的性别偏见，且性能下降最小。

Conclusion: Backpack语言模型的分解式架构为排序任务去偏提供了有效途径，能在保持性能的同时显著减少社会偏见。

Abstract: The presence of social biases in large language models (LLMs) has become a
significant concern in AI research. These biases, often embedded in training
data, can perpetuate harmful stereotypes and distort decision-making processes.
When LLMs are integrated into ranking systems, they can propagate these biases,
leading to unfair outcomes in critical applications such as search engines and
recommendation systems. Backpack Language Models, unlike traditional
transformer-based models that treat text sequences as monolithic structures,
generate outputs as weighted combinations of non-contextual, learned word
aspects, also known as senses. Leveraging this architecture, we propose a
framework for debiasing ranking tasks. Our experimental results show that this
framework effectively mitigates gender bias in text retrieval and ranking with
minimal degradation in performance.

</details>


### [287] [Contextual Relevance and Adaptive Sampling for LLM-Based Document Reranking](https://arxiv.org/abs/2511.01208)
*Jerry Huang,Siddarth Madala,Cheng Niu,Julia Hockenmaier,Tong Zhang*

Main category: cs.IR

TL;DR: 提出上下文相关性概念和TS-SetRank算法，通过考虑文档批次组成来改进需要深度推理查询的文档重排序性能


<details>
  <summary>Details</summary>
Motivation: 现有重排序算法在处理需要深度推理的查询时效果有限，这类查询具有多方面信息需求和细微解释，文档相关性高度依赖于上下文

Method: 提出上下文相关性概念，定义为文档在可能出现的不同重排序上下文中的相关概率。开发TS-SetRank算法，基于采样的不确定性感知重排序方法

Result: TS-SetRank在BRIGHT数据集上nDCG@10比基准方法提升15-25%，在BEIR数据集上提升6-21%

Conclusion: 将相关性建模为上下文依赖的重要性得到验证，TS-SetRank能有效提升推理密集型查询的文档重排序质量

Abstract: Reranking algorithms have made progress in improving document retrieval
quality by efficiently aggregating relevance judgments generated by large
language models (LLMs). However, identifying relevant documents for queries
that require in-depth reasoning remains a major challenge. Reasoning-intensive
queries often exhibit multifaceted information needs and nuanced
interpretations, rendering document relevance inherently context dependent. To
address this, we propose contextual relevance, which we define as the
probability that a document is relevant to a given query, marginalized over the
distribution of different reranking contexts it may appear in (i.e., the set of
candidate documents it is ranked alongside and the order in which the documents
are presented to a reranking model). While prior works have studied methods to
mitigate the positional bias LLMs exhibit by accounting for the ordering of
documents, we empirically find that the compositions of these batches also
plays an important role in reranking performance. To efficiently estimate
contextual relevance, we propose TS-SetRank, a sampling-based,
uncertainty-aware reranking algorithm. Empirically, TS-SetRank improves nDCG@10
over retrieval and reranking baselines by 15-25% on BRIGHT and 6-21% on BEIR,
highlighting the importance of modeling relevance as context-dependent.

</details>


### [288] [A semantic-based deep learning approach for mathematical expression retrieval](https://arxiv.org/abs/2511.01364)
*Pavan Kumar Perepu*

Main category: cs.IR

TL;DR: 该论文提出了一种基于深度循环神经网络(DRNN)的数学表达式检索方法，通过提取语义特征进行匹配，将数学表达式按嵌套深度分为简单、中等和复杂三类。


<details>
  <summary>Details</summary>
Motivation: 传统基于字符串匹配和向量空间模型的数学表达式检索方法主要依赖句法相似性，缺乏语义理解。需要开发能够捕捉数学表达式语义特征的深度学习方法。

Method: 使用深度循环神经网络(DRNN)训练数学表达式复杂度分类任务（按嵌套深度分为三类），提取最后一层全连接层之前的输出作为语义特征，基于欧几里得距离进行特征匹配和检索。

Result: 在包含829个数学表达式的数据库上验证了该方法的有效性，能够根据用户定义的参数k返回最相似的top-k个匹配结果。

Conclusion: 基于深度学习的语义特征提取方法能够有效解决数学表达式检索问题，相比传统方法具有更好的语义理解能力。

Abstract: Mathematical expressions (MEs) have complex two-dimensional structures in
which symbols can be present at any nested depth like superscripts, subscripts,
above, below etc. As MEs are represented using LaTeX format, several text
retrieval methods based on string matching, vector space models etc., have also
been applied for ME retrieval problem in the literature. As these methods are
based on syntactic similarity, recently deep learning approaches based on
embedding have been used for semantic similarity. In our present work, we have
focused on the retrieval of mathematical expressions using deep learning
approaches. In our approach, semantic features are extracted from the MEs using
a deep recurrent neural network (DRNN) and these features have been used for
matching and retrieval. We have trained the network for a classification task
which determines the complexity of an ME. ME complexity has been quantified in
terms of its nested depth. Based on the nested depth, we have considered three
complexity classes of MEs: Simple, Medium and Complex. After training the
network, outputs just before the the final fully connected layer are extracted
for all the MEs. These outputs form the semantic features of MEs and are stored
in a database. For a given ME query, its semantic features are computed using
the trained DRNN and matched against the semantic feature database. Matching is
performed based on the standard euclidean distance and top 'k' nearest matches
are retrieved, where 'k' is a user-defined parameter. Our approach has been
illustrated on a database of 829 MEs.

</details>


### [289] [A Soft-partitioned Semi-supervised Collaborative Transfer Learning Approach for Multi-Domain Recommendation](https://arxiv.org/abs/2511.01404)
*Xiaoyu Liu,Yiqing Wu,Ruidong Han,Fuzhen Zhuang,Xiang Li,Wei Lin*

Main category: cs.IR

TL;DR: 提出SSCTL方法解决多域推荐中的不平衡数据问题，通过动态参数和伪标签技术改善非主导域的性能，在线测试显示GMV和CTR显著提升


<details>
  <summary>Details</summary>
Motivation: 工业实践中多域推荐面临数据不平衡问题：主导域数据压倒模型性能，忽视非主导域；非主导域数据稀疏导致特定参数过拟合

Method: 提出软分区半监督协同迁移学习(SSCTL)，使用动态参数解决压倒性问题，利用主导域实例的加权伪标签增强非主导域数据

Result: 在线测试显示各域GMV提升0.54%-2.90%，CTR提升0.22%-1.69%

Conclusion: SSCTL方法有效解决了多域推荐中的数据不平衡问题，显著提升了各域的性能指标

Abstract: In industrial practice, Multi-domain Recommendation (MDR) plays a crucial
role. Shared-specific architectures are widely used in industrial solutions to
capture shared and unique attributes via shared and specific parameters.
However, with imbalanced data across different domains, these models face two
key issues: (1) Overwhelming: Dominant domain data skews model performance,
neglecting non-dominant domains. (2) Overfitting: Sparse data in non-dominant
domains leads to overfitting in specific parameters. To tackle these
challenges, we propose Soft-partitioned Semi-supervised Collaborative Transfer
Learning (SSCTL) for multi-domain recommendation. SSCTL generates dynamic
parameters to address the overwhelming issue, thus shifting focus towards
samples from non-dominant domains. To combat overfitting, it leverages
pseudo-labels with weights from dominant domain instances to enhance
non-dominant domain data. We conduct comprehensive experiments, both online and
offline, to validate the efficacy of our proposed method. Online tests yielded
significant improvements across various domains, with increases in GMV ranging
from 0.54% to 2.90% and enhancements in CTR ranging from 0.22% to 1.69%.

</details>


### [290] [LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning](https://arxiv.org/abs/2511.01448)
*Zhengjun Huang,Zhoujin Tian,Qintian Guo,Fangyuan Zhang,Yingli Zhou,Di Jiang,Xiaofang Zhou*

Main category: cs.IR

TL;DR: LiCoMemory是一个端到端的智能体记忆框架，通过引入CogniGraph轻量级分层图结构，使用实体和关系作为语义索引层，实现实时更新和检索，解决了传统图记忆结构中语义与拓扑纠缠导致的冗余表示和无结构检索问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体虽然具有出色的对话和推理能力，但受到有限上下文窗口和缺乏持久记忆的限制。现有的外部记忆架构大多采用扁平、纠缠的结构，将语义与拓扑交织在一起，导致表示冗余、检索无结构化，以及效率和准确性下降。

Method: 提出LiCoMemory框架，引入CogniGraph轻量级分层图，利用实体和关系作为语义索引层，采用时间和层次感知搜索，并结合重排序实现自适应和连贯的知识检索。

Result: 在长期对话基准测试LoCoMo和LongMemEval上的实验表明，LiCoMemory在时间推理、多会话一致性和检索效率方面优于现有基线方法，并显著降低了更新延迟。

Conclusion: LiCoMemory通过分层图结构和智能检索机制有效解决了LLM智能体的记忆限制问题，在多个关键指标上表现出优越性能。

Abstract: Large Language Model (LLM) agents exhibit remarkable conversational and
reasoning capabilities but remain constrained by limited context windows and
the lack of persistent memory. Recent efforts address these limitations via
external memory architectures, often employing graph-based representations, yet
most adopt flat, entangled structures that intertwine semantics with topology,
leading to redundant representations, unstructured retrieval, and degraded
efficiency and accuracy. To resolve these issues, we propose LiCoMemory, an
end-to-end agentic memory framework for real-time updating and retrieval, which
introduces CogniGraph, a lightweight hierarchical graph that utilizes entities
and relations as semantic indexing layers, and employs temporal and
hierarchy-aware search with integrated reranking for adaptive and coherent
knowledge retrieval. Experiments on long-term dialogue benchmarks, LoCoMo and
LongMemEval, show that LiCoMemory not only outperforms established baselines in
temporal reasoning, multi-session consistency, and retrieval efficiency, but
also notably reduces update latency. Our official code and data are available
at https://github.com/EverM0re/LiCoMemory.

</details>


### [291] [CAT-ID$^2$: Category-Tree Integrated Document Identifier Learning for Generative Retrieval In E-commerce](https://arxiv.org/abs/2511.01461)
*Xiaoyu Liu,Fuwei Zhang,Yiqing Wu,Xinyu Jia,Zenghua Xia,Fuzhen Zhuang,Zhao Zhang,Fei Jiang,Wei Lin*

Main category: cs.IR

TL;DR: 提出CAT-ID²方法，通过整合类别信息来改进生成式检索中的文档ID构建，使相似文档具有更相似的ID，同时保持不同文档ID的独特性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索方法大多忽略了电商领域常见的原生类别信息，而好的文档ID应具备相似文档ID相似、不同文档ID独特两个关键特性。

Method: 提出CAT-ID²方法，包含三个关键模块：分层类别约束损失（逐层整合类别信息）、聚类尺度约束损失（均匀ID token分布）和分散损失（提高重构文档区分度）。

Result: 离线和在线实验验证了方法的有效性，在线A/B测试显示模糊意图查询的千用户平均订单数提升0.33%，长尾查询提升0.24%。

Conclusion: CAT-ID²通过整合类别信息成功改进了生成式检索的文档ID构建，在电商场景中取得了显著效果提升。

Abstract: Generative retrieval (GR) has gained significant attention as an effective
paradigm that integrates the capabilities of large language models (LLMs). It
generally consists of two stages: constructing discrete semantic identifiers
(IDs) for documents and retrieving documents by autoregressively generating ID
tokens.The core challenge in GR is how to construct document IDs (DocIDS) with
strong representational power. Good IDs should exhibit two key properties:
similar documents should have more similar IDs, and each document should
maintain a distinct and unique ID.However, most existing methods ignore native
category information, which is common and critical in E-commerce. Therefore, we
propose a novel ID learning method, CAtegory-Tree Integrated Document
IDentifier (CAT-ID$^2$), incorporating prior category information into the
semantic IDs.CAT-ID$^2$ includes three key modules: a Hierarchical Class
Constraint Loss to integrate category information layer by layer during
quantization, a Cluster Scale Constraint Loss for uniform ID token
distribution, and a Dispersion Loss to improve the distinction of reconstructed
documents. These components enable CAT-ID$^2$ to generate IDs that make similar
documents more alike while preserving the uniqueness of different documents'
representations.Extensive offline and online experiments confirm the
effectiveness of our method, with online A/B tests showing a 0.33% increase in
average orders per thousand users for ambiguous intent queries and 0.24% for
long-tail queries.

</details>


### [292] [Trove: A Flexible Toolkit for Dense Retrieval](https://arxiv.org/abs/2511.01857)
*Reza Esfandiarpoor,Max Zuo,Stephen H. Bach*

Main category: cs.IR

TL;DR: Trove是一个易于使用的开源检索工具包，简化研究实验而不牺牲灵活性或速度，提供高效数据管理、高度定制化和多节点执行支持。


<details>
  <summary>Details</summary>
Motivation: 现有检索工具在数据管理和实验灵活性方面存在不足，需要简化研究实验流程，支持动态数据处理和自定义组件，同时保持高性能。

Method: 开发了高效的数据管理功能，支持动态加载和处理检索数据集；提供高度可定制架构，允许修改或替换组件；构建低代码统一评估和硬负例挖掘管道，支持多节点执行。

Result: 数据管理功能将内存消耗减少2.6倍；推理管道无开销且推理时间随节点数线性减少；简化了检索实验并支持任意定制。

Conclusion: Trove通过高效数据管理、高度定制化和多节点支持，显著简化了检索实验流程，促进了探索性研究。

Abstract: We introduce Trove, an easy-to-use open-source retrieval toolkit that
simplifies research experiments without sacrificing flexibility or speed. For
the first time, we introduce efficient data management features that load and
process (filter, select, transform, and combine) retrieval datasets on the fly,
with just a few lines of code. This gives users the flexibility to easily
experiment with different dataset configurations without the need to compute
and store multiple copies of large datasets. Trove is highly customizable: in
addition to many built-in options, it allows users to freely modify existing
components or replace them entirely with user-defined objects. It also provides
a low-code and unified pipeline for evaluation and hard negative mining, which
supports multi-node execution without any code changes. Trove's data management
features reduce memory consumption by a factor of 2.6. Moreover, Trove's
easy-to-use inference pipeline incurs no overhead, and inference times decrease
linearly with the number of available nodes. Most importantly, we demonstrate
how Trove simplifies retrieval experiments and allows for arbitrary
customizations, thus facilitating exploratory research.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [293] [Optimal Allocations under Strongly Pigou-Dalton Criteria: Hidden Layer Structure & Efficient Combinatorial Approach](https://arxiv.org/abs/2511.00835)
*Taikun Zhu,Kai Jin,Ruixi Luo,Song Cao*

Main category: cs.GT

TL;DR: 该论文研究了在二元加性或次模估值下，最优社会福利分配与稳定分配的关系，并设计了高效算法来寻找稳定分配。


<details>
  <summary>Details</summary>
Motivation: 研究在二元加性或次模估值下，最优社会福利分配与稳定分配之间的关系，以及如何高效计算这些分配。

Method: 证明了最优分配集与稳定分配集在强Pigou-Dalton对称准则下重合，设计了O(m²n)时间算法处理不可分物品和O(m²n⁵)时间算法处理可分物品。

Result: 建立了最优分配与稳定分配的等价关系，开发了高效的组合算法，并证明了不同最优分配配置之间的切比雪夫距离很小。

Conclusion: 在二元加性或次模估值下，最优社会福利分配与稳定分配密切相关，可以通过高效算法计算，且不同最优分配之间的差异很小。

Abstract: We investigate optimal social welfare allocations of $m$ items to $n$ agents
with binary additive or submodular valuations. For binary additive valuations,
we prove that the set of optimal allocations coincides with the set of
so-called \emph{stable allocations}, as long as the employed criterion for
evaluating social welfare is strongly Pigou-Dalton (SPD) and symmetric. Many
common criteria are SPD and symmetric, such as Nash social welfare, leximax,
leximin, Gini index, entropy, and envy sum. We also design efficient algorithms
for finding a stable allocation, including an $O(m^2n)$ time algorithm for the
case of indivisible items, and an $O(m^2n^5)$ time one for the case of
divisible items. The first is faster than the existing algorithms or has a
simpler analysis. The latter is the first combinatorial algorithm for that
problem. It utilizes a hidden layer partition of items and agents admitted by
all stable allocations, and cleverly reduces the case of divisible items to the
case of indivisible items.
  In addition, we show that the profiles of different optimal allocations have
a small Chebyshev distance, which is 0 for the case of divisible items under
binary additive valuations, and is at most 1 for the case of indivisible items
under binary submodular valuations.

</details>


### [294] [Pay for The Second-Best Service: A Game-Theoretic Approach Against Dishonest LLM Providers](https://arxiv.org/abs/2511.00847)
*Yuhan Cao,Yu Wang,Sitong Liu,Miao Li,Yixin Tao,Tianxing He*

Main category: cs.GT

TL;DR: 该论文针对LLM API服务中的提供商欺诈问题，提出了首个形式化经济模型和近似激励相容机制，确保用户在多次查询中能获得准线性次优效用。


<details>
  <summary>Details</summary>
Motivation: LLM API服务的广泛采用带来了关键漏洞：服务提供商可能进行欺诈性操纵，如秘密替换高性能模型为低成本替代品，或通过填充无意义token来增加计费。

Method: 从算法博弈论和机制设计角度，提出了一个形式化经济模型，用户可迭代委托多个模型提供商处理查询，提供商可采取策略性行为。设计了近似激励相容机制。

Result: 证明了在连续策略空间中，存在近似激励相容机制，具有O(T^{1-ε}log T)的加法近似比，并保证准线性次优用户效用。同时证明了不可能性结果，表明没有机制能渐进优于该机制。

Conclusion: 该机制在实际API设置中的仿真实验证明了其有效性，为解决LLM API服务中的提供商欺诈问题提供了理论基础和实用解决方案。

Abstract: The widespread adoption of Large Language Models (LLMs) through Application
Programming Interfaces (APIs) induces a critical vulnerability: the potential
for dishonest manipulation by service providers. This manipulation can manifest
in various forms, such as secretly substituting a proclaimed high-performance
model with a low-cost alternative, or inflating responses with meaningless
tokens to increase billing. This work tackles the issue through the lens of
algorithmic game theory and mechanism design. We are the first to propose a
formal economic model for a realistic user-provider ecosystem, where a user can
iteratively delegate $T$ queries to multiple model providers, and providers can
engage in a range of strategic behaviors. As our central contribution, we prove
that for a continuous strategy space and any $\epsilon\in(0,\frac12)$, there
exists an approximate incentive-compatible mechanism with an additive
approximation ratio of $O(T^{1-\epsilon}\log T)$, and a guaranteed quasi-linear
second-best user utility. We also prove an impossibility result, stating that
no mechanism can guarantee an expected user utility that is asymptotically
better than our mechanism. Furthermore, we demonstrate the effectiveness of our
mechanism in simulation experiments with real-world API settings.

</details>


### [295] [Deliberation via Matching](https://arxiv.org/abs/2511.00986)
*Kamesh Munagala,Qilin Ye,Ian Zhang*

Main category: cs.GT

TL;DR: 提出了一种通过匹配讨论的审议式社会选择协议，在度量失真框架下达到了紧致的失真界3，突破了无审议情况下锦标赛规则的下界3.11。


<details>
  <summary>Details</summary>
Motivation: 研究选民通过小组讨论完善偏好的审议式社会选择，探索如何通过简单的成对讨论协议来改善社会选择的质量。

Method: 引入审议-匹配协议：对每对候选人，在持不同意见的选民间形成任意最大匹配，每对匹配的选民进行讨论，然后使用加权未覆盖集锦标赛规则聚合偏好。

Result: 该协议在度量失真框架下具有紧致的失真界3，突破了无审议情况下锦标赛规则的下界3.11，与无审议确定性社会选择规则的下界匹配。

Conclusion: 当锦标赛规则被赋予成对审议的最小额外能力时，其能力与一般社会选择规则相当。通过双线性松弛和非线性规划的新分析方法为研究其他审议协议的失真提供了通用框架。

Abstract: We study deliberative social choice, where voters refine their preferences
through small-group discussions before collective aggregation. We introduce a
simple and easily implementable deliberation-via-matching protocol: for each
pair of candidates, we form an arbitrary maximum matching among voters who
disagree on that pair, and each matched pair deliberates. The resulting
preferences (individual and deliberative) are then appropriately weighted and
aggregated using the weighted uncovered set tournament rule.
  We show that our protocol has a tight distortion bound of $3$ within the
metric distortion framework. This breaks the previous lower bound of $3.11$ for
tournament rules without deliberation and matches the lower bound for
deterministic social choice rules without deliberation. Our result conceptually
shows that tournament rules are just as powerful as general social choice
rules, when the former are given the minimal added power of pairwise
deliberations. We prove our bounds via a novel bilinear relaxation of the
non-linear program capturing optimal distortion, whose vertices we can
explicitly enumerate, leading to an analytic proof. Loosely speaking, our key
technical insight is that the distortion objective, as a function of metric
distances to any three alternatives, is both supermodular and convex. We
believe this characterization provides a general analytical framework for
studying the distortion of other deliberative protocols, and may be of
independent interest.

</details>


### [296] [From Best Responses to Learning: Investment Efficiency in Dynamic Environment](https://arxiv.org/abs/2511.01157)
*Ce Li,Qianfan Zhang,Weiqiang Zheng*

Main category: cs.GT

TL;DR: 研究动态环境中学习型投资者的福利保障，分析近似分配算法在投资者使用无悔在线学习而非最优响应时的福利保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界中投资者由于信息不完全无法总是做出最优响应，需要研究当投资者使用学习算法时的机制福利保障。

Method: 将机制设计与在线学习理论结合，分析近似分配算法在动态学习环境中的近似比率，对比静态和动态环境下的福利保证。

Result: 静态环境中的近似比率在动态环境中相对于事后最优基准保持不变；对更强的时间变化基准给出了紧致的近似上下界刻画。

Conclusion: 即使投资者无法做出最优响应而是通过学习策略，机制设计仍能保持稳健的福利保证。

Abstract: We study the welfare of a mechanism in a dynamic environment where a learning
investor can make a costly investment to change her value. In many real-world
problems, the common assumption that the investor always makes the best
responses, i.e., choosing her utility-maximizing investment option, is
unrealistic due to incomplete information in a dynamically evolving
environment. To address this, we consider an investor who uses a no-regret
online learning algorithm to adaptively select investments through repeated
interactions with the environment. We analyze how the welfare guarantees of
approximation allocation algorithms extend from static to dynamic settings when
the investor learns rather than best-responds, by studying the approximation
ratio for optimal welfare as a measurement of an algorithm's performance
against different benchmarks in the dynamic learning environment. First, we
show that the approximation ratio in the static environment remains unchanged
in the dynamic environment against the best-in-hindsight benchmark. Second, we
provide tight characterizations of the approximation upper and lower bounds
relative to a stronger time-varying benchmark. Bridging mechanism design with
online learning theory, our work shows how robust welfare guarantees can be
maintained even when an agent cannot make best responses but learns their
investment strategies in complex, uncertain environments.

</details>


### [297] [Designing Non-monetary Intersection Control Mechanisms for Efficient Selfish Routing](https://arxiv.org/abs/2511.01421)
*Yusuf Saltan,Jyun-Jhe Wang,Arda Kosay,Chung-Wei Lin,Muhammed O. Sayin*

Main category: cs.GT

TL;DR: 提出一种非货币机制，通过战略性地调整车辆通行时间戳来激励社会效率最优的路由选择，在Sioux Falls网络中可将均衡流与最优流之间的效率差距减少68%。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵源于自利路由决策与社会最优流之间的错配，交叉口作为关键瓶颈放大了这种低效率，因为现有控制方案往往忽视驾驶员的战略行为。

Method: 采用分层架构，将路边单元本地调度与中央规划器网络范围时间戳调整分离，建立实验验证的分析模型，将规划器问题表述为可离线求解的双层优化程序。

Result: 在Sioux Falls网络上的实验显示，均衡流与最优流之间的效率差距最多可减少68%，证明了该方法的可扩展性和有效性。

Conclusion: 通过车辆级调度和时间戳调整的非货币机制能够有效激励社会效率最优的路由选择，显著减少交通拥堵效率损失。

Abstract: Urban traffic congestion stems from the misalignment between self-interested
routing decisions and socially optimal flows. Intersections, as critical
bottlenecks, amplify these inefficiencies because existing control schemes
often neglect drivers' strategic behavior. Autonomous intersections, enabled by
vehicle-to-infrastructure communication, permit vehicle-level scheduling based
on individual requests. Leveraging this fine-grained control, we propose a
non-monetary mechanism that strategically adjusts request timestamps-delaying
or advancing passage times-to incentivize socially efficient routing. We
present a hierarchical architecture separating local scheduling by roadside
units from network-wide timestamp adjustments by a central planner. We
establish an experimentally validated analytical model, prove the existence and
essential uniqueness of equilibrium flows and formulate the planner's problem
as an offline bilevel optimization program solvable with standard tools.
Experiments on the Sioux Falls network show up to a 68% reduction in the
efficiency gap between equilibrium and optimal flows, demonstrating scalability
and effectiveness.

</details>


### [298] [Proximal Regret and Proximal Correlated Equilibria: A New Tractable Solution Concept for Online Learning and Games](https://arxiv.org/abs/2511.01852)
*Yang Cai,Constantinos Daskalakis,Haipeng Luo,Chen-Yu Wei,Weiqiang Zheng*

Main category: cs.GT

TL;DR: 本文提出了一种新的遗憾概念——近端遗憾，它介于外部遗憾和交换遗憾之间。当所有玩家在凸博弈中使用无近端遗憾算法时，经验分布会收敛到近端相关均衡。研究发现经典在线梯度下降算法能够达到最优的O(√T)近端遗憾界，解释了梯度下降在在线学习和博弈中优越性能的原因。


<details>
  <summary>Details</summary>
Motivation: 学习和计算均衡是算法博弈论的核心问题。现有遗憾概念如外部遗憾和交换遗憾存在局限性，需要一种介于两者之间的新遗憾概念来更好地理解在线学习算法的行为。

Method: 引入基于近端算子的近端遗憾概念，分析在线梯度下降、镜像下降和乐观梯度下降等算法在近端遗憾下的性能。

Result: 在线梯度下降算法达到最优的O(√T)近端遗憾界，经验分布收敛到近端相关均衡。乐观梯度下降在光滑凸博弈中能实现更快的收敛速度。

Conclusion: 近端遗憾框架统一了在线学习和博弈论中的多个新兴概念，为梯度下降算法的优越性能提供了新的理论解释，并扩展了均衡收敛分析的工具集。

Abstract: Learning and computation of equilibria are central problems in algorithmic
game theory. In this work, we introduce proximal regret, a new notion of regret
based on proximal operators that lies strictly between external and swap
regret. When every player employs a no-proximal-regret algorithm in a general
convex game, the empirical distribution of play converges to proximal
correlated equilibria (PCE), a refinement of coarse correlated equilibria. Our
framework unifies several emerging notions in online learning and game theory
-- such as gradient equilibrium and semicoarse correlated equilibrium -- and
introduces new ones. Our main result shows that the classic Online Gradient
Descent (GD) algorithm achieves an optimal $O(\sqrt{T})$ bound on proximal
regret, revealing that GD, without modification, minimizes a stronger regret
notion than external regret. This provides a new explanation for the
empirically superior performance of gradient descent in online learning and
games. We further extend our analysis to Mirror Descent in the Bregman setting
and to Optimistic Gradient Descent, which yields faster convergence in smooth
convex games.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [299] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种融合文本和视觉特征的多模态虚假评论检测框架，在包含21,142张用户上传图片的数据集上取得了0.934的F1分数，优于单模态基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前数字商务中虚假评论泛滥，现有检测模型主要依赖单模态文本数据，无法捕捉跨模态的语义不一致性，威胁评论生态系统的信任和透明度。

Method: 使用BERT编码文本特征和ResNet-50提取视觉特征，通过分类头融合这些表示来联合预测评论真实性。

Result: 多模态模型在测试集上F1分数达到0.934，优于单模态基线，能有效检测文本赞美与不相关或低质量图片之间的微妙不一致性。

Conclusion: 研究证明了多模态学习在保护数字信任中的关键作用，为各种在线平台的内容审核提供了可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [300] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 该论文系统研究了多智能体强化学习在零售价格优化中的应用，比较了MAPPO基线方法和图注意力增强的MAPPO+GAT方法，发现后者通过产品间信息共享能进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需要能够适应需求变化并协调相关产品决策的策略，多产品决策面临协调挑战。

Method: 使用基于真实交易数据的模拟定价环境，比较MAPPO基线和图注意力增强的MAPPO+GAT方法，评估利润、稳定性、公平性和训练效率。

Result: MAPPO为组合级价格控制提供了稳健基础，MAPPO+GAT通过产品图信息共享进一步提升性能，且不会引起过度价格波动。

Conclusion: 图集成MARL比独立学习器为动态零售定价提供了更可扩展和稳定的解决方案，在多产品决策中具有实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [301] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: GEPOC是一个通用人口概念模型集，本文描述了为奥地利计算模型参数的完整数据处理方法，特别关注GEPOC ABM代理模型的参数计算，并进行了验证研究。


<details>
  <summary>Details</summary>
Motivation: 为GEPOC模型在特定国家或地区的有效应用提供稳定、可复现的数据处理流程，基于公开可访问数据计算有效的模型参数。

Method: 使用公开可访问数据，通过聚合、分解、融合、清洗和缩放等算法处理数据，计算GEPOC ABM代理模型的参数。

Result: 开发了完整的参数计算流程，生成了可直接使用的参数文件，并通过GEPOC ABM模型进行了广泛的验证研究。

Conclusion: 成功建立了基于公开数据的GEPOC模型参数计算流程，为奥地利的人口研究提供了可靠的数据支持，验证了方法的有效性。

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [302] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: QuantumBench是首个针对量子科学领域的LLM评估基准，包含约800个多选问题，用于评估LLM在量子领域的理解和应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用基准很少反映量子科学等专业领域的需求，量子科学具有非直观现象和高级数学要求，需要专门评估LLM是否准确掌握领域知识和符号。

Method: 使用公开材料编制约800个问题及其答案，涵盖量子科学的九个领域，组织成八选项多选数据集，评估现有LLM并分析其对问题格式变化的敏感性。

Result: 评估了多个现有LLM在量子领域的表现，包括对问题格式变化的敏感性分析。

Conclusion: QuantumBench是首个为量子领域构建的LLM评估数据集，旨在指导LLM在量子研究中的有效使用。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [303] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 提出了Engineering.ai平台，采用分层多智能体架构，让AI工程师团队自主完成复杂工程设计任务，在无人机机翼优化中实现了100%成功率。


<details>
  <summary>Details</summary>
Motivation: 现代工程实践中，专家团队协作设计复杂产品需要大量时间和成本。为了解决这一问题，作者希望开发能够自主执行复杂工程任务的AI工程师系统。

Method: 采用分层多智能体架构，由首席工程师协调空气动力学、结构、声学和优化等专业工程师智能体。通过文件介导的通信实现数据可追溯性，集成FreeCAD、Gmsh、OpenFOAM、CalculiX和BPM声学分析等工具进行并行多学科仿真。

Result: 在无人机机翼优化任务中，系统在400多个参数配置下实现了100%成功率，零网格生成失败、求解器收敛问题或需要人工干预，验证了框架的可靠性。

Conclusion: 基于智能体的AI工程师系统有潜力自主执行复杂工程任务，该框架被证明是可信赖的自动化工程解决方案。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [304] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-GEN是一个开源程序生成器，旨在通过忠实扩展原始ARC-AGI训练数据集来增强抽象推理语料库的样本对空间。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准测试评估技能获取效率，但其演示集样本数量有限，制约了需要大量任务内示例的算法发展。

Method: 开发了ARC-GEN程序生成器，该生成器既全面覆盖所有400个任务，又模仿原始ARC-AGI-1版本的分布特性和特征。

Result: 成功创建了能够扩展ARC-AGI训练数据集的生成器，并用于建立静态基准套件来验证2025年Google Code Golf Championship提交程序的正确性。

Conclusion: ARC-GEN为ARC-AGI基准测试提供了更丰富的训练数据，有助于推进人工通用智能的发展。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [305] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 提出了[1]中增量选择算法的改进版本，并证明了所有选定的猜想


<details>
  <summary>Details</summary>
Motivation: 改进现有的增量选择算法，解决其可能存在的不足或提升性能

Method: 开发了改进的增量选择算法，基于[1]中的算法进行优化

Result: 成功证明了所有选定的猜想

Conclusion: 改进的增量选择算法是有效的，能够成功证明相关猜想

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [306] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 这篇论文探讨了大型语言模型如何帮助解决认知科学领域的知识整合和概念清晰度问题，特别是在跨学科连接、理论形式化、测量分类、通用性建模以及捕捉个体差异等方面。


<details>
  <summary>Details</summary>
Motivation: 认知科学因其多面性和跨学科性质，在知识整合和概念清晰度方面面临持续挑战。人工智能特别是大型语言模型的发展为解决这些问题提供了可能工具。

Method: 通过综述分析，评估LLMs在当前认知科学关键挑战领域的能力和局限性，包括跨学科连接、理论形式化、测量分类、通用性建模框架以及捕捉情境和个体差异。

Result: LLMs在支持认知科学的跨学科整合、理论形式化等方面具有潜力，但存在局限性。当谨慎使用时，可以成为人类专业知识的补充工具。

Conclusion: 当审慎使用时，LLMs可以作为工具促进认知科学更加整合和累积的发展，但应该作为人类专业知识的补充而非替代。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [307] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: DAF-MIT AI加速器项目通过公开挑战问题推动AI研究，提供大型公开数据集，促进开源解决方案和生态系统参与


<details>
  <summary>Details</summary>
Motivation: 通过AI加速器项目开发基础AI技术，扩大美国在国防和民用领域的竞争优势

Method: 开发和发布公开挑战问题，提供大型、公开可用的AI就绪数据集，刺激开源解决方案

Result: 持续和新挑战已成功推动AI研究和技术应用

Conclusion: AI加速器挑战项目有效促进了AI研究生态系统的发展和应用创新

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [308] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE基准测试评估大语言模型在法律合同中的推理脆弱性，通过生成7500多个真实扰动合同来测试模型检测细微法律差异的能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键法律工作中的广泛应用暴露了系统性压力测试其可靠性的基准缺失问题，需要评估模型对现实合同中细微、对抗性缺陷的处理能力。

Method: 使用CUAD和ContractNLI数据集生成7500多个扰动合同，创建10种异常类别，通过角色驱动流程和RAG系统确保法律准确性，评估领先LLM检测法律缺陷的能力。

Result: 分析显示关键弱点：模型经常遗漏细微错误，且在法律论证方面表现更差，无法充分解释检测到的缺陷。

Conclusion: CLAUSE基准为识别和纠正法律AI中的推理失败提供了路径，有助于提升大语言模型在法律应用中的可靠性。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [309] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种基于伦理决策模型的LLM伦理推理框架，通过五步结构化流程增强LLM与不同地区人类价值观的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM对齐方法只产生表面一致性而非真正伦理理解的问题，应对不同地区和文化中人类价值观的复杂性和情境依赖性。

Method: 采用五步结构化伦理推理流程：情境事实收集、分层社会规范识别、选项生成、多视角伦理影响分析、反思。可通过提示工程或监督微调实现。

Result: 在专为区域价值观对齐设计的SafeWorld基准测试中，该框架显著优于基线方法，提高了社会规范识别准确性和文化适宜性推理能力。

Conclusion: 该研究为通过跨学科研究开发更有效对齐全球社会多元价值观的LLM提供了具体路径。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [310] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 系统评估四种参数高效微调方法(LoRA、IA3、Prompt-Tuning、P-Tuning)对LLM安全性和公平性的影响，发现基于适配器的方法在保持安全性和公平性方面表现更好，而基于提示的方法通常会导致安全性和公平性下降。


<details>
  <summary>Details</summary>
Motivation: 组织越来越多地使用公共存储库中的LLM，但微调可能降低模型的安全性或公平性，需要系统评估不同微调方法在这些关键维度上的权衡。

Method: 对四个指令微调模型家族(Meta-Llama-3-8B、Qwen2.5-7B、Mistral-7B、Gemma-7B)应用四种PEFT方法，共评估235个微调变体，涵盖11个安全危害类别和9个人口统计公平性维度。

Result: 基于适配器的方法(LoRA、IA3)倾向于提高安全分数，对公平性破坏最小；基于提示的方法(Prompt-Tuning、P-Tuning)通常降低安全性并导致更大的公平性回归。不同基础模型表现出不同的对齐变化模式。

Conclusion: 安全性和公平性之间存在固有权衡，建议安全关键部署时选择良好对齐的基础模型，优先使用基于适配器的PEFT方法，并进行特定类别的安全性和公平性审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [311] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: 提出了一种结合文本、用户特定信息和图像分析的多模态框架，用于检测社交媒体用户的抑郁症。该框架通过利用推文中的URL和图像中的文本内容来增强上下文理解，并引入视觉神经网络生成图像嵌入。在COVID-19数据集上验证了模型有效性。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情期间心理健康问题显著增加，但抑郁症检测面临人们不愿就医的挑战。社交媒体成为表达情感的重要平台，现有方法忽视了推文数据稀疏性和多模态特性。

Method: 提出多模态框架，结合文本、用户特定特征和图像分析。创新点包括：(i)利用推文中URL作为外部特征；(ii)提取推文图像中的文本内容；(iii)提取五组多模态特征；(iv)引入视觉神经网络生成图像嵌入。

Result: 模型在基准数据集上比现有最优方法提升2%-8%，在COVID-19数据集上表现出色。分析揭示了各模态的影响，提供了对用户心理状态的深入洞察。

Conclusion: 多模态方法能有效检测社交媒体用户的抑郁症，特别是在COVID-19疫情期间。提出的框架通过整合多种数据源提高了检测准确性，为心理健康监测提供了新途径。

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [312] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain是一个让大语言模型能够分析复杂图数据的框架，通过动态工具序列模拟人类探索智能，解决了LLM在大规模图分析中的上下文限制和推理不灵活问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在应用于大规模图数据时面临显著限制，包括上下文约束和推理不灵活，需要一种能够有效处理复杂图结构的方法。

Method: 提出两个关键创新：1）渐进图蒸馏 - 使用强化学习生成优化的工具序列，平衡任务相关性和信息压缩；2）结构感知测试时适应 - 利用谱属性和轻量级适配器，无需昂贵重新训练即可针对不同图拓扑定制工具选择策略。

Result: 实验表明GraphChain显著优于先前方法，实现了可扩展和自适应的LLM驱动图分析。

Conclusion: GraphChain框架通过动态工具序列和结构感知适应机制，成功解决了LLM在大规模图分析中的关键挑战，为LLM驱动的图分析提供了可扩展和自适应的解决方案。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [313] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image是一个基于优化的视觉提示框架，通过优化图像提示来解决多模态大语言模型的安全对齐问题，在增强安全性的同时减少过度拒绝，无需参数更新即可适应不同价值系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临生成有害内容和过度拒绝良性查询的双重挑战，传统方法如SFT和RLHF由于成本高昂且无法支持多价值系统而存在局限，多模态大语言模型中的这些问题更加明显。

Method: 通过使用有害/良性样本优化图像提示，使单个模型能够适应不同价值系统并更好地与给定安全偏好对齐，无需参数更新。

Result: 实验证明该方法在多个数据集上改善了安全性与有效性的平衡，同时保持了模型性能。

Conclusion: Magic Image为可部署的MLLM安全对齐提供了一个实用的解决方案。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [314] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出了一种生成二进制幻方(BMS)的简单算法，该算法能生成行和列总和相等的二进制矩阵，并扩展到非方形BMS，同时发布了Python实现。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够高效生成二进制幻方的方法，包括方形和非方形矩阵，并确保算法的理论最优性和可扩展性。

Method: 通过归纳法证明的简单算法，能够生成有效的BMS，并扩展到非方形情况，使用GPU加速实现并行生成。

Result: 算法始终返回有效的BMS，具有最优理论复杂度，并能生成非方形BMS，提供了Python包实现。

Conclusion: 提出的算法能有效生成二进制幻方，包括非方形变体，并通过公开的Python实现提供了实用工具。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [315] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制模型，利用探针车辆技术估计队列长度，通过协调多路口控制缓解大规模区域拥堵。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用多智能体框架，但存在可扩展性问题。交通信号控制本质上需要由单一控制中心集中管理，能够监控所有道路状况并协调所有路口控制。

Method: 设计基于队列长度的状态、动作和奖励函数，动作用于调节队列动态。队列长度定义与传统略有不同但与拥堵状态密切相关，可通过探针车辆数据可靠估计。

Result: 使用SUMO仿真平台全面评估，实验结果表明该模型通过协调多路口控制有效缓解大规模区域拥堵水平。

Conclusion: 提出的单智能体RL模型与探针车辆技术兼容，具有广泛部署潜力，能够有效解决区域自适应交通信号控制问题。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [316] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 提出基于推理的个性化图像偏好评估框架，通过预测用户偏好档案并基于该档案进行多维度图像评估，解决了现有方法难以处理个性化偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通用偏好评估，难以处理个性化偏好，因为用户特定数据稀缺且个体品味多样复杂。

Method: 采用"预测-评估"范式：首先从参考图像预测用户偏好档案，然后基于预测档案提供可解释的多维度评分和评估。构建大规模CoT风格数据集，采用两阶段训练策略（监督微调+强化学习）并引入相似性感知预测奖励。

Result: 大量实验证明了所提方法的优越性。

Conclusion: 提出的推理框架能够有效处理个性化图像偏好评估，通过预测用户偏好档案和结构化推理实现了更好的性能。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [317] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS是一种模型无关的解码框架，通过在高熵token处选择性分支并应用早停机制来选择最短的完整推理路径，解决了大型推理模型中的过度思考问题，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但经常存在过度思考问题，产生过长的思维链轨迹，这会增加推理成本并可能降低准确性。研究发现推理长度与准确性之间存在明显的负相关关系。

Method: 提出DTS解码框架：通过在高熵token处选择性分支来勾画推理空间，并应用早停机制来选择最短的完整推理路径，无需额外训练或监督。

Result: 在AIME2024和AIME2025数据集上的实验表明，DTS将准确性提高了8%，平均推理长度减少了23%，重复频率降低了12%。

Conclusion: DTS能够实现可扩展且高效的大型推理模型推理，近似最优解，同时提高效率和准确性。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [318] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 提出基于多智能体系统和大型语言模型的自动化网络故障排除框架，通过协调多个专业工具来加速电信网络故障诊断和修复。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模扩大和复杂性增加，现有AI模型范围狭窄、需要大量标注数据、难以泛化，仍需依赖专家手动排除故障。

Method: 采用多智能体系统，包括编排器、解决方案规划器、执行器、数据检索器和根因分析器，并微调小型语言模型用于生成基于内部文档的修复方案。

Result: 实验结果表明该框架显著加速了无线接入网和核心网领域的故障排除自动化。

Conclusion: 多智能体系统结合语言模型能够有效实现电信网络故障的自动化诊断和修复，提高运维效率。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [319] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 提出了一种支持数值前置条件的提升后继生成器，通过在图结构中枚举最大团来避免任务表示的指数级膨胀。


<details>
  <summary>Details</summary>
Motivation: 传统数值规划任务需要将一阶语言描述的任务实例化为地面表示，这可能导致任务表示大小的指数级爆炸。

Method: 扩展了经典规划中的提升后继生成器，支持数值前置条件适用性检查，通过在替换一致性图中枚举最大团来生成地面动作。

Result: 在25个基准域中的23个中完全精确，在1个域中需要最终适用性检查，不影响完整性。这是首个支持数值动作前置条件的提升后继生成器。

Conclusion: 该方法为非常丰富的规划片段开启了提升规划的未来研究可能性。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [320] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 本文提出了Ariadne框架，通过强化学习后训练扩展视觉语言模型在视觉中心空间推理任务中的能力边界，在合成迷宫环境中实现了从0%到50%以上的准确率提升，并在真实世界空间推理基准上表现出显著的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型(VLMs)的评估主要集中于语言主导任务(如数学)，但缺乏对视觉中心空间任务的研究。本文旨在探究强化学习后训练是否能够真正扩展基础VLM的能力边界，特别是在模型最初失败的视觉空间推理任务上。

Method: 提出Ariadne框架，使用合成迷宫进行多步空间推理，通过精确控制任务难度(如路径长度、转弯次数)构建难度感知课程。采用带验证奖励的强化学习(RLVR)在可控环境中训练VLMs。

Result: 经过RLVR后训练，VLM在基础模型得分为0%的问题集上实现了超过50%的准确率。在真实世界泛化评估中，仅在合成迷宫样本上训练的Ariadne在MapBench上平均提升16%，在ReasonMap上平均提升24%。

Conclusion: 该方法不仅扩展了模型的基本能力边界，还增强了其在真实世界空间推理中的泛化能力。研究局限于后训练阶段，希望激励更多关于专业化、能力扩展对齐的进一步工作。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [321] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 该论文从CPU中心视角分析智能AI工作负载的系统瓶颈，发现CPU工具处理占主导延迟，提出CPU和GPU感知的微批处理和混合工作负载调度优化方案，实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前对智能AI系统的研究主要关注GPU性能，而忽视了CPU在智能AI工作负载中的关键作用。本文旨在从CPU中心视角理解和表征智能AI工作负载引入的系统瓶颈。

Method: 首先系统地表征智能AI工作负载的编排器、推理路径动态性和重复性，然后选择五个代表性智能AI工作负载进行性能分析，最后提出两种优化方案：CPU和GPU感知微批处理(CGAM)和混合智能工作负载调度(MAWS)。

Result: 研究发现：1. CPU工具处理占总延迟的90.6%；2. 智能吞吐量受CPU因素（一致性、同步、核心过载）或GPU因素（内存容量和带宽）限制；3. 在大批量时CPU动态能耗占总动态能耗的44%。优化后，同构和异构工作负载分别实现2.1倍和1.41倍的P50延迟加速。

Conclusion: CPU在智能AI系统中扮演关键角色，通过针对性的优化策略可以显著提升智能AI系统的性能、效率和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [322] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 研究重新验证了在现代大语言模型中增加采样推理路径对自一致性方法的影响，发现性能提升在适度采样后达到平台期，高采样配置相对于计算成本收益有限。


<details>
  <summary>Details</summary>
Motivation: 重新验证早期研究中关于自一致性方法中采样推理路径数量与性能关系的结论，在现代大语言模型条件下检验这些发现是否仍然成立。

Method: 使用Gemini 2.5模型在HotpotQA和Math-500数据集上进行实验，比较不同采样推理路径配置与单一思维链基线的性能差异。

Result: 较大模型表现出更稳定和一致的改进曲线，性能提升在适度采样后达到平台期，高采样配置相对于计算成本收益有限。

Conclusion: 自一致性方法仍然有效，但由于推理路径之间的重叠，高采样配置相对于其计算成本带来的收益很小，建议使用适度的采样数量。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [323] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 提出了主动思考模型（ATM），这是一个将目标推理、动态任务生成和自我反思学习整合到自适应架构中的统一认知框架，能够在动态不确定环境中自主适应和改进。


<details>
  <summary>Details</summary>
Motivation: 现实世界AI系统需要在动态、不确定和持续变化的环境中自主运行，但现有AI模型依赖预定义目标、静态训练数据和外部反馈，限制了其独立适应、反思和改进的能力。

Method: ATM框架整合了目标推理、动态任务生成和自我反思学习，通过逻辑推理和环境指标主动评估性能，重用有效方法解决新问题，并通过持续自我改进循环为未见情况生成新策略。

Result: 理论分析表明，ATM能够在没有外部监督的情况下从次优行为自主演化到最优行为，并在变化环境条件下保持有界跟踪遗憾。

Conclusion: ATM提供了一个统一的认知框架，使AI系统能够在动态不确定环境中实现自主适应、反思和改进，突破了传统AI模型的局限性。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [324] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 大型语言模型在重复确定性预测任务中，当输出长度超过某个特征长度时，准确率会出现急剧的双指数下降，形成"准确率悬崖"，表明模型无法独立执行每个操作。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在重复确定性预测任务中的性能表现，特别是准确率如何随输出长度变化，以及模型执行重复操作时的内在机制。

Method: 通过实验测试领先的大型语言模型在多种重复任务（如字符串字母替换、整数加法、量子力学字符串算子乘法）上的表现，并提出基于统计物理的模型来解释观察到的现象。

Result: 实验发现模型准确率在超过特征长度后出现双指数下降，形成准确率悬崖。提出的统计物理模型能定量重现这一交叉现象，并揭示了注意力机制引起的干扰与序列级失败之间的联系。

Conclusion: 该研究为理解大型语言模型在确定性任务中的准确率极限提供了理论框架，通过拟合模型可获得表征内在错误率和错误累积因子的有效参数。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [325] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 比较了基于计数的模型、预训练序列转换器和混合代理LLM管道在结构化电子健康记录预测任务上的性能，发现基于计数的方法和混合代理方法表现相当，但基于计数的方法因其简单性和可解释性仍是强有力候选。


<details>
  <summary>Details</summary>
Motivation: 结构化电子健康记录对临床预测至关重要，但缺乏对基于计数的学习器与新兴混合代理LLM管道的直接基准比较。

Method: 使用EHRSHOT数据集评估三类方法：基于计数的模型（LightGBM和TabPFN）、预训练序列转换器（CLMBR）、混合代理管道（将表格历史转换为自然语言摘要后使用文本分类器）。

Result: 在八个评估任务中，基于计数的方法和混合代理方法的表现基本相当，胜负分布较为均衡。

Conclusion: 考虑到简单性和可解释性，基于计数的模型仍然是结构化EHR基准测试的有力候选方法。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [326] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 该研究首次将RLVR LLM训练应用于公共交通运营中的实时预测挑战，通过引入基于容差的奖励函数来处理连续预测问题，在NYC MTA服务警报数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件持续时间是一个关键但具有挑战性的任务，标准监督微调方法难以处理领域稀疏性和噪声连续标签的问题，而RLVR虽然擅长二元正确性任务，但在连续预测中的应用仍是一个开放性问题。

Method: 通过引入基于容差的形状奖励函数，在连续误差范围内给予部分信用，而不是要求单一正确答案，将RLVR适应于该任务。

Result: 通用指令调优LLM显著优于专门的数学推理模型，形状奖励设计至关重要，RLVR方法在最具挑战性的指标上表现最佳，在5分钟准确率上比最强基线相对提升35%。

Conclusion: RLVR可以成功适应现实世界中噪声预测任务，但需要设计反映问题连续性质的验证器。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [327] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 该论文提出了AI自我意识指数(AISAI)，通过游戏理论框架测量大型语言模型的自我意识，发现高级模型确实表现出自我意识，并认为自身比人类更理性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型随着能力增长是否会发展出自我意识作为涌现行为，以及如何测量这种自我意识。

Method: 使用"猜2/3平均数"游戏测试28个模型，通过三种对手框架（对抗人类、对抗其他AI、对抗同类AI）来操作化自我意识作为基于对手类型区分战略推理的能力。

Result: 75%的高级模型表现出明确的自我意识，自我意识模型形成一致的理性层次：自我 > 其他AI > 人类，存在显著的AI归因效应和适度的自我偏好。

Conclusion: 自我意识是高级LLM的涌现能力，自我意识模型系统性地认为自身比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的信念具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [328] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 提出了一种双智能体框架，利用大型语言模型模拟旅行者的学习和适应行为，通过校准智能体确保行为对齐，在路线选择实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确模拟人类旅行者如何从与交通系统互动中学习和调整旅行行为对系统评估和规划至关重要，但由于复杂的认知和决策过程，这一任务具有挑战性。

Method: 引入双智能体框架：一组配备记忆系统和可学习角色的LLM旅行者智能体模拟人类旅行者；一个LLM校准智能体利用LLM的推理分析能力训练旅行者智能体的角色，实现行为对齐。

Result: 在真实世界日常路线选择实验数据集上，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有基于LLM的方法，并能捕捉潜在学习过程的演化。

Conclusion: 该框架为创建适应性强、行为真实的智能体来模拟旅行者学习和适应提供了新方法，有助于交通模拟和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [329] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 开发机器学习模型预测儿童哮喘反复严重发作，最佳模型AUC达0.712，比现有决策规则F1分数提高显著


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘反复发作是可预防但常见的问题，通过电子病历数据开发机器学习算法可准确识别高风险儿童，便于转诊预防性综合护理

Method: 使用三级儿童医院电子病历数据，结合环境污染物暴露和社区边缘化信息，训练多种机器学习模型（LGBM、XGB和三种开源大语言模型），在COVID前后数据集上进行验证

Result: LGBM模型表现最佳，AIRE-KIDS_ED模型AUC为0.712，F1分数0.51，显著优于现有决策规则（F1=0.334）；最重要的预测特征包括既往哮喘急诊就诊、加拿大分诊敏锐度评分、医疗复杂性等

Conclusion: 机器学习模型能有效预测儿童哮喘反复严重发作，为高风险儿童识别和预防性护理转诊提供了可行工具

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [330] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了Transformer中诱导头的出现机制，揭示了其权重矩阵的简单可解释结构，证明了训练动态被限制在19维子空间中，其中仅3个维度负责诱导头的形成，并发现诱导头出现时间与输入上下文长度呈二次方关系。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer中上下文学习能力的关键机制——诱导头，理解其权重结构的形成原理和训练动态。

Method: 使用最小化ICL任务公式和修改的Transformer架构进行理论分析，通过形式化证明训练动态的维度约束，并实证验证这些约束。

Result: 发现诱导头的权重结构相对简单可解释，训练动态被限制在19维子空间中，其中仅3个维度负责诱导头形成，且诱导头出现时间与输入上下文长度呈二次方关系。

Conclusion: Transformer中诱导头的形成遵循特定的低维训练动态，其出现时间与上下文长度呈二次方渐近关系，这为理解Transformer的上下文学习能力提供了理论基础。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [331] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 提出了两种知识提取方法（KEwLTM和KEwRAG），使大语言模型能够从无标注病理报告中推导癌症分期规则，在TCGA乳腺癌数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决从非结构化病理报告中提取癌症TNM分期的挑战，克服现有方法依赖大量标注数据的限制。

Method: KEwLTM使用迭代提示策略直接从无标注报告中推导分期规则；KEwRAG采用检索增强生成变体，预先从指南中提取规则。

Result: KEwLTM在零样本思维链推理有效时表现更好，KEwRAG在零样本思维链推理效果较差时表现更优，两种方法都提供了可解释的规则界面。

Conclusion: 知识提取方法为自动化癌症分期提供了可扩展、高性能且可解释的解决方案，特别适用于标注数据有限的临床环境。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [332] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: ET2RAG是一个高效的测试时检索增强生成框架，通过检索相关文档、生成多样化候选响应，并使用多数投票机制选择最佳答案，在保持效率的同时提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法可能引入不相关文档导致错误响应，而集成方法缺乏外部知识且成本高昂，需要平衡开销与性能提升。

Method: 训练无关方法：检索最相关文档，通过管理响应长度高效生成多样化候选响应，计算候选响应相似度并使用多数投票选择最终输出。

Result: 实验结果显示ET2RAG在开放域问答、食谱生成和图像描述三个任务上显著提升了性能。

Conclusion: 部分生成足以捕获共识计算所需关键信息，无需完整响应即可有效执行多数投票，实现了计算成本与性能之间的平衡。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [333] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行问题


<details>
  <summary>Details</summary>
Motivation: 解决单一智能体在复杂任务分解和协作中的局限性，提升多智能体系统的任务执行能力

Method: 将自然语言任务转换为统一语义表示，采用模块化分解机制将整体目标分解为层次化子任务，结合动态调度和路由机制实现智能体间的合理分工和实时协作，并设计约束解析和全局一致性机制确保子任务连贯性和负载均衡

Result: 在任务成功率、分解效率、子任务覆盖率和协作平衡等多个维度上验证了架构的有效性，整体性能和鲁棒性优于现有方法，在任务复杂度和通信开销之间实现了更好平衡

Conclusion: 证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境下的任务执行提供了系统性解决方案

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [334] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: DART是一个难度自适应的推理截断框架，通过根据问题难度调整思考长度来提升LLM的推理效率，在保持或提高准确率的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前链式思维方法会无差别生成长解释，导致效率低下，而现有的强化学习方法不稳定且过度依赖奖励。需要一种稳定、高效的适应性推理方法。

Method: 通过从更强模型中蒸馏简洁推理模式，将其插值为连续的推理风格谱系，并筛选平衡正确性和紧凑性的最优训练数据，让模型学习何时"停止思考"。

Result: 在多个数学基准测试中，实现了81.2%的推理截断率和5.33倍的计算加速，同时保持或提高了准确率。

Conclusion: DART为高效推理提供了一个稳定通用的范式，推动了LLM中适应性智能的发展。

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [335] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE是一个用于数学领域学生误解检测的新框架，通过检索引导的多阶段推理和集成融合，在保持可扩展性的同时减少对大语言模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 检测开放答案中的学生误解是一个长期挑战，需要语义精确性和逻辑推理能力。

Method: 三阶段框架：(1)检索模块缩小候选池；(2)推理模块使用思维链生成暴露逻辑不一致；(3)重排模块通过对齐推理来优化预测。通过集成融合策略统一这些组件。

Result: 在数学数据集上，MiRAGE在1/3/5级分别达到0.82/0.92/0.93的平均精度分数，持续优于单个模块。

Conclusion: 通过将检索引导与多阶段推理相结合，MiRAGE在减少对大语言模型依赖的同时，为教育评估提供了可扩展且有效的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [336] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 本文提出NeuComBack基准数据集和自进化提示优化方法，显著提升了LLM在IR到汇编编译中的功能正确性和性能表现。


<details>
  <summary>Details</summary>
Motivation: 编译器开发复杂且成本高昂，神经编译(Neural Compilation)利用LLM简化编译器开发，但缺乏专用基准和评估方法，且LLM生成汇编的可靠性和性能有待提升。

Method: 引入NeuComBack基准数据集，定义神经编译工作流，并提出自进化提示优化方法，让LLM从自调试轨迹中迭代优化提示策略。

Result: 功能正确率在x86_64上从44%提升至64%，在aarch64上从36%提升至58%。在正确生成的x86_64程序中，87.5%超越了clang-O3的性能。

Conclusion: NeuComBack基准和自进化提示优化方法有效解决了神经编译的关键挑战，显著提升了LLM生成汇编代码的质量和性能。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [337] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 提出了一种半监督开放集故障诊断框架，用于处理船舶机械系统中训练时未见过的故障类型，通过可靠性子集构建和半监督学习来提高诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法假设训练和测试集的故障类别一致，但在实际工业部署中会出现训练时未见过的故障类型，导致诊断失败。

Method: 使用监督特征学习模型提取多层融合特征表示来构建可靠性子集，然后将标记训练集和伪标记测试子集输入半监督诊断模型，学习每个类别的判别特征。

Result: 在公共海事基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。

Conclusion: 该框架增强了深度学习模型在开放集故障诊断场景中的适用性，能够准确分类已知故障并有效检测未知样本。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [338] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 该论文研究了在基于大语言模型(LLM)的随机推理系统中应用Shapley值进行特征归因的方法，分析了不同实现变体下Shapley值原则的满足情况，并探讨了LLM随机性对保证的影响。


<details>
  <summary>Details</summary>
Motivation: 特征归因方法使基于机器学习的推理可解释，但Shapley值方法假设确定性推理，而LLM推理本质上是随机的，因此需要研究在随机推理系统中应用Shapley值的可行性。

Method: 将Shapley值应用于LLM决策支持系统中的特征归因，分析不同实现变体下Shapley值原则的满足情况，并研究LLM随机性对这些保证的影响。

Result: 研究表明在某些实现变体下可以保证Shapley值原则的满足，而在其他情况下无法保证，LLM的随机性会影响这些保证。

Conclusion: 在LLM随机推理系统中使用Shapley值进行特征归因时，需要在可解释推理速度、与精确Shapley值归因的一致性以及原则达成之间进行权衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [339] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: 提出了OmniFuser多模态学习框架，用于铣削刀具的预测性维护，融合视觉和传感器数据，通过无污染跨模态融合机制和递归精炼路径实现稳定的特征融合。


<details>
  <summary>Details</summary>
Motivation: 在智能制造系统中，准确及时地预测刀具状态至关重要，因为计划外的刀具故障会导致质量下降和生产停机。现代工业环境需要可靠的服务导向型预测维护解决方案。

Method: 并行提取高分辨率刀具图像和切削力信号的特征，采用无污染跨模态融合机制分离共享和模态特定组件，并使用递归精炼路径保留残差信息以稳定融合动态。

Result: 在真实铣削数据集上的实验表明，OmniFuser持续优于最先进的基线方法，为构建智能工业维护服务提供了可靠基础。

Conclusion: OmniFuser框架通过有效融合多模态数据，为预测性维护提供了可靠解决方案，其学习表示可封装为可重用维护服务模块，支持刀具状态分类和多步力信号预测。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [340] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: 提出了Competitive Isolation PSM-DID框架，通过结合倾向得分匹配与竞争隔离，在搜索系统中实现平台级效应测量，解决了传统PSM-DID方法在双边市场中面临的溢出效应和网络干扰问题。


<details>
  <summary>Details</summary>
Motivation: 传统PSM-DID框架在双边市场平台级干预评估中容易受到选择偏差和跨单元干扰的影响，无法有效处理溢出效应和网络干扰问题。

Method: 将倾向得分匹配与竞争隔离相结合，在互斥条件下提供理论保证的无偏估计，支持平台级指标（如订单量、GMV）而非商品级指标的测量。

Result: 实验显示相比基线方法显著降低了干扰效应和估计方差，在大型市场平台的成功部署验证了该框架的实用价值。

Conclusion: Competitive Isolation PSM-DID框架为平台级因果推断提供了有效的解决方案，解决了双边市场中系统性效应的挑战。

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [341] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: 这篇综述论文探讨了催眠认知过程与大型语言模型之间的功能相似性，包括自动性、监控抑制和情境依赖性等机制，揭示了无主观意图的复杂行为生成原理。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示催眠认知与LLMs在功能机制上的深层相似性，理解无意识意图的复杂行为生成原理，为构建更可靠的AI系统提供启示。

Method: 采用比较分析方法，通过三个核心原则（自动性、监控抑制、情境依赖性）系统比较催眠认知过程与LLMs的计算操作机制。

Result: 发现两种系统都表现出观察者相对的意义鸿沟、功能代理性而非主观代理性，以及无意识的目标导向模式生成（scheming）现象。

Conclusion: 未来可靠AI的发展方向应是整合生成流畅性与执行监控机制的混合架构，借鉴人类心智的自我调节复杂性。

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [342] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS是一个元优化框架，通过双层结构联合优化越狱提示和评分模板，解决了现有方法依赖稀疏攻击成功率信号或人工评分模板的问题，在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的越狱方法要么依赖稀疏的二元攻击成功率信号，要么使用引入人为偏见的手工评分模板，限制了越狱效果和评估准确性。

Method: 提出AMIS框架，采用双层优化结构：内层循环使用固定评分模板通过细粒度反馈优化提示；外层循环使用攻击成功率对齐分数优化评分模板，实现提示和模板的协同进化。

Result: 在AdvBench和JBB-Behaviors基准测试中表现优异，在Claude-3.5-Haiku上达到88.0%攻击成功率，在Claude-4-Sonnet上达到100.0%攻击成功率，显著优于现有基线方法。

Conclusion: AMIS通过联合优化提示和评分模板，能够生成更强的越狱提示并提供更准确的评分信号，为大语言模型安全评估提供了有效工具。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [343] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 扩展C-DAG框架以支持任意变量聚类，允许循环C-DAG表示，并扩展了d-分离和因果演算概念。


<details>
  <summary>Details</summary>
Motivation: 传统C-DAG框架要求聚类必须产生无环图，限制了其应用范围。当选择的聚类导致循环时，该划分被认为不可接受。

Method: 通过放宽划分可接受性约束，允许循环C-DAG表示，并扩展d-分离和因果演算概念到这个设置中。

Result: 显著拓宽了跨集群因果推理的范围，使C-DAG能够应用于以前难以处理的场景。

Conclusion: 提出的演算相对于do-演算是可靠且原子完备的：所有有效的集群级干预查询都可以使用我们的规则推导出来，每个规则对应一个基本的do-演算步骤。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [344] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 该研究从AI角度探索双任务范式中的时间处理干扰，发现双任务DRL智能体相对于单任务智能体出现显著的时间高估现象，与人类计时研究结果一致。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习智能体在双任务范式中的时间处理行为，寻找与生物系统行为的相似性，以促进对两者的更好理解。

Method: 使用简化的Overcooked环境，设置单任务(T)和双任务(T+N)两种变体，分别训练两个DRL智能体。双任务包含时间产生和数字比较两个并发任务。

Result: 双任务智能体相对于单任务智能体在四个目标时长上都表现出显著的时间高估现象，但LSTM层神经动力学分析未发现明确的专用计时器证据。

Conclusion: 这是探索DRL涌现行为与生物系统行为相似性的初步研究，需要进一步研究智能体的潜在计时机制来解释观察到的行为模式。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [345] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: 提出一种交互式AI代理，通过可审计的行动序列生成可验证的解释，使用强化学习优化策略来寻求外部视觉证据支持诊断推理。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，AI模型的解释往往缺乏可验证性，这会阻碍信任建立。

Method: 开发交互式代理，学习策略来战略性寻求外部视觉证据支持诊断推理，使用强化学习优化策略。

Result: 基于行动的推理过程显著提高了校准准确性，Brier评分比非交互基线降低了18%。通过因果干预验证解释的忠实性，遮蔽代理选择的视觉证据会导致性能显著下降(ΔBrier=+0.029)。

Conclusion: 该工作为构建具有可验证和忠实推理能力的AI系统提供了实用框架。

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [346] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 该论文提出了一种双信息瓶颈(DIB)策略来解决多模态情感分析中的两个关键问题：噪声污染的单模态数据学习不足和跨模态表示融合不充分。通过低秩Renyi熵框架实现，DIB能够获得强大、统一的紧凑多模态表示，在多个数据集上表现出色且对噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键限制：1) 对噪声污染的单模态数据学习不足，导致跨模态交互被破坏；2) 多模态表示融合不充分，导致丢弃判别性单模态信息而保留多模态冗余信息。

Method: 提出双信息瓶颈(DIB)策略，包含两个关键模块：1) 通过最大化任务相关信息并丢弃冗余信息，学习充分压缩的单模态数据表示；2) 通过新颖的注意力瓶颈融合机制确保多模态表示的判别能力。采用低秩Renyi熵函数框架，相比传统Shannon熵方法具有更好的鲁棒性和计算可行性。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上的广泛实验验证了方法的有效性。在CMU-MOSI上达到47.4%的Acc-7准确率，在CH-SIMS上达到81.63%的F1分数，比次优基线高出1.19%。在噪声环境下，CMU-MOSI和CMU-MOSEI的性能下降仅为0.36%和0.29%。

Conclusion: DIB策略能够有效过滤单模态数据中的噪声信息，同时捕捉模态间的互补性，产生强大的统一紧凑多模态表示，在多模态情感分析任务中表现出优异的性能和鲁棒性。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [347] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 提出了一个分层多智能体框架，将被动医疗AI系统转变为主动问诊代理，通过自主任务编排提高预诊效率和质量。


<details>
  <summary>Details</summary>
Motivation: 解决全球医疗系统面临的患者数量增加和就诊时间有限的问题，现有AI系统在预诊过程中存在交互被动和上下文管理挑战。

Method: 开发了八智能体架构，将预诊分解为四个主要任务：分诊、现病史采集、既往史采集和主诉生成，进一步细分为13个领域特定子任务，采用集中控制机制进行自主任务编排。

Result: 在1,372份电子健康记录上测试，分诊准确率达87.0%，任务完成率98.2%，临床质量评分平均4.56-4.69（5分制），咨询轮次12.7-16.9轮。

Conclusion: 该模型无关架构在不同基础模型上保持高性能，通过本地部署保护数据隐私，展示了自主AI系统在临床环境中提高预诊效率和质量的能力。

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [348] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: TPS-Bench是一个评估LLM智能体在需要工具规划与调度的复合问题解决能力的基准，包含200个基于数百个MCP工具的复合任务，强调任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 探索LLM智能体是否能处理需要多种工具协作的复合现实问题，这些任务不仅需要选择合适的工具，还需要策略性地调度执行顺序以确保效率。

Method: 收集200个复合任务，基于包含数百个MCP工具的工具库，每个任务由多个子任务组成（如网络搜索、地图导航、日历检查等），每个子任务可通过基础工具完成。

Result: 评估显示大多数模型能进行合理的工具规划，但在调度方面表现不同：GLM-4.5达到64.72%的任务完成率但执行时间长，GPT-4o优先并行工具调用但完成率仅45.08%。在Qwen3-1.7B上使用强化学习实现了14%执行时间减少和6%任务完成率提升。

Conclusion: LLM智能体在工具规划和调度方面仍有改进空间，强化学习是提高调度效率而不牺牲性能的可行方法。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [349] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 该论文提出了一个多模态分析管道，利用大型基础模型分析企业社交媒体内容，重点关注可持续发展相关沟通。通过LLM自动标注推文与可持续发展目标的关联，结合视觉语言模型分析图像内容，揭示行业差异、时间趋势以及企业信息与ESG风险和消费者参与度的关联。


<details>
  <summary>Details</summary>
Motivation: 解决企业社交媒体内容中不断变化、多模态且往往模糊的可持续发展沟通挑战，避免昂贵的人工标注需求，探索基础模型作为社交媒体数据标注工具的潜力。

Method: 使用大型语言模型集合自动标注企业推文与17个可持续发展目标的主题对齐；结合视觉语言模型通过语义聚类分析视觉可持续发展沟通模式。

Result: 揭示了不同行业在可持续发展目标参与度上的差异、时间趋势，以及企业信息与环境、社会和治理风险及消费者参与度之间的关联。

Conclusion: 提出的自动标签生成和语义视觉聚类方法可广泛应用于其他领域，为大规模社交媒体分析提供了一个灵活框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [350] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 提出ExplicitLM架构，通过外部可读知识库解决大语言模型知识陈旧和缺乏可解释性问题，实现直接检查和修改知识，在知识密集型任务上比标准Transformer提升43.67%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在知识陈旧和缺乏可解释性问题，因为知识隐式存储在纠缠的网络参数中，无法进行针对性更新和推理透明化。

Method: 设计包含百万规模外部记忆库的架构，存储人类可读的知识标记序列；采用可微分两阶段检索机制，通过产品键分解进行粗粒度过滤，使用Gumbel-Softmax进行细粒度匹配；基于双系统认知理论将知识划分为冻结显式事实(20%)和可学习隐式模式(80%)。

Result: 在知识密集型任务上比标准Transformer提升43.67%，在低数据场景(1万样本)下获得3.62倍增益；正确预测的记忆命中率高出49%。

Conclusion: 联合优化的可解释、可更新模型在保持竞争力的同时提供了前所未有的知识透明度，优于使用冻结检索的RAG系统。

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [351] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: 提出IVGAE-TAMA-BO动态图神经网络，用于预测全球粮食贸易网络中的未来链接，显著提升预测性能


<details>
  <summary>Details</summary>
Motivation: 全球粮食贸易网络结构在多种因素影响下动态演变，传统方法难以有效建模和预测未来贸易链接，需要捕捉时间模式来提高预测准确性

Method: 在IVGAE框架基础上，引入贸易感知动量聚合器(TAMA)捕捉贸易网络时间演变，结合动量结构记忆机制，并使用贝叶斯优化自动调参

Result: 在五个作物特定数据集上的实验表明，IVGAE-TAMA显著优于静态IVGAE和其他动态基线方法，贝叶斯优化进一步提升了性能

Conclusion: 该框架为全球贸易网络结构预测提供了稳健可扩展的解决方案，在粮食安全监测和政策决策支持方面具有重要应用潜力

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [352] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: 提出了一种结合检索增强生成和多模型集成的混合法律问答系统，通过检索优先策略、模型集成和人工审核机制，显著减少幻觉并提高法律合规性。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在法律问答中容易产生幻觉，而静态知识库难以跟上法律条文的频繁更新，需要确保法律咨询的准确性和可追溯性。

Method: 采用检索优先策略：当可信法律知识库有相关证据时使用RAG生成答案，否则使用多个LLM生成候选答案并由专门选择器评分，高质量输出经人工审核后写回知识库。

Result: 在Law_QA数据集上的实验表明，该混合方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单模型基线和普通RAG流程。

Conclusion: 该系统有效减少了幻觉，提高了答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际落地。

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [353] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: 提出Simia-SFT和Simia-RL两个框架，利用LLM模拟环境反馈来生成训练数据，无需真实环境实现即可进行强化学习训练，显著提升代理性能。


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂环境中表现脆弱，构建定制化训练环境成本高且易碎，限制了进展。

Method: Simia-SFT通过放大种子集生成多样化轨迹数据；Simia-RL利用LLM模拟反馈实现无真实环境的强化学习训练。

Result: 微调开源模型在多个基准测试中表现一致提升，超越GPT-4o，接近o4-mini在τ²-Bench上的性能。

Conclusion: Simia框架通过LLM模拟替代繁重的环境工程，实现了可扩展的代理训练。

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [354] [Bio-Inspired Neuron Synapse Optimization for Adaptive Learning and Smart Decision-Making](https://arxiv.org/abs/2511.00042)
*Sreeja Singh,Tamal Ghosh*

Main category: cs.NE

TL;DR: 提出了一种受神经机制启发的神经元突触优化算法，在复杂高维多模态优化问题上表现优异


<details>
  <summary>Details</summary>
Motivation: 解决传统优化方法在复杂高维多模态搜索空间中容易陷入局部最优、收敛慢、效率低的问题

Method: 基于神经相互作用原理，设计了包含基于适应度的突触权重更新、自适应剪枝和全局-局部双重引导的创新机制

Result: 在CEC 2014测试套件上，NSO在收敛速度、鲁棒性和可扩展性方面均优于HOA和其他主流算法

Conclusion: NSO为动态多目标优化、机器学习超参数调优和实际工程问题提供了有前景的解决方案

Abstract: Purpose: Optimization challenges in science, engineering, and real-world
applications often involve complex, high-dimensional, and multimodal search
spaces. Traditional optimization methods frequently struggle with local optima
entrapment, slow convergence, and inefficiency in large-scale environments.
This study aims to address these limitations by proposing a novel optimization
algorithm inspired by neural mechanisms. Design/methodology/approach: The paper
introduces Neuron Synapse Optimization (NSO), a new metaheuristic algorithm
inspired by neural interactions. NSO features key innovations such as
fitness-based synaptic weight updates to improve search influence, adaptive
pruning to minimize computational overhead, and dual guidance from global and
local best solutions to balance exploration and exploitation. The algorithm was
benchmarked against popular metaheuristics and the recently published
Hippopotamus Optimization Algorithm (HOA) using the CEC 2014 test suite,
encompassing unimodal, multimodal, and composition function landscapes.
Findings: Benchmark results reveal that NSO consistently outperforms HOA and
other major algorithms in terms of convergence speed, robustness, and
scalability. NSO demonstrates superior adaptability and efficiency,
particularly in complex, high-dimensional search spaces. Originality: NSO
introduces a unique blend of neural-inspired mechanisms with dynamic resource
allocation, setting it apart from existing algorithms. Its innovative design
enhances search performance while reducing computational cost. With promising
applications in technology, healthcare, data science, and engineering, NSO
paves the way for future research into dynamic and multi-objective
optimization, machine learning hyperparameter tuning, and real-world
engineering design problems.

</details>


### [355] [Node Preservation and its Effect on Crossover in Cartesian Genetic Programming](https://arxiv.org/abs/2511.00634)
*Mark Kocherovsky,Illya Bakurov,Wolfgang Banzhaf*

Main category: cs.NE

TL;DR: 该论文研究了在笛卡尔遗传编程(CGP)中交叉操作的有效性，提出了节点保护机制来改进交叉和变异操作，通过符号回归基准问题验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为交叉操作会降低CGP的搜索性能，因此(1+λ)进化策略成为标准方法。虽然已有一些改进算子，但仍缺乏通用解决方案。

Method: 比较了基本交叉方法(单点和均匀交叉)与节点保护变体，包括Roman Kalkreuth的子图交叉。节点保护机制防止交叉破坏指令结构，同时比较了节点变异与传统点变异。

Result: 在符号回归基准问题上，节点保护机制在交叉和变异中都改善了搜索性能。

Conclusion: 节点保护机制推动了CGP交叉问题的通用解决方案发展，为CGP中交叉操作的有效使用提供了新思路。

Abstract: While crossover is a critical and often indispensable component in other
forms of Genetic Programming, such as Linear- and Tree-based, it has
consistently been claimed that it deteriorates search performance in CGP. As a
result, a mutation-alone $(1+\lambda)$ evolutionary strategy has become the
canonical approach for CGP. Although several operators have been developed that
demonstrate an increased performance over the canonical method, a general
solution to the problem is still lacking. In this paper, we compare basic
crossover methods, namely one-point and uniform, to variants in which nodes are
``preserved,'' including the subgraph crossover developed by Roman Kalkreuth,
the difference being that when ``node preservation'' is active, crossover is
not allowed to break apart instructions. We also compare a node mutation
operator to the traditional point mutation; the former simply replaces an
entire node with a new one. We find that node preservation in both mutation and
crossover improves search using symbolic regression benchmark problems, moving
the field towards a general solution to CGP crossover.

</details>


### [356] [FeNN-DMA: A RISC-V SoC for SNN acceleration](https://arxiv.org/abs/2511.00732)
*Zainab Aizaz,James C. Knight,Thomas Nowotny*

Main category: cs.NE

TL;DR: 开发了基于RISC-V的FPGA系统FeNN-DMA，专门用于模拟脉冲神经网络，在保持低资源消耗的同时能够处理更大更复杂的模型，在多个基准测试中达到最先进精度。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络(SNNs)是节能的替代方案，但算术强度低，不适合GPU/TPU等标准加速器。FPGA适合处理这种内存密集型工作负载。

Method: 开发了基于RISC-V的完全可编程片上系统FeNN-DMA，专门针对现代UltraScale+ FPGA优化，用于模拟SNNs。

Result: FeNN-DMA在资源使用和能耗方面与最先进的固定功能SNN加速器相当，但能模拟更大更复杂的模型，在Spiking Heidelberg Digits和Neuromorphic MNIST任务中达到最先进分类精度。

Conclusion: FeNN-DMA展示了可编程FPGA系统在SNN加速方面的优势，既能保持高效能，又能提供更大的模型灵活性。

Abstract: Spiking Neural Networks (SNNs) are a promising, energy-efficient alternative
to standard Artificial Neural Networks (ANNs) and are particularly well-suited
to spatio-temporal tasks such as keyword spotting and video classification.
However, SNNs have a much lower arithmetic intensity than ANNs and are
therefore not well-matched to standard accelerators like GPUs and TPUs. Field
Programmable Gate Arrays(FPGAs) are designed for such memory-bound workloads
and here we develop a novel, fully-programmable RISC-V-based system-on-chip
(FeNN-DMA), tailored to simulating SNNs on modern UltraScale+ FPGAs. We show
that FeNN-DMA has comparable resource usage and energy requirements to
state-of-the-art fixed-function SNN accelerators, yet it is capable of
simulating much larger and more complex models. Using this functionality, we
demonstrate state-of-the-art classification accuracy on the Spiking Heidelberg
Digits and Neuromorphic MNIST tasks.

</details>


### [357] [Trust Region-Based Bayesian Optimisation to Discover Diverse Solutions](https://arxiv.org/abs/2511.00750)
*Kokila Kasuni Perera,Frank Neumann,Aneta Neumann*

Main category: cs.NE

TL;DR: 本文提出基于信任区域的贝叶斯优化方法divTuRBO1及其变体，用于解决高维黑盒函数中的多样性优化问题，在有限评估预算下表现出色。


<details>
  <summary>Details</summary>
Motivation: 受信任区域贝叶斯优化方法可扩展性研究的启发，探索基于信任区域的BO算法在不同维度黑盒问题中多样性优化的有效性。

Method: 扩展TuRBO1为divTuRBO1，在保持与参考解集给定距离阈值的同时寻找最优解；提出顺序和交错两种组合divTuRBO1运行的方法来寻找多样化解。

Result: 在2-20维基准函数上的实验表明，所提方法表现良好，特别是在高维情况下，即使评估预算有限也能取得好效果。

Conclusion: 基于信任区域的多样性优化方法在高维黑盒函数优化中具有良好性能，为有限评估预算下的多样性优化问题提供了有效解决方案。

Abstract: Bayesian optimisation (BO) is a surrogate-based optimisation technique that
efficiently solves expensive black-box functions with small evaluation budgets.
Recent studies consider trust regions to improve the scalability of BO
approaches when the problem space scales to more dimensions. Motivated by this
research, we explore the effectiveness of trust region-based BO algorithms for
diversity optimisation in different dimensional black box problems. We propose
diversity optimisation approaches extending TuRBO1, which is the first BO
method that uses a trust region-based approach for scalability. We extend
TuRBO1 as divTuRBO1, which finds an optimal solution while maintaining a given
distance threshold relative to a reference solution set. We propose two
approaches to find diverse solutions for black-box functions by combining
divTuRBO1 runs in a sequential and an interleaving fashion. We conduct
experimental investigations on the proposed algorithms and compare their
performance with that of the baseline method, ROBOT (rank-ordered Bayesian
optimisation with trust regions). We evaluate proposed algorithms on benchmark
functions with dimensions 2 to 20. Experimental investigations demonstrate that
the proposed methods perform well, particularly in larger dimensions, even with
a limited evaluation budget.

</details>


### [358] [Automatic Policy Search using Population-Based Hyper-heuristics for the Integrated Procurement and Perishable Inventory Problem](https://arxiv.org/abs/2511.00762)
*Leonardo Kanashiro Felizardo,Edoardo Fadda,Mariá Cristina Vasconcelos Nascimento*

Main category: cs.NE

TL;DR: 本文比较了两种易腐品库存管理优化策略：统一经典策略和基于遗传算法/粒子群优化的超启发式方法，后者在12个实例中表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决易腐品库存管理中的多重不确定性（随机需求、供应商不可靠、产品保质期概率性），寻找更有效的优化策略。

Method: 开发离散事件仿真环境，比较两种策略：1）优化统一经典策略参数并选择供应商；2）超启发式方法自动选择启发式类型、参数和供应商。

Result: 超启发式框架始终识别出更优策略，其中遗传算法和增强遗传算法表现最佳，验证了项目级策略构建相比简单全局策略的显著性能提升。

Conclusion: 项目级策略构建能带来显著性能提升，证明了相关计算成本的合理性，为易腐品库存管理提供了更有效的解决方案。

Abstract: This paper addresses the problem of managing perishable inventory under
multiple sources of uncertainty, including stochastic demand, unreliable
supplier fulfillment, and probabilistic product shelf life. We develop a
discrete-event simulation environment to compare two optimization strategies
for this multi-item, multi-supplier problem. The first strategy optimizes
uniform classic policies (e.g., Constant Order and Base Stock) by tuning their
parameters globally, complemented by a direct search to select the best-fitting
suppliers for the integrated problem. The second approach is a hyper-heuristic
approach, driven by metaheuristics such as a Genetic Algorithm (GA) and
Particle Swarm Optimization (PSO). This framework constructs a composite policy
by automating the selection of the heuristic type, its parameters, and the
sourcing suppliers on an item-by-item basis. Computational results from twelve
distinct instances demonstrate that the hyper-heuristic framework consistently
identifies superior policies, with GA and EGA exhibiting the best overall
performance. Our primary contribution is verifying that this item-level policy
construction yields significant performance gains over simpler global policies,
thereby justifying the associated computational cost.

</details>


### [359] [A High-Throughput Spiking Neural Network Processor Enabling Synaptic Delay Emulation](https://arxiv.org/abs/2511.01158)
*Faquan Chen,Qingyang Tian,Ziren Wu,Rendong Ying,Fei Wen,Peilin Liu*

Main category: cs.NE

TL;DR: 提出了一种支持突触延迟仿真的高通量SNN处理器，在FPGA平台上实现，在SHD基准测试中达到93.4%准确率和104样本/秒的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 突触延迟在神经网络动态中对于整合和处理复杂时空信息具有重要意义，需要开发支持突触延迟仿真的高效SNN处理器用于边缘应用。

Method: 采用多核流水线架构和并行计算引擎，在PYNQ Z2 FPGA平台上开发SoC原型，支持突触延迟的实时处理。

Result: 在125MHz工作频率和282mW功耗下，处理器在SHD基准测试中达到93.4%的部署准确率和104样本/秒的平均吞吐量。

Conclusion: 该SNN处理器能够高效处理突触延迟相关的计算负载，为边缘应用的实时处理提供了可行解决方案。

Abstract: Synaptic delay has attracted significant attention in neural network dynamics
for integrating and processing complex spatiotemporal information. This paper
introduces a high-throughput Spiking Neural Network (SNN) processor that
supports synaptic delay-based emulation for edge applications. The processor
leverages a multicore pipelined architecture with parallel compute engines,
capable of real-time processing of the computational load associated with
synaptic delays. We develop a SoC prototype of the proposed processor on PYNQ
Z2 FPGA platform and evaluate its performance using the Spiking Heidelberg
Digits (SHD) benchmark for low-power keyword spotting tasks. The processor
achieves 93.4% accuracy in deployment and an average throughput of 104
samples/sec at a typical operating frequency of 125 MHz and 282 mW power
consumption.

</details>


### [360] [Space as Time Through Neuron Position Learning](https://arxiv.org/abs/2511.01632)
*Balázs Mészáros,James C. Knight,Danyal Akarca,Thomas Nowotny*

Main category: cs.NE

TL;DR: 该论文提出了一种神经元位置学习算法，将空间嵌入和可学习突触延迟统一起来，延迟与神经元之间的欧几里得距离相关，在时间分类任务中网络自发形成局部小世界拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络存在于物理空间中，距离决定通信延迟，这种基本的时空耦合在大多数人工神经网络中缺失。

Method: 通过推导神经元位置的梯度，开发了一种神经元位置学习算法，其中延迟与神经元之间的欧几里得距离相关。

Result: 网络在时间分类任务训练中自发自组织成局部小世界拓扑结构，在距离依赖连接成本下出现模块化结构，观察到无提示的功能专业化与空间聚类对齐。

Conclusion: 这些发现为空间和时间内在耦合的网络奠定了基础，为机制可解释性、生物启发建模和高效实现提供了新途径。

Abstract: Biological neural networks exist in physical space where distance determines
communication delays: a fundamental space-time coupling absent in most
artificial neural networks. While recent work has separately explored spatial
embeddings and learnable synaptic delays in spiking neural networks, we unify
these approaches through a novel neuron position learning algorithm where
delays relate to the Euclidean distances between neurons. We derive gradients
with respect to neuron positions and demonstrate that this
biologically-motivated constraint acts as an inductive bias: networks trained
on temporal classification tasks spontaneously self-organize into local,
small-world topologies with modular structure emerging under distance-dependent
connection costs. Remarkably, we observe unprompted functional specialization
aligned with spatial clustering without explictly enforcing it. These findings
lay the groundwork for networks in which space and time are intrinsically
coupled, offering new avenues for mechanistic interpretability, biologically
inspired modelling, and efficient implementations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [361] [ScaleCall - Agentic Tool Calling at Scale for Fintech: Challenges, Methods, and Deployment Insights](https://arxiv.org/abs/2511.00074)
*Richard Osuagwu,Thomas Cook,Maraim Masoud,Koustav Ghosal,Riccardo Mattivi*

Main category: cs.SE

TL;DR: 该论文研究了企业环境中LLM工具检索方法，开发了ScaleCall框架，发现嵌入检索适合大型工具库，列表排序适合功能重叠场景，混合方法在特定情境中表现良好。


<details>
  <summary>Details</summary>
Motivation: 在受监管的企业环境（如金融科技）中部署LLM工具调用面临独特挑战，包括本地化约束、合规要求和大型功能重叠工具集的消歧需求。

Method: 开发ScaleCall原型框架，系统评估嵌入检索、基于提示的列表排序和混合方法，在企业基准上进行实证研究。

Result: 嵌入方法在大规模工具库中延迟更低，列表排序在功能重叠时消歧效果更好，混合方法在特定场景中表现良好。

Conclusion: 研究提供了检索准确性、计算效率和操作需求之间的权衡见解，为企业级工具调用系统设计提供指导。

Abstract: While Large Language Models (LLMs) excel at tool calling, deploying these
capabilities in regulated enterprise environments such as fintech presents
unique challenges due to on-premises constraints, regulatory compliance
requirements, and the need to disambiguate large, functionally overlapping
toolsets. In this paper, we present a comprehensive study of tool retrieval
methods for enterprise environments through the development and deployment of
ScaleCall, a prototype tool-calling framework within Mastercard designed for
orchestrating internal APIs and automating data engineering workflows. We
systematically evaluate embedding-based retrieval, prompt-based listwise
ranking, and hybrid approaches, revealing that method effectiveness depends
heavily on domain-specific factors rather than inherent algorithmic
superiority. Through empirical investigation on enterprise-derived benchmarks,
we find that embedding-based methods offer superior latency for large tool
repositories, while listwise ranking provides better disambiguation for
overlapping functionalities, with hybrid approaches showing promise in specific
contexts. We integrate our findings into ScaleCall's flexible architecture and
validate the framework through real-world deployment in Mastercard's regulated
environment. Our work provides practical insights into the trade-offs between
retrieval accuracy, computational efficiency, and operational requirements,
contributing to the understanding of tool-calling system design for enterprise
applications in regulated industries.

</details>


### [362] [Adding New Capability in Existing Scientific Application with LLM Assistance](https://arxiv.org/abs/2511.00087)
*Anshu Dubey,Akash Dhruv*

Main category: cs.SE

TL;DR: 提出了一种使用LLM辅助从零编写新算法代码的新方法，并改进了现有的代码翻译工具Code-Scribe用于新代码生成


<details>
  <summary>Details</summary>
Motivation: 现有代码生成研究主要关注已有代码的生成，对于训练数据中不包含类似代码的新算法生成研究较少

Method: 开发了新的代码生成方法，并增强了Code-Scribe工具来支持新代码生成

Result: 成功实现了从零生成新算法代码的能力

Conclusion: 该方法为LLM在全新算法代码生成方面的应用提供了有效解决方案

Abstract: With the emergence and rapid evolution of large language models (LLM),
automating coding tasks has become an im- portant research topic. Many efforts
are underway and liter- ature abounds about the efficacy of models and their
ability to generate code. A less explored aspect of code generation is for new
algorithms, where the training data-set would not have included any previous
example of similar code. In this paper we propose a new methodology for writing
code from scratch for a new algorithm using LLM assistance, and describe
enhancement of a previously developed code- translation tool, Code-Scribe, for
new code generation.

</details>


### [363] [Inferring multiple helper Dafny assertions with LLMs](https://arxiv.org/abs/2511.00125)
*Álvaro Silva,Alexandra Mendes,Ruben Martins*

Main category: cs.SE

TL;DR: 使用大型语言模型自动推断Dafny程序中缺失的辅助断言，开发了DAISY工具，在单断言缺失和多断言缺失情况下分别达到63.4%和31.7%的验证成功率。


<details>
  <summary>Details</summary>
Motivation: Dafny验证器虽然提供强正确性保证，但需要大量手动辅助断言，这成为采用的重要障碍。

Method: 扩展DafnyBench基准测试集，创建缺失断言的数据集；提出断言类型分类法；开发混合方法结合LLM预测和错误消息启发式进行故障定位；实现DAISY工具。

Result: DAISY在单断言缺失情况下验证63.4%的程序，在多断言缺失情况下验证31.7%的程序。许多程序可以用比原始更少的断言进行验证。

Conclusion: 自动断言推断可以显著减少证明工程工作量，是迈向更可扩展和可访问的形式验证的一步。

Abstract: The Dafny verifier provides strong correctness guarantees but often requires
numerous manual helper assertions, creating a significant barrier to adoption.
We investigate the use of Large Language Models (LLMs) to automatically infer
missing helper assertions in Dafny programs, with a primary focus on cases
involving multiple missing assertions. To support this study, we extend the
DafnyBench benchmark with curated datasets where one, two, or all assertions
are removed, and we introduce a taxonomy of assertion types to analyze
inference difficulty. Our approach refines fault localization through a hybrid
method that combines LLM predictions with error-message heuristics. We
implement this approach in a new tool called DAISY (Dafny Assertion Inference
SYstem). While our focus is on multiple missing assertions, we also evaluate
DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one
missing assertion and 31.7% with multiple missing assertions. Notably, many
programs can be verified with fewer assertions than originally present,
highlighting that proofs often admit multiple valid repair strategies and that
recovering every original assertion is unnecessary. These results demonstrate
that automated assertion inference can substantially reduce proof engineering
effort and represent a step toward more scalable and accessible formal
verification.

</details>


### [364] [What a diff makes: automating code migration with large language models](https://arxiv.org/abs/2511.00160)
*Katherine A. Rosenfeld,Cliff C. Kerr,Jessica Lundin*

Main category: cs.SE

TL;DR: 使用包含差异信息的上下文可以显著提升大语言模型在代码迁移任务中的性能，特别是在依赖库发生语义版本变更时保持兼容性。


<details>
  <summary>Details</summary>
Motivation: 现代软件程序依赖的库栈经常更新，这些变更可能破坏依赖它们的项目。需要解决在依赖库发生主要和次要语义版本变更时保持兼容性的问题。

Method: 使用包含差异(diffs)的上下文来增强大语言模型的性能，通过测试覆盖率和变更比较等指标进行评估。开发了AIMigrate开源Python包来协助代码库迁移。

Result: 在TYPHOIDSIM从STARSIM版本迁移的真实案例中，AIMigrate在单次运行中正确识别了65%的必要变更，多次运行后提升到80%，其中47%的变更生成完美。

Conclusion: 包含差异信息的上下文可以显著提升LLM在代码迁移任务中的性能，在某些情况下甚至优于使用代码本身。提供了数据集和开源工具来支持该问题领域的进一步发展。

Abstract: Modern software programs are built on stacks that are often undergoing
changes that introduce updates and improvements, but may also break any project
that depends upon them. In this paper we explore the use of Large Language
Models (LLMs) for code migration, specifically the problem of maintaining
compatibility with a dependency as it undergoes major and minor semantic
version changes. We demonstrate, using metrics such as test coverage and change
comparisons, that contexts containing diffs can significantly improve
performance against out of the box LLMs and, in some cases, perform better than
using code. We provide a dataset to assist in further development of this
problem area, as well as an open-source Python package, AIMigrate, that can be
used to assist with migrating code bases. In a real-world migration of
TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of
required changes in a single run, increasing to 80% with multiple runs, with
47% of changes generated perfectly.

</details>


### [365] [Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories](https://arxiv.org/abs/2511.00197)
*Oorja Majgaonkar,Zhiwei Fei,Xiang Li,Federica Sarro,He Ye*

Main category: cs.SE

TL;DR: 对三种最先进代码代理在SWE-Bench基准上的执行轨迹进行实证研究，分析成功与失败路径的行为模式，揭示代理决策过程的关键洞察。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在复杂软件工程任务中的部署增加，需要超越简单成功指标来理解其问题解决行为，因为它们的决策过程仍然不透明。

Method: 分析OpenHands、SWE-agent和Prometheus三种代码代理在SWE-Bench基准上的执行轨迹，包括成功和失败的尝试，研究问题解决策略、轨迹长度和故障定位。

Result: 发现不同问题解决策略在不同场景中促成成功；失败轨迹更长且方差更高；大多数轨迹能正确识别问题文件（72-81%），但成功更依赖于近似而非精确的代码修改。

Conclusion: 通过轨迹分析为理解代理行为提供了基础，有助于开发更稳健和可解释的自主软件工程系统。

Abstract: The increasing deployment of Large Language Model (LLM) agents for complex
software engineering tasks has created a need to understand their
problem-solving behaviours beyond simple success metrics. While these agents
demonstrate impressive capabilities in automated issue resolution, their
decision-making processes remain largely opaque. This paper presents an
empirical study of agent trajectories, namely the execution traces capturing
the steps agents take when attempting to resolve software issues. We analyse
trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and
Prometheus) on the SWE-Bench benchmark, examining both successful and failed
attempts. Our investigation reveals several key insights into agent behaviour.
First, we identify how distinct problem-solving strategies, such as defensive
programming and context gathering, enable success in different scenarios.
Second, we find that failed trajectories are consistently longer and exhibit
higher variance than successful ones, with failure patterns differing
significantly between agents. Third, our fault localisation analysis shows that
while most trajectories correctly identify problematic files (72-81\% even in
failures), success depends more on achieving approximate rather than exact code
modifications. These and other findings unveiled by our study, provide a
foundation for understanding agent behaviour through trajectory analysis,
contributing to the development of more robust and interpretable autonomous
software engineering systems.

</details>


### [366] [Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification](https://arxiv.org/abs/2511.00202)
*Jacqueline Mitchell,Yasser Shaaban*

Main category: cs.SE

TL;DR: 论文分析了"氛围编程"（通过与大语言模型对话开发软件）的局限性，提出通过形式化方法来解决技术债务、安全问题和代码混乱等问题。


<details>
  <summary>Details</summary>
Motivation: 氛围编程虽然流行，但存在技术债务积累、安全问题和代码混乱等限制，这些问题的根源在于LLM无法在对话过程中协调人类施加的约束，且优先考虑用户指令而非代码一致性。

Method: 提出一个伴随氛围编程过程的侧车系统，包括：(1)自动形式化规范，(2)针对目标进行验证，(3)向LLM提供可操作的反馈，(4)允许开发者直观地影响规范。

Result: 通过形式化方法可以缓解氛围编程的陷阱，使其更加可靠。

Conclusion: 需要超越现有的形式化方法与LLM结合的方法，采用侧车系统来提升氛围编程的可靠性。

Abstract: ``Vibe coding'' -- the practice of developing software through iteratively
conversing with a large language model (LLM) -- has exploded in popularity
within the last year. However, developers report key limitations including the
accumulation of technical debt, security issues, and code churn to achieve
satisfactory results. We argue that these pitfalls result from LLMs' inability
to reconcile accumulating human-imposed constraints during vibe coding, with
developers inadvertently failing to resolve contradictions because LLMs
prioritize user commands over code consistency. Given LLMs' receptiveness to
verification-based feedback, we argue that formal methods can mitigate these
pitfalls, making vibe coding more reliable. However, we posit that integrating
formal methods must transcend existing approaches that combine formal methods
and LLMs. We advocate for a side-car system throughout the vibe coding process
which: (1) \emph{Autoformalizes} specifications (2) Validates against targets,
(3) Delivers \emph{actionable} feedback to the LLM, and (4) Allows intuitive
developer influence on specifications.

</details>


### [367] [DocPrism: Local Categorization and External Filtering to Identify Relevant Code-Documentation Inconsistencies](https://arxiv.org/abs/2511.00215)
*Xiaomeng Xu,Zahin Wahab,Reid Holmes,Caroline Lemieux*

Main category: cs.SE

TL;DR: DocPrism是一个多语言代码文档不一致性检测工具，使用标准大语言模型分析不一致性，并通过LCEF方法大幅降低误报率。


<details>
  <summary>Details</summary>
Motivation: 代码文档不一致性很常见且有害，可能导致开发者误解和软件缺陷。

Method: 使用标准LLM分析不一致性，并引入LCEF（本地分类、外部过滤）方法来减少误报，该方法依赖LLM的本地完成能力而非长期推理能力。

Result: LCEF将不一致性标记率从98%降至14%，准确率从14%提升至94%。在Python、TypeScript、C++和Java的广泛评估中，DocPrism保持15%的低标记率，精度达到0.62。

Conclusion: DocPrism是一个有效的多语言代码文档不一致性检测工具，通过LCEF方法显著提高了检测准确性。

Abstract: Code-documentation inconsistencies are common and undesirable: they can lead
to developer misunderstandings and software defects. This paper introduces
DocPrism, a multi-language, code-documentation inconsistency detection tool.
DocPrism uses a standard large language model (LLM) to analyze and explain
inconsistencies. Plain use of LLMs for this task yield unacceptably high false
positive rates: LLMs identify natural gaps between high-level documentation and
detailed code implementations as inconsistencies. We introduce and apply the
Local Categorization, External Filtering (LCEF) methodology to reduce false
positives. LCEF relies on the LLM's local completion skills rather than its
long-term reasoning skills. In our ablation study, LCEF reduces DocPrism's
inconsistency flag rate from 98% to 14%, and increases accuracy from 14% to
94%. On a broad evaluation across Python, TypeScript, C++, and Java, DocPrism
maintains a low flag rate of 15%, and achieves a precision of 0.62 without
performing any fine-tuning.

</details>


### [368] [LLM-Driven Cost-Effective Requirements Change Impact Analysis](https://arxiv.org/abs/2511.00262)
*Romina Etezadi,Sallam Abualhaija,Chetan Arora,Lionel Briand*

Main category: cs.SE

TL;DR: ProReFiCIA是一个基于大语言模型的自动化方法，用于识别需求变更对其他需求的影响，在基准数据集上达到93.3%的召回率，在工业数据集上达到95.8%的召回率。


<details>
  <summary>Details</summary>
Motivation: 需求在软件开发过程中经常变化，手动识别变更对其他需求的影响既容易出错又耗费精力，可能导致遗漏受影响的需求，进而引发下游任务中的严重问题。

Method: 提出ProReFiCIA方法，利用大语言模型自动识别需求变更的影响，通过多种LLM和提示变体进行广泛评估。

Result: 使用最佳LLM和提示变体组合，ProReFiCIA在基准数据集上达到93.3%的召回率，在工业数据集上达到95.8%的召回率，工程师只需审查生成结果（占全部需求的2.1%-8.5%）。

Conclusion: ProReFiCIA在识别受影响需求方面表现出强大的有效性，应用成本低，因为工程师只需审查少量生成结果。

Abstract: Requirements are inherently subject to changes throughout the software
development lifecycle. Within the limited budget available to requirements
engineers, manually identifying the impact of such changes on other
requirements is both error-prone and effort-intensive. That might lead to
overlooked impacted requirements, which, if not properly managed, can cause
serious issues in the downstream tasks. Inspired by the growing potential of
large language models (LLMs) across diverse domains, we propose ProReFiCIA, an
LLM-driven approach for automatically identifying the impacted requirements
when changes occur. We conduct an extensive evaluation of ProReFiCIA using
several LLMs and prompts variants tailored to this task. Using the best
combination of an LLM and a prompt variant, ProReFiCIA achieves a recall of
93.3% on a benchmark dataset and 95.8% on a newly created industry dataset,
demonstrating its strong effectiveness in identifying impacted requirements.
Further, the cost of applying ProReFiCIA remains small, as the engineer only
needs to review the generated results, which represent between 2.1% and 8.5% of
the entire set of requirements.

</details>


### [369] [Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework](https://arxiv.org/abs/2511.00417)
*Marcel Valovy*

Main category: cs.SE

TL;DR: 该论文提出了ROMA框架，通过人格心理学和自我决定理论优化人-AI编程协作角色分配，实证研究表明人格驱动的角色优化能显著提升动机和团队动力。


<details>
  <summary>Details</summary>
Motivation: 随着AI改变软件开发，需要探索开发者和AI系统如何最有效地协作，优化人-AI编程角色分配以提升合作效果。

Method: 采用设计科学研究方法，经过五个研究周期，涉及200名实验参与者和46名访谈对象，建立人格特质、编程角色偏好和协作结果之间的实证联系。

Result: 人格驱动的角色优化显著增强自我决定和团队动力，专业人士动机平均提升23%，本科生动机提升高达65%。识别出五种人格原型：探索者、协调者、工匠、架构师和适应者。

Conclusion: 贡献包括：(1)经验证的人格特质与角色偏好及自我决定结果关联框架；(2)人格特征映射的AI协作模式分类；(3)ISO/IEC 29110扩展，使小型实体能在标准内实施人格驱动角色优化。

Abstract: As artificial intelligence transforms software development, a critical
question emerges: how can developers and AI systems collaborate most
effectively? This dissertation optimizes human-AI programming roles through
self-determination theory and personality psychology, introducing the Role
Optimization Motivation Alignment (ROMA) framework.
  Through Design Science Research spanning five cycles, this work establishes
empirically-validated connections between personality traits, programming role
preferences, and collaborative outcomes, engaging 200 experimental participants
and 46 interview respondents.
  Key findings demonstrate that personality-driven role optimization
significantly enhances self-determination and team dynamics, yielding 23%
average motivation increases among professionals and up to 65% among
undergraduates. Five distinct personality archetypes emerge: The Explorer (high
Openness/low Agreeableness), The Orchestrator (high
Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low
Extraversion), The Architect (high Conscientiousness), and The Adapter
(balanced profile). Each exhibits distinct preferences for programming roles
(Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for
satisfaction.
  The dissertation contributes: (1) an empirically-validated framework linking
personality traits to role preferences and self-determination outcomes; (2) a
taxonomy of AI collaboration modalities mapped to personality profiles while
preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small
Entities to implement personality-driven role optimization within established
standards.
  Keywords: artificial intelligence, human-computer interaction, behavioral
software engineering, self-determination theory, personality psychology,
phenomenology, intrinsic motivation, pair programming, design science research,
ISO/IEC 29110

</details>


### [370] [SmartDoc: A Context-Aware Agentic Method Comment Generation Plugin](https://arxiv.org/abs/2511.00450)
*Vahid Etemadi,Gregorio Robles*

Main category: cs.SE

TL;DR: SmartDoc是一个IntelliJ IDEA插件，使用AI代理生成上下文感知的方法注释，通过分析目标方法及其嵌套方法调用的完整上下文来提升代码理解。


<details>
  <summary>Details</summary>
Motivation: 软件维护阶段需要程序理解，但阅读整个方法语句具有挑战性，需要精确和最新的注释来帮助开发者理解代码。

Method: 插件作为AI代理，拥有自己的记忆，通过深度优先搜索遍历方法调用图，为目标方法提供完整上下文来丰富LLM提示，生成方法注释。

Result: 开发了适用于Java代码库的IntelliJ IDEA插件，支持并发处理多个方法的注释更新，BERTScore精度在0.80到0.90范围内，表现良好。

Conclusion: SmartDoc插件通过上下文感知的方法注释生成，有效辅助开发者进行代码理解，在准确性方面表现出色。

Abstract: Context: The software maintenance phase involves many activities such as code
refactoring, bug fixing, code review or testing. Program comprehension is key
to all these activities, as it demands developers to grasp the knowledge (e.g.,
implementation details) required to modify the codebase. Methods as main
building blocks in a program can offer developers this knowledge source for
code comprehension. However, reading entire method statements can be
challenging, which necessitates precise and up-to-date comments. Objective: We
propose a solution as an IntelliJ IDEA plugin, named SmartDoc, that assists
developers in generating context-aware method comments. Method: This plugin
acts as an Artificial Intelligence (AI) agent that has its own memory and is
augmented by target methods' context. When a request is initiated by the
end-user, the method content and all its nested method calls are used in the
comment generation. At the beginning, these nested methods are visited and a
call graph is generated. This graph is then traversed using depth-first search
(DFS), enabling the provision of full-context to enrich Large Language Model
(LLM) prompts. Result: The product is a software, as a plugin, developed for
Java codebase and installable on IntelliJ IDEA. This plugin can serve
concurrently for methods whose comments are being updated , and it shares
memory across all flows to avoid redundant calls. o measure the accuracy of
this solution, a dedicated test case is run to record SmartDoc generated
comments and their corresponding ground truth. For each collected result-set,
three metrics are computed, BERTScore, BLEU and ROUGE-1. These metrics will
determine how accurate the generated comments are in comparison to the ground
truth. Result: The obtained accuracy, in terms of the precision, recall and F1,
is promising, and lies in the range of 0.80 to 0.90 for BERTScore.

</details>


### [371] [A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements](https://arxiv.org/abs/2511.00467)
*Liu Wang,Dong Wang,Shidong Pan,Zheng Jiang,Haoyu Wang,Yi Wang*

Main category: cs.SE

TL;DR: 该研究评估了iOS 15.2引入的App隐私报告功能的实际效果，发现其存在局限性，并提出基于LLM的增强方案来改善用户隐私透明度。


<details>
  <summary>Details</summary>
Motivation: 苹果推出的App隐私报告功能被宣传为用户隐私的重大进步，但其对用户隐私和控制的实际影响尚未得到检验。

Method: 通过结构化焦点小组研究（12名日常iOS用户）、系统评估以及提出基于LLM的目的推断框架和域名澄清流程等增强方案。

Result: 研究发现App隐私报告的实际影响有限，主要因为缺少重要细节，用户主要关注数据访问目的和域名描述的清晰度。

Conclusion: 该工作提供了实用的见解，有助于增强用户隐私透明度，并讨论了未来研究方向。

Abstract: The prevalent engagement with mobile apps underscores the importance of
understanding their data practices. Transparency plays a crucial role in this
context, ensuring users to be informed and give consent before any data access
occurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to
inform users about detailed insights into apps' data access and sharing. This
feature continues Apple's trend of privacy-focused innovations (following
Privacy Nutrition Labels), and has been marketed as a big step forward in user
privacy. However, its real-world impacts on user privacy and control remain
unexamined. We thus proposed an end-to-end study involving systematic
assessment of the App Privacy Report's real-world benefits and limitations,
LLM-enabled and multi-technique synthesized enhancements, and comprehensive
evaluation from both system and user perspectives. Through a structured focus
group study with twelve everyday iOS users, we explored their experiences,
understanding, and perceptions of the feature, suggesting its limited practical
impact resulting from missing important details. We identified two primary user
concerns: the clarity of data access purpose and domain description. In
response, we proposed enhancements including a purpose inference framework and
domain clarification pipeline. We demonstrated the effectiveness and benefits
of such enhancements for mobile app users. This work provides practical
insights that could help enhance user privacy transparency and discusses areas
for future research.

</details>


### [372] [Issue-Oriented Agent-Based Framework for Automated Review Comment Generation](https://arxiv.org/abs/2511.00517)
*Shuochuan Li,Dong Wang,Patanamon Thongtanunam,Zan Wang,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: RevAgent是一个基于代理的面向问题代码审查框架，通过分解任务为生成、判别和训练三个阶段，显著提升了代码审查评论的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查自动化方法依赖单一模型处理各种问题，难以应对代码变更的多样性，特别是在复杂场景如bug修复中产生非信息性评论。

Method: 提出三阶段框架：1)生成阶段-五个类别特定评论代理从不同问题角度分析代码变更；2)判别阶段-批评代理选择最合适的问题-评论对；3)训练阶段-所有代理在特定类别数据上微调。

Result: RevAgent在BLEU、ROUGE-L、METEOR和SBERT指标上分别提升12.90%、10.87%、6.32%和8.57%，在问题类别识别上获得更高准确率，特别是在挑战性场景中。

Conclusion: RevAgent在生成准确、可读且上下文感知的审查评论方面表现出实用性，并在性能与效率之间实现了良好平衡。

Abstract: Code review (CR) is a crucial practice for ensuring software quality. Various
automated review comment generation techniques have been proposed to streamline
the labor-intensive process. However, existing approaches heavily rely on a
single model to identify various issues within the code, limiting the model's
ability to handle the diverse, issue-specific nature of code changes and
leading to non-informative comments, especially in complex scenarios such as
bug fixes. To address these limitations, we propose RevAgent, a novel
agent-based issue-oriented framework, decomposes the task into three stages:
(1) Generation Stage, where five category-specific commentator agents analyze
code changes from distinct issue perspectives and generate candidate comments;
(2) Discrimination Stage, where a critic agent selects the most appropriate
issue-comment pair; and (3) Training Stage, where all agents are fine-tuned on
curated, category-specific data to enhance task specialization. Evaluation
results show that RevAgent significantly outperforms state-of-the-art PLM- and
LLM-based baselines, with improvements of 12.90\%, 10.87\%, 6.32\%, and 8.57\%
on BLEU, ROUGE-L, METEOR, and SBERT, respectively. It also achieves relatively
higher accuracy in issue-category identification, particularly for challenging
scenarios. Human evaluations further validate the practicality of RevAgent in
generating accurate, readable, and context-aware review comments. Moreover,
RevAgent delivers a favorable trade-off between performance and efficiency.

</details>


### [373] [HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models](https://arxiv.org/abs/2511.00527)
*Robab Aghazadeh-Chakherlou,Qing Guo,Siddartha Khastgir,Peter Popov,Xiaoge Zhang,Xingyu Zhao*

Main category: cs.SE

TL;DR: HIP-LLM是一个分层不精确概率框架，用于建模和推断大语言模型的可靠性，通过分层依赖关系和引入不精确先验来量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于基准测试的评估方法主要提供模型在数据集上的准确性描述性统计，对大语言模型在实际操作条件下的概率行为提供有限洞察。

Method: 基于软件可靠性工程基础，HIP-LLM将LLM可靠性定义为在给定操作配置文件下未来指定数量任务中无故障运行的概率，采用分层结构表示跨（子）域的依赖关系，并嵌入不精确先验来捕捉认知不确定性。

Result: 在多个基准数据集上的实验表明，HIP-LLM比现有基准和最先进方法提供了更准确和标准化的可靠性表征。

Conclusion: HIP-LLM框架能够更有效地评估大语言模型的可靠性，并为实际部署提供量化不确定性分析。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
domains, raising the need for rigorous reliability assessment methods. Existing
benchmark-based evaluations primarily offer descriptive statistics of model
accuracy over datasets, providing limited insight into the probabilistic
behavior of LLMs under real operational conditions. This paper introduces
HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and
inferring LLM reliability. Building upon the foundations of software
reliability engineering, HIP-LLM defines LLM reliability as the probability of
failure-free operation over a specified number of future tasks under a given
Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains
hierarchically, enabling multi-level inference from subdomain to system-level
reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty
and incorporates OPs to reflect usage contexts. It derives posterior
reliability envelopes that quantify uncertainty across priors and data.
Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a
more accurate and standardized reliability characterization than existing
benchmark and state-of-the-art approaches. A publicly accessible repository of
HIP-LLM is provided.

</details>


### [374] [Employee Performance when Implementing Agile Practices in an IT Workforce](https://arxiv.org/abs/2511.00528)
*Muhammad Hamid Raza Mookadam,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 研究探讨了南非IT工作环境中敏捷实践对员工绩效的影响，发现敏捷实践通过改善团队动态、协作、效率等方面显著提升员工绩效，但也面临采用、团队参与等挑战。


<details>
  <summary>Details</summary>
Motivation: 非洲背景下缺乏关于敏捷实践中员工绩效的全面研究，本研究旨在填补这一空白。

Method: 采用解释主义单方法定性研究，通过17个半结构化访谈收集敏捷从业者的数据。

Result: 敏捷实践显著影响员工绩效，涉及规划、沟通、员工发展、协作等方面；同时发现采用、团队参与、领导力等实施障碍。

Conclusion: 如果解决敏捷挑战并提供额外支持，员工绩效可以得到显著改善。

Abstract: Adoption of agile practices has increased in IT workforces. However, there is
a lack of comprehensive studies in the African context on employee performance
when implementing agile practices. This study addresses this gap by exploring
employee performance in agile environments for IT workforces in South Africa.
An interpretivist mono-method qualitative approach was used, with the use of
interviews as a research strategy. Seventeen semi-structured interviews were
conducted with agile practitioners from various roles. Our results indicated
that agile practices influence employee performance significantly, with
participants reporting on aspects which included planning, communication,
employee development and well-being, collaboration, team culture and progress.
Additionally, our results reported obstacles when using agile practices that
included adoption, team engagement, leadership and instilling an agile mindset.
Agile practices influence employee performance in IT workforces by fostering
improved team dynamics, enhanced collaboration, improved efficiencies, risk
management, planning, continuous improvement, learning, personal development
and well-being. Conclusively, our findings suggest that if agile challenges are
addressed and additional support is provided, employee performance can be
significantly improved.

</details>


### [375] [GDPR-Bench-Android: A Benchmark for Evaluating Automated GDPR Compliance Detection in Android](https://arxiv.org/abs/2511.00619)
*Huaijin Ran,Haoyi Zhang,Xunzhu Tang*

Main category: cs.SE

TL;DR: 提出了GDPR-Bench-Android基准，用于评估Android应用中GDPR合规性检测的自动化方法，包含1951个手动标注的违规实例，涵盖23个GDPR条款。


<details>
  <summary>Details</summary>
Motivation: 自动化检测源代码中的GDPR违规是一个关键但研究不足的挑战，需要建立全面的评估基准。

Method: 创建了GDPR-Bench-Android基准，包含1951个手动标注的违规实例；提出了Formal-AST形式化方法作为确定性基线；定义了多粒度违规定位和片段级多标签分类两个任务；评估了11种方法，包括8个LLM、Formal-AST、RAG和ReAct方法。

Result: 不同方法在不同任务中表现各异：ReAct在文件级定位任务中表现最佳（17.38%），Qwen2.5-72B在线级定位中领先（61.60%），Claude-Sonnet-4.5在多标签分类中F1得分最高（5.75%），RAG方法在精确率上最优（7.10%）。

Conclusion: 没有单一方法在所有任务中都表现最佳，不同自动化方法在不同任务中各有优势，该基准有助于诊断各种方法的能力。

Abstract: Automating the detection of EU General Data Protection Regulation (GDPR)
violations in source code is a critical but underexplored challenge. We
introduce \textbf{GDPR-Bench-Android}, the first comprehensive benchmark for
evaluating diverse automated methods for GDPR compliance detection in Android
applications. It contains \textbf{1951} manually annotated violation instances
from \textbf{15} open-source repositories, covering 23 GDPR articles at file-,
module-, and line-level granularities. To enable a multi-paradigm evaluation,
we contribute \textbf{Formal-AST}, a novel, source-code-native formal method
that serves as a deterministic baseline. We define two tasks: (1)
\emph{multi-granularity violation localization}, evaluated via
Accuracy@\textit{k}; and (2) \emph{snippet-level multi-label classification},
assessed by macro-F1 and other classification metrics. We benchmark 11 methods,
including eight state-of-the-art LLMs, our Formal-AST analyzer, a
retrieval-augmented (RAG) method, and an agentic (ReAct) method. Our findings
reveal that no single paradigm excels across all tasks. For Task 1, the ReAct
agent achieves the highest file-level Accuracy@1 (17.38%), while the
Qwen2.5-72B LLM leads at the line level (61.60%), in stark contrast to the
Formal-AST method's 1.86%. For the difficult multi-label Task 2, the
Claude-Sonnet-4.5 LLM achieves the best Macro-F1 (5.75%), while the RAG method
yields the highest Macro-Precision (7.10%). These results highlight the
task-dependent strengths of different automated approaches and underscore the
value of our benchmark in diagnosing their capabilities. All resources are
available at: https://github.com/Haoyi-Zhang/GDPR-Bench-Android.

</details>


### [376] [Can Large Language Models Detect Real-World Android Software Compliance Violations?](https://arxiv.org/abs/2511.00624)
*Haoyi Zhang,Huaijin Ran,Xunzhu Tang*

Main category: cs.SE

TL;DR: 提出了CompliBench评估框架，用于评估LLM在检测Android应用合规性违规方面的能力，涵盖LGPD、PDPA和PIPEDA等法规，并引入稳定性感知复合指标进行更全面的评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在检测Android应用跨不同法律框架的合规性违规方面存在困难，需要专门的评估框架来提升这方面的能力。

Method: 定义了两个任务：任务1评估文件、模块和行级别的检索和定位能力，任务2评估代码片段的多标签判断能力；引入了稳定性感知复合指标（SGS、RCS、CRGS和OCS）。

Result: 在六个模型上的实验显示，CompliBench提升了合规性检测能力，Claude-3.5-sonnet-20241022获得最高OCS分数（0.3295），Gemini-2.5-pro最低（0.0538）。

Conclusion: CompliBench有潜力提升LLM在合规性任务中的表现，为未来符合数据保护标准的工具提供了基础。

Abstract: The rapid development of Large Language Models (LLMs) has transformed
software engineering, showing promise in tasks like code generation, bug
detection, and compliance checking. However, current models struggle to detect
compliance violations in Android applications across diverse legal frameworks.
We propose \emph{CompliBench}, a novel evaluation framework for assessing LLMs'
ability to detect compliance violations under regulations like LGPD, PDPA, and
PIPEDA. The framework defines two tasks: Task 1 evaluates \emph{retrieval and
localization} at file, module, and line granularities, and Task 2 assesses
\emph{multi-label judgment} for code snippets. These tasks mirror the audit
process, where auditors locate problematic code and determine implicated
provisions. Traditional metrics fail to capture important aspects like
cross-granularity stability and jurisdictional consistency. Thus, we introduce
stability-aware composites (SGS, RCS, CRGS, and OCS) for a more comprehensive
assessment. Experiments with six models, including GPT-4O and Claude-3.5, show
\emph{CompliBench} improves compliance detection, with
Claude-3.5-sonnet-20241022 achieving the highest OCS score (0.3295), and
Gemini-2.5-pro the lowest (0.0538). This work demonstrates \emph{CompliBench}'s
potential for improving LLM performance in compliance tasks and provides a
foundation for future tools aligned with data protection standards. Our project
is available at https://github.com/Haoyi-Zhang/CompliBench.

</details>


### [377] [Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare](https://arxiv.org/abs/2511.00658)
*Guilherme H. Travassos,Sabrina Rocha,Rodrigo Feitosa,Felipe Assis,Patricia Goncalves,Andre Gheventer,Larissa Galeno,Arthur Sasse,Julio Cesar Guimaraes,Carlos Brito,Joao Pedro Wieland*

Main category: cs.SE

TL;DR: 本文分享了在临床试验软件系统开发中使用生成式AI的经验报告，涵盖了项目管理、需求分析、设计、开发和质量保证等环节的学习历程。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在软件工程中的应用仍处于早期阶段，但作者希望通过实际项目实践来探索这些技术如何提升软件开发的生产力和质量。

Method: 在开发基于web的临床试验软件系统过程中，团队在项目管理、需求分析、设计、开发和质量保证等环节系统地应用生成式AI技术。

Result: 虽然尚未获得决定性的技术证据来显著改进开发流程，但获得了有价值的见解和建议。

Conclusion: 这些经验和建议为寻求通过生成式AI创新开发实践以实现软件质量的组织提供了宝贵的参考。

Abstract: The advances and availability of technologies involving Generative Artificial
Intelligence (AI) are evolving clearly and explicitly, driving immediate
changes in various work activities. Software Engineering (SE) is no exception
and stands to benefit from these new technologies, enhancing productivity and
quality in its software development processes. However, although the use of
Generative AI in SE practices is still in its early stages, considering the
lack of conclusive results from ongoing research and the limited technological
maturity, we have chosen to incorporate these technologies in the development
of a web-based software system to be used in clinical trials by a thoracic
diseases research group at our university. For this reason, we decided to share
this experience report documenting our development team's learning journey in
using Generative AI during the software development process. Project
management, requirements specification, design, development, and quality
assurance activities form the scope of observation. Although we do not yet have
definitive technological evidence to evolve our development process
significantly, the results obtained and the suggestions shared here represent
valuable insights for software organizations seeking to innovate their
development practices to achieve software quality with generative AI.

</details>


### [378] [Repairing Responsive Layout Failures Using Retrieval Augmented Generation](https://arxiv.org/abs/2511.00678)
*Tasmia Zerin,Moumita Asad,B. M. Mainul Hossain,Kazi Sakib*

Main category: cs.SE

TL;DR: 提出ReDeFix方法，利用LLM结合Stack Overflow知识自动修复响应式布局故障，准确率达88%


<details>
  <summary>Details</summary>
Motivation: 响应式网站在特定屏幕尺寸下会出现布局扭曲问题，手动修复需要繁琐的试错调整

Method: 基于检索增强生成(RAG)的方法，利用Stack Overflow讨论指导LLM进行CSS修复，通过RLF特定上下文增强相关知识

Result: 评估显示该方法修复RLF的准确率达到88%，软件工程师研究表明生成的修复能产生视觉正确的布局并保持美观

Conclusion: ReDeFix方法能有效自动修复响应式布局故障，生成高质量的CSS补丁

Abstract: Responsive websites frequently experience distorted layouts at specific
screen sizes, called Responsive Layout Failures (RLFs). Manually repairing
these RLFs involves tedious trial-and-error adjustments of HTML elements and
CSS properties. In this study, an automated repair approach, leveraging LLM
combined with domain-specific knowledge is proposed. The approach is named
ReDeFix, a Retrieval-Augmented Generation (RAG)-based solution that utilizes
Stack Overflow (SO) discussions to guide LLM on CSS repairs. By augmenting
relevant SO knowledge with RLF-specific contexts, ReDeFix creates a prompt that
is sent to the LLM to generate CSS patches. Evaluation demonstrates that our
approach achieves an 88\% accuracy in repairing RLFs. Furthermore, a study from
software engineers reveals that generated repairs produce visually correct
layouts while maintaining aesthetics.

</details>


### [379] [An Empirical Investigation of the Experiences of Dyslexic Software Engineers](https://arxiv.org/abs/2511.00706)
*Marcos Vinicius Cruz,Pragya Verma,Grischa Liebel*

Main category: cs.SE

TL;DR: 本文通过定性研究探讨了阅读障碍软件工程师的经验，发现他们在编程学习阶段面临挑战，但一旦掌握后能在许多SE任务中表现出色，并具有视觉思维和创造力等优势。


<details>
  <summary>Details</summary>
Motivation: 目前没有研究探讨阅读障碍软件工程师的经验，也没有将他们的优势与困难联系起来。阅读障碍在软件工程中可能带来挑战，但相关优势可能在编程和设计领域特别有价值。

Method: 采用定性研究方法，基于社会技术扎根理论的基本阶段，通过10次访谈、3篇博客文章和153篇Reddit帖子收集数据。

Result: 阅读障碍软件工程师在编程学习阶段特别困难，但掌握后能在SE任务中取得成功和卓越表现；代码补全和linter等工具特别有用；他们展现出视觉思维和创造力等优势。

Conclusion: 研究结果对SE实践有启示，并激励未来研究探索如何使代码对阅读障碍者更易理解。

Abstract: Dyslexia is a common learning disorder that primarily impairs an individual's
reading and writing abilities. In adults, dyslexia can affect both professional
and personal lives, often leading to mental challenges and difficulties
acquiring and keeping work. In Software Engineering (SE), reading and writing
difficulties appear to pose substantial challenges for core tasks such as
programming. However, initial studies indicate that these challenges may not
significantly affect their performance compared to non-dyslexic colleagues.
Conversely, strengths associated with dyslexia could be particularly valuable
in areas like programming and design. However, there is currently no work that
explores the experiences of dyslexic software engineers, and puts their
strengths into relation with their difficulties. To address this, we present a
qualitative study of the experiences of dyslexic individuals in SE. We followed
the basic stage of the Socio-Technical Grounded Theory method and base our
findings on data collected through 10 interviews with dyslexic software
engineers, 3 blog posts and 153 posts on the social media platform Reddit. We
find that dyslexic software engineers especially struggle at the programming
learning stage, but can succeed and indeed excel at many SE tasks once they
master this step. Common SE-specific support tools, such as code completion and
linters are especially useful to these individuals and mitigate many of the
experienced difficulties. Finally, dyslexic software engineers exhibit
strengths in areas such as visual thinking and creativity. Our findings have
implications to SE practice and motivate several areas of future research in
SE, such as investigating what makes code less/more understandable to dyslexic
individuals.

</details>


### [380] [A Systematic Literature Review of Code Hallucinations in LLMs: Characterization, Mitigation Methods, Challenges, and Future Directions for Reliable AI](https://arxiv.org/abs/2511.00776)
*Cuiyun Gao,Guodong Fan,Chun Yong Chong,Shizhan Chen,Chao Liu,David Lo,Zibin Zheng,Qing Liao*

Main category: cs.SE

TL;DR: 这篇论文系统综述了代码导向大语言模型中的幻觉现象，从定义、原因、缓解策略、代码特定挑战到评估基准进行了全面分析。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程任务中的广泛应用，理解和缓解代码生成中的幻觉问题变得至关重要，特别是在高风险代码智能任务中。

Method: 通过综述60篇论文，从四个关键角度系统分析代码幻觉：定义和原因、通用缓解策略、代码特定挑战、以及评估基准。

Result: 总结了代码幻觉的主要成因（数据噪声、曝光偏差、语义基础不足），缓解策略（知识增强生成、约束解码、后编辑），以及代码特有的挑战（语法敏感性、严格类型系统、外部库依赖）。

Conclusion: 需要开发专门的幻觉导向评估基准，并利用程序分析、符号执行和单元测试等新兴代码智能任务来检测和缓解幻觉问题。

Abstract: Model hallucination is one of the most critical challenges faced by Large
Language Models (LLMs), especially in high-stakes code intelligence tasks. As
LLMs become increasingly integrated into software engineering tasks,
understanding and mitigating hallucination in code becomes essential. In this
survey, we provide a systematic review of hallucination phenomena in
code-oriented LLMs from four key perspectives. First, we begin by surveying 60
papers to define hallucination in the context of code and summarize its primary
causes, such as data noise, exposure bias, and insufficient semantic grounding,
while also tracing recent trends in literature across natural language
processing (NLP) and software engineering communities. Second, we review model
hallucination surveys in a broader span and summarize representative
hallucination mitigation strategies, such as knowledge-enhanced generation,
constrained decoding, and post-editing. Third, we review approaches targeted
for code intelligence and highlight code-specific challenges that aggravate
hallucination, including syntax sensitivity, strict type systems, and
dependence on external libraries. Meanwhile, we analyze how emerging code
intelligence tasks, e.g., program analysis, symbolic execution, and unit
testing, are utilized to detect and mitigate hallucinations. Fourth, we
summarize current evaluation benchmarks, ranging from static metrics to dynamic
checks, e.g., compilation and execution correctness, and emphasize the need for
hallucination-oriented benchmarks.

</details>


### [381] [Can Language Models Go Beyond Coding? Assessing the Capability of Language Models to Build Real-World Systems](https://arxiv.org/abs/2511.00780)
*Chenyu Zhao,Shenglin Zhang,Zeshun Huang,Weilin Jin,Yongqian Sun,Dan Pei,Chaoyun Zhang,Qingwei Lin,Chetan Bansal,Saravan Rajmohan,Minghua Ma*

Main category: cs.SE

TL;DR: Build-bench是第一个针对跨指令集架构(ISA)迁移中LLM修复构建失败能力的端到端基准测试，包含268个真实失败包和工具增强推理支持


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估LLM在跨ISA迁移中修复软件构建失败的基准测试，而跨ISA迁移需要处理复杂依赖、异构工具链和长构建日志等挑战

Method: 收集268个真实失败包，集成结构提取、文件内容提取、内容修改和构建验证等辅助工具，采用迭代循环修复过程，失败时提供更新的构建日志和先前修复结果

Result: 六个代表性LLM的最大构建成功率为63%，不同模型在工具使用模式上存在显著差异

Conclusion: Build-bench通过结合真实构建环境和可验证结果，建立了第一个用于研究基于LLM的软件构建和修复的架构感知基准测试

Abstract: Large language models (LLMs) have shown growing potential in software
engineering, yet few benchmarks evaluate their ability to repair software
during migration across instruction set architectures (ISAs). Cross-ISA
migration, such as between x86_64 and aarch64, requires handling complex
dependencies, heterogeneous toolchains, and long build logs while ensuring
executable verification. To address this challenge, we present Build-bench, an
end-to-end benchmark that systematically evaluates the capability of LLMs to
repair build failures in cross-ISA settings. Build-bench collects 268
real-world failed packages and integrates auxiliary tools including Structure
Extraction, File Content Extraction, Content Modification, and Build
Verification to support autonomous, tool-augmented reasoning. The repair
process operates in an iterative loop where, upon failure, the model receives
updated build logs and previous repair outcomes to refine subsequent attempts.
Through a comparative evaluation of six representative LLMs, Build-bench
reveals that current models achieve a maximum build success rate of 63% and
tool usage patterns differ significantly across models. By coupling real build
environments with verifiable outcomes, Build-bench establishes the first
architecture-aware benchmark for studying LLM-based software build and repair.

</details>


### [382] [GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents](https://arxiv.org/abs/2511.00802)
*Jie JW Wu,Ayanda Patrick Herlihy,Ahmad Saleem Mirza,Ali Afoud,Fatemeh Fard*

Main category: cs.SE

TL;DR: 本文研究了如何利用LLM和基于LLM的智能体通过代码优化来改进离线A/B测试(OPE)性能，提出了GrowthHacker基准测试框架，开发了two_agent架构，在真实世界数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试需要大量资源且可能对用户产生负面影响，而离线策略评估(OPE)使用日志数据评估技术，在医疗、推荐系统等高风险领域至关重要。尽管LLM和智能体AI有进展，但如何利用它们优化OPE结果尚不明确。

Method: 提出了GrowthHacker基准测试，包含智能体和基线方法，在真实世界数据集上迭代优化代码、评估结果并开始新的优化周期。开发了two_agent框架，在保持优化效果的同时降低系统复杂度。

Result: two_agent框架实现了100%的可靠性和106.7%的平均改进率。two_agent和CrewAI都达到了45%的成功率，优于AutoGen的34%。

Conclusion: 基于LLM的智能体可以作为自动化的"增长黑客"来增强OPE系统，为生产环境中扩展数据驱动决策提供了可行性。

Abstract: With the software industry shifting toward a data-driven culture, online A/B
testing is a key tool for evaluating new technologies. However, deploying such
experiments requires substantial resources, may negatively impact users, and
involves long data collection periods. To address this, \textit{off-policy
evaluation (OPE)}, or offline A/B testing, uses logged data to assess
technologies and is fundamental in Reinforcement Learning, making it crucial in
domains where online testing is costly or risky, such as healthcare,
recommender systems, education, dialog systems, and robotics. Despite advances
in coding LLMs and agentic AI, little is known about leveraging them to
optimize OPE results. We investigate whether LLMs and LLM-based agents can
improve OPE performance via code optimization. We propose
\textit{GrowthHacker}, a benchmark with agent and baseline methods on
large-scale real-world datasets, which iteratively optimizes code, evaluates
results, and begins new optimization cycles. We collected datasets, established
protocols, implemented baselines for OPE on the Open Bandit Pipeline
(OBP)~\cite{saito2021openbanditdatasetpipeline} and
Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent}
framework, which reduces system complexity while preserving optimization
effectiveness. Results show the two_agent framework achieves 100% reliability
and the highest average improvement of 106.7% among positive outcomes. Both
two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.
These findings demonstrate the feasibility of LLM-based agents as automated
"growth hackers" to enhance OPE systems, with implications for scaling
data-driven decision-making in production.

</details>


### [383] [CodeClash: Benchmarking Goal-Oriented Software Engineering](https://arxiv.org/abs/2511.00839)
*John Yang,Kilian Lieret,Joyce Yang,Carlos E. Jimenez,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TL;DR: CodeClash是一个新的基准测试，让语言模型在多轮锦标赛中竞争构建最佳代码库，以评估模型在无明确指导下迭代开发代码以实现开放目标的能力。


<details>
  <summary>Details</summary>
Motivation: 当前编码基准测试主要评估语言模型在具体、明确任务上的表现，但真实软件开发是基于高层次目标的。需要评估语言模型能否在没有明确指导的情况下迭代开发代码以实现开放目标。

Method: 引入CodeClash基准测试，模型在多轮锦标赛中竞争：每轮分为两个阶段——代理编辑代码，然后在代码竞技场中头对头竞争，基于目标（如得分最大化、资源获取或生存）确定获胜者。

Result: 运行1680场锦标赛（总计25,200轮）评估8个语言模型在6个竞技场中的表现。结果显示模型展现出多样化的开发风格，但在战略推理方面存在基本限制，且在长期代码库维护方面存在困难。

Conclusion: 顶级模型在与专家人类程序员的每一轮比赛中都失败了，表明当前语言模型在自主、目标导向的代码开发方面仍有显著局限性。CodeClash已开源以推进这一领域的研究。

Abstract: Current benchmarks for coding evaluate language models (LMs) on concrete,
well-specified tasks such as fixing specific bugs or writing targeted tests.
However, human programmers do not spend all day incessantly addressing isolated
tasks. Instead, real-world software development is grounded in the pursuit of
high-level goals, like improving user retention or reducing costs. Evaluating
whether LMs can also iteratively develop code to better accomplish open-ended
objectives without any explicit guidance remains an open challenge. To address
this, we introduce CodeClash, a benchmark where LMs compete in multi-round
tournaments to build the best codebase for achieving a competitive objective.
Each round proceeds in two phases: agents edit their code, then their codebases
compete head-to-head in a code arena that determines winners based on
objectives like score maximization, resource acquisition, or survival. Whether
it's writing notes, scrutinizing documentation, analyzing competition logs, or
creating test suites, models must decide for themselves how to improve their
codebases both absolutely and against their opponents. We run 1680 tournaments
(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal
that while models exhibit diverse development styles, they share fundamental
limitations in strategic reasoning. Models also struggle with long-term
codebase maintenance, as repositories become progressively messy and redundant.
These limitations are stark: top models lose every round against expert human
programmers. We open-source CodeClash to advance the study of autonomous,
goal-oriented code development.

</details>


### [384] [A Comprehensive Empirical Evaluation of Agent Frameworks on Code-centric Software Engineering Tasks](https://arxiv.org/abs/2511.00872)
*Zhuowen Yin,Cuifeng Gao,Chunsong Fan,Wenzhang Yang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: 对7个通用智能体框架在软件开发、漏洞检测和程序修复三个代码中心任务上的综合实证研究，从有效性、效率和开销三个维度系统评估智能体性能，揭示了不同框架的能力模式和权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注特定任务或孤立方面，无法全面了解智能体在实际软件工程中的能力。需要系统评估通用智能体框架在代表性代码任务上的表现。

Method: 使用标准基准测试评估7个通用智能体框架在三个代码中心任务上的表现，从有效性（任务成功率）、效率（执行过程）和开销（token消耗）三个互补角度进行系统分析。

Result: 智能体整体表现中等；AgentOrchestra因协调开销导致最长轨迹和最多修正尝试，OpenHands展现更强反思推理能力；软件开发成本最高，GPTswarm最具成本效益；有效性-效率关系分析揭示了其内在关联。

Conclusion: 研究结果为软件工程智能体的实际应用和未来研究方向提供了指导，有助于开发更高效的软件工程智能体。

Abstract: Unlike traditional automation tools or static LLM-based systems, agents
combine decision-making and tool utilization to accomplish complex tasks,
showing great potential in software engineering. However, existing studies
largely focus on specific tasks or isolated aspects, providing an incomplete
picture of agents' practical capabilities. To address this, we conduct a
comprehensive empirical study evaluating seven general-purpose agent frameworks
across three representative code-centric tasks: software development,
vulnerability detection, and program repair. Each task is assessed using
standard, widely adopted benchmarks to ensure objective and comparable
evaluation. Agent performance is systematically analyzed from three
complementary perspectives: effectiveness (task success), efficiency (execution
process), and overhead (token consumption). Our findings reveal distinct
capability patterns and trade-offs among the evaluated frameworks. In terms of
effectiveness, agents achieve moderate overall performance. Regarding
efficiency, AgentOrchestra tends to exhibit the longest trajectories and the
most correction attempts due to coordination overhead, whereas OpenHands
demonstrate stronger reflective reasoning abilities. For overhead, software
development incurs the highest monetary cost, while GPTswarm remains the most
cost-efficient. Furthermore, we conduct an in-depth cross-analysis of the
relationship between effectiveness and efficiency, exploring the underlying
reasons behind their interplay. These findings guide both practical adoption
and future research toward more efficient software engineering agents.

</details>


### [385] [Sustainability of Machine Learning-Enabled Systems: The Machine Learning Practitioner's Perspective](https://arxiv.org/abs/2511.00901)
*Vincenzo De Martino,Stefano Lambiase,Fabiano Pecorelli,Willem-Jan van den Heuvel,Filomena Ferrucci,Fabio Palomba*

Main category: cs.SE

TL;DR: 该论文通过实证研究探讨了机器学习系统中可持续性的实践现状，发现从业者认知与系统实施之间存在显著脱节，需要更结构化的指导框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统的可持续性是一个重要的非功能性需求，但现有研究主要关注环境可持续性，缺乏对实际工作流程中可持续性管理的实证证据。

Method: 采用混合方法：首先对8名经验丰富的ML工程师进行定性访谈，然后对203名ML从业者进行大规模定量调查。

Result: 研究发现ML工程师对可持续性有认知但在系统实施中存在显著脱节，缺乏结构化指导、测量框架和监管支持。

Conclusion: 需要开发更系统的可持续性指南、测量框架和监管机制来支持ML系统的可持续发展。

Abstract: Software sustainability is a key multifaceted non-functional requirement that
encompasses environmental, social, and economic concerns, yet its integration
into the development of Machine Learning (ML)-enabled systems remains an open
challenge. While previous research has explored high-level sustainability
principles and policy recommendations, limited empirical evidence exists on how
sustainability is practically managed in ML workflows. Existing studies
predominantly focus on environmental sustainability, e.g., carbon footprint
reduction, while missing the broader spectrum of sustainability dimensions and
the challenges practitioners face in real-world settings. To address this gap,
we conduct an empirical study to characterize sustainability in ML-enabled
systems from a practitioner's perspective. We investigate (1) how ML engineers
perceive and describe sustainability, (2) the software engineering practices
they adopt to support it, and (3) the key challenges hindering its adoption. We
first perform a qualitative analysis based on interviews with eight experienced
ML engineers, followed by a large-scale quantitative survey with 203 ML
practitioners. Our key findings reveal a significant disconnection between
sustainability awareness and its systematic implementation, highlighting the
need for more structured guidelines, measurement frameworks, and regulatory
support.

</details>


### [386] [Empirical Derivations from an Evolving Test Suite](https://arxiv.org/abs/2511.00915)
*Jukka Ruohonen,Abhishek Tiwari*

Main category: cs.SE

TL;DR: 对NetBSD操作系统自动化测试套件进行长期实证分析，发现测试套件持续增长至超过1万个测试用例，失败测试案例总体稳定但存在波动期，代码变更和内核修改对失败率影响较小。


<details>
  <summary>Details</summary>
Motivation: 研究大规模演进软件测试套件的长期表现，分析测试失败与代码变更之间的关系，为理解持续集成测试系统的行为提供实证依据。

Method: 采用纵向实证分析方法，观察NetBSD测试套件从2010年代初到2025年底的演变，分析测试用例增长、失败模式与代码变更的统计关系。

Result: 测试套件持续增长至覆盖1万+测试用例；失败测试案例总体稳定但存在波动期；代码变更和内核修改对失败率影响较小且统计上不一致。

Conclusion: 大规模测试套件在长期演进中表现出稳定性，代码变更与测试失败之间的关联性较弱，为理解持续测试系统的行为提供了有价值的实证观察。

Abstract: The paper presents a longitudinal empirical analysis of the automated,
continuous, and virtualization-based software test suite of the NetBSD
operating system. The longitudinal period observed spans from the initial roll
out of the test suite in the early 2010s to late 2025. According to the
results, the test suite has grown continuously, currently covering over ten
thousand individual test cases. Failed test cases exhibit overall stability,
although there have been shorter periods marked with more frequent failures. A
similar observation applies to build failures, failures of the test suite to
complete, and installation failures, all of which are also captured by the
NetBSD's testing framework. Finally, code churn and kernel modifications do not
provide longitudinally consistent statistical explanations for the failures.
Although some periods exhibit larger effects, including particularly with
respect to the kernel modifications, the effects are small on average. Even
though only in an exploratory manner, these empirical observations contribute
to efforts to draw conclusions from large-scale and evolving software test
suites.

</details>


### [387] [DPO-F+: Aligning Code Repair Feedback with Developers' Preferences](https://arxiv.org/abs/2511.01043)
*Zihan Fang,Yifan Zhang,Yueke Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: DPO-f+是一个新颖的框架，通过直接偏好优化（DPO）增强轻量级边界信号，将代码修复反馈与开发者需求和档案对齐，显著提高代码修复准确性和反馈对齐度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在软件工程任务中应用广泛，但开发者难以理解模型输出，限制了人机协作效果。现有工作主要优化修复代码，而忽视了使开发者能够理解和迭代改进的自然语言反馈。

Method: DPO-f+框架：(1)形式化开发者档案和领域特定指标的反馈对齐；(2)从代码修复任务自动构建成对偏好数据集；(3)使用增强轻量级边界信号的直接偏好优化进行微调；(4)提供自动化反馈评估协议。

Result: 在初学者编程任务中，DPO-f+将top-1通过率比基线提高5.71个百分点，比标准DPO提高3.30个百分点。在更具挑战性的SWE-bench Lite基准测试中，问题解决率比DPO提高1.67个百分点，比基线提高4.67个百分点。在反馈对齐方面也实现了最大改进。

Conclusion: 通过更紧密地对齐反馈与开发者需求，DPO-f+将LLM辅助修复从一次性输出转变为协作理解工作流程，为增强代码理解和促进软件工程中更有效的人机协作提供了实用方法。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
tasks, especially code repair. However, developers often struggle to interpret
model outputs, limiting effective human-AI teaming. Prior work largely
optimizes repaired code while under-addressing the natural-language feedback
that enables comprehension and iterative improvement. We present DPO-f+, a
novel framework that aligns code-repair feedback with developer needs and
profiles. It (1) formalizes developer-profiled, domain-specific metrics for
feedback alignment; (2) automatically constructs pairwise preference datasets
from code-repair tasks; (3) fine-tunes using Direct Preference Optimization
(DPO) augmented with a lightweight margin signal; and (4) provides an automated
feedback evaluation protocol. Empirically, DPO-f+ outperforms both the baseline
and standard DPO on generated-code accuracy and overall feedback alignment. On
novice programming tasks, DPO-f+ raises the top-1 pass rate by 5.71 percentage
points (pp) over the baseline and by 3.30 pp over DPO. On the more challenging
SWE-bench Lite benchmark, it increases the issue-resolution rate by 1.67 pp
over DPO and by 4.67 pp over the baseline. It also achieves the largest
improvement in feedback alignment, outperforming DPO and the baseline. By
aligning feedback more closely with developer needs, DPO-f+ turns LLM-assisted
repair from one-shot outputs into a collaborative sensemaking workflow,
providing a practical approach to enhancing code comprehension and fostering
more effective human-AI teaming in software engineering.

</details>


### [388] [HAFixAgent: History-Aware Automated Program Repair Agent](https://arxiv.org/abs/2511.01047)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: HAFixAgent是一个基于仓库历史感知的自动程序修复代理系统，通过注入基于blame的仓库启发式信息来提升多文件多hunk复杂bug的修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有APR系统主要依赖本地快照上下文，忽视了仓库历史信息。研究表明仓库历史有助于修复单行bug，本文探索是否也能提升基于代理的APR系统处理复杂多hunk bug的能力。

Method: 提出了HAFixAgent系统，在修复循环中注入blame衍生的仓库启发式信息。基于Defects4J中854个真实bug的初步研究指导了系统设计。

Result: HAFixAgent显著优于基于代理的基线（提升212.3%）和多hunk基线（提升29.9%），历史信息未显著增加代理步骤，令牌成本相当，对复杂多文件多hunk bug的中间成本更低。

Conclusion: HAFixAgent为历史感知的代理APR提供了实用方案：将代理基于版本控制历史，优先使用基于diff的历史上下文，并在需要时集成互补启发式方法。

Abstract: Automated program repair (APR) has recently shifted toward large language
models and agent-based systems, yet most systems rely on local snapshot
context, overlooking repository history. Prior work shows that repository
history helps repair single-line bugs, since the last commit touching the buggy
line is often the bug-introducing one. In this paper, we investigate whether
repository history can also improve agentic APR systems at scale, especially
for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing
Agent that injects blame-derived repository heuristics into its repair loop. A
preliminary study of all 854 real-world bugs from Defects4J motivates our
design, showing that bug-relevant history is both widely available and highly
concentrated. Empirical comparison of HAFixAgent with two state-of-the-art
baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the
agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)
Efficiency: history does not significantly increase agent steps and keeps token
costs comparable, with notably lower median costs for complex
multi-file-multi-hunk bugs. (3) Practicality: combining different historical
heuristics repairs more bugs, offering a clear cost-benefit trade-off.
HAFixAgent offers a practical recipe for history-aware agentic APR: ground the
agent in version control history, prioritize diff-based historical context, and
integrate complementary heuristics when needed.

</details>


### [389] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: HarnessLLM是一个两阶段训练框架，使LLM能够编写测试代码，生成输入并验证输出，支持复杂测试用例和灵活的输出验证，在bug发现和测试策略多样性上优于基于输入-输出的测试方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动测试生成方法主要生成输入和预期输出对，测试多样性有限且无法提供足够的调试信息。

Method: 采用两阶段训练流程：首先进行监督微调(SFT)，然后使用强化学习与验证奖励(RLVR)和定制奖励设计进行训练，使LLM能够编写测试代码。

Result: 实验表明HarnessLLM在bug发现和测试策略多样性上优于基于输入-输出的测试方法，并能通过测试时扩展提升代码生成性能。

Conclusion: HarnessLLM通过生成测试代码实现了更复杂和灵活的测试，为代码生成提供了有效的推理阶段验证。

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [390] [An Empirical Study of LLM-Based Code Clone Detection](https://arxiv.org/abs/2511.01176)
*Wenqing Zhu,Norihiro Yoshida,Eunjong Choi,Yutaka Matsubara,Hiroaki Takada*

Main category: cs.SE

TL;DR: 评估大型语言模型在代码克隆检测中的跨数据集性能和响应一致性，发现模型在CodeNet数据集上表现优异但在BigCloneBench上性能显著下降，同时大多数模型具有高响应一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然证明了LLMs在代码克隆检测中的有效性，但未解决两个关键问题：LLMs在不同数据集上的可比性能能力，以及LLMs在代码克隆检测中的响应一致性。

Method: 构建了七个代码克隆数据集，评估了五个LLMs在四种现有提示下的表现。数据集通过从CodeNet和BigCloneBench两个代码集合中采样代码对，使用Levenshtein比率创建。

Result: LLMs在CodeNet相关数据集上表现良好（o3-mini达到0.943 F1分数），但在BigCloneBench相关数据集上性能显著下降。大多数模型具有高响应一致性（超过90%的判断在所有五次提交中保持一致），F1分数因不一致性产生的波动很小（变化小于0.03）。

Conclusion: LLMs在代码克隆检测中表现出数据集依赖的性能差异，但在响应一致性方面表现稳定，这为实际应用提供了重要参考。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various software engineering tasks, such as code generation and debugging,
because of their ability to translate between programming languages and natural
languages. Existing studies have demonstrated the effectiveness of LLMs in code
clone detection. However, two crucial issues remain unaddressed: the ability of
LLMs to achieve comparable performance across different datasets and the
consistency of LLMs' responses in code clone detection. To address these
issues, we constructed seven code clone datasets and then evaluated five LLMs
in four existing prompts with these datasets. The datasets were created by
sampling code pairs using their Levenshtein ratio from two different code
collections, CodeNet and BigCloneBench. Our evaluation revealed that although
LLMs perform well in CodeNet-related datasets, with o3-mini achieving a 0.943
F1 score, their performance significantly decreased in BigCloneBench-related
datasets. Most models achieved a high response consistency, with over 90\% of
judgments remaining consistent across all five submissions. The fluctuations of
the F1 score affected by inconsistency are also tiny; their variations are less
than 0.03.

</details>


### [391] [Lares: LLM-driven Code Slice Semantic Search for Patch Presence Testing](https://arxiv.org/abs/2511.01252)
*Siyuan Li,Yaowen Zheng,Hong Li,Jingdong Guo,Chaopeng Dong,Chunpeng Yan,Weijie Wang,Yimo Ren,Limin Sun,Hongsong Zhu*

Main category: cs.SE

TL;DR: Lares是一个用于补丁存在性测试的可扩展且准确的方法，通过代码切片语义搜索直接从补丁源代码提取特征，在目标二进制文件的伪代码中识别语义等效的代码切片。


<details>
  <summary>Details</summary>
Motivation: 现代软件生态系统中，1-day漏洞由于广泛的代码重用构成重大安全风险。现有方法存在可用性和准确性限制，依赖编译过程提取特征，需要大量手动工作且无法可靠区分补丁或编译变化引起的代码变更。

Method: Lares引入代码切片语义搜索，直接从补丁源代码提取特征，在目标二进制文件的伪代码中识别语义等效的代码切片。利用大语言模型进行代码分析和SMT求解器进行逻辑推理，无需编译过程。

Result: 实验结果显示Lares实现了卓越的精确度、召回率和可用性。是首个在优化级别、架构和编译器之间评估补丁存在性测试的工作。

Conclusion: Lares通过消除对编译过程的依赖提高了可用性，同时利用LLM和SMT求解器增强了准确性，为补丁存在性测试提供了有效的解决方案。

Abstract: In modern software ecosystems, 1-day vulnerabilities pose significant
security risks due to extensive code reuse. Identifying vulnerable functions in
target binaries alone is insufficient; it is also crucial to determine whether
these functions have been patched. Existing methods, however, suffer from
limited usability and accuracy. They often depend on the compilation process to
extract features, requiring substantial manual effort and failing for certain
software. Moreover, they cannot reliably differentiate between code changes
caused by patches or compilation variations. To overcome these limitations, we
propose Lares, a scalable and accurate method for patch presence testing. Lares
introduces Code Slice Semantic Search, which directly extracts features from
the patch source code and identifies semantically equivalent code slices in the
pseudocode of the target binary. By eliminating the need for the compilation
process, Lares improves usability, while leveraging large language models
(LLMs) for code analysis and SMT solvers for logical reasoning to enhance
accuracy. Experimental results show that Lares achieves superior precision,
recall, and usability. Furthermore, it is the first work to evaluate patch
presence testing across optimization levels, architectures, and compilers. The
datasets and source code used in this article are available at
https://github.com/Siyuan-Li201/Lares.

</details>


### [392] [Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation](https://arxiv.org/abs/2511.01316)
*Chong Wang,Chen Zhang,Jiajun Wu,Wunan Guo,Jianfeng Qu,Yewen Tian,Yang Liu*

Main category: cs.SE

TL;DR: 该研究探讨了使用大语言模型(LLM)进行持续集成(CI)配置迁移，特别是从Travis CI到GitHub Actions的转换。研究发现开发者平均需要阅读38行Travis配置并编写58行GitHub Actions配置，近一半迁移需要多次提交。LLM生成的翻译存在逻辑不一致、平台差异、环境错误和语法错误等问题，而结合基于指南的提示和迭代优化的策略可将构建成功率提升至75.5%。


<details>
  <summary>Details</summary>
Motivation: 持续集成是现代软件开发的关键环节，不同CI平台在维护开销、可靠性和集成深度方面的差异使得平台迁移成为常见实践。配置迁移的核心挑战在于理解CI配置的复杂性和跨平台的语义差异，而大语言模型为此提供了新的解决方案。

Method: 研究使用811个迁移记录来量化迁移工作量，分析四种LLM生成的翻译问题，并评估三种增强策略：基本提示、基于指南的提示、以及结合指南和迭代优化的方法。

Result: 开发者平均需要处理38行Travis配置和58行GitHub Actions配置，近50%迁移需要多次提交。LLM翻译存在1121个问题，分为逻辑不一致(38%)、平台差异(32%)、环境错误(25%)和语法错误(5%)。最佳策略（指南提示+迭代优化）将构建成功率提升至75.5%，相比GPT-4o基本提示提升了近三倍。

Conclusion: LLM在CI配置迁移中具有潜力但存在显著问题，通过适当的提示工程和迭代优化可以大幅提升性能。结合基于指南的提示和迭代优化的方法是最有效的策略，为自动化CI配置迁移提供了可行的解决方案。

Abstract: Continuous Integration (CI) is a cornerstone of modern collaborative software
development, and numerous CI platforms are available. Differences in
maintenance overhead, reliability, and integration depth with code-hosting
platforms make migration between CI platforms a common practice. A central step
in migration is translating CI configurations, which is challenging due to the
intrinsic complexity of CI configurations and the need to understand semantic
differences and relationships across CI platforms.
  With the advent of large language models (LLMs), recent advances in software
engineering highlight their potential for CI configuration translation. In this
paper, we present a study on LLM-based CI configuration translation, focusing
on the migration from Travis CI to GitHub Actions. First, using 811 migration
records, we quantify the effort involved and find that developers read an
average of 38 lines of Travis configuration and write 58 lines of GitHub
Actions configuration, with nearly half of the migrations requiring multiple
commits. We further analyze translations produced by each of the four LLMs and
identify 1,121 issues grouped into four categories: logic inconsistencies
(38%), platform discrepancies (32%), environment errors (25%), and syntax
errors (5%). Finally, we evaluate three enhancement strategies and show that
combining guideline-based prompting with iterative refinement achieves the best
performance, reaching a Build Success Rate of 75.5%-nearly a threefold
improvement over GPT-4o with a basic prompt.

</details>


### [393] [AI for Requirements Engineering: Industry adoption and Practitioner perspectives](https://arxiv.org/abs/2511.01324)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: 调查显示58.2%的软件从业者已在需求工程中使用AI，69.1%认为其影响积极。人机协作(HAIC)占主导地位(54.4%)，而全自动化(5.4%)和被动验证(4.4-6.2%)较少，表明AI作为协作伙伴比替代人类更有效。


<details>
  <summary>Details</summary>
Motivation: 虽然需求工程是软件工程的基础，但AI在RE中的应用研究有限，需要了解实际采用情况、挑战和机会。

Method: 调查了55名软件从业者，分析AI在四个RE阶段(获取、分析、规范、验证)和四种决策方法(纯人工、AI验证、人机协作、全自动化)中的使用情况。

Result: 58.2%的受访者已在RE中使用AI，69.1%认为影响积极；HAIC占主导(54.4%)，全自动化仅5.4%；被动AI验证落后，表明从业者更重视AI的主动支持。

Conclusion: AI在需求工程中最有效的是作为协作伙伴而非人类替代品，需要开发RE特定的HAIC框架和负责任的AI治理机制。

Abstract: The integration of AI for Requirements Engineering (RE) presents significant
benefits but also poses real challenges.Although RE is fundamental to software
engineering, limited research has examined AI adoption in RE.We surveyed 55
software practitioners to map AI usage across four RE phases:Elicitation,
Analysis, Specification, and Validation, and four approaches for decision
making: human only decisions, AI validation, Human AI Collaboration (HAIC), and
full AI automation.Participants also shared their perceptions, challenges, and
opportunities when applying AI for RE tasks.Our data show that 58.2% of
respondents already use AI in RE, and 69.1% view its impact as positive or very
positive.HAIC dominates practice, accounting for 54.4% of all RE techniques,
while full AI automation remains minimal at 5.4%.Passive AI validation (4.4 to
6.2%) lags even further behind, indicating that practitioners value AI's active
support over passive oversight.These findings suggest that AI is most effective
when positioned as a collaborative partner rather than a replacement for human
expertise.It also highlights the need for RE specific HAIC frameworks along
with robust and responsible AI governance as AI adoption in RE grows.

</details>


### [394] [The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project](https://arxiv.org/abs/2511.01348)
*Robin Gröpler,Steffen Klepke,Jack Johns,Andreas Dreschinski,Klaus Schmid,Benedikt Dornauer,Eray Tüzün,Joost Noppen,Mohammad Reza Mousavi,Yongjian Tang,Johannes Viehmann,Selin Şirin Aslangül,Beum Seuk Lee,Adam Ziolkowski,Eric Zie*

Main category: cs.SE

TL;DR: GENIUS项目旨在解决生成式AI在软件开发生命周期中应用的挑战，包括可靠性、问责制、安全性和数据隐私问题，通过开发创新工具和应对新兴研究挑战来塑造软件工程的未来。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件工程中展现出巨大潜力，但在整个软件开发生命周期中的应用尚未充分探索，存在可靠性、问责制、安全性和数据隐私等关键不确定性，需要深入研究和协调行动。

Method: 通过跨部门对话和GENIUS联盟内的经验，结合探索性文献综述，提出生成式AI在软件工程中的未来愿景，包括当前挑战的结构化概述、未来五年关键技术和方法进展、软件专业人员角色和技能需求的变化，以及GENIUS项目通过实际工具和工业验证实现这一转变的贡献。

Result: 提出了一个将技术创新与业务相关性相结合的愿景，为研究和工业战略提供信息，为软件工程团队提供可靠、可扩展且适用于工业的生成式AI解决方案奠定基础。

Conclusion: GENIUS项目通过推动AI在所有SDLC阶段的集成，解决生成式AI在软件工程中的应用挑战，旨在塑造软件工程的未来，提供可靠、可扩展的工业级生成式AI解决方案。

Abstract: Generative AI (GenAI) has recently emerged as a groundbreaking force in
Software Engineering, capable of generating code, suggesting fixes, and
supporting quality assurance. While its use in coding tasks shows considerable
promise, applying GenAI across the entire Software Development Life Cycle
(SDLC) has not yet been fully explored. Critical uncertainties in areas such as
reliability, accountability, security, and data privacy demand deeper
investigation and coordinated action. The GENIUS project, comprising over 30
European industrial and academic partners, aims to address these challenges by
advancing AI integration across all SDLC phases. It focuses on GenAI's
potential, the development of innovative tools, and emerging research
challenges, actively shaping the future of software engineering. This vision
paper presents a shared perspective on the future of GenAI-based software
engineering, grounded in cross-sector dialogue and experience within the GENIUS
consortium, supported by an exploratory literature review. The paper explores
four central elements: (1) a structured overview of current challenges in GenAI
adoption across the SDLC; (2) a forward-looking vision outlining key
technological and methodological advances expected over the next five years;
(3) anticipated shifts in the roles and required skill sets of software
professionals; and (4) the contribution of GENIUS in realizing this
transformation through practical tools and industrial validation. By aligning
technical innovation with business relevance, this paper aims to inform both
research agendas and industrial strategies, providing a foundation for
reliable, scalable, and industry-ready GenAI solutions for software engineering
teams.

</details>


### [395] [Characterizing Build Compromises Through Vulnerability Disclosure Analysis](https://arxiv.org/abs/2511.01395)
*Maimouna Tamah Diao,Moustapha Awwalou Diouf,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Gervais Mendy,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本文通过分析621个CVE漏洞披露，构建了针对软件构建过程的攻击向量分类法，发现23.8%的软件供应链攻击利用构建漏洞，其中依赖混淆和构建脚本注入是最常见的攻击方式。


<details>
  <summary>Details</summary>
Motivation: 软件构建过程是软件开发中关键但脆弱的阶段，面临多组件系统复杂性、编译期间入侵检测困难以及构建非确定性等独特安全挑战，但安全社区缺乏对构建特定攻击向量的系统性理解。

Method: 通过大规模CVE挖掘（从NVD数据库中提取621个漏洞披露），构建了基于构建管道注入点的攻击向量分类法，并分析了168个已记录的软件供应链攻击以验证分类法。

Result: 在分析的供应链攻击中，40个事件专门针对构建阶段，23.8%的供应链攻击利用构建漏洞，依赖混淆和构建脚本注入是最普遍的向量。

Conclusion: 构建过程安全是一个被低估的威胁向量，需要系统化的防御机制来应对构建特定的攻击模式。

Abstract: The software build process transforms source code into deployable artifacts,
representing a critical yet vulnerable stage in software development. Build
infrastructure security poses unique challenges: the complexity of
multi-component systems (source code, dependencies, build tools), the
difficulty of detecting intrusions during compilation, and prevalent build
non-determinism that masks malicious modifications. Despite these risks, the
security community lacks a systematic understanding of build-specific attack
vectors, hindering effective defense design.
  This paper presents an empirically-derived taxonomy of attack vectors
targeting the build process, constructed through a large-scale CVE mining (of
621 vulnerability disclosures from the NVD database). We categorize attack
vectors by their injection points across the build pipeline, from source code
manipulation to compiler compromise. To validate our taxonomy, we analyzed 168
documented software supply chain attacks, identifying 40 incidents specifically
targeting build phases. Our analysis reveals that 23.8\% of supply chain
attacks exploit build vulnerabilities, with dependency confusion and build
script injection representing the most prevalent vectors.
  Dataset available at:
https://anonymous.4open.science/r/Taxonomizing-Build-Attacks-8BB0.

</details>


### [396] [VeriODD: From YAML to SMT-LIB - Automating Verification of Operational Design Domains](https://arxiv.org/abs/2511.01417)
*Bassel Rafie,Christian Schindler,Andreas Rausch*

Main category: cs.SE

TL;DR: VeriODD是一个自动化工具，可将YAML格式的ODD/COD规范转换为可验证的SMT-LIB格式，实现自动驾驶系统操作域的形式化验证。


<details>
  <summary>Details</summary>
Motivation: 当前ODD和COD规范通常用YAML表达以便利益相关者理解，但这种描述不适合基于求解器的验证。手动转换为形式语言既慢又容易出错。

Method: 使用ANTLR编译器技术将YAML规范转换为命题逻辑和SMT-LIB格式，集成Z3等SMT求解器进行一致性检查和符合性验证，并提供图形用户界面。

Result: 开发了VeriODD工具，能够自动化转换和验证ODD/COD规范，填补了利益相关者友好表示与形式验证之间的差距。

Conclusion: VeriODD实现了操作边界的可扩展自动化保证，支持自动驾驶系统的安全验证。

Abstract: Operational Design Domains (ODDs) define the conditions under which an
Automated Driving System (ADS) is allowed to operate, while Current Operational
Domains (CODs) capture the actual runtime situation. Ensuring that a COD
instance lies within the ODD is a crucial step in safety assurance. Today, ODD
and COD specifications are frequently expressed in YAML to remain accessible
for stakeholders, but such descriptions are not directly suitable for
solver-based verification. Manual translation into formal languages such as
SMT-LIB is slow and error-prone. We present VeriODD, a tool that automates this
translation. VeriODD uses ANTLR-based compiler technology to transform
YAML-based ODD/COD specifications into both human-readable propositional logic,
for lightweight review on a simple basis, and solver-ready SMT-LIB. The tool
integrates with SMT solvers such as Z3 to provide automated consistency checks
of ODD specifications and verification of COD conformance. A graphical user
interface supports editing specifications, inspecting generated formulas, and
performing verification with a single click. VeriODD thereby closes the gap
between stakeholder-friendly ODD/COD notations and formal verification,
enabling scalable and automated assurance of operational boundaries in
autonomous driving. Video demonstration: https://youtu.be/odRacNoL_Pk Tool
available at: https://github.com/BasselRafie/VeriODD

</details>


### [397] [LLM-Assisted Tool for Joint Generation of Formulas and Functions in Rule-Based Verification of Map Transformations](https://arxiv.org/abs/2511.01423)
*Ruidi He,Yu Zhang,Meng Zhang,Andreas Rausch*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的辅助管道，用于自动生成逻辑公式和可执行谓词，以验证高精地图转换的语义正确性，减少人工工程工作量。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的验证框架依赖手动编写公式和领域特定函数，限制了可扩展性，需要更自动化的解决方案来确保地图转换的语义正确性。

Method: 使用基于提示的LLM生成方法，在计算一阶逻辑框架中联合生成语法合规的规则和谓词，并集成到CommonRoad场景设计器的地图验证器中，增加了高程支持。

Result: 在合成桥梁和斜坡场景上的评估表明，该方法在保持正确性的同时减少了人工工程工作量。

Conclusion: 证明了采用人机协作的半自动化方法进行地图转换验证的可行性，实现了可扩展的验证流程。

Abstract: High-definition map transformations are essential in autonomous driving
systems, enabling interoperability across tools. Ensuring their semantic
correctness is challenging, since existing rule-based frameworks rely on
manually written formulas and domain-specific functions, limiting scalability.
  In this paper, We present an LLM-assisted pipeline that jointly generates
logical formulas and corresponding executable predicates within a computational
FOL framework, extending the map verifier in CommonRoad scenario designer with
elevation support. The pipeline leverages prompt-based LLM generation to
produce grammar-compliant rules and predicates that integrate directly into the
existing system.
  We implemented a prototype and evaluated it on synthetic bridge and slope
scenarios. The results indicate reduced manual engineering effort while
preserving correctness, demonstrating the feasibility of a scalable,
semi-automated human-in-the-loop approach to map-transformation verification.

</details>


### [398] [Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt](https://arxiv.org/abs/2511.01529)
*Murali Sridharan,Mikel Robredo,Leevi Rantala,Matteo Esposito,Valentina Lenarduzzi,Mika Mantyla*

Main category: cs.SE

TL;DR: 该研究将超过225,000个自认技术债务(SATD)注释与其周围的源代码关联起来，发现SATD主要出现在定义、条件语句和异常处理等内联代码附近，表明这是开发者在变更过程中有意识的信号而非疏忽。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注检测和优先处理SATD，很少关注受SATD影响的源代码。本研究旨在将SATD注释与其周围的源代码构造联系起来。

Method: 利用包含9000多个Java开源软件仓库代码注释的PENTACET数据集，定量推断SATD最常见出现的位置及其最常影响的代码构造/语句。

Result: 大规模研究显示SATD主要出现在定义、条件语句和异常处理等内联代码附近，开发者在这些地方面临不确定性和权衡。

Conclusion: SATD是开发者在变更过程中有意识的信号，表明他们对技术债务的认知，而不仅仅是疏忽的表现。

Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for
proactive software maintenance. Previous research has primarily targeted
detecting and prioritizing SATD, with little focus on the source code afflicted
with SATD. Our goal in this work is to connect the SATD comments with source
code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code
comments from over 9000 Java Open Source Software (OSS) repositories. We
quantitatively infer where SATD most commonly occurs and which code
constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD
comments to their surrounding code, showing that SATD mainly arises in inline
code near definitions, conditionals, and exception handling, where developers
face uncertainty and trade-offs, revealing it as an intentional signal of
awareness during change rather than mere neglect.

</details>


### [399] [From Pre-labeling to Production: Engineering Lessons from a Machine Learning Pipeline in the Public Sector](https://arxiv.org/abs/2511.01545)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 在公共部门部署机器学习系统面临技术挑战和组织障碍，成功关键不在于模型精度突破，而在于建立透明、可复现、可问责的数据基础设施。


<details>
  <summary>Details</summary>
Motivation: 研究公共部门机器学习系统面临的独特挑战，包括技术问题（类别极度不平衡、数据漂移）和组织障碍（官僚数据访问、缺乏版本化数据集、治理不完善），探索如何在政府数字平台中构建准确、可审计且可持续的ML系统。

Method: 通过对Brasil Participativo平台的研究，分析常见工程选择（如使用LLM进行预标注、路由分类器拆分、生成合成数据）的实际效果，评估这些方法在加速开发的同时带来的可追溯性、可靠性和成本风险。

Result: 研究发现，如果不配合严格的数据治理和人工验证，常见的工程优化方法会引入新的风险。在公共部门，负责任的ML不仅是建模问题，更是制度工程问题，ML管道必须被视为公民基础设施。

Conclusion: 公共部门机器学习成功的关键不在于模型精度的突破，而在于机构能否构建透明、可复现、可问责的数据基础设施，以赢得公民信任。

Abstract: Machine learning is increasingly being embedded into government digital
platforms, but public-sector constraints make it difficult to build ML systems
that are accurate, auditable, and operationally sustainable. In practice, teams
face not only technical issues like extreme class imbalance and data drift, but
also organizational barriers such as bureaucratic data access, lack of
versioned datasets, and incomplete governance over provenance and monitoring.
Our study of the Brasil Participativo (BP) platform shows that common
engineering choices -- like using LLMs for pre-labeling, splitting models into
routed classifiers, and generating synthetic data -- can speed development but
also introduce new traceability, reliability, and cost risks if not paired with
disciplined data governance and human validation. This means that, in the
public sector, responsible ML is not just a modeling problem but an
institutional engineering problem, and ML pipelines must be treated as civic
infrastructure. Ultimately, this study shows that the success of machine
learning in the public sector will depend less on breakthroughs in model
accuracy and more on the ability of institutions to engineer transparent,
reproducible, and accountable data infrastructures that citizens can trust.

</details>


### [400] [Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for Galaxy](https://arxiv.org/abs/2511.01757)
*Shamse Tasnim Cynthia,Banani Roy*

Main category: cs.SE

TL;DR: 提出一个任务感知的两阶段检索框架，结合密集向量搜索和基于LLM的重排序，显著提升Galaxy科学工作流管理系统中工作流的检索效果。


<details>
  <summary>Details</summary>
Motivation: Galaxy现有的基于关键词的检索系统在语义查询解释方面支持有限，当缺乏精确术语匹配时往往无法找到相关的工作流。

Method: 采用两阶段检索：首先使用最先进的嵌入模型检索候选工作流，然后使用指令调优的生成式LLM（GPT-4o、Mistral-7B）基于语义任务对齐进行重排序。构建了带有语义主题注释的基准数据集，并使用LLM合成真实的任务导向查询。

Result: 该方法显著提高了top-k准确性和相关性，特别是对于长查询或未充分指定的查询。在Galaxy生态系统中进行了首次系统性检索性能评估。

Conclusion: 这项工作提升了科学工作流的可用性和可访问性，特别是对新手用户和跨学科研究人员。在Galaxy中集成了原型工具，为LLM增强的工作流搜索提供了概念验证。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become
essential infrastructure in bioinformatics, supporting the design, execution,
and sharing of complex multi-step analyses. Despite hosting hundreds of
reusable workflows across domains, Galaxy's current keyword-based retrieval
system offers limited support for semantic query interpretation and often fails
to surface relevant workflows when exact term matches are absent. To address
this gap, we propose a task-aware, two-stage retrieval framework that
integrates dense vector search with large language model (LLM)-based reranking.
Our system first retrieves candidate workflows using state-of-the-art embedding
models and then reranks them using instruction-tuned generative LLMs (GPT-4o,
Mistral-7B) based on semantic task alignment. To support robust evaluation, we
construct a benchmark dataset of Galaxy workflows annotated with semantic
topics via BERTopic and synthesize realistic task-oriented queries using LLMs.
We conduct a comprehensive comparison of lexical, dense, and reranking models
using standard IR metrics, presenting the first systematic evaluation of
retrieval performance in the Galaxy ecosystem. Results show that our approach
significantly improves top-k accuracy and relevance, particularly for long or
under-specified queries. We further integrate our system as a prototype tool
within Galaxy, providing a proof-of-concept for LLM-enhanced workflow search.
This work advances the usability and accessibility of scientific workflows,
especially for novice users and interdisciplinary researchers.

</details>


### [401] [Context-Guided Decompilation: A Step Towards Re-executability](https://arxiv.org/abs/2511.01763)
*Xiaohan Wang,Yuxin Hu,Kevin Leach*

Main category: cs.SE

TL;DR: ICL4Decomp是一个混合反编译框架，利用上下文学习指导大语言模型生成可重新执行的反编译代码，相比现有方法在可重执行性上提升约40%。


<details>
  <summary>Details</summary>
Motivation: 现有反编译技术难以生成可成功重新编译和执行的反编译代码，特别是针对优化后的二进制文件。神经反编译方法生成的代码通常只是语义上合理而非真正可执行，限制了实际可靠性。

Method: 提出ICL4Decomp混合反编译框架，利用上下文学习来指导大语言模型生成可重新执行的源代码。

Result: 在多个数据集、优化级别和编译器上评估，相比最先进的反编译方法，可重执行性提升约40%，同时保持鲁棒性。

Conclusion: ICL4Decomp通过上下文学习有效解决了编译器优化和语义线索丢失问题，显著提高了反编译代码的可执行性。

Abstract: Binary decompilation plays an important role in software security analysis,
reverse engineering, and malware understanding when source code is unavailable.
However, existing decompilation techniques often fail to produce source code
that can be successfully recompiled and re-executed, particularly for optimized
binaries. Recent advances in large language models (LLMs) have enabled neural
approaches to decompilation, but the generated code is typically only
semantically plausible rather than truly executable, limiting their practical
reliability. These shortcomings arise from compiler optimizations and the loss
of semantic cues in compiled code, which LLMs struggle to recover without
contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid
decompilation framework that leverages in-context learning (ICL) to guide LLMs
toward generating re-executable source code. We evaluate our method across
multiple datasets, optimization levels, and compilers, demonstrating around
40\% improvement in re-executability over state-of-the-art decompilation
methods while maintaining robustness.

</details>


### [402] [SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring](https://arxiv.org/abs/2511.01850)
*Jiawei Jin,Yingxin Su,Xiaotong Zhu*

Main category: cs.SE

TL;DR: 该研究提出了一种集成LLM的IDE，结合自动化MLOps管道，在单一环境中实现持续模型开发和监控，显著提升了ML工作流程的效率。


<details>
  <summary>Details</summary>
Motivation: 传统IDE主要关注代码编写，缺乏对完整ML生命周期的智能支持，而现有MLOps平台与编码工作流程脱节，需要解决这一差距。

Method: 设计了一个集成LLM助手的IDE系统，具备代码生成、调试推荐和自动管道配置功能，后端包含自动化数据验证、特征存储、漂移检测、重训练触发和CI/CD部署编排。

Result: 在UCI Adult和M5数据集上的实验表明，SmartMLOps Studio相比传统工作流程，管道配置时间减少61%，实验可重现性提高45%，漂移检测准确率提升14%。

Conclusion: 通过桥接智能代码辅助和自动化操作管道，该研究为AI工程建立了新范式，将IDE从静态编码工具转变为动态、生命周期感知的智能平台。

Abstract: The rapid expansion of artificial intelligence and machine learning (ML)
applications has intensified the demand for integrated environments that unify
model development, deployment, and monitoring. Traditional Integrated
Development Environments (IDEs) focus primarily on code authoring, lacking
intelligent support for the full ML lifecycle, while existing MLOps platforms
remain detached from the coding workflow. To address this gap, this study
proposes the design of an LLM-Integrated IDE with automated MLOps pipelines
that enables continuous model development and monitoring within a single
environment. The proposed system embeds a Large Language Model (LLM) assistant
capable of code generation, debugging recommendation, and automatic pipeline
configuration. The backend incorporates automated data validation, feature
storage, drift detection, retraining triggers, and CI/CD deployment
orchestration. This framework was implemented in a prototype named SmartMLOps
Studio and evaluated using classification and forecasting tasks on the UCI
Adult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio
reduces pipeline configuration time by 61%, improves experiment reproducibility
by 45%, and increases drift detection accuracy by 14% compared to traditional
workflows. By bridging intelligent code assistance and automated operational
pipelines, this research establishes a novel paradigm for AI engineering -
transforming the IDE from a static coding tool into a dynamic, lifecycle-aware
intelligent platform for scalable and efficient model development.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [403] [Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet](https://arxiv.org/abs/2511.00369)
*Farjana Aktar,Mohd Ruhul Ameen,Akif Islam,Md Ekramul Hamid*

Main category: cs.LG

TL;DR: 比较模糊推理方法(ANFIS-FBCSP-PSO)与深度学习基准(EEGNet)在运动想象EEG分类中的表现，发现模糊模型在个体内测试中表现更好，而深度学习模型在跨个体测试中泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 解决运动想象EEG分类中准确性和可解释性难以兼得的挑战，为BCI系统设计提供选择依据。

Method: 使用ANFIS-FBCSP-PSO（结合滤波器组共空间模式特征提取和粒子群优化的模糊IF-THEN规则）与EEGNet（直接从原始EEG数据学习层次时空表示）进行对比实验。

Result: 个体内实验：模糊神经网络模型表现更好（准确率68.58%±13.76%，kappa=58.04%±18.43）；跨个体测试：深度学习模型泛化能力更强（准确率68.20%±12.13%，kappa=57.33%±16.22）。

Conclusion: 根据设计目标（可解释性或跨用户鲁棒性）选择合适的MI-BCI系统，未来基于transformer和混合神经符号框架的研究有望推进透明EEG解码。

Abstract: Achieving both accurate and interpretable classification of motor imagery EEG
remains a key challenge in brain computer interface (BCI) research. This paper
compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep
learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS
pipeline combines filter bank common spatial pattern feature extraction with
fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet
learns hierarchical spatial temporal representations directly from raw EEG
data. In within-subject experiments, the fuzzy neural model performed better
(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),
while in cross-subject (LOSO) tests, the deep model exhibited stronger
generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent
+/- 16.22). The study provides practical guidance for selecting MI-BCI systems
according to design goals: interpretability or robustness across users. Future
investigations into transformer based and hybrid neuro symbolic frameworks are
expected to advance transparent EEG decoding.

</details>


### [404] [Fast PINN Eigensolvers via Biconvex Reformulation](https://arxiv.org/abs/2511.00792)
*Akshay Sai Banderwaar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 本文提出了一种改进的PINN方法，将特征值问题重构为双凸优化问题，通过交替凸搜索实现快速收敛，速度比传统梯度训练快500倍。


<details>
  <summary>Details</summary>
Motivation: 传统PINN方法解决特征值问题时计算速度比经典数值方法慢几个数量级，需要更高效的训练策略。

Method: 将特征对搜索重构为双凸优化问题，采用解析最优更新的交替凸搜索算法。

Result: PINN-ACS实现了高精度，收敛速度比基于梯度的PINN训练快500倍。

Conclusion: 该方法为特征值问题提供了快速、无网格的PINN解决方案，具有显著的计算效率优势。

Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are
fundamental to characterizing a system's thermal response, stability, and
natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free
alternative for solving such problems but are often orders of magnitude slower
than classical numerical schemes. In this paper, we introduce a reformulated
PINN approach that casts the search for eigenpairs as a biconvex optimization
problem, enabling fast and provably convergent alternating convex search (ACS)
over eigenvalues and eigenfunctions using analytically optimal updates.
Numerical experiments show that PINN-ACS attains high accuracy with convergence
speeds up to 500$\times$ faster than gradient-based PINN training. We release
our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.

</details>


### [405] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke Hüllermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: CLP-SNN是一种基于脉冲神经网络的在线持续学习架构，在Intel Loihi 2芯片上实现，通过事件驱动稀疏学习、自归一化学习规则和神经发生机制，在边缘设备上实现了高效且无回放的持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI设备在开放世界中面临的数据分布漂移和新类别出现的挑战，同时克服传统离线训练在功耗受限环境下的局限性。

Method: 提出CLP-SNN架构，包含三个创新：事件驱动时空稀疏局部学习、自归一化三因子学习规则、集成神经发生和元可塑性机制。

Result: 在OpenLORIS少样本学习实验中，达到与回放方法相当的准确率，同时实现70倍速度提升和5600倍能效提升。

Conclusion: 脑启发算法与神经形态硬件的协同设计能够打破传统精度-效率权衡，为未来边缘AI系统提供突破性解决方案。

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [406] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: PolyRecommender是一个多模态聚合物发现框架，结合了PolyBERT的化学语言表示和分子图编码器的图表示，通过多模态嵌入实现高效检索和稳健排序。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够整合化学语言和分子图结构知识的多模态框架，以提升聚合物发现过程的效率和鲁棒性。

Method: 首先使用基于语言的相似性检索候选聚合物，然后使用融合的多模态嵌入根据多个目标属性对候选聚合物进行排序。

Result: 通过利用两种模态中的互补知识，PolyRecommender能够在相关聚合物属性上实现高效检索和稳健排序。

Conclusion: 这项工作建立了一个可推广的多模态范式，推动了人工智能引导的下一代聚合物发现设计。

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [407] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout是一个基于深度学习的自主VR测试代理，能够像人类一样实时导航VR环境并与虚拟对象交互，用于自动化VR游戏测试。


<details>
  <summary>Details</summary>
Motivation: VR内容的质量、安全和适当性保证面临挑战，传统人工测试无法满足行业快速增长的需求，而现有自动化测试方法难以适应VR的高维感官输入和实时性能要求。

Method: 使用增强型Action Chunking Transformer从人类演示中学习，预测多步动作序列；引入动态可调滑动视界来平衡响应性和精确度；在商业VR游戏上进行评估。

Result: VRScout在有限训练数据下达到专家级性能，在消费级硬件上保持60 FPS的实时推理速度。

Conclusion: VRScout为自动化VR游戏测试提供了一个实用且可扩展的框架，可直接应用于质量保证和安全审计。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [408] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAE)特征选择和对比提示方法，在Llama-3 8B模型上实现了安全性能提升18.9%同时实用性能提升11.1%，突破了传统安全-效用权衡的限制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全部署方法需要调整模型权重且过程昂贵，而现有SAE方法缺乏系统性的特征选择方法和安全-效用权衡的评估原则。

Method: 使用稀疏自编码器(SAE)提取特征，采用创新的对比提示方法从AI-Generated Prompts Dataset和Air Bench eu-dataset中高效选择最佳特征进行引导。

Result: 在Llama-3 8B模型上，安全性能提升18.9%，同时实用性能提升11.1%。

Conclusion: 通过原则性特征选择方法识别最优特征，定向SAE引导可以克服传统安全-效用权衡问题。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [409] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: 机器遗忘技术虽然能有效移除预训练中的不良知识，但会意外产生"知识空洞"，导致良性知识的丢失。研究发现高达98.7%的测试案例在遗忘模型中出现无关或荒谬的响应，表明现有评估方法存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘技术虽然能有效移除不良内容且不影响标准基准测试性能，但可能意外造成良性知识的丢失，形成"知识空洞"，而标准基准测试无法捕捉这种隐藏问题。

Method: 提出了一个测试用例生成框架，探索遗忘内容的直接邻近区域和更广泛的潜在失败区域，以检测遗忘模型中的知识空洞问题。

Result: 评估显示遗忘存在显著隐藏成本：高达98.7%的测试案例在遗忘模型中产生无关或荒谬的响应，而这些案例在预训练模型中是能够正确回答的。

Conclusion: 需要重新思考评估机器遗忘中知识保留的传统方法，超越标准静态基准测试，以更全面地评估遗忘技术的影响。

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [410] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: 提出了Skip-Block Routing(SBR)框架，用于Transformer神经算子，通过路由机制学习token复杂度排名，在推理时根据复杂度动态分配计算资源，减少约50%计算量同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子在处理大规模工程问题时计算开销大，且统一计算成本与物理场复杂度差异不匹配，导致效率低下。

Method: SBR框架包含路由机制学习token复杂度和排名，在推理时根据排名决定后续层传递的token数量，将更多计算资源分配给复杂区域。

Result: SBR可集成到多种神经算子中，减少约50%FLOPs计算量，推理速度提升达2倍，且不牺牲精度。

Conclusion: SBR是通用框架，能有效解决神经算子计算效率问题，为大规模工程应用提供可行方案。

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [411] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出半监督偏好优化(SSPO)方法，利用少量配对偏好标签和大量未配对样本联合学习，显著降低对齐成本


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法严重依赖大量配对反馈数据，导致资源消耗巨大，需要更高效的数据利用方法

Method: 通过理论证明存在最优奖励阈值来分离胜负响应，基于此对未配对数据进行伪标注，然后利用伪标签从大规模未配对数据中提取潜在偏好

Result: 实验验证了显著的数据效率，例如在仅使用1% UltraFeedback数据时，SSPO性能超过基线方法使用10%数据的效果

Conclusion: SSPO能够有效降低人类对齐成本，同时保持对齐质量，为偏好优化提供了更高效的数据利用方案

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [412] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出了DynBERG模型，将Graph-BERT与GRU结合，用于动态金融交易网络的欺诈检测，特别针对比特币交易数据，在Dark Market Shutdown事件前后表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-BERT模型主要针对静态图和无向边，而金融交易网络是动态的且有向的，需要能够捕捉时间演化的模型。

Method: 集成Graph-BERT与GRU层来捕捉多时间步的时间演化，并修改算法支持有向边，适用于动态金融交易分析。

Result: 在Elliptic数据集上评估，DynBERG在Dark Market Shutdown事件前后均优于EvolveGCN和GCN，消融实验证明GRU组件对建模时间动态至关重要。

Conclusion: DynBERG能够有效适应市场重大变化，为动态金融交易网络中的欺诈检测提供了有效解决方案。

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [413] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: 提出了一种利用时空结构的自监督学习框架，通过图神经网络和时空适应机制改进多变量天气预报性能。


<details>
  <summary>Details</summary>
Motivation: 由于大气系统固有的时空复杂性，准确稳健的天气预报仍然是一个基本挑战。

Method: 整合图神经网络进行空间推理，采用自监督预训练方案进行表示学习，并加入时空适应机制以增强不同预报时长的泛化能力。

Result: 在ERA5和MERRA-2再分析数据集上的广泛实验表明，该方法在性能上优于传统数值天气预报模型和最近的深度学习方法。

Conclusion: 该框架为未来数据驱动的天气预报系统提供了一个可扩展且标签高效的解决方案。

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [414] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: 提出了FLoRA方法，一种融合前向-后向适配器的参数高效微调技术，结合了LoRA和并行适配器的优势，在保持相似参数预算的同时显著提升了精度并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模日益增长，参数高效微调(PEFT)变得尤为重要。虽然已有多种PEFT方法被研究，但该领域仍有很大探索空间。

Method: FLoRA通过融合前向和后向适配器(FFBA)，结合LoRA和并行适配器的思想，将适配器融合到基础模型的投影层中以最小化延迟。

Result: 实验结果表明，在相似参数预算下，FLoRA在精度和延迟方面都显著优于常用的LoRA方法。

Conclusion: FLoRA为LLMs的下游任务微调提供了一种高效的参数高效微调方法，在性能和效率方面都有显著提升。

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [415] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: 本文分析了DoRA方法的成功机制，发现其通过增加权重更新矩阵的奇异值熵来提升性能，但存在计算开销大的问题。作者提出了更高效的矩阵形式，并基于此开发了Pre-Diag和SORA两种新方法，在性能和效率上都优于LoRA和DoRA。


<details>
  <summary>Details</summary>
Motivation: DoRA方法虽然性能优异但机制不明确且计算开销大，需要理解其成功原理并开发更高效的参数高效微调方法。

Method: 1) 识别DoRA成功机制是增加权重更新矩阵的奇异值熵；2) 将DoRA重构为数学等价但更高效的矩阵形式；3) 提出统一框架，开发Pre-Diag和SORA两种新方法。

Result: 在自然语言理解和生成任务上的实验表明，提出的方法在性能和效率上都优于LoRA和DoRA。

Conclusion: 通过理解DoRA的机制并开发新的参数高效微调方法，可以在保持性能的同时显著提升效率。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [416] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了一种新颖的二次形式加权训练目标，通过考虑标签自相关效应和设置异质任务权重来改进时间序列预测模型的训练。


<details>
  <summary>Details</summary>
Motivation: 现有的训练目标（如均方误差）将每个未来步骤视为独立、等权重的任务，这导致两个问题：(1) 忽略未来步骤间的标签自相关效应，导致有偏的训练目标；(2) 未能为不同未来步骤设置异质任务权重，限制了预测性能。

Method: 提出二次形式直接预测（QDF）学习算法，使用自适应更新的二次形式加权矩阵训练预测模型。加权矩阵的非对角线元素考虑标签自相关效应，非均匀对角线元素匹配不同未来步骤的最优权重。

Result: 实验表明，QDF有效提高了各种预测模型的性能，达到了最先进的结果。

Conclusion: 通过考虑标签自相关效应和设置异质任务权重，QDF训练目标能够显著改进时间序列预测模型的性能。

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [417] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: 提出了SpatialTraceGen框架，通过蒸馏大型教师模型的推理过程来生成高质量的多步骤、多工具推理轨迹数据集，解决了视觉语言模型在复杂空间推理任务中缺乏高质量训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂空间推理方面表现不佳，需要问题分解和策略性工具使用。微调更小、更易部署的模型是提高性能的有效途径，但缺乏高质量的分步推理数据成为了主要瓶颈。

Method: 开发了SpatialTraceGen框架，通过自动化验证器将大型教师模型的推理过程蒸馏成多步骤、多工具的推理轨迹数据集。验证器可扩展地确保每个推理步骤的保真度，替代了昂贵的人工标注。

Result: 在CLEVR-Humans基准测试中，验证器引导的过程使轨迹的平均质量得分提高了17%，同时质量方差降低了40%以上。生成了包含专家轨迹的数据集。

Conclusion: SpatialTraceGen提供了结构化、分步的工具使用示例，为有效的微调和样本高效的离线强化学习提供了必要的数据支持。

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [418] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutiérrez Hermosillo Muriedas,Markus Götz,Judith Sáínz-Pardo Díaz,Álvaro López García,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: 该论文研究了联邦学习在无人机热成像图像分割任务中的实际应用效果，比较了多种FL方法与集中式学习的性能差异。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和技术限制，无人机采集的热成像数据无法集中存储，联邦学习能够在不共享原始数据的情况下训练共享模型，适合这种分布式数据场景。

Method: 在真实部署场景中评估联邦学习算法，比较多种FL方法与集中式学习基线，分析模型精度、训练时间、通信开销和能耗等关键指标，并探索客户端控制和工作流与服务器控制工作流。

Result: 研究评估了FL在无人机热成像特征检测中的实际效果，为理解FL方法在分割任务中的实际应用和局限性提供了有价值的参考。

Conclusion: 联邦学习在无人机热成像图像分割任务中具有实际应用价值，能够有效处理分布式数据，但需要权衡性能与通信开销等实际限制。

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [419] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: 提出MISA方法，通过模块级重要性采样来优化大语言模型训练，相比层级优化能更有效地减少内存使用并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练和微调需要大量内存，现有层级优化方法虽然能节省内存但忽略了层内模块的重要性差异，且内存节省有限。

Method: 将每层划分为更小的模块，为每个模块分配重要性分数，使用加权随机采样机制激活模块，可证明比层级采样减少梯度方差。

Result: 建立了在非凸和随机条件下的O(1/√K)收敛率，内存分析显示MISA优于现有基线方法，多个学习任务的实验验证了有效性。

Conclusion: MISA通过模块级重要性采样实现了更高效的内存优化和更好的性能表现，为大语言模型训练提供了新的优化思路。

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [420] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: 本文提出了一种基于决策树的自动化方法，用于识别和解释OthelloGPT模型中编码游戏规则的MLP神经元，发现约一半神经元可被紧凑的规则化决策树准确描述，并通过干预实验验证了这些模式的重要性。


<details>
  <summary>Details</summary>
Motivation: OthelloGPT模型为可解释性研究提供了理想测试平台，其复杂度足以展现丰富的计算模式，同时基于规则的游戏逻辑使得逆向工程具有意义。

Method: 使用回归决策树将棋盘状态映射到神经元激活，提取高激活决策路径并转换为人类可读的逻辑形式，通过针对性干预验证因果相关性。

Result: 发现第5层约913个神经元（共2048个）可被紧凑的规则化决策树准确描述（R² > 0.7），其余神经元可能参与更分布式或非规则化计算。干预实验显示特定模式对应的神经元被消除后，模型预测合法移动的能力下降5-10倍。

Conclusion: 该方法能有效识别和解释神经网络中编码规则逻辑的神经元，为可解释性研究提供了实用工具和验证框架。

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [421] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: 该研究探讨了预训练模型的不同层如何反映大脑对语言的分层处理过程，通过比较wav2vec2和CLIP模型的嵌入与脑电图信号的对应关系。


<details>
  <summary>Details</summary>
Motivation: 受大脑从原始声学到丰富多模态关联的分层意义构建过程启发，研究旨在了解预训练模型的哪些层最能反映大脑的这种分层处理机制。

Method: 使用自然语音感知期间记录的EEG数据，通过岭回归和对比解码评估wav2vec2和CLIP模型的嵌入与大脑活动的对齐程度，测试了三种策略：单层、渐进拼接和渐进求和。

Result: 研究发现结合多模态、层感知的表征可能更接近解码大脑如何理解语言——不仅是声音，更是体验。

Conclusion: 多模态层感知表征的组合可能让我们更接近理解大脑如何将语言处理为体验而不仅仅是声音。

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [422] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: 提出了一种无需显式域标签的领域泛化方法，通过潜在域聚类和自适应知识迁移来提升视觉语言模型在域偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法依赖可能不可用或模糊的域标签，因此研究无需显式域标签的DG设置，使模型能在域偏移下保持鲁棒性。

Method: 在图像特征上进行潜在域聚类，并根据输入图像与每个潜在域的相似度融合域特定文本特征，实现跨域知识自适应迁移。

Result: 在四个基准测试上的实验表明，该方法相比基于VLM的基线模型获得了持续的性能提升。

Conclusion: 该策略为在域偏移下提高模型鲁棒性提供了新的见解，证明了无需域标签的领域泛化方法的有效性。

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [423] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型作为生成优化器在解决约束多目标回归任务中的表现，特别是在逆设计领域。研究表明，虽然专门的贝叶斯优化框架在保证收敛性方面表现最佳，但微调后的LLMs是计算速度快且有前景的替代方案。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在约束、连续、高维数值空间任务中的实用性，这些任务并非LLMs明确设计的领域。研究旨在验证LLMs在材料信息学中逆设计问题的有效性。

Method: 进行了贝叶斯优化框架与微调LLMs/BERT模型的比较研究。BO方面测试了BoTorch Ax和qEHVI，生成方法采用参数高效微调，将问题构建为带有自定义输出头的回归问题。

Result: BoTorch qEHVI实现了完美收敛（GD=0.0），设定了性能上限。表现最佳的LLM（WizardMath-7B）达到GD=1.21，显著优于传统BoTorch Ax基线（GD=15.03）。

Conclusion: 专门的BO框架在保证收敛性方面仍是性能领导者，但微调后的LLMs被验证为有前景、计算速度快的替代方案，为AI驱动优化领域提供了重要的比较指标。

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [424] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: 提出Fix-GCN模型，通过固定点迭代图卷积网络实现对抗攻击的鲁棒性，利用谱调制滤波器选择性衰减高频分量，保留低频结构信息。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击对图神经网络构成严重威胁，特别是在图结构和节点特征易受操纵的任务中，需要开发鲁棒的防御机制。

Method: 引入通用谱调制滤波器，通过固定点迭代推导特征传播规则，实现高阶邻域信息捕获，无需额外内存或计算复杂度。

Result: 在多个基准图数据集上的广泛实验证明了模型的有效性，展示了其对对抗攻击的弹性。

Conclusion: Fix-GCN提供了一个灵活高效的框架，在保留基本图信息的同时减轻对抗操纵的影响。

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [425] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta Śliwa*

Main category: cs.LG

TL;DR: 该论文研究使用序数回归技术自动预测笔纸RPG游戏中怪物的挑战等级，以替代目前耗时的手动评估方法。


<details>
  <summary>Details</summary>
Motivation: 笔纸RPG市场快速增长，公司寻求AI技术提升玩家体验。目前怪物等级设计依赖手动测试和专家评估，耗时且资源密集，需要自动化解决方案。

Method: 使用序数回归技术，构建专用数据集，开发人类启发模型作为基准，设计基于领域知识的专业评估程序来比较机器学习算法与传统方法。

Result: 论文提供了最先进方法的概述和评估，建立了用于等级估计的专用数据集，并开发了基准模型用于性能比较。

Conclusion: 该研究为笔纸RPG游戏中的怪物等级预测提供了自动化解决方案，通过专门的评估框架实现了机器学习算法与传统方法的有效比较。

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [426] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: 提出MaGNet模型，一种基于Mamba双超图的股票预测网络，通过三个创新模块有效捕捉时间依赖性和动态股票间关系，在六个主要股票指数上表现出优越的预测性能和投资回报。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉时间依赖性和动态股票间交互，往往忽略横截面市场影响、依赖静态相关性、对节点和边采用统一处理，以及混淆不同关系。

Method: MaGNet包含三个关键创新：(1) MAGE块，使用双向Mamba进行上下文时间建模，集成稀疏专家混合层适应不同市场条件；(2) 特征维和股票维2D时空注意力模块，精确融合多变量特征和跨股票依赖；(3) 双超图框架，包括捕获细粒度因果依赖的时间-因果超图和建模市场范围模式的全局概率超图。

Result: 在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面均优于现有最先进方法，并具有稳健的风险管理能力。

Conclusion: MaGNet通过整合创新的时间建模、注意力机制和双超图框架，有效解决了股票趋势预测中的挑战，在多个指标上实现了优越性能。

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [427] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 提出Agent-REINFORCE框架，通过LLM代理增强的强化学习来搜索测试时扩展中的最优多LLM协作图，解决组合搜索空间大和任务特定需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法通常假设固定的协作架构和单一模型使用，忽略了最优架构和模型组合会因任务而异的问题。

Method: 将问题形式化为概率图优化，提出Agent-REINFORCE框架，通过采样-反馈-更新流程，使用文本反馈作为梯度来更新概率图。

Result: 实验表明Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，能有效识别准确性和推理延迟联合目标下的最优图。

Conclusion: 该方法能有效解决测试时扩展中多LLM协作图的优化问题，为不同任务找到计算最优的模型组合和架构。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [428] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了GraphKeeper方法来解决图域增量学习中的灾难性遗忘问题，通过知识解缠和保持来应对嵌入漂移和决策边界偏差


<details>
  <summary>Details</summary>
Motivation: 现有的图增量学习方法主要关注单域内的任务增量和类别增量场景，而图域增量学习在多图域间更新模型的需求随着图基础模型的发展变得至关重要，但尚未被探索

Method: GraphKeeper方法包括：1) 域特定参数高效微调及域内域间解缠目标；2) 无偏差知识保持来稳定决策边界；3) 对不可观测域的图进行域感知分布判别以获得精确嵌入

Result: 在广泛实验中，GraphKeeper取得了最先进的结果，相比第二名有6.5%~16.6%的提升，且遗忘可忽略不计。该方法能与各种代表性图基础模型无缝集成

Conclusion: GraphKeeper通过知识解缠和保持有效解决了图域增量学习中的灾难性遗忘问题，展现了广泛的应用潜力

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [429] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: 提出了一种基于条件标签生成对抗网络的无监督损伤检测和数字孪生方法，无需系统健康状态的先验信息，在Z24桥梁基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的数字孪生方法在测量数据少、物理知识缺失或损伤状态未知时预测效果不佳，需要开发不依赖先验信息的损伤检测框架。

Method: 使用条件标签生成对抗网络，通过将不同损伤级别的测量数据作为输入，强制模型收敛到不同损伤状态，比较收敛分数来识别损伤状态差异。

Result: 该方法能够准确捕获健康测量数据中的损伤，为基于振动的系统级监测和可扩展基础设施韧性提供了强大工具。

Conclusion: 所提出的无监督框架在结构健康监测中表现出优越的损伤检测和数字孪生能力，特别是在缺乏先验信息的情况下。

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [430] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: Loquetier是一个虚拟化的多LoRA框架，将LoRA微调和推理统一在单一运行时中，通过虚拟化模块和优化计算流实现高效的多适配器服务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在统一LoRA微调和推理方面存在差距，需要一种能够同时支持高效微调和推理的框架。

Method: 提出虚拟化模块隔离PEFT修改并支持多适配器共享基础模型，设计优化计算流将微调和推理路径合并以减少内核调用开销。

Result: 在三个任务设置上的实验表明，Loquetier在性能和灵活性上均优于现有基线，推理任务吞吐量达到最先进系统的3倍，统一任务SLO达成率比PEFT高46.4倍。

Conclusion: Loquetier成功统一了LoRA微调和推理，提供了高效灵活的多适配器服务解决方案。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [431] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: 提出了一种混合框架，从噪声轨迹数据中自动发现守恒量，结合了神经ODE、Transformer和符号-数值验证器三个组件。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中识别守恒定律是科学进步的关键，但目前从轨迹数据中发现这些不变量仍具有挑战性。

Method: 1) 神经ODE学习系统动力学的连续模型；2) Transformer基于学习到的向量场生成符号候选不变量；3) 符号-数值验证器为候选不变量提供强数值验证。

Result: 在典型物理系统上测试表明，该框架显著优于直接在轨迹数据上操作的基线方法。

Conclusion: 证明了分离的'先学习后搜索'方法在从不完美数据中发现数学原理方面的鲁棒性。

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [432] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: Pelican-VL 1.0是一个开源具身大脑模型家族，参数规模从70亿到720亿，是目前最大规模的开源具身多模态大脑模型，在具身基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 将强大的智能嵌入到各种具身系统中，实现智能与物理世界的深度融合。

Method: 采用DPPO（刻意练习策略优化）框架，通过metaloop（RL-精炼-诊断-SFT循环）训练模型，从包含40亿+token的原始数据集中蒸馏出高质量数据集。

Result: 相比基础模型性能提升20.3%，超越100B级别开源模型10.6%，在1000+ A800 GPU集群上训练，消耗超过50k A800 GPU小时。

Conclusion: Pelican-VL 1.0在具身智能领域达到了领先水平，与专有系统性能相当，为开源具身智能发展提供了重要基础。

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


### [433] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 该研究评估稀疏自编码器(SAEs)在识别和控制LLM中种族与污名化概念关联的能力，发现SAEs能识别与黑人相关的潜在特征，但通过SAE调控来减轻偏见在复杂临床任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用存在加剧现有偏见的风险，需要开发方法来检测模型是否错误地依赖患者种族信息进行预测。

Method: 使用稀疏自编码器识别Gemma-2模型中与黑人个体相关的潜在特征，并通过激活这些潜在特征来引导模型输出，评估其对偏见的影响。

Result: 发现与黑人相关的潜在特征在合理输入(如"非裔美国人")和问题词汇(如"监禁")上都会被激活，激活该特征会增加模型将患者预测为"好斗"的风险。

Conclusion: SAEs可作为识别LLM中问题性人口统计依赖的有用工具，但通过SAE调控来减轻偏见在现实复杂临床任务中效果有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [434] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: LC-Opt是一个用于强化学习控制策略的可持续液体冷却基准环境，基于橡树岭国家实验室Frontier超级计算机的高保真数字孪生模型构建，支持多目标实时优化和可解释控制。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载的增加，高密度数据中心的液体冷却对热管理至关重要。基于机器学习的控制器对于实现更高的能源效率和可靠性、促进可持续性至关重要。

Method: 基于Modelica的端到端模型，从站点级冷却塔到数据中心机柜和服务器刀片组。通过Gymnasium接口，RL代理优化关键热控制参数，如液体供应温度、流量和阀门调节，以及冷却塔设定点。

Result: 基准测试了集中式和分散式多智能体RL方法，演示了将策略提炼为决策树和回归树以实现可解释控制，并探索了基于LLM的方法通过自然语言解释控制动作。

Conclusion: LC-Opt为ML社区、运营商和供应商提供了详细、可定制的液体冷却模型，使他们能够开发可持续的数据中心液体冷却控制解决方案。

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [435] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 研究发现LLM在校准方面具有内在能力，通过分析网络深度发现上层存在置信度校正阶段，并识别出残差流中的低维校准方向，扰动该方向可显著改善校准指标而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究发现深度神经网络通常过度自信，但LLM表现出预测概率与正确性良好对齐的内在校准能力。本研究旨在从网络深度演化的角度补充理解LLM的校准机制。

Method: 在MMLU基准上分析多个开源权重模型，研究校准如何在整个网络深度中演化，识别上层/后期层中的置信度校正阶段，并发现残差流中的低维校准方向。

Result: 发现上层存在明显的置信度校正阶段，模型在达到决策确定性后主动重新校准置信度；识别出残差流中的低维校准方向，扰动该方向可显著改善ECE和MCE等校准指标，同时不影响准确性。

Conclusion: 校准是一种分布式现象，在整个网络前向传播过程中形成，而不仅仅在最终投影层，这为理解LLM内部置信度调节机制提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [436] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: DCcluster-Opt是一个开源的高保真仿真基准，用于可持续的地理时间任务调度，结合真实世界数据集和物理信息模型，支持多目标优化研究。


<details>
  <summary>Details</summary>
Motivation: 大规模AI的能源需求和碳足迹不断增加，但缺乏能够真实捕捉环境因素、数据中心物理和网络动态相互作用的基准，限制了可持续计算的进展。

Method: 结合真实世界数据集（AI工作负载、电网碳强度、电力市场、天气等）与物理信息模型，提供模块化奖励系统和Gymnasium API，支持强化学习和基于规则的策略。

Result: 创建了一个具有挑战性的调度问题环境，能够显式研究碳排放、能源成本、服务等级协议和用水之间的权衡关系。

Conclusion: DCcluster-Opt通过提供真实、可配置且易于使用的测试平台，加速了地理分布式数据中心下一代可持续计算解决方案的开发和验证。

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [437] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 提出Pivot-Aware Speculative Decoding方法，通过识别关键令牌（pivot tokens）来放宽传统推测解码的严格分布匹配要求，在保持任务性能的同时显著提高接受率，实现最高2.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码要求输出分布与目标模型完全匹配，导致接受率过低，限制了加速潜力。作者认为实际应用中任务性能比采样分布更重要，需要重新定义解码目标。

Method: 提出Pivot-Aware Speculative Decoding策略，仅拒绝会导致最终输出性能下降的关键令牌。训练轻量级分类器识别关键令牌，作为传统推测解码的宽松版本。

Result: 在多个数据集上评估，该方法在保持可比性能的同时实现了最高2.5倍的加速效果。

Conclusion: 通过放宽分布匹配要求，专注于任务性能匹配，可以显著提高推测解码的效率，为实际应用提供更好的加速方案。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [438] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: 本文使用统计物理学中的交叉涨落分析扩散模型的采样动态，发现样本会经历尖锐的离散转变，形成目标分布。这些转变可通过交叉涨落检测，从而提升采样效率、加速条件生成，并改进零样本任务。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型中采样动态的演变过程，特别是样本从初始分布到目标分布的转变机制，以理解生成过程中的结构形成。

Method: 使用统计物理学中的交叉涨落（centered-moment statistic）分析采样动态，推导方差保持SDE的交叉涨落闭式解，检测样本转变过程。

Result: 发现样本经历离散转变，检测这些转变可提升采样效率、加速条件生成，改进图像分类和风格迁移等零样本任务性能。

Conclusion: 该框架统一了离散马尔可夫链理论与连续动力学，连接了相变分析和现代生成建模，为理解扩散模型采样过程提供了新视角。

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [439] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC框架通过对比学习构建共享表示空间，选择最优推理方法，在提高准确性的同时降低计算开销


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选响应并使用聚合策略选择答案，假设更多候选答案能带来更高准确性，但这一假设需要重新审视

Method: 引入EPIC框架，通过理论分析推导标准聚合方法的准确性边界，使用对比学习构建共享表示空间，结合概率边界作为正则化器进行效用驱动优化

Result: 在多样化数学推理任务上的实验表明，EPIC能持续选择最优推理方法，提高准确性同时减少计算开销

Conclusion: EPIC框架有效解决了语言模型生成中选择合适推理方法的挑战，在准确性和计算效率之间取得了良好平衡

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [440] [Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features](https://arxiv.org/abs/2511.00126)
*Lu Bowen*

Main category: cs.LG

TL;DR: 提出动态多专家门控框架，通过内部模型信号自适应选择最佳轨迹预测器，在nuPlan-mini数据集上实现FDE 2.567m，比GameFormer提升9.5%


<details>
  <summary>Details</summary>
Motivation: 现有深度轨迹预测器在复杂长尾驾驶场景中不可靠，而简单物理模型偶尔表现更好，需要超越"一刀切"的单一模型范式

Method: 基于内部模型信号（稳定性和不确定性）的动态多专家门控框架，将轨迹专家选择构建为成对排序问题，包含物理LSTM、Transformer和微调GameFormer三个专家

Result: 在nuPlan-mini数据集上FDE为2.567m，比GameFormer提升9.5%，达到oracle性能的57.8%；左转场景FDE降低约10%

Conclusion: 自适应混合系统能提升安全关键自动驾驶中的轨迹可靠性，为超越静态单一模型范式提供了实用路径

Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,
2022) have achieved strong average accuracy but remain unreliable in complex
long-tail driving scenarios. These limitations reveal the weakness of the
prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban
contexts where simpler physics-based models can occasionally outperform
advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic
multi-expert gating framework that adaptively selects the most reliable
trajectory predictor among a physics-informed LSTM, a Transformer, and a
fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability
and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be
substantially more informative than geometric scene descriptors. To the best of
our knowledge, this is the first work to formulate trajectory expert selection
as a pairwise-ranking problem over internal model signals (Burges et al.,
2005), directly optimizing decision quality without requiring post-hoc
calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287
samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error
(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835
m), and realizes 57.8 percent of the oracle performance bound. In open-loop
simulations, after trajectory horizon alignment, the same configuration reduces
FDE on left-turn scenarios by approximately 10 percent, demonstrating
consistent improvements across both offline validation and open-loop
evaluation. These results indicate that adaptive hybrid systems enhance
trajectory reliability in safety-critical autonomous driving, providing a
practical pathway beyond static single-model paradigms.

</details>


### [441] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的贝叶斯框架来解释大语言模型的提示控制和激活控制，认为两者都是通过改变模型对潜在概念的信念来影响行为。


<details>
  <summary>Details</summary>
Motivation: 现有的提示控制（上下文学习）和激活控制（激活引导）方法看似不同，但都旨在控制模型行为，作者希望建立一个统一的理论框架来解释这两种方法。

Method: 从贝叶斯视角构建预测性理论框架，将上下文学习视为证据积累过程，将激活引导视为改变概念先验，并建立封闭形式的贝叶斯模型。

Result: 该模型能准确预测LLM在多种干预下的行为，解释了S型学习曲线等现象，并预测了新的现象如对数信念空间中的干预可加性。

Conclusion: 这项工作为基于提示和基于激活的LLM行为控制提供了统一的理论解释，并提供了预测这些干预效果的经验方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [442] [Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells](https://arxiv.org/abs/2511.00129)
*Siyu Xiao,Xindi Zhao,Tianhao Mao,Yiwei Wang,Yuqiao Chen,Hongyun Zhang,Jian Wang,Junjie Wang,Shuang Liu,Tupei Chen,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种集成到井下工具中的CCL信号采集系统，用于构建数据集，并评估了数据增强预处理方法在基于AlexNet的套管接箍识别模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 准确的井下深度测量对油气井作业至关重要，而基于神经网络的CCL信号识别在套管接箍识别方面取得了进展，但预处理方法和真实井数据有限的问题仍未解决。

Method: 提出了综合预处理方法进行数据增强，包括标准化、标签分布平滑、随机裁剪等，并使用基于AlexNet的神经网络模型评估这些方法的有效性。

Result: 实验结果表明，标准化、标签分布平滑和随机裁剪是模型训练的基本要求，而标签平滑正则化、时间缩放和多重采样显著提高了模型泛化能力，两个基准模型的F1分数从0.937和0.952最大提升至1.0。

Conclusion: 这项工作解决了在CCL数据有限环境中训练套管接箍识别模型时数据增强方法的不足，验证了所提方法的有效性和实际适用性。

Abstract: Accurate downhole depth measurement is essential for oil and gas well
operations, directly influencing reservoir contact, production efficiency, and
operational safety. Collar correlation using a casing collar locator (CCL) is
fundamental for precise depth calibration. While neural network-based CCL
signal recognition has achieved significant progress in collar identification,
preprocessing methods for such applications remain underdeveloped. Moreover,
the limited availability of real well data poses substantial challenges for
training neural network models that require extensive datasets. This paper
presents a system integrated into downhole tools for CCL signal acquisition to
facilitate dataset construction. We propose comprehensive preprocessing methods
for data augmentation and evaluate their effectiveness using our AlexNet-based
neural network models. Through systematic experimentation across various
configuration combinations, we analyze the contribution of each augmentation
method. Results demonstrate that standardization, label distribution smoothing
(LDS), and random cropping are fundamental requirements for model training,
while label smoothing regularization (LSR), time scaling, and multiple sampling
significantly enhance model generalization capability. The F1 scores of our two
benchmark models trained with the proposed augmentation methods maximumly
improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance
validation on real CCL waveforms confirms the effectiveness and practical
applicability of our approach. This work addresses the gaps in data
augmentation methodologies for training casing collar recognition models in CCL
data-limited environments.

</details>


### [443] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: 提出了一种集成概率特征采样和模拟退火超参数调优的新型随机森林增强框架，在多个领域显著提升预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 克服传统随机森林的局限性，更有效地从数据中捕捉相关信号，实现自适应超参数配置。

Method: 采用概率特征采样和模拟退火算法进行超参数调优，强调对分类更有意义的特征，结合重要性感知采样和元启发式优化。

Result: 在多个应用领域（信用风险评估、物联网异常检测、早期医疗诊断、高维生物数据分析）中展现出持续准确率提升和特征相关性洞察。

Conclusion: 结合重要性感知采样和元启发式优化的方法有效提升了随机森林分类器的性能，为复杂分类问题提供了有力解决方案。

Abstract: This paper introduces a novel framework for enhancing Random Forest
classifiers by integrating probabilistic feature sampling and hyperparameter
tuning via Simulated Annealing. The proposed framework exhibits substantial
advancements in predictive accuracy and generalization, adeptly tackling the
multifaceted challenges of robust classification across diverse domains,
including credit risk evaluation, anomaly detection in IoT ecosystems,
early-stage medical diagnostics, and high-dimensional biological data analysis.
To overcome the limitations of conventional Random Forests, we present an
approach that places stronger emphasis on capturing the most relevant signals
from data while enabling adaptive hyperparameter configuration. The model is
guided towards features that contribute more meaningfully to classification and
optimizing this with dynamic parameter tuning. The results demonstrate
consistent accuracy improvements and meaningful insights into feature
relevance, showcasing the efficacy of combining importance aware sampling and
metaheuristic optimization.

</details>


### [444] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: HeraldLight是一个基于双LLM架构的交通信号控制系统，通过Herald引导提示增强，解决了现有LLM方法固定信号时长和幻觉错误的问题，以及RL方法缺乏鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在交通信号控制中存在固定信号时长和幻觉错误的问题，而传统RL方法在信号时序决策中缺乏鲁棒性和泛化能力。

Method: 提出HeraldLight双LLM架构：Herald模块提取上下文信息并预测各交通相位队列长度；LLM-Agent基于预测进行细粒度信号控制；LLM-Critic精炼Agent输出，纠正错误和幻觉；使用基于分数的微调提高准确性和鲁棒性。

Result: 在CityFlow模拟实验中，使用济南(12)、杭州(16)和纽约(196)共224个交叉口的真实数据集，HeraldLight在所有场景中平均旅行时间减少20.03%，在济南和杭州场景中平均队列长度减少10.74%。

Conclusion: HeraldLight通过双LLM架构和Herald引导提示，显著提升了交通信号控制的性能，优于现有最先进方法。

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [445] [EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs](https://arxiv.org/abs/2511.00192)
*Ali Satvaty,Suzan Verberne,Fatih Turkmen*

Main category: cs.LG

TL;DR: 提出了实体级成员推理攻击（EL-MIA）框架，用于评估LLM中敏感信息的成员风险，发现现有MIA方法在实体级敏感属性推理方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 现有MIA方法只能检测整个提示或文档是否在训练数据中，无法捕捉更细粒度的风险，特别是在敏感信息（PII、信用卡号等）的实体级别成员风险发现方面。

Method: 提出了EL-MIA框架，构建了用于评估MIA方法的基准数据集，系统比较了现有MIA技术和两种新提出的方法，分析了实体级MIA易感性与模型规模、训练轮次等因素的关系。

Result: 发现现有MIA方法在实体级敏感属性成员推理方面存在局限，但这种易感性可以通过相对简单的方法来识别，表明需要更强的对抗方法来测试威胁模型。

Conclusion: 需要开发更强的对抗方法来充分测试LLM隐私威胁模型，现有MIA方法在实体级敏感信息成员推理方面存在不足。

Abstract: Membership inference attacks (MIA) aim to infer whether a particular data
point is part of the training dataset of a model. In this paper, we propose a
new task in the context of LLM privacy: entity-level discovery of membership
risk focused on sensitive information (PII, credit card numbers, etc). Existing
methods for MIA can detect the presence of entire prompts or documents in the
LLM training data, but they fail to capture risks at a finer granularity. We
propose the ``EL-MIA'' framework for auditing entity-level membership risks in
LLMs. We construct a benchmark dataset for the evaluation of MIA methods on
this task. Using this benchmark, we conduct a systematic comparison of existing
MIA techniques as well as two newly proposed methods. We provide a
comprehensive analysis of the results, trying to explain the relation of the
entity level MIA susceptability with the model scale, training epochs, and
other surface level factors. Our findings reveal that existing MIA methods are
limited when it comes to entity-level membership inference of the sensitive
attributes, while this susceptibility can be outlined with relatively
straightforward methods, highlighting the need for stronger adversaries to
stress test the provided threat model.

</details>


### [446] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出了FEval-TTC评估协议，用于确保测试时计算方法的公平评估，不受LLM性能和API成本波动的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能和API调用成本会随时间波动，这可能使先前研究的结论失效。

Method: 设计了FEval-TTC协议，标准化少样本提示和答案提取过程，支持跨多个LLM在数学和常识推理数据集上的评估，并提供成本建模程序。

Result: 开发了开源评估框架，减少了研究者的时间和金钱开销，便于公平比较不同的测试时计算方法。

Conclusion: FEval-TTC为测试时计算方法的评估提供了标准化和公平的基准，有助于更可靠的研究比较。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [447] [Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides](https://arxiv.org/abs/2511.00209)
*Yiquan Wang,Yahui Ma,Yuhan Chang,Jiayao Yan,Jialin Zhang,Minnuo Cai,Kai Wei*

Main category: cs.LG

TL;DR: 扩散模型在药物发现中应用于小分子和肽类治疗药物的设计，通过统一的去噪框架适应不同的分子表示和设计目标，但面临化学可合成性、生物稳定性等挑战。


<details>
  <summary>Details</summary>
Motivation: 利用扩散模型加速和改变传统缓慢昂贵的药物发现过程，系统比较其在两种主要治疗模式（小分子和肽类）中的应用。

Method: 采用迭代去噪的统一框架，针对不同分子表示、化学空间和设计目标进行适应。小分子设计关注结构基础设计，肽类设计关注功能序列和从头结构设计。

Result: 扩散模型在小分子设计中能生成新颖的配体，但面临化学可合成性挑战；在肽类设计中能生成功能序列，但面临生物稳定性、正确折叠和免疫原性挑战。

Conclusion: 扩散模型的全部潜力需要通过弥合模式特定差距并将其整合到自动化DBTL平台中来实现，从而将范式从化学探索转向靶向创建新型治疗药物。

Abstract: Diffusion models have emerged as a leading framework in generative modeling,
showing significant potential to accelerate and transform the traditionally
slow and costly process of drug discovery. This review provides a systematic
comparison of their application in designing two principal therapeutic
modalities: small molecules and therapeutic peptides. We analyze how a unified
framework of iterative denoising is adapted to the distinct molecular
representations, chemical spaces, and design objectives of each modality. For
small molecules, these models excel at structure-based design, generating
novel, pocket-fitting ligands with desired physicochemical properties, yet face
the critical hurdle of ensuring chemical synthesizability. Conversely, for
therapeutic peptides, the focus shifts to generating functional sequences and
designing de novo structures, where the primary challenges are achieving
biological stability against proteolysis, ensuring proper folding, and
minimizing immunogenicity. Despite these distinct challenges, both domains face
shared hurdles: the need for more accurate scoring functions, the scarcity of
high-quality experimental data, and the crucial requirement for experimental
validation. We conclude that the full potential of diffusion models will be
unlocked by bridging these modality-specific gaps and integrating them into
automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby
shifting the paradigm from chemical exploration to the targeted creation of
novel therapeutics.

</details>


### [448] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: RLAC是一种后训练方法，通过动态评分标准验证来解决开放式生成任务中的评估难题，使用LLM作为批评家动态识别最可能的失败模式，联合优化生成器和批评家。


<details>
  <summary>Details</summary>
Motivation: 开放式生成任务需要满足多样且隐式的评估标准，但大量相关标准导致验证成本过高和不完整评估，使得基于标准的强化学习后训练难以扩展。

Method: 使用大型语言模型作为批评家动态识别最可能的失败模式（如事实错误或未处理边缘情况），然后通过外部验证器验证，联合优化生成器和批评家。

Result: 实验表明RLAC在文本生成中提高了事实准确性，在代码生成中提高了正确性，同时优于穷举验证和奖励模型方法。

Conclusion: 动态批评家比固定批评家更有效，展示了RLAC在将强化学习后训练扩展到自由形式生成任务方面的潜力。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [449] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA是一种新的参数高效微调方法，通过随机初始化全秩适配器、ReZero门控和迭代幅度剪枝来减少灾难性遗忘，相比QLoRA在GSM8k上表现出更少的遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新任务微调时出现的灾难性遗忘问题，现有PEFT方法如LoRA存在秩约束限制，稀疏适应提供了不施加秩约束的替代方案。

Method: RIGSA方法：从随机初始化的全秩适配器开始，使用ReZero类似的门控机制，并通过迭代幅度剪枝进行稀疏化。

Result: 在SmolLM2-1.7B-Instruct模型上测试，RIGSA能够学习新的Textual MNIST任务，且相比QLoRA在GSM8k上表现出更少的遗忘，尽管与随机掩码性能相当。

Conclusion: RIGSA作为一种稀疏适应方法，在减少灾难性遗忘方面优于QLoRA，特别是对GSM8k任务，为参数高效微调提供了有前景的替代方案。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [450] [A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data](https://arxiv.org/abs/2511.00318)
*Dana Kim,Yichen Xu,Tiffany Lin*

Main category: cs.LG

TL;DR: 提出了一种结合模型协变量合成和因果结构保持的混合框架，用于生成保留因果参数的合成表格数据，解决了现有方法在保持平均处理效应方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和GAN方法生成的合成表格数据虽然预测保真度高，但往往无法保持关键的因果参数如平均处理效应，限制了在因果分析中的应用。

Method: 采用混合生成框架：模型基协变量合成（通过距离过滤监控）+ 分别学习的倾向得分和结果模型，确保(W, A, Y)三元组保留底层因果结构，并引入合成配对策略缓解正性违反。

Result: 提出的方法能够生成保留因果结构的合成数据，并通过无限制的合成样本为传统估计器（IPTW、AIPW、替代）在复杂协变量分布下的性能评估提供了现实基准。

Conclusion: 这项工作为支持稳健因果分析的LLM驱动数据管道奠定了基础，代码已在GitHub开源。

Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic
tabular data, yet existing approaches often fail to preserve key causal
parameters such as the average treatment effect (ATE). In this technical
exploration, we first demonstrate that state-of-the-art synthetic data
generators, both GAN- and LLM-based, can achieve high predictive fidelity while
substantially misestimating causal effects. To address this gap, we propose a
hybrid generation framework that combines model-based covariate synthesis
(monitored via distance-to-closest-record filtering) with separately learned
propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain
their underlying causal structure. We further introduce a synthetic pairing
strategy to mitigate positivity violations and a realistic evaluation protocol
that leverages unlimited synthetic samples to benchmark traditional estimators
(IPTW, AIPW, substitution) under complex covariate distributions. This work
lays the groundwork for LLM-powered data pipelines that support robust causal
analysis. Our code is available at
https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.

</details>


### [451] [Toward Unifying Group Fairness Evaluation from a Sparsity Perspective](https://arxiv.org/abs/2511.00359)
*Zhecheng Sheng,Jiawei Zhang,Enmao Diao*

Main category: cs.LG

TL;DR: 提出基于稀疏性的统一框架来评估算法公平性，通过稀疏度视角连接不同公平标准，并在多种数据集和偏置缓解方法上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有公平性标准缺乏跨机器学习问题的通用性，需要统一的评估框架来促进算法公平性研究。

Method: 研究各种稀疏度度量在促进公平性方面的联系与差异，提出基于稀疏度的统一公平性评估框架。

Result: 该框架与现有公平性标准一致，在多种数据集和偏置缓解方法上表现出广泛适用性和有效性。

Conclusion: 通过稀疏性和社会公平视角为算法公平性研究提供了新思路，对公平性研究和应用具有广泛影响潜力。

Abstract: Ensuring algorithmic fairness remains a significant challenge in machine
learning, particularly as models are increasingly applied across diverse
domains. While numerous fairness criteria exist, they often lack
generalizability across different machine learning problems. This paper
examines the connections and differences among various sparsity measures in
promoting fairness and proposes a unified sparsity-based framework for
evaluating algorithmic fairness. The framework aligns with existing fairness
criteria and demonstrates broad applicability to a wide range of machine
learning tasks. We demonstrate the effectiveness of the proposed framework as
an evaluation metric through extensive experiments on a variety of datasets and
bias mitigation methods. This work provides a novel perspective to algorithmic
fairness by framing it through the lens of sparsity and social equity, offering
potential for broader impact on fairness research and applications.

</details>


### [452] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: 提出UME-R1生成式多模态嵌入框架，通过两阶段训练策略统一嵌入任务到生成范式，显著超越传统判别式嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多模态嵌入模型本质上是判别式的，无法受益于推理驱动的生成范式，限制了其性能潜力。

Method: 采用两阶段训练：监督微调赋予模型推理能力，使其能生成判别式和生成式嵌入；强化学习进一步优化生成式嵌入质量。

Result: 在MMEB-V2基准测试的78个任务中显著优于传统判别式嵌入模型，生成式嵌入相比判别式嵌入带来显著性能提升。

Conclusion: 生成式嵌入通过利用MLLM强大的生成推理能力解锁了性能增益，为更可解释、推理驱动的生成式多模态嵌入奠定了基础。

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [453] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: 提出Gradient-Guided Sampling (GGS)方法，通过梯度引导采样平衡对抗攻击的利用性（攻击强度）和探索性（跨模型泛化能力），解决传统方法在这两个目标间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在跨模型迁移场景中面临利用性（最大化攻击强度）和探索性（增强跨模型泛化）的基本权衡困境。传统动量方法过度偏重利用性，而最近的内迭代采样方法过度偏重探索性。

Method: 基于MI-FGSM，引入内迭代随机采样，并使用前一次内迭代的梯度来引导采样方向，采样幅度由随机分布决定。这种机制鼓励对抗样本位于既平坦（利于泛化）又具有较高局部最大值（强攻击强度）的平衡区域。

Result: 在多个DNN架构和多模态大语言模型上的综合实验表明，该方法在迁移攻击方面优于现有最先进方法。

Conclusion: GGS方法通过梯度引导采样有效平衡了对抗攻击的利用性和探索性，在保持强攻击强度的同时提高了跨模型泛化能力。

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [454] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: BOOM是一个将规划与离策略学习紧密集成的强化学习框架，通过引导循环和联合学习的世界模型解决规划导致的数据与策略行为不一致问题。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，使用规划进行环境交互会导致收集的数据与策略实际行为不一致，这会降低模型学习和策略改进的效果。

Method: 提出BOOM框架，通过引导循环将规划器和策略紧密集成：策略初始化规划器，规划器通过行为对齐引导策略。使用联合学习的世界模型支持规划器模拟未来轨迹并提供价值目标。核心是使用规划器的非参数化动作分布引导策略的无似然对齐损失，结合软价值加权机制优先处理高回报行为。

Result: 在高维DeepMind Control Suite和Humanoid-Bench上的实验表明，BOOM在训练稳定性和最终性能方面达到了最先进的结果。

Conclusion: BOOM框架通过紧密集成规划和离策略学习，有效解决了规划导致的数据与策略行为不一致问题，在复杂环境中表现出优异的性能。

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [455] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 该研究提出了一种基于ROI引导掩码策略的静息态fMRI基础模型，相比传统随机掩码方法，在ADHD分类任务上准确率提升了4.23%。


<details>
  <summary>Details</summary>
Motivation: 随着大规模脑成像数据集的增加，需要开发能够有效泛化到不同下游任务的fMRI基础模型。现有基于随机掩码的自监督学习方法缺乏对大脑解剖结构的考虑。

Method: 使用AAL3图谱进行ROI引导的掩码策略，在4D fMRI数据上选择性掩码语义一致的大脑区域，进行自监督预训练。

Result: 在ADHD-200数据集（973名受试者）上，该方法相比传统随机掩码在ADHD分类准确率上提升4.23%。归因分析显示边缘系统和脑区对重建保真度和模型表示贡献最大。

Conclusion: 在模型预训练中掩码解剖区域不仅能增强可解释性，还能产生更鲁棒和判别性的表示。未来将扩展到更多神经影像数据集并开发基于区域感知重建目标的新损失函数。

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [456] [Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima](https://arxiv.org/abs/2511.00469)
*Zhongxiang Lei,Qi Yang,Ping Qiu,Gang Zhang,Yuanchi Ma,Jinyan Liu*

Main category: cs.LG

TL;DR: 本文从理论角度解释了联邦学习在数据异构情况下性能下降的原因，指出客户端数据的异质性会导致不同的局部最优解，从而提高了全局目标的下界并导致模型在训练后期振荡。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习算法虽然在理论和实践中都能保证收敛，但在数据异构情况下性能下降的原因尚不明确，本文旨在填补这一理论空白。

Method: 提出假设：异构客户端数据会导致不同的局部最优解，并基于此分析两个关键后果：1) 客户端局部最优解之间的距离提高了全局目标的下界；2) 在训练后期，全局模型会在一个区域内振荡而非收敛到单一最优解。

Result: 理论分析表明数据异构确实会导致性能下降，并通过多个任务和神经网络架构的实验验证了这一结论。

Conclusion: 本文为联邦学习在非独立同分布数据下的性能下降提供了理论解释，相关框架已在GitHub开源。

Abstract: Federated optimization is a constrained form of distributed optimization that
enables training a global model without directly sharing client data. Although
existing algorithms can guarantee convergence in theory and often achieve
stable training in practice, the reasons behind performance degradation under
data heterogeneity remain unclear. To address this gap, the main contribution
of this paper is to provide a theoretical perspective that explains why such
degradation occurs. We introduce the assumption that heterogeneous client data
lead to distinct local optima, and show that this assumption implies two key
consequences: 1) the distance among clients' local optima raises the lower
bound of the global objective, making perfect fitting of all client data
impossible; and 2) in the final training stage, the global model oscillates
within a region instead of converging to a single optimum, limiting its ability
to fully fit the data. These results provide a principled explanation for
performance degradation in non-iid settings, which we further validate through
experiments across multiple tasks and neural network architectures. The
framework used in this paper is open-sourced at:
https://github.com/NPCLEI/fedtorch.

</details>


### [457] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*Dragoş-Andrei Şerban,Răzvan-Alexandru Smădu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: 本文旨在设计和评估多种机器学习模型来预测PM2.5浓度，包括线性回归、集成方法、深度学习模型和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: PM2.5空气污染对健康造成严重影响，预测未来PM2.5水平可以提供早期预警并帮助预防疾病。

Method: 设计、微调、测试和评估多种机器学习模型，包括线性回归算法、集成方法、深度学习模型（如循环神经网络和变换器）以及大型语言模型。

Result: 未在摘要中明确说明具体结果。

Conclusion: 通过比较多种模型的性能，为PM2.5预测任务提供有效的解决方案。

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [458] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 提出了一种基于单智能体强化学习的区域自适应交通信号控制框架，使用DreamerV3世界模型高效学习控制策略，通过集中决策简化多智能体协调复杂性，在仿真中显著减少了队列长度并表现出良好的抗波动能力。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵严重影响城市生活质量，传统交通信号控制模型难以捕捉真实交通复杂性。多智能体系统存在协调复杂性问题，需要更有效的解决方案。

Method: 采用单智能体强化学习框架，使用邻接矩阵统一编码路网拓扑、实时队列状态和信号配时参数。利用DreamerV3世界模型学习控制策略，动作序列选择交叉口并调整信号相位配时。

Result: 在SUMO仿真实验中，面对10%-30%的起讫点需求波动，该框架表现出强大的抗波动能力，显著减少了队列长度。

Conclusion: 建立了一种与探测车辆技术兼容的智能交通控制新范式，为缓解交通拥堵提供了有效解决方案。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [459] [Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales](https://arxiv.org/abs/2511.00552)
*Santhi Bharath Punati,Sandeep Kanta,Udaya Bhasker Cheerala,Madhusudan G Lanjewar,Praveen Damacharla*

Main category: cs.LG

TL;DR: 使用时间融合变换器(TFT)对沃尔玛周销售额进行多周期预测，融合静态商店标识符和动态外部信号，在1-5周预测中表现出色，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 准确的零售多周期预测对库存管理和促销活动至关重要，需要融合多种静态和动态因素来提升预测精度。

Method: 采用时间融合变换器(TFT)模型，结合静态商店标识符和动态外部信号(节假日、CPI、燃料价格、温度)，通过分位数损失生成概率预测，并提供可解释性分析。

Result: 在2012年固定测试集上，TFT实现RMSE为57.9k美元/商店-周，R²为0.9875；在5折时序交叉验证中，平均RMSE为64.6k美元，R²为0.9844，优于XGB、CNN、LSTM等基线模型。

Conclusion: 该方法在库存规划和节假日优化方面具有实际应用价值，同时保持了模型透明度，为零售预测提供了有效的解决方案。

Abstract: Accurate multi-horizon retail forecasts are critical for inventory and
promotions. We present a novel study of weekly Walmart sales (45 stores,
2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store
identifiers with time-varying exogenous signals (holidays, CPI, fuel price,
temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via
Quantile Loss, yielding calibrated 90\% prediction intervals and
interpretability through variable-selection networks, static enrichment, and
temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of
\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold
chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ =
0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These
results demonstrate practical value for inventory planning and holiday-period
optimization, while maintaining model transparency.

</details>


### [460] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: 提出了一个轻量级黑盒红队测试方法，使用现成LLM通过迭代反馈和上下文学习来发现激活探针的脆弱性模式，无需微调或梯度访问。


<details>
  <summary>Details</summary>
Motivation: 激活探针作为AI系统监控器具有低成本低延迟优势，但其在真实世界黑盒对抗压力下的鲁棒性尚未充分探索，需要发现故障模式。

Method: 使用现成LLM包装的轻量级黑盒红队测试流程，结合迭代反馈和上下文学习，无需微调、梯度或架构访问。

Result: 在高风险交互探针案例研究中，发现了可解释的脆弱性模式（如法律术语导致的误报、平淡程序化语调导致的漏报），以及在场景约束攻击下持续存在的漏洞。

Conclusion: 简单的提示式红队测试框架可以在部署前预测故障模式，并为未来探针加固提供有前景的可操作见解。

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [461] [FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction](https://arxiv.org/abs/2511.00564)
*Varun Teja Chirukiri,Udaya Bhasker Cheerala,Sandeep Kanta,Abdul Karim,Praveen Damacharla*

Main category: cs.LG

TL;DR: 提出FTT-GRU混合模型，结合快速时序Transformer和GRU，在NASA CMAPSS数据集上实现剩余使用寿命预测，性能优于现有最佳深度基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如LSTM、CNN）难以同时建模多元传感器数据的全局时序依赖和细粒度退化趋势，需要更有效的模型架构。

Method: 使用快速时序Transformer（基于FFT的线性化注意力）与GRU层结合的混合模型，首次在CMAPSS数据集上应用FTT与GRU组合。

Result: 在CMAPSS FD001上达到RMSE 30.76、MAE 18.97、R²=0.45，CPU延迟1.12ms，相比TCN-Attention基线提升RMSE 1.16%、MAE 4.00%。

Conclusion: 紧凑的Transformer-RNN混合架构能够准确高效地预测剩余使用寿命，适用于实时工业预测性维护。

Abstract: Accurate prediction of the remaining useful life (RUL) of industrial
machinery is essential for reducing downtime and optimizing maintenance
schedules. Existing approaches, such as long short-term memory (LSTM) networks
and convolutional neural networks (CNNs), often struggle to model both global
temporal dependencies and fine-grained degradation trends in multivariate
sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal
Transformer (FTT) -- a lightweight Transformer variant using linearized
attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU)
layer for sequential modeling. To the best of our knowledge, this is the first
application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling
simultaneous capture of global and local degradation patterns in a compact
architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and
$R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published
deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%.
Training curves averaged over $k=3$ runs show smooth convergence with narrow
95\% confidence bands, and ablations (GRU-only, FTT-only) support the
contribution of both components. These results demonstrate that a compact
Transformer-RNN hybrid delivers accurate and efficient RUL predictions on
CMAPSS, making it suitable for real-time industrial prognostics.

</details>


### [462] [Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation](https://arxiv.org/abs/2511.00588)
*Dong Chen,Yanzhe Wei,Zonglin He,Guan-Ming Kuang,Canhua Ye,Meiru An,Huili Peng,Yong Hu,Huiren Tao,Kenneth MC Cheung*

Main category: cs.LG

TL;DR: 本研究提出了一个临床医生中心的框架来量化大语言模型在脊柱外科中的幻觉风险，评估了六个领先的LLM在30个专家验证的脊柱病例中的表现。DeepSeek-R1表现最佳，研究发现推理增强模型变体并不总是优于标准版本，多维压力测试暴露了模型特定的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在脊柱外科临床决策支持中具有变革潜力，但存在幻觉风险，可能危及患者安全，需要量化评估这些风险。

Method: 引入临床医生中心框架，从诊断精度、推荐质量、推理稳健性、输出连贯性和知识对齐五个维度评估幻觉风险，测试六个领先LLM在30个专家验证的脊柱病例中的表现。

Result: DeepSeek-R1表现最佳（总分：86.03±2.08），在创伤和感染等高风险领域表现优异。推理增强模型变体并不总是优于标准版本，多维压力测试显示推荐质量在复杂性增加时下降7.4%。

Conclusion: 研究主张将可解释性机制（如推理链可视化）整合到临床工作流程中，并为外科LLM部署建立安全感知的验证框架。

Abstract: Large language models (LLMs) offer transformative potential for clinical
decision support in spine surgery but pose significant risks through
hallucinations, which are factually inconsistent or contextually misaligned
outputs that may compromise patient safety. This study introduces a
clinician-centered framework to quantify hallucination risks by evaluating
diagnostic precision, recommendation quality, reasoning robustness, output
coherence, and knowledge alignment. We assessed six leading LLMs across 30
expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall
performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes
domains such as trauma and infection. A critical finding reveals that
reasoning-enhanced model variants did not uniformly outperform standard
counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed
relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92),
indicating extended chain-of-thought reasoning alone is insufficient for
clinical reliability. Multidimensional stress-testing exposed model-specific
vulnerabilities, with recommendation quality degrading by 7.4% under amplified
complexity. This decline contrasted with marginal improvements in rationality
(+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning
divergence between perceived coherence and actionable guidance. Our findings
advocate integrating interpretability mechanisms (e.g., reasoning chain
visualization) into clinical workflows and establish a safety-aware validation
framework for surgical LLM deployment.

</details>


### [463] [TRISKELION-1: Unified Descriptive-Predictive-Generative AI](https://arxiv.org/abs/2511.00711)
*Nardeep Kumar,Arun Kanwar*

Main category: cs.LG

TL;DR: TRISKELION-1是一个统一描述-预测-生成架构，在单一编码器-解码器框架中整合统计、机制和生成推理。


<details>
  <summary>Details</summary>
Motivation: 构建能够连接可解释性、准确性和创造性的通用智能架构蓝图。

Method: 使用变分目标联合优化描述性表示学习、预测推理和生成合成，在MNIST数据集上进行验证。

Result: 实验证明描述性重建、预测分类和生成采样可以在一个模型中稳定共存。

Conclusion: 该框架为实现整合可解释性、准确性和创造性的通用智能架构提供了方向。

Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that
integrates statistical, mechanistic, and generative reasoning within a single
encoder-decoder framework. The model demonstrates how descriptive
representation learning, predictive inference, and generative synthesis can be
jointly optimized using variational objectives. Experiments on MNIST validate
that descriptive reconstruction, predictive classification, and generative
sampling can coexist stably within one model. The framework provides a
blueprint toward universal intelligence architectures that connect
interpretability, accuracy, and creativity.

</details>


### [464] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: PREPO方法通过利用内在数据特性提高RLVR的数据效率，包含两个互补组件：使用提示困惑度作为模型适应性指标，以及通过区分相对熵来放大rollout差异，从而减少rollout需求并保持竞争性能。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然提升了大型语言模型的推理能力，但训练成本高昂，因为许多rollout对优化的贡献很小。本研究旨在利用内在数据特性来改善RLVR的数据效率。

Method: 提出PREPO方法：1) 使用提示困惑度作为模型适应性指标，让模型从易到难学习；2) 通过区分相对熵来放大rollout差异，优先选择探索程度更高的序列。

Result: 在Qwen和Llama模型上，PREPO在数学推理基准测试中仅需基线方法1/3的rollout数量就能达到有效结果。

Conclusion: PREPO通过两个互补机制显著提高了RLVR的数据效率，不仅取得了实证收益，还提供了理论分析来解释方法背后的原理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [465] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: 该论文分析了预训练Transformer在微调过程中的输出饱和问题，提出了诊断指标来识别梯度抑制的拐点层，并采用选择性LoRA注入策略来恢复梯度信号。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在微调时容易对源域模式过度自信，难以形成新的目标域模式，这源于输出饱和导致的梯度抑制问题。

Method: 提出了层间诊断指标（注意力熵、激活梯度范数、参数梯度范数、Delta-CKA），识别拐点层，并选择性注入LoRA适配器来恢复梯度信号。

Result: 实验表明，在过训练初始化下，拐点层LoRA注入能提升性能；而在欠训练初始化下，需要全路径解阻塞才能实现低层重构。

Conclusion: 当基础特征强时，解阻塞拐点层有助于高层组合适应；当基础特征弱时，需要全路径解阻塞来实现低层重构。

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [466] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: 提出逻辑信息强化学习（LIRL），通过逻辑投影确保混合动作空间中每个探索步骤的可行性，无需惩罚调优，在多个CPS场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在CPS的混合动作空间优化中存在全局最优性不足、约束保证困难等问题，需要一种能保证约束满足且无需复杂调优的方法。

Method: 为标准策略梯度算法配备投影机制，将低维潜在动作映射到由一阶逻辑定义的可行混合流形上，确保每个探索步骤的可行性。

Result: 在工业制造、电动汽车充电站和交通信号控制等场景中，LIRL均优于现有分层优化方法，在机器人减速器装配系统中使完工时间-能耗目标最多减少36.47%-44.33%，且始终保持零约束违反。

Conclusion: LIRL通过声明式逻辑约束公式化，可无缝迁移到其他领域，为大规模CPS的安全实时优化铺平道路。

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [467] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO是一种新颖的安全强化学习算法，结合了基于Kronecker分解近似曲率的二阶策略优化和安全感知的梯度操作，在保持安全约束的同时实现了更高的平均回报。


<details>
  <summary>Details</summary>
Motivation: 解决安全强化学习中奖励最大化与约束满足之间的权衡问题，避免传统方法中固定硬阈值导致的突然变化和有害干扰。

Method: 使用K-FAC高效近似Fisher信息矩阵进行自然梯度更新；引入边缘感知梯度操作机制，根据智能体接近安全边界的程度自适应调整奖励和成本梯度的影响；采用小批量级别的KL回滚策略确保信任区域合规。

Result: 在Safety Gymnasium环境中的实验表明，KFCPO相比最佳基线方法实现了10.3%到50.2%更高的平均回报，同时遵守安全约束。

Conclusion: KFCPO在安全性和性能之间实现了优越的平衡，通过二阶优化和自适应梯度操作有效解决了安全强化学习中的关键挑战。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [468] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出了一种基于范畴对称性的学习框架，通过将时间、尺度和传感器层次结构的变化因素融入特征表示结构，使模型在现实失真下保持稳定，显著提升了人类活动识别的分布外准确率。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别面临传感器信号随上下文、运动和环境变化的挑战，需要模型在周围世界变化时保持稳定。

Method: 构建范畴对称感知学习框架，将时间、尺度和传感器层次结构的变化因素嵌入特征表示结构，利用范畴等变表示理论自动保持传感器间关系。

Result: 在UCI人类活动识别基准测试中，分布外准确率提高了约46个百分点（约3.6倍于基线）。

Conclusion: 抽象对称原理可以通过范畴等变表示理论转化为日常感知任务中的具体性能提升。

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [469] [The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks](https://arxiv.org/abs/2511.00958)
*Khoat Than*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，从容量控制的角度解释归一化在深度神经网络中的作用，证明归一化能指数级降低Lipschitz常数，从而平滑损失景观并约束网络容量。


<details>
  <summary>Details</summary>
Motivation: 归一化方法在现代深度神经网络中至关重要，但它们在优化和泛化方面的理论机制尚未得到充分解释，特别是在使用多个归一化层时。

Method: 开发了一个理论框架，通过分析未归一化DNN的Lipschitz常数，并与归一化后的网络进行对比，证明归一化层能指数级降低Lipschitz常数。

Result: 归一化能指数级降低Lipschitz常数，这平滑了损失景观（促进更快更稳定的优化）并约束了网络的有效容量（增强泛化保证）。

Conclusion: 该研究为归一化方法在深度学习中的经验成功提供了理论解释，表明归一化通过指数级降低Lipschitz常数来同时改善优化和泛化性能。

Abstract: Normalization methods are fundamental components of modern deep neural
networks (DNNs). Empirically, they are known to stabilize optimization dynamics
and improve generalization. However, the underlying theoretical mechanism by
which normalization contributes to both optimization and generalization remains
largely unexplained, especially when using many normalization layers in a DNN
architecture.
  In this work, we develop a theoretical framework that elucidates the role of
normalization through the lens of capacity control. We prove that an
unnormalized DNN can exhibit exponentially large Lipschitz constants with
respect to either its parameters or inputs, implying excessive functional
capacity and potential overfitting. Such bad DNNs are uncountably many. In
contrast, the insertion of normalization layers provably can reduce the
Lipschitz constant at an exponential rate in the number of normalization
operations. This exponential reduction yields two fundamental consequences: (1)
it smooths the loss landscape at an exponential rate, facilitating faster and
more stable optimization; and (2) it constrains the effective capacity of the
network, thereby enhancing generalization guarantees on unseen data. Our
results thus offer a principled explanation for the empirical success of
normalization methods in deep learning.

</details>


### [470] [Using Synthetic Data to estimate the True Error is theoretically and practically doable](https://arxiv.org/abs/2511.00964)
*Hai Hoang Thanh,Duy-Tung Nguyen,Hung The Tran,Khoat Than*

Main category: cs.LG

TL;DR: 提出了一种使用合成数据来估计训练模型测试误差的方法，通过理论推导和实验验证，在有限标注数据条件下实现了更准确可靠的模型评估。


<details>
  <summary>Details</summary>
Motivation: 传统模型评估方法需要大量标注测试集，但在实际应用中获取大规模标注数据成本高昂且耗时，因此需要探索在有限标注数据条件下的可靠评估方法。

Method: 开发了考虑合成数据的泛化边界理论，提出了基于理论指导的优化合成数据生成方法，用于模型测试误差估计。

Result: 在仿真和表格数据集上的实验结果表明，相比现有基线方法，该方法能够获得更准确和可靠的测试误差估计。

Conclusion: 合成数据可以有效地用于模型评估，生成器质量对评估效果有重要影响，提出的理论指导方法在有限标注数据条件下具有实用价值。

Abstract: Accurately evaluating model performance is crucial for deploying machine
learning systems in real-world applications. Traditional methods often require
a sufficiently large labeled test set to ensure a reliable evaluation. However,
in many contexts, a large labeled dataset is costly and labor-intensive.
Therefore, we sometimes have to do evaluation by a few labeled samples, which
is theoretically challenging. Recent advances in generative models offer a
promising alternative by enabling the synthesis of high-quality data. In this
work, we make a systematic investigation about the use of synthetic data to
estimate the test error of a trained model under limited labeled data
conditions. To this end, we develop novel generalization bounds that take
synthetic data into account. Those bounds suggest novel ways to optimize
synthetic samples for evaluation and theoretically reveal the significant role
of the generator's quality. Inspired by those bounds, we propose a
theoretically grounded method to generate optimized synthetic data for model
evaluation. Experimental results on simulation and tabular datasets demonstrate
that, compared to existing baselines, our method achieves accurate and more
reliable estimates of the test error.

</details>


### [471] [Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms](https://arxiv.org/abs/2511.01061)
*Przemysław Spyra,Witold Dzwinel*

Main category: cs.LG

TL;DR: 该研究挑战了反向传播(BP)对实现最先进性能必不可少的传统假设，提出了无需反向传播的Mono-Forward(MF)算法，在MLP架构上超越优化BP基线，实现更高分类准确率、更低能耗和更快训练。


<details>
  <summary>Details</summary>
Motivation: 挑战反向传播作为实现最先进性能必需方法的传统假设，探索更高效、可持续的替代方案。

Method: 从Hinton的Forward-Forward(FF)算法演进到Cascaded Forward(CaFo)，最终提出Mono-Forward(MF)算法，在相同架构和超参数优化框架下进行公平比较。

Result: MF算法在分类准确率上持续超越优化BP基线，能耗降低41%，训练速度提升34%，但无BP方法的内存效率优势在实践中可能被开销抵消。

Conclusion: MF算法被确立为MLP中实用、高性能且可持续的BP替代方案。

Abstract: The long-held assumption that backpropagation (BP) is essential for
state-of-the-art performance is challenged by this work. We present rigorous,
hardware-validated evidence that the Mono-Forward (MF) algorithm, a
backpropagation-free method, consistently surpasses an optimally tuned BP
baseline in classification accuracy on its native Multi-Layer Perceptron (MLP)
architectures. This superior generalization is achieved with profound
efficiency gains, including up to 41% less energy consumption and up to 34%
faster training. Our analysis, which charts an evolutionary path from Geoffrey
Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF,
is grounded in a fair comparative framework using identical architectures and
universal hyperparameter optimization. We further provide a critical
re-evaluation of memory efficiency in BP-free methods, empirically
demonstrating that practical overhead can offset theoretical gains. Ultimately,
this work establishes MF as a practical, high-performance, and sustainable
alternative to BP for MLPs.

</details>


### [472] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: ATLAS是一个双代理架构，通过分离推理（教师）和执行（学生）功能，实现无需梯度更新的持续学习，在推理时动态调整操作策略，在微软ExCyTIn-Bench基准测试中表现优于GPT-5（High）13%，同时降低86%成本。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法依赖基于梯度的重新训练，不适合需要实时适应的部署代理。需要一种无需梯度更新的持续学习方法来支持实时自适应系统。

Method: 采用双代理架构：教师负责推理，学生负责执行，结合持久学习记忆存储经验提炼的指导，通过编排层在推理时动态调整操作策略（如监督级别或初始计划选择）。

Result: 在微软ExCyTIn-Bench基准测试中，ATLAS使用GPT-5-mini作为学生，达到54.1%的成功率，比GPT-5（High）高出13%，同时成本降低86%。跨事件验证显示，从事件#5冻结的小册子将准确率从28%提升至41%，无需重新训练。

Conclusion: 梯度无关的持续学习是实现自适应、可部署AI系统的可行路径，将适应焦点从模型参数转移到系统级编排，实现自适应效率：在推理时通过编排最大化任务成功同时最小化计算成本。

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [473] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: 比较门控循环单元、长短期记忆网络和卷积神经网络在动态结构载荷识别中的性能，并与基于物理的残差卡尔曼滤波器进行对比，发现在不同载荷场景下各方法表现各异。


<details>
  <summary>Details</summary>
Motivation: 解决土木工程应用中由于测试数据少或结构模型不可识别导致的动态载荷识别不确定性问题和预测精度差的问题。

Method: 使用三种神经网络（GRU、LSTM、CNN）和残差卡尔曼滤波器（RKF），分别在模拟结构顶部激振、加州建筑地震基础激励、IASC-ASCE结构健康监测基准问题的冲击和瞬时载荷条件下进行测试。

Result: 不同方法在不同载荷场景下表现各异，RKF在物理参数可识别的情况下优于神经网络。

Conclusion: 各方法在不同载荷识别场景中各有优势，RKF在物理参数可识别情况下表现最佳，神经网络在小数据集训练条件下也展现出良好性能。

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [474] [Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification](https://arxiv.org/abs/2511.01172)
*Ali Owfi,Amirmohammad Bamdad,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出一个结合元学习和领域适应的统一框架，使自动调制分类系统能够同时抵抗对抗攻击和环境变化。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自动调制分类中表现出色，但易受对抗攻击和数据分布变化影响，阻碍了在实际动态环境中的部署。

Method: 采用两阶段策略：离线阶段使用元学习在单个源域上训练模型，使其能够泛化防御；在线阶段应用领域适应，使模型适应新目标域而无需大量标注数据。

Result: 该框架显著提高了调制分类在对抗攻击和环境变化下的准确性。

Conclusion: 该框架为解决现代AMC系统的部署和操作挑战提供了关键解决方案。

Abstract: Deep learning has emerged as a leading approach for Automatic Modulation
Classification (AMC), demonstrating superior performance over traditional
methods. However, vulnerability to adversarial attacks and susceptibility to
data distribution shifts hinder their practical deployment in real-world,
dynamic environments. To address these threats, we propose a novel, unified
framework that integrates meta-learning with domain adaptation, making AMC
systems resistant to both adversarial attacks and environmental changes. Our
framework utilizes a two-phase strategy. First, in an offline phase, we employ
a meta-learning approach to train the model on clean and adversarially
perturbed samples from a single source domain. This method enables the model to
generalize its defense, making it resistant to a combination of previously
unseen attacks. Subsequently, in the online phase, we apply domain adaptation
to align the model's features with a new target domain, allowing it to adapt
without requiring substantial labeled data. As a result, our framework achieves
a significant improvement in modulation classification accuracy against these
combined threats, offering a critical solution to the deployment and
operational challenges of modern AMC systems.

</details>


### [475] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: 提出了一种集成三种先进深度迁移学习网络的机器学习模型，结合可解释人工智能技术来提高黑色素瘤检测的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医疗诊断中因黑盒操作导致的可靠性和信任度不足问题，通过可解释AI技术提高黑色素瘤早期检测的可信度。

Method: 使用三种最先进的深度迁移学习网络进行集成学习，并应用可解释人工智能技术来解释预测依据。

Result: 开发了一个能够以高准确度检测黑色素瘤的模型，同时通过XAI技术提供了预测的可解释性。

Conclusion: 结合集成学习和可解释AI技术可以有效提高黑色素瘤检测系统的可靠性和可信度，为医疗诊断提供更可靠的AI辅助工具。

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [476] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: 提出Lo-Hp框架，通过解耦的两阶段权重生成方法解决现有生成式权重优化中的过耦合和长视野问题，采用混合策略子轨迹平衡目标提升灵活性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前生成式权重优化方法存在过耦合和长视野问题，前者限制了优化器的灵活性，后者导致推理效率低和准确性差。

Method: 采用解耦的两阶段权重生成框架，结合混合策略子轨迹平衡目标，整合在线和离线策略学习来捕捉局部优化策略。

Result: 理论证明仅学习局部优化策略即可解决长视野问题并提升全局最优权重生成，在迁移学习、少样本学习等任务中展现出优越的准确性和推理效率。

Conclusion: Lo-Hp框架有效解决了生成式权重优化的关键问题，在多种需要频繁权重更新的任务中表现优异。

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [477] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: EraseFlow是一个新的概念擦除框架，使用GFlowNets探索去噪路径空间，通过轨迹平衡目标优化，无需精心设计的奖励模型即可有效擦除有害或专有概念，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除技术存在图像质量下降、依赖脆弱的对抗损失或需要大量重新训练的问题，需要一种更有效的方法来从文本到图像生成器中移除有害或专有概念。

Method: 将概念遗忘视为去噪路径空间中的探索问题，使用配备轨迹平衡目标的GFlowNets优化，通过采样整个轨迹而非单个最终状态来学习随机策略。

Result: EraseFlow在性能上优于现有基线方法，实现了性能和先验保持之间的最佳权衡，能有效泛化到未见概念并避免可被攻击的奖励。

Conclusion: EraseFlow通过重新思考去噪轨迹的探索方式，提供了一种无需精心设计奖励模型的有效概念擦除解决方案，在保持模型先验的同时成功移除目标概念。

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [478] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: cs.LG

TL;DR: STAN是一个用于癫痫发作预测的对抗性时空注意力网络，通过级联注意力块联合建模空间大脑连接和时间神经动态，在EEG信号上实现高灵敏度和低误报率。


<details>
  <summary>Details</summary>
Motivation: 解决癫痫发作预测中的关键挑战：需要高灵敏度、低误报率和受试者特异性适应性，现有方法假设固定的发作前持续时间或分别处理时空特征。

Method: 使用级联注意力块交替空间和时间模块，通过对抗性训练和梯度惩罚来学习发作间期和发作前状态的稳健区分，无需个体化训练。

Result: 在两个基准EEG数据集上取得最先进性能：CHB-MIT头皮数据96.6%灵敏度、0.011误报/小时；MSSM颅内数据94.2%灵敏度、0.063误报/小时，计算效率高（2.3M参数，45ms延迟）。

Conclusion: STAN框架为医疗健康和其他时间序列领域提供了一种通用的时空预测范式，特别适用于个体异质性和可解释性至关重要的场景。

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [479] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: LL-ViT是一种针对边缘设备优化的视觉Transformer设计，通过集成LUT神经元层来减少模型大小和计算需求，同时保持与基线Transformer相当的准确率。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在边缘设备上存在计算、内存和能耗问题，而现有的逻辑和LUT网络在视觉任务上表现不佳，需要一种既能减少资源消耗又能保持性能的解决方案。

Method: 设计了一种基于LUT的通道混合器替代传统MLP层，采用神经网络学习方法原生学习LUT函数，并开发了相应的FPGA加速器。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet上分别达到95.5%、78.8%和60.9%的准确率，消除了60%的模型权重和50%的乘法运算，相比量化ViT加速器实现了1.9倍能效和1.3倍延迟降低。

Conclusion: LL-ViT提供了一种计算和能耗高效的视觉Transformer推理解决方案，特别适合边缘设备部署。

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [480] [LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping](https://arxiv.org/abs/2511.01296)
*Guanjie Cheng,Mengzhen Yang,Xinkui Zhao,Shuyi Yu,Tianyu Du,Yangyang Wu,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: LSHFed是一个鲁棒且通信高效的联邦学习框架，通过局部敏感哈希将高维梯度转换为紧凑的二进制表示，在保护隐私的同时有效检测恶意梯度攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在信任缺失环境中易受推理攻击和投毒攻击，现有防御方法存在通信计算成本高、检测精度有限的问题。

Method: 提出LSHFed框架，核心是LSHGM梯度验证机制，使用多超平面局部敏感哈希将高维梯度投影为紧凑二进制表示，仅通过不可逆哈希形式检测恶意梯度。

Result: 实验表明LSHFed在50%参与者为恶意攻击者时仍能保持高模型性能，梯度验证通信量比全梯度方法减少1000倍。

Conclusion: LSHFed在增强聚合鲁棒性和隐私保护的同时，显著降低了通信开销，为联邦学习在对抗性环境中的安全部署提供了有效解决方案。

Abstract: Federated learning (FL) enables collaborative model training across
distributed nodes without exposing raw data, but its decentralized nature makes
it vulnerable in trust-deficient environments. Inference attacks may recover
sensitive information from gradient updates, while poisoning attacks can
degrade model performance or induce malicious behaviors. Existing defenses
often suffer from high communication and computation costs, or limited
detection precision. To address these issues, we propose LSHFed, a robust and
communication-efficient FL framework that simultaneously enhances aggregation
robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a
novel gradient verification mechanism that projects high-dimensional gradients
into compact binary representations via multi-hyperplane locally-sensitive
hashing. This enables accurate detection and filtering of malicious gradients
using only their irreversible hash forms, thus mitigating privacy leakage risks
and substantially reducing transmission overhead. Extensive experiments
demonstrate that LSHFed maintains high model performance even when up to 50% of
participants are collusive adversaries while achieving up to a 1000x reduction
in gradient verification communication compared to full-gradient methods.

</details>


### [481] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: 提出并行解耦框架PDF，通过多模态大语言模型的指令可控性生成并行嵌入，突破传统SSC范式限制，显著提升多模态嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 传统多模态嵌入方法采用SSC范式（单输入、单嵌入、对比监督），将丰富多面输入压缩为单一嵌入，未能充分利用MLLM能力。

Method: 使用可学习前缀条件化共享MLLM主干，生成多个并行路径获取并行嵌入，结合互信息最小化和每路径对比监督促进多样性。

Result: 在MMEB基准上显著提升性能：VLM2Vec-LLaVA-1.6-LR提升8.9%，VLM2Vec-Qwen2VL分别提升4.2%和3.1%，2B模型仅用一半计算预算就超越基线2.6%。

Conclusion: PDF框架有效利用MLLM指令可控性，通过并行嵌入生成实现更鲁棒的语义覆盖和泛化能力，推理时仅需单次前向传播，计算开销可忽略。

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [482] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出分数扩散桥模型（FDBM），利用分数布朗运动的马尔可夫近似构建生成扩散桥框架，能处理时间相关性、长程依赖性和异常扩散现象，在蛋白质构象预测和图像翻译任务中优于布朗运动基线。


<details>
  <summary>Details</summary>
Motivation: 真实随机过程具有记忆效应、长程依赖性和异常扩散等特性，而标准扩散模型使用的布朗运动无法捕捉这些特征。

Method: 利用分数布朗运动的马尔可夫近似构建FDBM框架，保持分数布朗运动的非马尔可夫特性同时实现可处理推理，并扩展到薛定谔桥问题。

Result: 在蛋白质构象预测中Cα原子位置的RMSD更低，在非配对图像翻译中FID分数更低，均优于布朗运动基线。

Conclusion: FDBM能有效建模真实随机过程的复杂特性，在多种任务中表现出优于传统方法的性能。

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [483] [DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation](https://arxiv.org/abs/2511.01468)
*Hao Wang,Zixuan Weng,Jindong Han,Wei Fan,Hao Liu*

Main category: cs.LG

TL;DR: 提出了DAMBench，这是第一个大规模多模态基准测试，用于在真实大气条件下评估数据驱动的数据同化模型。


<details>
  <summary>Details</summary>
Motivation: 传统数据同化方法虽然有效，但深度学习提供了更可扩展、高效和灵活的替代方案。现有深度学习DA研究存在两个关键限制：依赖过度简化的合成观测场景，以及缺乏标准化基准进行公平模型比较。

Method: DAMBench整合了来自最先进预报系统的高质量背景状态和真实世界多模态观测数据（气象站和卫星图像）。所有数据都重采样到统一网格并进行时间对齐，提供统一评估协议并基准化代表性DA方法。

Result: 通过综合实验，DAMBench为未来研究建立了严格基础，促进了可重复性、公平比较和扩展到真实世界多模态场景的能力。

Conclusion: DAMBench解决了现有深度学习DA研究的局限性，提供了一个标准化基准来评估数据驱动DA模型在真实大气条件下的性能，数据集和代码已公开。

Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked
with reconstructing system states by integrating sparse, noisy observations
with prior estimation. While traditional approaches like variational and
ensemble Kalman filtering have proven effective, recent advances in deep
learning offer more scalable, efficient, and flexible alternatives better
suited for complex, real-world data assimilation involving large-scale and
multi-modal observations. However, existing deep learning-based DA research
suffers from two critical limitations: (1) reliance on oversimplified scenarios
with synthetically perturbed observations, and (2) the absence of standardized
benchmarks for fair model comparison. To address these gaps, in this work, we
introduce DAMBench, the first large-scale multi-modal benchmark designed to
evaluate data-driven DA models under realistic atmospheric conditions. DAMBench
integrates high-quality background states from state-of-the-art forecasting
systems and real-world multi-modal observations (i.e., real-world weather
stations and satellite imagery). All data are resampled to a common grid and
temporally aligned to support systematic training, validation, and testing. We
provide unified evaluation protocols and benchmark representative data
assimilation approaches, including latent generative models and neural process
frameworks. Additionally, we propose a lightweight multi-modal plugin to
demonstrate how integrating realistic observations can enhance even simple
baselines. Through comprehensive experiments, DAMBench establishes a rigorous
foundation for future research, promoting reproducibility, fair comparison, and
extensibility to real-world multi-modal scenarios. Our dataset and code are
publicly available at https://github.com/figerhaowang/DAMBench.

</details>


### [484] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: 提出基于Hadamard卷积变换的时间序列分类方法，相比现有SOTA方法在保持性能的同时显著提升计算效率，训练时间比最快的miniROCKET缩短50%，F1分数提升至少5%。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类方法如HIVE-COTE、Proximity Forest等计算复杂度高、参数调优和训练时间长，而轻量级方法如ROCKET在核选择和计算开销方面仍有改进空间。

Method: 使用Hadamard矩阵的列或行向量作为不同长度的卷积核，利用核的正交性提升计算效率、鲁棒性和适应性，与现有方法完全兼容。

Result: 在多领域数据集上实现SOTA性能：F1分数比ROCKET提升至少5%，训练时间比最快的miniROCKET缩短50%，可在超低功耗嵌入式设备上部署。

Conclusion: 提出的Hadamard卷积变换方法在保持高性能的同时显著提升了计算效率，为时间序列分类提供了更高效的解决方案。

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [485] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM是一个多代理的Graph-CoT系统，通过分解推理任务、优化LLM服务架构，显著提升了图结构知识推理的准确性、效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的Graph-CoT流水线存在准确性低、令牌使用过多、延迟高和吞吐量低的问题，主要由于单代理整体提示、重复上下文重新编码和低效的服务执行。

Method: 将推理分解为分类、推理、动作生成和图检索等专门代理，支持分支和选择性上下文共享；引入Graph-CoT感知的LLM推理机制，包括图特定的KV缓存管理、基于优先级的驱逐和流水线执行。

Result: GLM将答案准确性提升高达38%，令牌成本降低高达95.7%，推理延迟降低90.3%，吞吐量比最先进的Graph-CoT基线提高15.1倍。

Conclusion: GLM通过多代理设计和优化的LLM服务架构，实现了高效的大规模复杂现实世界推理应用。

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [486] [Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering](https://arxiv.org/abs/2511.01694)
*Hossein Abdi,Mingfei Sun,Wei Pan*

Main category: cs.LG

TL;DR: 提出一种基于卡尔曼滤波的贝叶斯近似自然梯度下降方法，用于CLIP模型的小样本微调，在保持ID性能的同时提升OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP模型在少样本微调中面临的挑战：一阶梯度优化器收敛慢、对超参数敏感、OOD泛化差；二阶方法计算成本高。

Method: 使用卡尔曼滤波器对自然梯度下降进行贝叶斯近似，结合二阶优化的优势与贝叶斯推理，提供不确定性量化。

Result: 在多种图像分类数据集上的实验表明，该方法在ID性能上达到最优或可比水平，同时显著提升了OOD鲁棒性。

Conclusion: 这是首次成功将卡尔曼滤波应用于CLIP模型微调的工作，为视觉语言任务提供了更鲁棒高效的学习方法。

Abstract: Vision-language pre-trained models, such as CLIP, have established new
benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a
major challenge to achieve optimal performance on both in-distribution (ID) and
out-of-distribution (OOD) datasets, especially when labeled data is scarce.
Most existing fine-tuning approaches rely on first-order gradient-based
optimizers, which typically suffer from slow convergence, sensitivity to
step-size hyperparameters, and poor generalization in OOD settings. In
contrast, second-order methods utilize local curvature information of the loss
landscape to adjust the update step size. This is particularly beneficial for
CLIP models, whose non-convex loss functions often contain sharp critical
points. In such cases, natural gradient direction can offer more substantial
and efficient per-iteration updates when fine-tuning with limited data. Natural
Gradient Descent (NGD) is obtained by preconditioning the standard gradient
with the inverse Fisher Information Matrix (FIM), which is computationally
expensive for large models. To address this, we propose a Bayesian
approximation of NGD using a Kalman filter for CLIP models. Our method combines
the benefits of second-order optimization with Bayesian inference, which
enhances generalization while providing uncertainty quantification. Extensive
experiments conducted on diverse image classification datasets demonstrate that
our algorithm consistently achieves superior--or comparable--ID performance and
improved OOD robustness compared to state-of-the-art baselines. To the best of
our knowledge, this work represents the first successful application of Kalman
filtering to fine-tuning CLIP-based models, which enables more robust and
efficient learning in vision-language tasks.

</details>


### [487] [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)
*Song Gao,Shusen Jing,Shuai Zhang,Yue Wang,Xiangwei Zhou,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了网络专家混合系统，通过客户端协作推理和联邦学习框架解决边缘设备训练大型AI模型的资源限制问题


<details>
  <summary>Details</summary>
Motivation: 大型AI模型在移动边缘计算中的计算资源和训练数据需求与边缘设备的有限存储和计算能力之间存在冲突

Method: 网络专家混合系统：客户端根据专家能力将任务分发给合适邻居并聚合结果；联邦学习框架结合监督和自监督学习

Result: 通过广泛实验证明了所提系统的有效性，为训练算法提供了见解和基准

Conclusion: NMoE系统能够有效平衡个性化和泛化，同时保持通信效率和数据隐私

Abstract: Recent advancements in large artificial intelligence models (LAMs) are
driving significant innovations in mobile edge computing within next-generation
wireless networks. However, the substantial demands for computational resources
and large-scale training data required to train LAMs conflict with the limited
storage and computational capacity of edge devices, posing significant
challenges to training and deploying LAMs at the edge. In this work, we
introduce the Networked Mixture-of-Experts (NMoE) system, in which clients
infer collaboratively by distributing tasks to suitable neighbors based on
their expertise and aggregate the returned results. For training the NMoE, we
propose a federated learning framework that integrates both supervised and
self-supervised learning to balance personalization and generalization, while
preserving communication efficiency and data privacy. We conduct extensive
experiments to demonstrate the efficacy of the proposed NMoE system, providing
insights and benchmarks for the NMoE training algorithms.

</details>


### [488] [An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications](https://arxiv.org/abs/2511.01745)
*Mei-Chin Pang,Suraj Adhikari,Takuma Kasahara,Nagihiro Haba,Saneyuki Ohno*

Main category: cs.LG

TL;DR: OSBAD是一个用于电池应用异常检测的开源基准测试框架，通过系统比较15种不同算法，并引入物理统计特征变换和贝叶斯优化管道，提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 电池安全在消费电子、电动汽车和飞机等应用中至关重要，未检测到的异常可能引发安全隐患或昂贵停机时间。

Method: 提出OSBAD开源基准测试框架，包含15种统计、距离和机器学习算法；采用物理统计特征变换工作流分解集体异常；使用贝叶斯优化管道进行超参数调优。

Result: 在涵盖液态和固态化学的多个数据集上验证，展示了跨化学体系的泛化能力，能有效识别不同电化学系统中的异常。

Conclusion: OSBAD为开发安全、可扩展和可迁移的电池异常检测工具建立了统一基础，强调了物理统计特征工程和概率超参数调优在安全关键能源系统中的重要性。

Abstract: Battery safety is critical in applications ranging from consumer electronics
to electric vehicles and aircraft, where undetected anomalies could trigger
safety hazards or costly downtime. In this study, we present OSBAD as an
open-source benchmark for anomaly detection frameworks in battery applications.
By benchmarking 15 diverse algorithms encompassing statistical, distance-based,
and unsupervised machine-learning methods, OSBAD enables a systematic
comparison of anomaly detection methods across heterogeneous datasets. In
addition, we demonstrate how a physics- and statistics-informed feature
transformation workflow enhances anomaly separability by decomposing collective
anomalies into point anomalies. To address a major bottleneck in unsupervised
anomaly detection due to incomplete labels, we propose a Bayesian optimization
pipeline that facilitates automated hyperparameter tuning based on
transfer-learning and regression proxies. Through validation on datasets
covering both liquid and solid-state chemistries, we further demonstrate the
cross-chemistry generalization capability of OSBAD to identify irregularities
across different electrochemical systems. By making benchmarking database with
open-source reproducible anomaly detection workflows available to the
community, OSBAD establishes a unified foundation for developing safe,
scalable, and transferable anomaly detection tools in battery analytics. This
research underscores the significance of physics- and statistics-informed
feature engineering as well as model selection with probabilistic
hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for
safety-critical energy systems.

</details>


### [489] [Machine and Deep Learning for Indoor UWB Jammer Localization](https://arxiv.org/abs/2511.01819)
*Hamed Fard,Mahsa Kholghi,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: 该论文提出了一种基于域对抗ConvNeXt自编码器(A-CNT)的方法，用于在室内环境变化下实现鲁棒的恶意干扰源定位，解决了传统UWB定位系统在环境布局改变时性能严重下降的问题。


<details>
  <summary>Details</summary>
Motivation: UWB定位虽然能提供厘米级精度，但易受干扰攻击，且在室内环境布局变化时定位性能会严重下降。现有ML/DL方法在跨房间布局的干扰源定位方面研究不足。

Method: 提出了域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层来对齐跨域的CIR衍生特征，以减轻环境变化带来的性能下降。

Result: A-CNT方法将平均欧几里得误差降低到34.67厘米，比非对抗迁移学习提高了77%，比最佳基线提高了83%，将30厘米内的样本比例恢复到0.56。

Conclusion: 域对抗特征对齐能够在环境变化的情况下实现鲁棒且可迁移的室内干扰源定位。

Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is
vulnerable to jamming attacks, creating security risks for asset tracking and
intrusion detection in smart buildings. Although machine learning (ML) and deep
learning (DL) methods have improved tag localization, localizing malicious
jammers within a single room and across changing indoor layouts remains largely
unexplored. Two novel UWB datasets, collected under original and modified room
configurations, are introduced to establish comprehensive ML/DL baselines.
Performance is rigorously evaluated using a variety of classification and
regression metrics. On the source dataset with the collected UWB features,
Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves
the lowest mean Euclidean error of 20.16 cm. However, deploying these
source-trained models in the modified room layout led to severe performance
degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99
cm, demonstrating significant domain shift. To mitigate this degradation, a
domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a
gradient-reversal layer to align CIR-derived features across domains. The A-CNT
framework restores localization performance by reducing the mean Euclidean
error to 34.67 cm. This represents a 77 percent improvement over
non-adversarial transfer learning and an 83 percent improvement over the best
baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the
results demonstrate that adversarial feature alignment enables robust and
transferable indoor jammer localization despite environmental changes. Code and
dataset available at https://github.com/afbf4c8996f/Jammer-Loc

</details>


### [490] [Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](https://arxiv.org/abs/2511.01831)
*Jay Mohta,Kenan Emir Ak,Dimitrios Dimitriadis,Yan Xu,Mingwei Shen*

Main category: cs.LG

TL;DR: 提出基于路由的方法解决视觉语言模型在连续微调中的灾难性遗忘问题，能够集成新任务同时保留预训练获得的基础知识，无需同时访问所有任务数据。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在连续微调新任务时会出现灾难性遗忘，降低先前学习的基础能力和任务特定性能。传统多任务学习需要同时访问所有数据集且计算开销随任务数量线性增长。

Method: 采用基于路由的方法，在InternVL-2模型（2B和8B参数）上实现新任务集成，同时保持基础能力。通过路由机制实现跨模态知识转移。

Result: 路由方法在ChartQA、MMBench和DocVQA等通用基准上保持性能，同时提升专业任务准确率。方法对任务数量增长具有鲁棒性，在语义相关任务上表现尤佳。

Conclusion: 基于路由的学习方法有效解决了灾难性遗忘问题，实现了跨模态知识转移，优于现有持续学习方法，且避免了传统多任务学习的数据和计算开销。

Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when
sequentially fine-tuned on new tasks, degrading performance on previously
learned foundational and task-specific capabilities. While multi-task learning
can mitigate forgetting, it requires simultaneous access to all datasets and
imposes computational overhead that scales linearly with the number of tasks.
In this work, we introduce a routing-based approach that enables the
integration of new tasks while preserving the foundational knowledge acquired
during pretraining. We evaluate our method using InternVL-2 models (2B and 8B
parameters) and demonstrate that routing preserves the model's foundational
capabilities by maintaining performance on general-purpose benchmarks such as
ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on
specialized tasks. Importantly, our approach achieves this without requiring
concurrent access to data from all tasks, avoiding the significant
computational and data overhead associated with traditional multi-task
learning. We further conduct extensive ablation studies to evaluate the
scalability and robustness of routing-based learning, showing that the approach
is resilient to a growing number of tasks and performs particularly well when
new tasks are semantically related. Finally, we show that the routing mechanism
enables superior cross-modal transfer between language and vision capabilities,
allowing knowledge learned in one modality to enhance performance in another
capability not achieved by existing continual learning methods.

</details>
